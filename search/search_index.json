{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\udc4b Welcome!","text":"<p>Hi there! This is a collection of notes on Math, Computer, Economics, Finance and so much more. Feel free to share with everyone you know!</p> <ul> <li>Check out the \ud83d\udcf1 Installation Guide to install now</li> <li>Use the navbar \u2b05\ufe0f to go through the website</li> <li>For changes/corrections, click the edit button \ud83d\udcdd at the top of any page</li> </ul>"},{"location":"#contributions","title":"\u2728 Contributions","text":"<p>A big thank you to all contributors, as this initiative would not be possible without their support! You can check out the progress of the project here.</p> <p>Want to join the initiative? Check out the guidelines and start contributing now! Collaborators will be mentioned for their contributions. </p>"},{"location":"#installation-guide","title":"Installation Guide","text":"Platform Installation Step Android Chrome - Click the three dots at the top right- Click <code>Install App</code> iOS Safari - Click share button in the bottom- Click <code>Add to Home Screen</code> iOS Chrome \u274c\ud83d\ude15 I hate this; open the link on Safari Windows Click the <code>+</code> icon in the address bar MacOS Chrome Click the <code>+</code> icon in the address bar MacOS Safari \u274c\ud83d\ude15 I hate this; open the link on Chrome"},{"location":"#disclaimers","title":"\u26a0\ufe0f Disclaimers","text":"<ul> <li>Make sure to give due credit when sharing these notes, to help support this project :) Publishing this as your own without crediting and using the exact Open Software License 3.0 would be bypassing the license, and demeaning this project.</li> <li>This is meant to be a student-only knowledge sharing initiative - not a classroom portal.</li> <li>These notes are not affiliated to any university, club, association.</li> <li>Kindly note that since this is open source, and hence, correctness cannot be guaranteed.</li> </ul>"},{"location":"#license","title":"License","text":"<p>Open Source Licence</p>"},{"location":"CONTRIBUTING/","title":"\ud83d\udcdd Contributing","text":"<p>Welcome to our open-source project! We love your input! We want to make contributing to this project as easy and transparent as possible, whether it's:</p> <ul> <li>Reporting a Bug</li> <li>Discussing the Current State of the Code</li> <li>Submitting a Fix</li> <li>Proposing New Features</li> <li>Becoming a Moderator</li> </ul> <p>We appreciate your interest in contributing to our repository. Before you get started, please take a moment to review the guidelines below.</p>"},{"location":"CONTRIBUTING/#guidelines","title":"\ud83d\ude80 Guidelines","text":"<ol> <li>Fork the repository and clone it locally.</li> <li>Create a new branch for your changes.</li> <li>Make your changes and test them locally. All notes must be in markdown only. If you're not familiar with markdown, please refer to this.</li> <li>If you're fixing a bug, please include a test case that demonstrates the bug.</li> <li>Submit a pull request (PR) to the <code>main</code> branch of the original repository. All pull requests must address an issue. Write clear and concise commit messages and pull request descriptions.</li> <li>Your PR will be reviewed by a moderator. If there are any requested changes, make them and push them to your branch, and your PR will be updated automatically.</li> <li>Once your changes are approved and merged, you can delete your branch.</li> </ol>"},{"location":"CONTRIBUTING/#issues-and-feature-requests","title":"\ud83e\udd14 Issues and Feature Requests","text":"<p>If you find a bug or have a feature request, please open an issue in the repository. Please provide a clear and concise description of the issue or request, and include any relevant information, such as error messages or steps to reproduce.</p>"},{"location":"CONTRIBUTING/#introducing-new-course-or-new-notes","title":"\ud83e\udd29 Introducing new Course (or) new notes?","text":"<ol> <li>Fork the repo:</li> <li>Go to the repo on GitHub and click on the \"Fork\" button in the top right corner.</li> <li> <p>This will create a copy of the repository under you GitHub account.</p> </li> <li> <p>Clone the forked repo:</p> </li> <li>Using your terminal or Command Prompt, navigate to the directory where you want to clone the repo.  But before, download &amp; install git on you machine.</li> <li> <p>Then use the following command to clone the repository to you local machine: <pre><code>git clone &lt;forked_repository_url&gt;\n</code></pre> Replace <code>&lt;forked_repository_url&gt;</code> with the URL of your forked repository. You can find this URL in the repository page of your forked repository on GitHub.</p> </li> <li> <p>Configure upstream remote:</p> </li> <li>Change to the cloned repository's directory using the <code>cd</code> command. </li> <li> <p>Then, add the original repository as the upstream remote so that you can fetch any changes made to the original repository.  Use the following command: <pre><code>git remote add upstream &lt;original_repository_url&gt;\n</code></pre></p> </li> <li> <p>Create a new branch: </p> </li> <li>Before making any changes, create a new branch to work on. This keeps your changes separate from the main branch. </li> <li> <p>Use the following command to create a new branch: <pre><code>git checkout -b &lt;branch_name&gt;\n</code></pre> Replace <code>&lt;branch_name&gt;</code> with a descriptive name for your branch.</p> </li> <li> <p>Add your new files: </p> </li> <li> <p>Place the new files you want to upload into the cloned repository's directory on your local machine.</p> </li> <li> <p>Stage the changes: </p> </li> <li> <p>Use the following command to stage the changes (including the new files) for commit: <pre><code>git add .\n</code></pre></p> </li> <li> <p>Commit your changes: </p> </li> <li> <p>Commit the changes with a descriptive commit message using the following command: <pre><code>git commit -m \"Your commit message\"\n</code></pre></p> </li> <li> <p>Push your changes: </p> </li> <li> <p>Push the committed changes to your forked repository on GitHub using the following command: <pre><code>git push origin &lt;branch_name&gt;\n</code></pre> Replace <code>branch_name</code> with the name of the branch you created earlier.</p> </li> <li> <p>Create a pull request: </p> </li> <li>Go to the repository page of your forked repository on GitHub. </li> <li>You should see a prompt to create a pull request for the branch you just pushed. </li> <li>Click on it and provide a clear title and description for your pull request. </li> <li>Submit the pull request.</li> </ol> <p>Once your pull request is approved, your changes will be merged into the original repository. \ud83e\udd73</p>"},{"location":"CONTRIBUTING/#additional-tips","title":"\ud83d\udca1 Additional Tips","text":"<ul> <li>Use tables over lists whenever possible. This helps in grouping related concepts.</li> </ul> <ul> <li>Use mermaid flowcharts to simplify processes, flows, trees. </li> </ul> <ul> <li>Use LaTeX for mathematical/scientifical expressions. (Blurry images are not cool \ud83d\ude14\ud83d\udc4e)</li> </ul> \\[ \\int x^2 = \\frac{x^3}{3} \\]"},{"location":"CONTRIBUTING/#license","title":"\u2696\ufe0f License","text":"<p>By contributing, you agree that your contributions will be licensed under Open Software License 3.0.</p> <p>Check the license here</p>"},{"location":"CONTRIBUTING/#conclusion","title":"\ud83d\udc4b Conclusion","text":"<p>We welcome contributions from everyone, and we appreciate your help in making our project better. Thank you for your support!</p>"},{"location":"General_Notes/","title":"General","text":""},{"location":"General_Notes/#general-notes","title":"General Notes","text":"<ul> <li> Never directly trust anything you read; always do the derivations &amp; test everything for yourself</li> <li> How to read a paper</li> <li> Always be prepared; new opportunities may arise any time</li> <li> Always be eager to continuously learn &amp; explore</li> <li> Be wary about the Hype Cycle</li> </ul>"},{"location":"General_Notes/#hype-cycle","title":"Hype Cycle","text":"<p>Technology adoption cycle</p> <p></p> <p>Venture Capitalists know that 9/10 investments fail, but expect at least 1/10 makes enough money to compensate for the others</p>"},{"location":"General_Notes/#general-tasks-involved-in-job-roles","title":"General Tasks involved in Job Roles","text":"<p>O*Net</p>"},{"location":"1_Core/Bio_Lab/","title":"Lab","text":"<p>The Biology Laboratory course introduces first-year engineering students to essential experimental techniques in microscopy, micrometry, haemometry, cytometry, DNA isolation, chromatography, and spectrophotometry.</p> <p>Objectives: - Familiarize students with key experiments in core biology. - Enhance understanding of underlying biological theories. - Develop skills in experimental methodology and scientific instrument operation. - Integrate theoretical knowledge with practical applications.</p> <p>Here is the demonstration of every experiment in the biology lab. As of now I have only uploaded till midsem. </p> <p>PS : Please note that the links to videos on GDrive have been removed at the moment as they have been removed by the faculty. Once reuplaoded, will add them. </p> <ol> <li>Microscopy</li> <li>Isolation of Stomata</li> <li>Calculation of Mitotic Index</li> <li>Identification of Blood Group</li> <li>Total WBC Count</li> <li>Total RBC Count</li> <li>Estimation of Chlorophyll</li> <li>Micrometry</li> <li>Paper Chromatography</li> <li>Preparation of Onion Root Tip Sample (Refer Exp 3)</li> </ol>"},{"location":"1_Core/Bio_Lab/Exp_01/","title":"Exp 01","text":""},{"location":"1_Core/Bio_Lab/Exp_01/#microscopy-depends-on-specimen","title":"Microscopy [Depends on Specimen]","text":"<p>Fairly chill experiment, talks about the different parts of a microcope and how to use these parts. Also, involves looking at some basic slides and describing their overall strucutre. </p>"},{"location":"1_Core/Bio_Lab/Exp_02/","title":"Exp 02","text":""},{"location":"1_Core/Bio_Lab/Exp_02/#stomata-400x","title":"Stomata [400x]","text":""},{"location":"1_Core/Bio_Lab/Exp_02/#dicot-plant","title":"Dicot Plant:","text":"<ol> <li>We need lower epidermis, use cutter to isolate the tissue. </li> <li>Put one drop water and place the tissue. </li> <li>Now place cover slip in a tilted way such that there are no air bubbles. </li> <li>Now view it under microscope on 40x (4, 10, 40) magnification. </li> </ol> <p>Similar procedure for monocot except there is no upper and lower epidermis, either side works.  </p> <p>To calculate the stomatal density: </p> <ul> <li>Count the number of stomata in 3 different \"fields\" [x2, one for Monocot and other for Dicot]. </li> <li>Take the average of the 3 fields, and multiply it by 8 that will give you the stomatal density per mm^2. </li> </ul> <p>Q. Why are we multiplying by 8? </p>"},{"location":"1_Core/Bio_Lab/Exp_03/","title":"Exp 03","text":""},{"location":"1_Core/Bio_Lab/Exp_03/#calculation-of-mitotic-index-400x","title":"Calculation of Mitotic Index [400x]","text":"<ol> <li>Examine under 400x magnification. </li> <li>Pick a fieid where there's atleast one prophase, metaphase, anaphase, telophase. Count the total number of each type of cell. </li> <li>Identification : </li> <li>Prophase - Raisin, </li> <li>Metapahse - Concentrated in Center</li> <li>Anaphase - Concentrated in Sides</li> <li>Telophase - Raisins towards the edges. </li> <li>Go the center of the \"field\" count the number of cells in row an column with max count, multiplying those two numbers will give you the total cell count. Sum of the cells in phase can also be obtained. </li> <li>Number of Cells in Interphase = Total - Cells in Phase </li> <li>Find their corresponding percentages. </li> <li>Calculations: </li> <li>Mitotic Index = Number of Cells in Phase / Totoal Number of Cells.</li> <li>Generalized Version : Mitotic Index * 19 Hours</li> <li>Index of Cells = Number of Cells in that Phase / Total number of Cells in Phase</li> <li>Duration of Mitosis = Mitotic Index * Index of Cells</li> </ol>"},{"location":"1_Core/Bio_Lab/Exp_04/","title":"Exp 04","text":""},{"location":"1_Core/Bio_Lab/Exp_04/#micrometry-100x","title":"Micrometry [100x]","text":"<ol> <li>Place the calibration slide on the stage of the microscope. </li> <li>View the slide under 4x and 10x. </li> <li>Remove the light side lens, and place the ocular scale eyepiece on the right side lens. </li> <li>Observe which divisions correspond exactly with the divisions on the stage micrometer. </li> <li>Least Count of Stage of Micrometer is 0.01mm = 0.1 MicroMeter. </li> <li>Calculate the Calibration Constant: </li> <li>CC = (No. of Divisions in Stage Micrometer between two adjacent coincident points * Least Count) / Corr. no. of divisions on Ocular Microscope. </li> </ol>"},{"location":"1_Core/Bio_Lab/Exp_04/#measuring-specimens","title":"Measuring specimens:","text":"<ol> <li>Measure the Length and the Breadth of one cell of the given sample. Multuply the measured paramters by your calibration constant to obtain the final readings that in MicroMeters. </li> </ol>"},{"location":"1_Core/Bio_Lab/Exp_05/","title":"Exp 05","text":""},{"location":"1_Core/Bio_Lab/Exp_05/#total-white-blood-cell-count-100x","title":"Total White Blood Cell Count [100x]","text":"<ol> <li>Draw blood till 0.5 Mark in Thoma Dilution Pippete (with the white bead)</li> <li>Draw the RBC dilution flood up to 11 mark. Homogenize the sample for 2-3 min. </li> <li>Discard the first few drops of sample present at the tip as it only contains the dilution fluid. </li> <li>Charge the haemocytometer with the sample by gently touching the tip of the pippete of the counting platform. In contact with the edge of the cover slip, small amount of fluid will automatically by surface tension and capillary action. </li> <li>Wait for 5mins. </li> <li> <p>The WBCs are counted in 4 corners under 10x low power objective. You will see 16 boxes in each corner, apply the L - Rule. </p> </li> <li> <p>Total number of WBC's per MicroLiter is given by (D*N)/V</p> </li> <li>Dilution Factor is 10, Volume is 0.4 MicroLiter. </li> <li>Total Number of WBCs = N * 50 Cells</li> </ol> <p>Normal range of WBC in a human body is 5000 - 10000 cells/mm^3. </p>"},{"location":"1_Core/Bio_Lab/Exp_05/#common-reasons-for-false-high-results","title":"Common Reasons for False High Results:","text":"<ul> <li>Dilution Fluid taken below mark (Insufficient Dilution)</li> <li>Overloading</li> </ul>"},{"location":"1_Core/Bio_Lab/Exp_05/#common-reasons-for-false-low-results","title":"Common Reasons for False Low Results:","text":"<ul> <li>Excessive Dilution</li> <li>Insufficient Loading</li> </ul>"},{"location":"1_Core/Bio_Lab/Exp_06/","title":"Exp 06","text":""},{"location":"1_Core/Bio_Lab/Exp_06/#counting-of-rbcs-400x","title":"Counting of RBCs [400x]","text":"<ul> <li> <p>We use Hayem's Solution which is an isotonic fluid that that maintains tonicity, acts as a preservative and discourages clumping of blood. </p> </li> <li> <p>Draw blood gently up to 0.5 mark on the pipette. </p> </li> <li>Draw and fill in Hayem's fluid up to 101 mark and homogenize the blood with the fluid thoroughly by rotating the pipette in the horizontal position. The red bead aids in mixing. </li> <li>Discard the fiirst few drops of the fluid as it contains only dilution fluid. </li> <li>Charge the chamber with the sample by gently touching the tip of the pipette on the surface of the counting platform in contact with the edge of the cover slip. A small amount of fluid will be drawing in automatically by surface tension and capillary action. There should not be any air bubbles. </li> <li>Once the haemocytometer is charged, view it under 400x magnification. </li> <li> <p>Similar process as WBC Counting, we will only focus on the 4 corner squares. Each corner square has 16 boxes, we count the number of cells within each box using the L - Rule. </p> </li> <li> <p>Total number of WBC's per MicroLiter is given by (D*N)/V</p> </li> <li>Dilution Factor is 200, Volume is 0.02 MicroLiter. </li> <li>Total Number of WBCs = N * 10000 Cells</li> </ul>"},{"location":"1_Core/Bio_Lab/Exp_06/#common-reasons-for-false-high-results","title":"Common Reasons for False High Results:","text":"<ul> <li>Dilution Fluid taken below mark (Insufficient Dilution)</li> <li>Overloading</li> </ul>"},{"location":"1_Core/Bio_Lab/Exp_06/#common-reasons-for-false-low-results","title":"Common Reasons for False Low Results:","text":"<ul> <li>Excessive Dilution</li> <li>Insufficient Loading</li> </ul>"},{"location":"1_Core/Bio_Lab/Exp_07/","title":"Exp 07","text":""},{"location":"1_Core/Bio_Lab/Exp_07/#determination-of-blood-group-by-slide-agglutination-test-no-microscope","title":"Determination of Blood Group by Slide Agglutination Test [No Microscope]","text":"<ol> <li>Take a sample of blood and put 3 drops in different places in a slide. </li> <li>To the first drop of blood, add anti-serum A. </li> <li>To the second drop of blood, add anti-serum B. </li> <li> <p>To the third drop of blood, add anti-serum D. </p> </li> <li> <p>If there's agglutination in A, A is present. </p> </li> <li>If there's agglutination in B, B is present. </li> <li>If there's agglutination in D, it is +'ve. </li> <li>If there's no agglutination in D, it is -'ve. </li> <li>If there's agglutination only in D, it is O+'ve. </li> <li>If there's no agglutination in D, it is O-'ve. </li> </ol>"},{"location":"1_Core/Bio_Lab/Exp_08/","title":"Exp 08","text":""},{"location":"1_Core/Bio_Lab/Exp_08/#qualitative-estimation-of-chlorophyll","title":"Qualitative Estimation of Chlorophyll","text":"<ol> <li>Select tender green leaves. </li> <li>Wash them with water and dry them and weigh 100mg of leaf. </li> <li>Chop and Grind it well using a matter and pestle. </li> <li>Add 5ml acetone, since it is volatile in air, mix it fast and pour it back in the measuring thing.</li> <li>Now we centirfuge it, while doing that remember to have another liquid with same measurment of liquid on the exact opposite side such that there's a balance. The density of acetone and water are quite similar so as far as density is concerned, we are good to go. Put the setting on 5000rpm for 5min. (In Lab its 100x rpm, so look for 50 rpm)</li> <li>Fill the cuvette with 80% blank acetone solution. The volume of the cuvette is 4mL and it should be held on the transluscent sides. Conver the cuvette using the white top. Place it on the machine in such a way that the translucent side is facing us. Same process for the green sample. </li> <li>Getting absorbance: </li> <li>Make sure 100% mode (last button) is on and the reading is stable. </li> <li>Click on the first button for getting absorbance, it should say 0. </li> <li>Pull the black button until you here one click. You will get the reading. </li> </ol>"},{"location":"1_Core/Bio_Lab/Exp_08/#formulas","title":"Formulas:","text":"<ul> <li>Chlorophyll A: 12.25(A-663) - 2.55 (A-646)</li> <li>Chlorophyll B: 20.31(A-646) - 4.91 (A-663)</li> <li>Total : 17.76 (A-646)  + 7.34 (A-663)</li> </ul>"},{"location":"1_Core/Bio_Lab/Exp_09/","title":"Exp 09","text":""},{"location":"1_Core/Bio_Lab/Exp_09/#paper-chromatography","title":"Paper Chromatography","text":"<ol> <li>Add 6mL of mobile phase into chromatography chamber. Keep the chamber closed for 10 minutes. </li> <li>Now we prepare our chromatography paper (whatmann paper), make sure the scale doesn't touch the paper. Make a 6 x 4 cutout. Always hold the paper at the top. </li> <li>Now we draw a line 1cm above the bottom of the paper to mark the origin. Let us mark 3 equidistant points on the line. Mark them as A, B and M respectively. </li> <li>Dip the capillary tube on the PhenylAnaline solution, it will automaticlly draw up. Tap the tube on the spot A it will auto drop. </li> <li>Use another capillary tube for the next solution. Do the same for the mixture solution. On the paper mention your ID Number in the top right corner, and place the paper in the chromatograhy chamber till the mobile phase has covered \u00beth of your paper. Remove the chromatography paper and mark the dipped portion using a pencil. Then we have to airdry the paper. </li> <li> <p>Now we have to spray the ninhydrin spray. When you spray it be sure to not inhale the vapours yourself. After that we dry it in the oven. After drying we can notice different spots, we will mark the center of the spots and measure the distance between the origin to the center of the colored spots. </p> </li> <li> <p>Formula for RF Value: Distance travelled by the substance (spot) / Total distance travelled by solvent.</p> </li> </ol>"},{"location":"1_Core/Bio_Lab/Exp_10/","title":"Exp 10","text":""},{"location":"1_Core/Bio_Lab/Exp_10/#preparation-of-onion-root-tip-slide-for-calculation-of-mitotic-index-no-microscope-mandatory","title":"Preparation of Onion Root Tip Slide for Calculation of Mitotic Index [No Microscope] -- [Mandatory!!!]","text":"<ol> <li>Clean slide and cover slip</li> <li>Place onion root tip on the slide</li> <li>2-3 drops of HCl on the onion root tip such that it is fully covered. Let this remain for 15 minutes. Wipe it off gently once done. </li> <li>Wipe it off gently, and use nuclear stain (in our case -- Saffranin Blue), this imparts a pink color on the slide. Let this remain for 15 minutes. Wipe it off gently once done. </li> <li>Put a drop of water on the root tip and cover it uisng a cover slip, make sure to place the cover slip in a such a manner that there are no air bubbles. Remove the excess stain using a tissue. </li> <li>Gently tap the root tip using a pencil to produce a squashed homogenous effect. </li> </ol>"},{"location":"1_Core/Chem_Lab/","title":"Lab","text":"<p>The Chemistry Laboratory course provides first-year students with hands-on experience in various branches of chemistry. Key objectives include:</p> <ul> <li>Guiding students to gain expertise in conducting experiments and understanding the related theories.</li> <li>Training students to confidently handle and operate scientific instruments.</li> <li>Developing practical skills essential for future coursework and laboratory settings.</li> </ul> <p>No demonstration videos have been uplaoded for chemistry lab -- will have to rely on notes.  :/ </p>"},{"location":"1_Core/Chem_Lab/Exp_01/","title":"Exp 01","text":""},{"location":"1_Core/Chem_Lab/Exp_01/#preparation-and-charecterization-of-aspirin","title":"Preparation and Charecterization of Aspirin","text":"<ol> <li>Weigh 2g of Salicylic Acid. Transfer it to a dry 100mL beaker. </li> <li>4mL of Acetic Anhydride into 100mL beaker. </li> <li>5 Drops of conc. H2SO4 into 100ml beaker. [If excess H2SO4 is added the reaction gets suphonated and no product is formed.]</li> <li>Water Bath - Reaction mixture below the liquid level in the bath and water in boiling condition, continue for 20 mins. </li> <li>Add 10mL ice cold water, to hyrolyze the unreacted acetic anhydride. </li> <li>Chill in ice bath - occasionally swirling to cause precipitation. </li> <li>Add 25mL ice cold water. </li> <li>Scratch side of beaker with stirring rod to initialize crystallization. </li> <li>Filter, Dry, Weigh, Report. </li> <li>Report melting point. </li> </ol>"},{"location":"1_Core/Chem_Lab/Exp_02/","title":"Exp 02","text":""},{"location":"1_Core/Chem_Lab/Exp_02/#preparation-of-coordination-compound-tetrammine-sulphate-monohydrate","title":"Preparation of Coordination Compound Tetrammine Sulphate Monohydrate","text":"<ol> <li>1.6g CuSO4 salt in 5mL CuSO4 solution on a 250mL beaker. </li> <li>Add 10mL Ammonia Solution</li> <li>Add 15mL Ethanol dropwise till purple precipitate. </li> <li>Filter, Dry, Weigh. </li> </ol>"},{"location":"1_Core/Chem_Lab/Exp_03/","title":"Exp 03","text":""},{"location":"1_Core/Chem_Lab/Exp_03/#estimation-of-ferrous-sulphate-by-permanganometry","title":"Estimation of Ferrous Sulphate by Permanganometry","text":"<ol> <li>Weigh 1.4g of FeSO4.7H2O on watch glass. </li> <li>Transfer this amount into a 100mL volumetric flask. Add 10mL of dil. H2SO4</li> <li>Add enough volume of distilled water to the flask to make it reach the mark. </li> <li> <p>Calculate the Molarity of this solution. </p> </li> <li> <p>Fill the burette with KMnO4 Solution till 0. </p> </li> <li>Pipette 25mL of this standard solution in a conical flask. Add 10mL of silute H2SO4 to this. </li> <li> <p>Titrate till a light pink color is observed. Repeat twice for two concordant readings. </p> </li> <li> <p>Repeat this for the unknown solution. </p> </li> </ol>"},{"location":"1_Core/Computer_Programming/","title":"Computer Programming","text":"<p>This was very similar to the C++ course in CBSE 11<sup>th</sup>/12<sup>th</sup>, so haven't made detailed notes for this course, yet. Some basics have been provided though. </p> <p>The primary goals of this course are to introduce:</p> <ul> <li>Basic data representation and processing within a computer.</li> <li>Techniques for specifying data, operations, and problem-solving using a programming language.</li> <li>Systematic approaches for program construction.</li> </ul> <p>The course covers topics such as:</p> <ul> <li>Basic Model of a Computer</li> <li>Problem Solving: Basic Computing Steps and Flow Charting (Assignment, Sequencing, Conditionals, Iteration)</li> <li>Programming Constructs: Expressions, Statements, Conditionals, Loops, Functions/Procedures</li> <li>Data Types: Primitive Types, Tuples, Unions, Lists/Arrays, Pointers, and Dynamically Allocated Data</li> <li>Input, Output, and Files</li> </ul>"},{"location":"1_Core/Computer_Programming/#explainer-video","title":"Explainer Video","text":"<p>This video covers till midsem</p>"},{"location":"1_Core/Computer_Programming/#labs","title":"Labs","text":"<p>This junior's repository SivaaB/BITSPil-CSF111 contains solutions to all the lab and tutorial sheets given in class. Thank you Sivaa! \u2728</p> <p>I will also be adding my lab codes in this folder. Later, based on user feedback, we can filter to the simplest codes.</p>"},{"location":"1_Core/Computer_Programming/01_Intro/","title":"01 Intro","text":""},{"location":"1_Core/Computer_Programming/01_Intro/#basics","title":"Basics","text":"<p>Digital data is in binary form.</p> <ul> <li>0/1 (bit)</li> <li>8 bits = 1 byte</li> <li>1024 bytes = 1KB</li> </ul> <p>When write a C program to perform a task, it gets converted into binary instructions which a computer can understand</p>"},{"location":"1_Core/Computer_Programming/01_Intro/#variables","title":"Variables","text":"<pre><code>int x; // declaration\nx=5; // initialization\n\nint x = 5; // combined\n</code></pre>"},{"location":"1_Core/Computer_Programming/01_Intro/#tokens","title":"Tokens","text":"<ul> <li> <p>atomic unit of a code</p> </li> <li> <p>Key words</p> <pre><code>return\n  void\n</code></pre> </li> <li> <p>Identifiers</p> <pre><code>int x = 3;\n</code></pre> </li> </ul>"},{"location":"1_Core/Computer_Programming/01_Intro/#code-block","title":"Code Block","text":"<p>group of code within <code>{ ... }</code></p>"},{"location":"1_Core/Computer_Programming/01_Intro/#structure","title":"Structure","text":"<pre><code>print(\"hi\")\nprint(\"hello\")\n</code></pre> <pre><code>#include &lt;stdio.h&gt;\n\nvoid print_on_screen()\n{\n  printf(\"hi\"); // ; = terminator\n}\n\nint main() // driver function of your program\n{\n  // code\n  print_on_screen();\n\n  int x = 4; // \n\n  return 0;\n}\n</code></pre>"},{"location":"1_Core/Computer_Programming/01_Intro/#data-types","title":"Data Types","text":""},{"location":"1_Core/Computer_Programming/01_Intro/#primitive","title":"Primitive","text":"<code>void</code> (nothing) 0 <code>boolean</code> True/Falsetrue/false 1 <code>char</code> \u2018H\u2019 1 %c <code>int</code> 34444 2 %d <code>float</code> 334545.345534 4 %f <code>double</code> 334545345534334545345534.334545345534334545345534 8 %"},{"location":"1_Core/Computer_Programming/01_Intro/#user-defined","title":"User-defined","text":"<p>(not in scope of current exam)</p>"},{"location":"1_Core/Computer_Programming/01_Intro/#math-operators","title":"Math Operators","text":"\\[ + - * / \\% \\] <ul> <li> <p><code>int</code>/<code>int</code> = <code>int</code></p> </li> <li> <p>B</p> </li> <li>^</li> <li>Multiplication/Division (whichever comes first)</li> <li>Addition/Subtraction (whichever comes first)</li> </ul>"},{"location":"1_Core/Computer_Programming/01_Intro/#type-casting","title":"Type Casting","text":"<p>change in data type of a variable for a momentary purpose</p> <p>implicit (automatic)</p> <p>int/int \\(\\to\\) int</p> <ul> <li>3/2 = 1.5</li> <li>3/2 = 1</li> </ul> <p>int/float \\(\\to\\) float</p> <ul> <li>larger data type</li> </ul> <pre><code>int x=5;\nfloat z = 5/(double) 5;\n</code></pre> <p>explicit(user-defined) type casting</p> <pre><code>int x = -5;\nint y = 2;\n\n-5/2; // -2.5 -2\n5/-2; // -2.5 -2\n\n5/-2.0f // -2.5\n</code></pre>"},{"location":"1_Core/Computer_Programming/01_Intro/#relational-operators","title":"Relational Operators","text":"\\[ &gt; , \\ge , &lt; , \\le, ==, \\ne, != \\] <ul> <li>\\(=\\) and \\(==\\)<ul> <li>assignment</li> <li>equality operator</li> </ul> </li> </ul> <pre><code>const float pi = 3.14;\n</code></pre>"},{"location":"1_Core/Computer_Programming/01_Intro/#logical-operators","title":"Logical Operators","text":"\\[ ! \\\\ \\&amp;\\&amp; \\\\ || \\] \\[ [!0 \\&amp;\\&amp; 1 || 3\\&amp;\\&amp;!1] \\] <pre><code>int main()\n{\n  int x;\n\n  // take input from user\n  scanf(\"%d\", &amp;x); // storing the value in the address of x\n\n  printf(\"%d\", x);\n\n\n  return 0;\n}\n\n// x = 5\n</code></pre> <pre><code>int main()\n{\n  int x = 5;\n\n  printf(\"%d\", ++x); // 6\n  printf(\"%d\", x); // 6\n\n  return 0;\n}\n</code></pre>"},{"location":"1_Core/Computer_Programming/02_Flow_Control/","title":"02 Flow Control","text":""},{"location":"1_Core/Computer_Programming/02_Flow_Control/#if-else","title":"if \u2026 else","text":"<p>conditional statement</p> <p>branching</p> <pre><code>flowchart TB\ndb{1==0}\n\ndb --- |Yes|nesnt\ndb --- |No|blah</code></pre> <pre><code>int x = 4;\nif (x == 5)\n{\n  printf(\"Yes\");\n  printf(\"hi\");\n}\n\nprint(\"outside everything\");\n</code></pre> <pre><code>outside everything\n</code></pre> <pre><code>if(x&gt;10)\n{\n if(y &lt; 10) \n {\n   printf(\"hi\");\n }\n}\nelse\n{\n  printf(\"hi\");\n}\n</code></pre> <pre><code>if(x&gt;10 &amp;&amp; y&lt;10)\n{\n   printf(\"hi\");\n}\nelse\n{\n  printf(\"hi\");\n}\n</code></pre> <pre><code>elif\n</code></pre> <pre><code>hi\noutside everything\n</code></pre> <p><code>for</code>, <code>if</code> and <code>else</code> go to the immediate next block</p> <pre><code>x = 5\nif x == 5:\n  print(\"yes\")\n  print(\"block 1\")\nelse:\n  print(\"no\")\n  print(\"block 2\")\n</code></pre>"},{"location":"1_Core/Computer_Programming/02_Flow_Control/#ternary-operator","title":"Ternary Operator","text":"<p>cooler way of doing if else</p> <pre><code>(y&lt;10)?(x=10):(x=0)\n</code></pre>"},{"location":"1_Core/Computer_Programming/02_Flow_Control/#switch","title":"Switch","text":"<pre><code>switch(s):\n{\n  case 1: something; break;\n  case 2: something; break;\n  default: something;\n}\n\nswitch(s):\n{\n  case 'a': something; break;\n  case 'b': something; break;\n  default: something;\n}\n</code></pre>"},{"location":"1_Core/Computer_Programming/03_Loops/","title":"03 Loops","text":""},{"location":"1_Core/Computer_Programming/03_Loops/#types","title":"Types","text":"<ul> <li><code>while</code><ul> <li>Entry-controlled loop</li> </ul> </li> <li><code>do...while</code><ul> <li>Exit-controlled loops</li> </ul> </li> <li><code>for</code></li> </ul>"},{"location":"1_Core/Computer_Programming/03_Loops/#components","title":"Components","text":"<ul> <li>Initialization<ul> <li>\\(n\\)</li> <li>\\(i\\)</li> </ul> </li> <li>Loop condition checked</li> <li>Code to execute is run</li> <li>Updation occurs</li> </ul> <pre><code>flowchart LR\n\ni[Initialization] --&gt; c[Condition] --&gt; Code --&gt; Updation --&gt; c</code></pre> <pre><code>/*\nint n = 10;\nint i = 1; // comesntenret\n\nfor (; ; )\n{\n\n  if(i&gt;n)\n    break;\n\n  // code\n\n  i++;\n}\n*/\n\nfor (int i = 1, n=10; i&lt;=n; i++)\n{\n\n}\n\nint i = 1, n=10;\nfor (; i&lt;=n; i++)\n{\n  ;// nothing happens\n}\n\nprintf(\"%d\", i); 11\n</code></pre> <pre><code>for (int i = 1, n=10; i&lt;=n; i++)\n  printf(\"%d\\n\", i);\n</code></pre> <pre><code>1\n2\n...\n9\n10\n</code></pre> <pre><code>for (int i = 1, n=10; i&lt;=n; i++)\n  printf(\"%d\\n\", i);\n  printf(\"%d\\n\", i);\n\nfor (int i = 1, n=10; i&lt;=n; i++)\n{\n  printf(\"%d\\n\", i);\n}\nprintf(\"%d\\n\", i);\n\n// here\n</code></pre> <pre><code>1\n2\n3\n...\n9\n10\n11\n</code></pre> <pre><code>for (int i = 1, n=10; i&lt;=n; i++)\n{\n  printf(\"%d\\n\", i);\n  printf(\"%d\\n\", i);\n}\n\n// here\n</code></pre> <pre><code>1\n1\n2\n2\n...\n9\n9\n10\n10\n</code></pre> <pre><code>int i, n;\nfor (i = 1, n=10; i&lt;=n; i++);\n  printf(\"%d\\n\", i);\n\nint i, n;\nfor (i = 1, n=10; i&lt;=n; i++)\n{\n ; \n}\n\nprintf(\"%d\\n\", i);\n</code></pre> <pre><code>11\n</code></pre> <pre><code>int i = 1, n = 10;\n\n// curly brace\n// run 10 times\nwhile(i&lt;=n)\n{\n print(\"hi\");\n i++;\n}\n</code></pre> <pre><code>int i = 1, n = 10;\n// curly brace\n\ndo\n{\n  // code\n  i++;\n} while (i&lt;n); // has semi-colon\n</code></pre> <pre><code>x=5, y=10\n</code></pre>"},{"location":"1_Core/Computer_Programming/03_Loops/#pyramid-example","title":"Pyramid example","text":"<pre><code>*\n*\n*\n*\n</code></pre> <pre><code>#include &lt;stdio.h&gt;\n\nint main()\n{\n  int n = 4; // no of lines\n  for (int i=1; i&lt;=n; i++) // run the loop 4 times\n  {\n    printf(\"*\\n\");\n  }\n\n  return 0;\n}\n</code></pre> <pre><code>*\n**\n***\n****\n</code></pre> <pre><code>#include &lt;stdio.h&gt;\n\nint main()\n{\n  printf(\"*\\n\");\n    printf(\"**\\n\");\n    printf(\"***\\n\");\n    printf(\"****\\n\");\n\n  return 0;\n}\n</code></pre> <ol> <li>DUMB</li> <li>Not scalable</li> <li>Not elegant</li> </ol> <p>Hence, we need loops (iterative statements)</p> <pre><code>#include &lt;iostream.h&gt;\n\nint main()\n{\n  for (int i=1; i&lt;=n; i++)\n  {\n    for (int j=1; j&lt;=i; j++)\n    {\n      printf(\"*\");\n    }\n\n    printf(\"\\n\");\n  }\n  return 0;\n}\n</code></pre>"},{"location":"1_Core/Computer_Programming/03_Loops/#jump-statements","title":"jump statements","text":"<ul> <li>break<ul> <li>exits the current loop</li> </ul> </li> <li>continue<ul> <li>skips the below code of the current iteration</li> <li>goes to the update segment of (for)loop</li> <li>in other loops, it goes to condition</li> <li>then continues as usual</li> </ul> </li> </ul> <pre><code>for (int i=1; i&lt;=5; i++)\n{\n  if(i==3)\n  {\n    break;\n    printf(\"%d\\n\", i);\n  }\n\n   printf(\"%d\\n\", i);\n}\n\n// come here\n</code></pre> <pre><code>1\n2\n</code></pre> <pre><code>for (int i=1; i&lt;=5; i++)\n{\n  if(i==3)\n  {\n    continue;\n    printf(\"%d\\n\", i);\n  }\n\n   printf(\"%d\\n\", i);\n}\n</code></pre> <pre><code>1\n2\n4\n5\n</code></pre> <pre><code>int i=1;\nwhile(i&lt;=5)\n{\n  if(i==3)\n  {\n    continue;\n    printf(\"%d\\n\", i);\n  }\n\n   printf(\"%d\\n\", i);\n\n  i++;\n}\n</code></pre> <pre><code>1\n2\n(infinite loop)\n</code></pre> <pre><code>int i=1;\ndo\n{\n  if(i==3)\n  {\n    continue;\n    printf(\"%d\\n\", i);\n  }\n\n   printf(\"%d\\n\", i);\n\n  i++;\n} while(i&lt;=5); // has terminator\n</code></pre> <pre><code>1\n2\n(infinite loop)\n</code></pre>"},{"location":"1_Core/Computer_Programming/03_Loops/#loop-printining-questions","title":"Loop Printining Questions","text":"<pre><code>int n = 4; // no of lines\nint j = 1;\nfor (int i=1; i&lt;=n; i++) // controls no of lines\n{\n  printf(\"%d\", j);\n  j++;\n\n}\n</code></pre> <pre><code>1\n2\n3\n4\n</code></pre> <pre><code>int n = 4; // no of lines\n\nfor (int i=1; i&lt;=n; i++) // controls no of lines\n{\n  int j = 1;\n  printf(\"%d\", j);\n  j++;\n}\n</code></pre> <pre><code>1\n1\n1\n1\n</code></pre>"},{"location":"1_Core/Computer_Programming/04_Arrays/","title":"04 Arrays","text":"<p>linear data structure</p> <p>primitive data structure</p> <p>arrays are 0-indexed</p> <ul> <li>index start from 0</li> </ul> <p>it is non-mutable (non-changeable)</p> <p>statically-allocated</p> <p>size of array is fixed</p> <p>array is similar to tuple in python, but only same data type</p> <p>collection of elements of the same data type</p>"},{"location":"1_Core/Computer_Programming/04_Arrays/#ne-list","title":"\\(\\ne\\) List","text":"<p>In python, list is a collection of elements of the same/different type</p> <pre><code>list = [2, \"hi\", 3, 4, 5]\n</code></pre>"},{"location":"1_Core/Computer_Programming/04_Arrays/#declarationcreation","title":"Declaration/Creation","text":"<pre><code>int array[10];\n\nchar array[10];\nfloat array[10];\ndouble array[10];\nconst int length = 10;\nint array[length];\n</code></pre>"},{"location":"1_Core/Computer_Programming/04_Arrays/#initialization","title":"Initialization","text":"<pre><code>array[0]= 1;\narray[1] = 2;\n\nint num;\nscanf(\"%d\", &amp;num);\narray[2] = num;\nscanf(\"%d\", &amp;num);\narray[3] = num;\n</code></pre> <pre><code>for(int i=0; i&lt;10; i++)\n{\n  int num;\n  scanf(\"%d\", &amp;num);\n  array[i] = num;\n}\n\n// no error\n</code></pre> <pre><code>for(int i=0; i&lt;10; i++)\n{\n  scanf(\"%d\", array[i]); // no &amp;\n}\n\n// no error\n</code></pre> <pre><code>for(int i=1; i&lt;=9; i++)\n{\n  scanf(\"%d\", &amp;num);\n  array[i] = num;\n}\n\n// no error\n</code></pre>"},{"location":"1_Core/Computer_Programming/04_Arrays/#display","title":"Display","text":"<pre><code>for(int i=0; i&lt;10; i++)\n{\n  printf(\"%d\\n\", array[i]);\n}\n</code></pre>"},{"location":"1_Core/Computer_Programming/04_Arrays/#example","title":"example","text":"<pre><code>#include &lt;iostream.h&gt;\n\nint main()\n{\n  int a[100]; // assuming a random number\n\n\n  // input no of elements\n  int n;\n  printf(\"Please enter no of elements that you are goint to input\");\n  scanf(\"%d\", &amp;n);\n\n  // input the elements\n  for(int i=0; i&lt;n; i++)\n  {\n    printf(\"Enter number at index %d: \", i);\n    scanf(\"%d\", a[i]);\n  }\n\n  // display marks &gt; 60\n  for(int i=0; i&lt;n; i++)\n  {\n    if(a[i] &gt; 60)\n      printf(\"%d\", a[i]);\n  }\n  // or\n  for(int i=0; i&lt;n; i++)\n  {\n    int num = a[i];\n    if(num &gt; 60)\n      printf(\"%d\", num);\n  }\n\n  return 0;\n}\n</code></pre>"},{"location":"1_Core/Computer_Programming/05_Pointers/","title":"05 Pointers","text":"<p>a variable that stores address</p> <p>name of an array is a pointer to the first element of that array</p>"},{"location":"1_Core/Electrical_Science/","title":"Electrical Science","text":"<p>I\u2019ve got the notes for these, but need someone to digitize them.</p> <p>This course provides a comprehensive introduction to the fundamental principles of electrical sciences, focusing on circuit analysis, electrical machines, electronic devices, and digital electronics. Understanding these core topics is essential for students pursuing electrical and electronic engineering, as they form the foundation for advanced studies in the field.</p> <p>The course covers key learning objectives, including basic circuit elements and laws, circuit analysis techniques, and relevant theorems. Students will explore the behavior of circuits with energy-storing elements, three-phase circuits, and magnetic circuits. Additionally, the course delves into transformers, the principles of DC and AC machines, semiconductor fundamentals, and the operation of various devices, including bipolar junction transistors and field-effect transistors (FETs). Operational amplifiers and the basics of digital circuits are also included, providing students with a well-rounded understanding of electrical engineering concepts and their applications.</p>"},{"location":"1_Core/Electrical_Science/01/","title":"Introduction","text":""},{"location":"1_Core/Electrical_Science/01/#electric-vs-electronic","title":"Electric vs Electronic","text":"Electric Electronic Signal Discrete Continuous Example Bulb Fan"},{"location":"1_Core/Engineering_Graphics/","title":"Engineering Graphics","text":"<p>This course aims to equip students with the techniques and standard practices of technical drawing to effectively communicate and produce design ideas. It covers the theories of projection and the fundamentals of engineering drawing, utilizing AutoCAD, a widely used CAD application software. Students will be introduced to and emphasized on basic AutoCAD commands throughout the course.</p> <p>The course has two parts, Part - 1 (for midsem) includes orthographic projections, isometric projections and straight lines. Part - 2 (for compre) includes projection of lines + solids, sectional view of solids and cutting of solids. </p> <p>A problem set wiith questions from the textbook are given a week before midsems and compre which are really helpful for preparation. The solutions to those problems have been provided by this junior's repository SivaaB/BITSPil-CSF111. Thank you Sivaa! \u2728</p>"},{"location":"1_Core/General_Biology/","title":"General Biology","text":"<p>This course aims to provide a broad introduction to the major principles and topics in biology. It highlights the relationship of living organisms with their environment at the molecular level, aligning with modern research in biological sciences. </p> <p>The course is divided into two parts - one part includes introduction to biology, types of cell, different components of a cell, energy production in plants + animals, heridiitary and evolution ; the other part is post midsem. Part - 1 is all closed book. Part - 2 is all open book. So, preparation must be done accordingly. </p> <p>As of date, notes only upto midsem have been uploaded. Will proceed to put up notes for postmidsem portion as we progress thorugh the semester. </p>"},{"location":"1_Core/General_Biology/01_Getting_introduced_to_biology_and_its_scope/","title":"01 Getting introduced to biology and its scope","text":""},{"location":"1_Core/General_Biology/01_Getting_introduced_to_biology_and_its_scope/#steps-in-biology-to-conclude-a-statement","title":"Steps in biology to conclude a statement:","text":"<p>Observation, Question, Hypothesis, Prediction, Experiment. (convert this into mermaid flowchart) If the experiment does not support hypothesis then make a new one. If it supports hypothesis, make new predictions. </p>"},{"location":"1_Core/General_Biology/01_Getting_introduced_to_biology_and_its_scope/#mnemonic-for-remembering-oqhpe","title":"Mnemonic for remembering: [OQHPE]","text":"<ol> <li>**O**bservation</li> <li>**Q**uestion</li> <li>**H**ypothesis</li> <li>**P**rediction</li> <li>**E**xperiment</li> </ol>"},{"location":"1_Core/General_Biology/01_Getting_introduced_to_biology_and_its_scope/#major-life-charecteristics","title":"Major life charecteristics","text":"<p>Order, Regulation, Growth and Development, Energy Processing, Response to Environment, Reproduction, Evolution. </p>"},{"location":"1_Core/General_Biology/01_Getting_introduced_to_biology_and_its_scope/#mnemonic-for-remembering-orgerre","title":"Mnemonic for remembering: [ORGERRE]","text":"<ol> <li>**O**rder</li> <li>**R**egulation</li> <li>**G**rowth and Development</li> <li>**E**volution</li> <li>**R**esponse to Environment </li> <li>**R**eproduction </li> <li>**E**volution</li> </ol>"},{"location":"1_Core/General_Biology/01_Getting_introduced_to_biology_and_its_scope/#major-themes-in-biology","title":"Major themes in biology","text":"<p>Energy Processsing, Structure and Function, Information Flow, Evolution, Interconnection to other systems. (Pnemonic : ESIEI)</p>"},{"location":"1_Core/General_Biology/01_Getting_introduced_to_biology_and_its_scope/#mnemonic-for-remembering-esiei","title":"Mnemonic for remembering: [ESIEI]","text":"<ol> <li>**E**nergy Processing</li> <li>**S**tructure and Function</li> <li>**I**nformation Flow</li> <li>**E**volution</li> <li>**I**nterconnection to other systems.</li> </ol> <pre><code>graph LR;\n    A[Molecules and Atoms] --&gt; B[Organelles, Cells];\n    B --&gt; C[Tissues, Organs];\n    C --&gt; D[Organ System];\n    D --&gt; E[Organisms];\n    E --&gt; F[Population];\n    F --&gt; G[Communities];\n    G --&gt; H[Ecosystem];\n    H --&gt; I[Biosphere];</code></pre>"},{"location":"1_Core/General_Biology/01_Getting_introduced_to_biology_and_its_scope/#eubacteria","title":"Eubacteria:","text":"<ul> <li>Small, Prokaryotic, Single celled organisms ranging from 1 - 10um. </li> <li>Lack nucleus and cell membrane bound organelles. </li> <li>Cell wall contains a complex organic molecule called peptidoglycan. </li> </ul>"},{"location":"1_Core/General_Biology/01_Getting_introduced_to_biology_and_its_scope/#archea","title":"Archea:","text":"<ul> <li>Prokaryotic</li> <li>Cell membrane has Isoprenes. </li> <li>Extremophiles </li> <li>Has large proportions of genes that are diff from eubacteria.</li> </ul> <p>Protists : Microscopic Organisims like Amoeba and Seaweed.</p> <p>Kingdoms are distinguished as:  - Plantae : Produce their own food.  - Animalia : Obtain food by ingesting and digesting.  - Fungi : Decomposers, obtain food by decomposing dead and decaying organic matter. </p> <p>Prokaryotes have:  - Pilli : Short projections for attachment to surfaces.  - Flagella : Long projections to propel them through their liquid environment.  - Capsules : Makes the bacteria's surface more slippery, helping it to escape engulfment by phagocytic cells.</p> <p>Only Animal cells have Lysomes and Plant Cells have Chloroplasts.</p>"},{"location":"1_Core/General_Biology/02_Components_of_the_Cell_and_its_Internal_Working/","title":"02 Components of the Cell and its Internal Working","text":""},{"location":"1_Core/General_Biology/02_Components_of_the_Cell_and_its_Internal_Working/#molecules-of-life-tb-order","title":"Molecules of Life [TB Order]","text":"<p>Animals store excess glucose in the form of Glycogen. </p> <p>Cellulose can't be digested by humans, and only ruminating animals like cows can do so through the help of the microorganisims in their intestines which help them break them down. Although we can't digest it, it is good for us as the lining of our intestines secrete mucus which allows food to pass freely.</p> <p>A pound of fat contains more than the twice the amount of energy as compared to a pound of carb. The downside is it's very hard to get rid of them as they are stored deep in our adipose tissues which swell or shrink as we deposit or withdraw fat from them.</p> <p>Unsaturated fatty acids are those which have fewer than the max number of hydrogen atoms. They appear bent. For a fat to be saturated all 3 components need to be straight.</p> <p>Sometimes a food manufacturer wants to use vegetable oil but wants the end product to be solid, in that case adds hydrogen, this process is called hydrogenation. Unfortunately, hydrogenation also leads to trans fat which are very detrimental to our health. Some fats like omega 3 are very healthy for us, these are found in foods like nuts and salmons.</p> <p>Cholesterol is a base steroid on top of which other hormones like testosterone is made. Anabolic steroids mimic testosterone, these are given to treat diseases like cancer and AIDS, but athletes abuse this and the use of this has dangerous side effects such as depression, liver damage, violent behavior.</p> <p>Changing one element's sequence in proteins can be very detrimental as it affects the proteins ability to function. Eg. Substitution of one amino acid in hemoglobin incorrectly results in sickle cell disease, an inherited blood disorder.</p> <p>High Fever are dangerous for this very reason as with the inc in temp, the proteins tend to lose their shape.</p>"},{"location":"1_Core/General_Biology/02_Components_of_the_Cell_and_its_Internal_Working/#linking-of-ribonucleic-acid-can-be-represented-as","title":"Linking of Ribonucleic Acid can be represented as:","text":"Base-1 Base-2 Adenosine Guanine Cytosine Thymine      / Uracil"},{"location":"1_Core/General_Biology/02_Components_of_the_Cell_and_its_Internal_Working/#difference-between-rna-and-dna","title":"Difference between RNA and DNA:","text":"Category DNA RNA Full name Deoxyribonucleic acid Ribonucleic acid Sugar Deoxyribose Ribose Bases Adenine (A), Guanine (G), Cytosine (G), Thymine (T) Adenine (A), Guanine (G), Cytosine (C), Uracil (U) Structure Double-stranded helix Single-stranded helix Function Stores genetic information Transfers genetic information and assists in protein synthesis"},{"location":"1_Core/General_Biology/02_Components_of_the_Cell_and_its_Internal_Working/#termninology-in-this-module","title":"Termninology in this module:","text":"Term Equivalent Starch Amylose Indigestible fibres Cellulose Animal starch Glycogen"},{"location":"1_Core/General_Biology/02_Components_of_the_Cell_and_its_Internal_Working/#a-tour-of-the-cell-tb-order","title":"A Tour of the Cell [TB Order]","text":"<p>Plasma Membrane:  - Made of Phospholipids which comprise of phospholipid bilayer, one for hydrophilic head and other for hydrophobic tail. - Inside the bilayer, it mostly comprises of proteins which regulates what goes in and out. It can freely move and is called Fluid Mossaic.</p> <p>Superbug : Ruptures the plasma membrane, causing a staph infection. Mostly occurs in schools, hospitals and gyms.</p> <p>Animal cells lack cell wall, but compensate w the help of an extracellular matrix (protein collagen).</p> <p>Nucleus :  - It is separated from the cytoplasm by a double membrane called the nuclear envelope. It has holes. - Long DNA molecules and associated proteins from fibres called chromatin are present, each chromatin fibre has one chromosome.</p> <p>Ribosomes : - Some case the proteins made by ribosomes are made in nucleus and are transported to the cytoplasm through the pores of the nuclear envelope, in other cases they are in the cytoplasm itself, making proteins that remain fluid within the cell, sometimes even connected to the ER.  - Cells that make a lot of proteins have a lot of ribosomes.</p> <p>Endoplasmic Reticulum:  - Rough ER : To make more plasma membrane, it can budd off. Cells that secrete a lot of protein such as your salivary glands, which secrete enzymes in your mouth are rich in rough ER.  - Smooth ER: Repro hormones are rich in smooth ER. They help in detoxification, example liver cells as well. This eventually strengthen's the body's tolerance to that drug, therefore requiring more dosage to have the same effect.</p> <p>Golgi Apparatus : Is the ship port of the cell. </p> <p>Lysomes: Safe area for animale cells to digest large proteins, nucleic acids .etc. without decomposing itself. It can also engulf  organelles within the cell, essentially recycling it. Heriditary disorders occur as a result of lysome storage disease.</p> <p>Vacuoles : Can pump out or accept water based on need. Central Vacuole comprises of more than half the volume of a mature plant cell. Some cells of flowering plants, in their vacuoles may have pigments present in them to attract pollinating insects. Other plants store toxic elements in their vacuoles to protect themselves against plant eating animals.</p> <p>Plants have both chloroplasts and mitochondria but animals only have mitochondria. </p> <p>Cytoskeleton shape is maintained using microtubules.</p> <p>Movement in Cells: - Cilia : Short hair liked strucutres that are many in number and move back and forth in a coordinated fashion. Eg - Cilia in windpipe that are responsible to sweep mucus in the respiratory system get damages due to smoking leading to respiratory complications in smokers. - Flagella : They are singular, whip - like structures that help in movement os sperms during fertilization. The problems with flagella often times lead to infertility problems. - Pilli : Helps in attachment to surfaces. </p>"},{"location":"1_Core/General_Biology/03_Genetic_Regulation_and_the_Process_of_Cloning/","title":"03 Genetic Regulation and the Process of Cloning","text":""},{"location":"1_Core/General_Biology/03_Genetic_Regulation_and_the_Process_of_Cloning/#genetic-regulation-and-the-process-of-cloning","title":"Genetic Regulation and the Process of Cloning","text":""},{"location":"1_Core/General_Biology/03_Genetic_Regulation_and_the_Process_of_Cloning/#introduction","title":"Introduction:","text":"<p>Gene Regulation : A mechanism that turns on certain genes while other genes remain turned off. </p> <p>Cells with same genetic information can develop into different types of cells through gene regulation. Regulating gene activity within cells allows for specialization. </p> <p>Genes determine the nucleotide sequence of specific mRNA molecules, and the mRNA in turn determines the specific sequence of amino acid in proteins. A gene that is being turned on is being transcribed into mRNA and that message is being translated into specific proteins. The overall process by which genetic information flows from genes to proteins is called Gene Expression. </p> <pre><code>graph TD;\nA(Genes)--&gt;B(Nucleotide Sequence of mRNA);\nB--&gt;C(Sequence of Amino Acid Proteins);</code></pre> <p>Notice that genes for \"housekeeping\" enzymes such as those that provide energy via glycolysis are always \"on\". Whereas, more specific genes for some proteins like insulin and haemoglobin are expressed only by a particular kind of cell. </p>"},{"location":"1_Core/General_Biology/03_Genetic_Regulation_and_the_Process_of_Cloning/#gene-regulation-in-bacteria","title":"Gene Regulation in Bacteria:","text":"<p>Bacteria regulate their genes in response to environmental changes. </p> <p>Natural Selection has favored bacterial that express only genes whose products are needed by the cell. For example, imagine  eschericihia coli living in your intestine. </p> <ul> <li>If you drink a milk shake, there will be a sudden rush of lactose. </li> <li>In response, E.Coli will express only three genes for enzymes that will enable the bacterium to digest this sugar. </li> <li>After the sugar has been digested, these genes are turned \"off\". </li> </ul> <p>An operon is a sequence of DNA containing a cluster of genes under the control of a single promoter. An operon consists of a POS - promoter, operator and structural genes that function in a highly coordinated manner. </p> <p>A lac-operon as discussed in the example given above is a segment of DNA containing adjacent genes including a SOR - structural gene, an operator gene and a regulatory gene. </p>"},{"location":"1_Core/General_Biology/03_Genetic_Regulation_and_the_Process_of_Cloning/#gene-regulation-in-eukaryotes","title":"Gene Regulation in Eukaryotes:","text":"<p>Eukaryotic chromosomes are highly condensed. This prevents gene expression as RNA polymerase and other transcription proteins cannot bind to DNA. </p> <p>Introns : Sequence of a eukaryotes genes that are not transcribed into a protein. </p> <p>Exons : Sequence of a gene's DNA that transcribes into protein's structures. </p>"},{"location":"1_Core/General_Biology/03_Genetic_Regulation_and_the_Process_of_Cloning/#genetic-regulation-occurs-at-several-levels-uirstp","title":"Genetic Regulation occurs at several levels: [UIRSTP]","text":"<ul> <li>Unpacking of DNA </li> <li>Initiation of Transcription </li> <li>RNA Processing </li> <li>Stability of RNA </li> <li>Translation </li> <li> <p>Protein Activation an Breakdown</p> </li> <li> <p>Unpacking of DNA : </p> </li> <li>When lactose is absent the repressor remains bound to the operator and RNA Polymerase is therefore prevented from moving down the lac operon transcribing its genes. </li> <li> <p>When lactose is present the repressor is converted to its inactive form, which does not bind to the operator. RNA Polymerase can therefore move past the operator and transcribe the necessary genes into a single mRNA. </p> </li> <li> <p>Initiation of Transcription : </p> </li> <li>Enhancers and Silencers : These are sequences on the DNA that are binding sites for proteins. Enhancers increase, Silencers decrease protein synthesis. </li> <li> <p>Transcription Factors (TFs) : Are proteins that control the available promoter sequence for transcription. TFs bind to DNA around the gene's promoter region and influence RNA polymerase's ability to start transcription. A particular gene will not be expressed if its TFs are not available.  </p> </li> <li> <p>Eukaryotic Transcription is so strictly regulated that TFs always guide RNA polymerase to the promoter sequence. </p> </li> <li> <p>Prokaryotic cells also use proteins to block or encourage transcription but not to the extent that this stratergy is used in eukaryotic cells. </p> </li> <li> <p>RNA Processing: </p> </li> <li>One gene can code for multiple RNAs, and thus multiple proteins. </li> <li> <p>Alternative splicing increases phenotype variation within species and among different species. </p> </li> <li> <p>RNA Stability: </p> </li> <li>The lifespan of a mRNA molecule determines how much of a protein is made of microRNAs (miRNAs). These are small ss RNA molecules that bind to the complementary sequences on mRNA molecule in the cytoplasm. After binding, some miRNA trigger breakdown of their target mRNA, whereas others block translation. </li> </ul> <p>Cells signal each other by direct contact or by the release of a substance from one cell that is taken up by another cell. The three stages are: [R.ST.R]</p> <ul> <li>Reception, whereby the signal molecule binds to the receptor. </li> <li>Signal Transduction, where the chemical signal results in a series of enzyme activations. </li> <li>Response, which is cellular responses. </li> </ul>"},{"location":"1_Core/General_Biology/03_Genetic_Regulation_and_the_Process_of_Cloning/#cloning-in-plants","title":"Cloning in Plants:","text":"<p>Tissues removed from the stem of Orchid Plant and placed in a growth medium may begin dividing and eventually grow into an adult plant. The new plant is a genetic duplicate of the parent plant. This process proves that mature plant cells can reverse their differentiation and develop in to all the specialized cells of an adult plant. </p>"},{"location":"1_Core/General_Biology/03_Genetic_Regulation_and_the_Process_of_Cloning/#reproductive-cloning-of-animals","title":"Reproductive Cloning of Animals:","text":"<p>This is achieved through Somantic Cell Nuclear Transplantation (SCNT) which involves replacing the nucleus of an egg cell or a zygote with a nucleus removed from an adult body cell. If the animal to be cloned is a mammal, further development of zygote requires implanting the early embryo in the uterus of a surrogate mother. The result will be the clone of the donor (not the surrogate mother). </p>"},{"location":"1_Core/General_Chemistry/","title":"General Chemistry","text":"<p>This course provides a comprehensive survey of various topics, including the electronic structure of atoms and molecules, spectroscopy, chemical thermodynamics, and kinetics. </p> <p>It emphasizes the applications of these topics in understanding the structure and properties of organic compounds and transition metal complexes.</p> <p>The notes are organized into three main sections: inorganic chemistry, organic chemistry, and physical chemistry.</p> <p>In addition, useful yt links and tutorial sheet answers are provided for all the tutorial sheets to enhance your understanding of the material.</p>"},{"location":"1_Core/General_Chemistry/01_In-Organic/","title":"01 In Organic","text":"<ul> <li>In-Organic<ul> <li> <p>Co-ordination chem</p> <ul> <li> <p>Handwritten notes (14 pg)</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> </li> <li> <p>Tutorial 1 (organic)</p> <p></p> <p></p> </li> <li> <p>Tutorial 2 (organic)</p> <p></p> <p></p> </li> </ul> </li> </ul> </li> </ul>"},{"location":"1_Core/General_Chemistry/02_Organic/","title":"02 Organic","text":"<ul> <li> <p>Organic</p> <ul> <li> <p>Substitution and elimination reactions</p> <ul> <li> <p>Handwritten notes (17 pg)</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> </li> <li> <p>https://www.youtube.com/watch?v=hz-fSXifP9w&amp;t=286s&amp;ab_channel=TheOrganicChemistryTutor very good video that explains everything in sub elimination nd provides a good table for easy learning</p> </li> <li>https://www.youtube.com/watch?v=qTc2uud7TVU&amp;ab_channel=TheOrganicChemistryTutor test review</li> </ul> </li> <li> <p>Addition reactions</p> <p>These reactions are exothermic because the electron density around the carbon atoms becomes polarized, making them more commercially viable.</p> <ul> <li> <p>Addition reactions</p> <ul> <li> <p>Addition rxn general mechanism</p> <ul> <li>Since carbocation is preferred stability is 3&gt;2&gt;1</li> <li> <p>Order of reactivity of Hydrogen halides is HF&lt;&lt;&lt;&lt;HCl&lt;HI where HI is the fastest because its always polarized</p> <p>F**i**C**kle **B**err**I</p> <p></p> </li> <li> <p> Constitutional isomers and regioselectivity</p> <p>Constitutional isomers</p> <p></p> </li> <li> <p> Markovnikovs rule</p> <ul> <li>Product with more branches is preferred</li> <li>H goes to Carbon atom with more Hydrogen ( rich becomes richer ) and the product we get is the major product</li> <li>major requires lower Gibbs free energy</li> <li> <p>Both back side and front side attack is possible therefore this results in the formation of a racemic mixture of two possible enantiomers, since the addition can occur from either side of the double bond.</p> <p></p> </li> </ul> </li> <li> <p> Stereochemistry of ionic addition to alkene</p> <p></p> </li> <li> <p> Anti- Markovnikovs rule</p> <p></p> <ul> <li>Peroxide has no effect on HI and HCl ( R-O-O-R is a peroxide )</li> <li>Chain initiation and chain propagation ( Formation of free radical and formation of Halo hydrocarbon )</li> </ul> <p></p> <p>https://www.youtube.com/watch?v=ZMcCvD5dDMU explains this</p> <p>Basically, Chain initiation happens first where bond is broken and we get roh and Br</p> <p>and then propogation happens where the Br we got from before  attacks to form a stable carbocation which is then attacked by HBR again to provide the hydrogen</p> </li> </ul> </li> <li> <p>Addition rxn with H2SO4 to form alcohol</p> <ul> <li> <p> Addition of H2SO4 to alkene</p> <p></p> <p>It is stereo-specific and follows markonikovs rule</p> </li> </ul> </li> <li> <p>Addition rxn with water</p> <ul> <li> <p> Acid catalyzed Hydration</p> <p>Follows Markovnikovs reaction</p> <p></p> </li> </ul> <p>Step 2 and 3 are fast follows markovnikovs</p> </li> <li> <p>Le Chatliers principle</p> <p>The hydration dehydration equilibria is altered  towards hydration \u2014&gt; alcohol \u2014&gt; adding dil. acid at low temp towards dehydration \u2014&gt; Alkene \u2014&gt; adding conc. acid at high temp</p> </li> <li> <p>Oxymercuration Hydration</p> <p>Regiospecific rxn </p> <p>follows markovnikovs rxn we get Oh , nabh4 removes the HgOAc</p> <p></p> </li> <li> <p>Hydro Boration Hydration rxn</p> <p>https://www.youtube.com/watch?v=RBwOfhS6mBM</p> <p>Boron is less e-ve than hydrogen that's why the oppo happens ,  concerted rxn</p> <p>first step u get tri alky bromine  2<sup>nd</sup> step u add h202</p> <p>Anti markovnikovs rxn</p> <p></p> <p>remember ch3 in back or front and Oh in front and back they will be oppo of each others positions</p> <p>(your alcohol and hydrogen should be on the same side)</p> <p>https://www.youtube.com/watch?v=PFwYIkkOyzA&amp;ab_channel=FrankWong helps alot</p> <p>https://www.youtube.com/watch?v=I-pmeHcjD8M&amp;ab_channel=Leah4sci</p> <p>Mechanism</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> </li> <li> <p>Electrophilic addition of Br2/Cl2 to Alkenes</p> <p>General rxn we get enantiomers</p> <p>https://www.youtube.com/watch?v=ZfOcYntQmlk&amp;list=RDCMUCEWpbFLzoYGPfuWUMFPSaoA&amp;start_radio=1&amp;rv=ZfOcYntQmlk&amp;t=6</p> <p></p> <p></p> <p></p> <p></p> </li> <li> <p>Halo hydrin formation</p> </li> </ul> </li> </ul> <p></p> <p></p> <pre><code>- **Practice Qs + Tutorial**\n\n    ![Untitled 57](https://github.com/Muqaram0/Muqaram0/assets/130496042/01ffd97b-4ec7-495a-8af0-7cfcdaaa55da)\n\n    ![Untitled 58](https://github.com/Muqaram0/Muqaram0/assets/130496042/5fc7016a-f081-4a6a-988f-279eb2c671c7)\n\n    ![Untitled 59](https://github.com/Muqaram0/Muqaram0/assets/130496042/09629a50-0ef8-4314-882d-bbaea54640a1)\n\n    Fickle Beri :- Cl is more e-ve\n\n    ![Untitled 60](https://github.com/Muqaram0/Muqaram0/assets/130496042/9c39f85e-f3c4-48af-bc6f-538698234290)\n\n    ![Untitled 61](https://github.com/Muqaram0/Muqaram0/assets/130496042/dd3ade01-17b2-4b2c-8315-528839d2555e)\n</code></pre> </li> <li> <p>Aromaticity</p> <ul> <li> <p>Conditions to check for aromaticity</p> <ul> <li>Acidic strength is proportional to stability</li> <li>https://www.youtube.com/watch?v=qDZIvjcM_Gk&amp;list=PLaySzQJTCO1mdBoL-BLBfKrhgdwRpbOnx helpful video on this</li> <li>It has to be cyclic ( self-explanatory)</li> <li> <p>Planar</p> <p>Has to have a 2-D structure and not a 3D structure</p> <p>sp3 - tetrahedral is 3D and wont be aromatic ,  sp2 - trigonal planar is aromatic because flat</p> <p></p> </li> <li> <p>Conjugated at the ring</p> <p></p> <p>Should be able to follow resonance , in the other two figures it cant follow resonance because the carbon atom is sp3 hybridized and cant accept more ( mainly depends on the availability of the p orbital )</p> <p></p> <p></p> <p>             - Should follow huckels rule</p> <p>4n + 2 = pi electrons where n should be a whole number , if n is not a whole number then it is not aromatic</p> <p></p> <p>\ud83d\udca1SHORTCUT :- if there are even no. of electron pairs then we will get n as a fraction therefore not aromatic or we can say it does not obey huckels rule , if odd then its aromatic </p> <p>eg:- </p> <p></p> <p>11 pairs :- which is odd and therefore follows huckels rule no need to do lengthy calc</p> <p></p> <p>IMPORTANT :- Here it follows huckels rule because we are counting the lone pair from -ve charge also as an e- pair</p> </li> <li> <p>There are 3 classes :- Aromatic , Anti Aromatic and non aromatic</p> <p>Aromatic and Anti aromatic they both look the same but one follows huckels rule and the other does not</p> <p>Non aromatic don't follow all 4 properties </p> <p></p> </li> <li> <p>Heterocyclic aromatic compounds are compounds where not all are carbon atoms</p> <p>They want to be aromatic which means it wants to resonate </p> <p>check if the atom is participating in resonance or not , if its participating with a pi bond you don't add the electrons in , if its not participating you use the lone pair</p> <p>(or) if the atom has a pi bond you dont use the lone pair , if it does not have a pi bond you use the lone pair , if it has 2 lone pairs you use only one</p> <p></p> <p>eg:- </p> <p></p> <p></p> <p>we are not using the lone pair , that is why we wont count it in the huckel rule and get it as aromatic and not anti-aromatic</p> </li> </ul> </li> <li> <p>Annulenes</p> <ul> <li> <p>Nomenclature</p> <p></p> <p>Annulenes must have even number of electrons</p> <p>branches into two , either anti aromatic annulenes or aromatic annulenes (refer to huckels rule)</p> </li> <li> <p>Some examples</p> <p></p> <p>due to repulsion or ang. strain</p> <p>happens with [8] annulene, [10] annulene , [14] annulene and [18] annulene (non aromatic)</p> </li> </ul> </li> <li> <p>Practice Qs</p> <p></p> <p>notice how there is -ve charge (lone pair) after treating it with strong base</p> <p></p> <p>h) is anti atomaticc not aromatic*</p> <p></p> <p>order is pyridine&gt;pyrolidene&gt;pyrole not pyridene&gt;pyrole&gt;pyrolidene</p> <p>depends on basicity which depends on the no. of lp</p> <p>if we look at pyridine and pyrole , if we dont consider the lp it is less basic then pyrole , but because of the presence of lp it is more basic as it has more donating power now</p> <p>similarly, if we look at pyrole and pyrolidene pyrolidine will be more basic than pyrole because pyrole has less donating power as its lone pair is being used </p> </li> </ul> </li> <li> <p>Stereochem</p> <p>Stereochemistry</p> <ul> <li> <p>Identification of chirality or achirality</p> <p>https://www.youtube.com/watch?v=KztTL3FTcOw helpful video</p> <p></p> <p>It needs to have 4 different functional groups attached to the carbon atom for it to be chiral</p> <p>2^n is the max no. of stereomers you can have , where n is the no. of chiral centers </p> <p>if we have 2 chiral centers then molecule may or may not be chiral depending on the symmetry</p> <p></p> <p>Chiral means active , Achiral means inactive </p> </li> <li> <p>Diff. Types of Isomers</p> <ul> <li> <p>Types of isomers</p> <ul> <li> <p>Isomerism :- Different compound but same molecular formula</p> <p></p> </li> <li> <p>Constitutional Isomers :- Atoms have diff connectivity</p> <p></p> <p>IMPORTANT :- Numbering diff = constitutional isomer </p> </li> <li> <p>Stereoisomers :- Atoms differ in arrangement of their atoms in space</p> <p></p> </li> <li> <p>Enantiomers :- Non superimposable mirror images</p> <p></p> </li> <li> <p>Diastereomers :- Non superimposable non mirror images</p> <p></p> </li> <li> <p>Cis-trans geometric isomers</p> <p></p> </li> </ul> </li> <li> <p>Some examples</p> </li> </ul> <p></p> <p></p> <p>Achiral can never be enantiomer instead its a meso compound ( have a plane of symmetry )</p> <p></p> <p>All chiral centers change in enantiomers</p> <p></p> <p></p> <p>They are the same  because of achirality</p> <p></p> <p>Constitutional isomers ^</p> <p>https://www.youtube.com/watch?v=KztTL3FTcOw helpful vid</p> <ul> <li>Chirality<ul> <li>Diff functional group on Carbon</li> <li>Absence of Planar ( Internal Symmetry \u2014&gt; condition for chirality )</li> </ul> </li> <li>Chirality Center<ul> <li>Asymmetric Carbon atom</li> <li>Stereocenter :- when the interchange of 2 groups gives stereoisomers ( Asymmetric carbons and cis-trans isomers )</li> <li>Achirality :- When the images can be superimposed and has a plane of symmetry</li> </ul> </li> </ul> </li> <li> <p>Cahn Ingold Prelog priority system + R and S configuration</p> <p>Fancy name for what reason </p> <p>https://www.youtube.com/watch?v=yzfcrwJ37kI helpful video</p> <p></p> <p>3 chiral centers therefore using 2^n there will be 8 possible stereomers</p> <p></p> <p>Chiral carbon will have 4 different functional groups , to assign it a R or S order we rank the Functional groups using the priority system</p> <p>Priority is based on atomic no. </p> <p>I&gt;Br&gt;Cl&gt;S&gt;F&gt;O&gt;N&gt;13C&gt;12C&gt;2H1&gt;1H1</p> <p>assign numbers to each atom and count from 1 to 4 , 4<sup>th</sup> group should be in the back </p> <p>if anti- clockwise we assign it S config and if clock wise we give it R config R:-rolling\u2014&gt; clockwise</p> <p>atomic no:- of carbon is 6</p> <p></p> <p></p> <p></p> <p>switch the configuration from R to S or S to R  if the last hydrogen atom is not in the Back (hatched wedge)</p> <p>Nomenclature</p> <p></p> <p>for double and triple bonds you open them up</p> <p></p> <p>here it was not shown if H is in the front or the back , so we do the double flip ( interchange oppo groups ) to get the position of H</p> <p>         - Fischer's Projection</p> <p></p> <p>groups on the horizontal edge are in the front and groups on vertical ends are in the back</p> </li> <li> <p>Optical Activity and Specific Rotation</p> <p>Enantiomers rotate the plane polarised light in oppo. direction, but same no. of degrees </p> <p>Clockwise :- Dextro rotatory (+)</p> <p>Anti-Clockwise :- Levo rotatory (-) levo-left</p> <p>S makes light go right , R makes light go left</p> <ul> <li> <p>Specific rotation</p> <p>alpha = observed rotation</p> <p>l = path length</p> <p>C = concentration</p> <p>)</p> <p>Q. When one of the enantiomers of 2-butanol is placed in a polarimeter, the observed rotation is 4.05 degrees anti-clockwise . The soln. was made by diluting 6g of 2-butanol to a total of 40ml , and the solution was placed into 200mm polarimeter tube for measurement . Determine the specific rotation for this enantiomer of 2-Butanol.</p> <p></p> </li> </ul> </li> <li> <p>Racemic mixtures</p> <p></p> <p>will not rotate polarized light because equal amounts of left polarizing and right polarizing light is there in the mixture </p> </li> <li> <p>E/Z system</p> <p>https://www.youtube.com/watch?v=frtnEDTSzi8 helpful video</p> <p></p> <p>when highest priority group is on same side we get Z isomer  (Zame) (Zusammen)</p> <p>when highest priority group is on oppo. sides we get E isomer (OppositE) (Entgegen)</p> <p></p> </li> <li> <p>Practice Qs + Tutorial</p> <p></p> </li> </ul> </li> <li> <p>Conformational Analysis</p> <ul> <li> <p>Basic</p> <p>Conformations:- Different Spatial arrangements that a molecule can adapt due to rotation about the internal bonds </p> <p>Conformers:- Structurers that differ based on rotation</p> <p>Conformational analysis :- Study of energy changes that occur during rotations</p> <p></p> </li> <li> <p>Newmanns Projection</p> <p></p> <p></p> </li> <li> <p>Energy</p> <p>\u2022 \u00a0Torsional strain is an increase in energy caused by electron-electron repulsions between the eclipsing C-H bonds</p> <p>Why is staggered more stable than eclipse?</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>The chair conformation is very stable because it eliminates angle strain\u00a0 and torsional strain (all hydrogens on adjacent C atoms are staggered)</p> <p></p> <p></p> <p></p> <p>Axial is less stable than Equatorial pos because it minimizes steric repulsion</p> <p></p> <p>Axial becomes Equatorial and vice versa numbering changes  </p> <p>up stays up and down stays down they dont change</p> <p> if we go from form A to B the numbering shifts to the next carbon clockwise</p> <p>if vice versa then numbering shifts back and anti clockwise</p> <p>The bigger arrow shows that the equilibrium favors the more stable product</p> </li> <li> <p>Problems</p> <p></p> <p></p> <p></p> <p>bulky group on equatorial thats why more stable than bulky group on axial</p> <p></p> <p></p> </li> <li> <p>Tutorial</p> <p>https://apps.dso.iastate.edu/si/documentdb/spring_2016/CHEM_331_Kraus_ihazlett_331_Worksheet_Week_4_Key_2_.pdf very useful worksheet</p> <p>https://www.garybreton.com/CHM_223HF/ewExternalFiles/4_6 KEY.pdf Very useful worksheet</p> <p></p> <p></p> <p></p> <p></p> <p></p> </li> </ul> </li> </ul> </li> </ul>"},{"location":"1_Core/General_Chemistry/03_Physical/","title":"03 Physical","text":"<ul> <li> <p>Physical Chem</p> <ul> <li> <p>Quantum theory</p> <ul> <li>Energy can be transferred between systems in discrete amts only</li> <li>Radiation (light) has a particle character</li> <li> <p>Electron has a wave character</p> <ul> <li> <p>Planks Law</p> <p>The internal modes of atoms and molecules can posses only certain energies </p> <p>the modes are quantized</p> <p>E=nhv</p> </li> <li> <p>Light as a particle</p> <p>The photoelectric effect</p> <ul> <li>No electrons are ejected regardless of the intensity of the radiation unless its frequency exceeds a threshold value characteristic of the metal</li> <li>The kinetic energy of the ejected electron increases linearly with the frequency of the incident radiation but is independent of the intensity of radiation</li> <li>Even at low light intensities, electrons are ejected immediately if the frequency is above the threshold</li> <li> <p>According to conversation of energy,</p> <p></p> <p>where \u00bdmeV^2 is the kinetic energy of the ejected electron </p> <p>hv is the energy of the photon </p> <p>theta is the work function ( characteristic of the metal ) - energy required to remove an electron from the metal \u201cto infinity and beyond\u201d</p> <p>hv no photoejection where h is 6.626 x 10^-34 <li> <p>Electron as a wave</p> <p>Davisson-Germer experiment </p> <ul> <li>Diffraction of electrons by a single crystal of Ni</li> <li>Diffraction is the interference caused by an object in the path of waves</li> <li>Depending on whether the interference is constructive or destructive , the result is a region of enhanced or diminished intensity  of the wave</li> <li>particles are wave-like             - Wave-Particle Duality</li> </ul> <p>Any particle travelling with a linear momentum p=mv should have a wavelength lambda.</p> <p></p> <p>particle with high linear momentum has short wavelength </p> </li> <li> <p>Matter wave and de Broglie's relation</p> <p>Matter wave is expressed as </p> <p></p> <p>The de-broglie relation implies that the wavelength of a matter wave should decrease as the particles speed increases</p> <p>Large particles only manifest their particle nature and never their wave nature because their momenta will be so high due big mass making their wavelengths very small </p> </li> <li> <p>Dynamics of microscopic systems</p> <p>According to classical mechanics a particle has specific trajectory</p> <p>position and momenta are specified at each instant</p> <p>According to quantum mechanics a particle cannot have specific trajectory</p> <p>The wavefunction determines the probability distribution , darker the area higher the probability of finding the particle </p> <p></p> </li> <li> <p>Schrodinger's Equation</p> <p>The schrodingers equation for a single particle of mass m moving with energy E in one dimension is </p> <p></p> </li> <li> <p>Physical Significance of wave function</p> <p>The born interpretation</p> <p></p> <p></p> <p></p> <p></p> <p></p> </li> <li> <p>Uncertainty principle</p> <p>It is impossible to specify simultaneously , with arbitrary precision, both the momentum and the position of a particle</p> <p></p> </li> <li> <p>Application of quantum physics</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> </li> <li> <p>Atomic Structure</p> <ul> <li> <p>Hydrogenic atoms</p> <p>A hydrogenic atom is a one-electron atom </p> <p>Schrodinger equation can be solved for them and their structures can be discussed exactly</p> <p>Spectrum of atomic hydrogen </p> <p></p> <p></p> <p></p> <p>Boundary condition</p> <ul> <li>The wavefunction must not become infinite anywhere</li> <li>it must repeat itself ( like the particle on the surface of a sphere )</li> </ul> <p></p> <p></p> <p></p> <p></p> <p></p> </li> <li> <p>Quantum numbers</p> <p></p> </li> <li> <p>Spectral transition and rules</p> <p></p> <p></p> </li> </ul> </li> <li> <p>Tutorial Quantum + Atomic + Practice qs</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>     - Chemical bonding</p> <p></p> <p></p> <p></p> <p></p> <p>Molecular orbital theory</p> <p></p> <p></p> <p></p> <p>show 1s2 also</p> <p></p> <p></p> <p></p> <p></p> </li> <li> <p>Molecular spectroscopy</p> <ul> <li> <p>Vibrational spectroscopy</p> <p></p> <p></p> <p></p> <p></p> </li> <li> <p>Electronic transitions</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>sigma to anti sigma in saturated</p> <p>n to anti sigma in saturated with lone pairs</p> <p>pi to anti pi in unsaturated</p> <p>n to anti pi in unsaturated with lone pairs</p> <p></p> <p></p> <p></p> </li> </ul> </li> <li> <p>Chemical kinetics</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> </li> <li> <p>Tutorial Chemical bonding + spectro + kinetics</p> <p></p> <p></p> <p></p> <p></p> <p>Correction :- B2H6 is IR active because of selection rule</p> <p>In quantum mechanics, a selection rule is a criterion that determines whether a particular physical process or transition is allowed or forbidden. It specifies the conditions under which a transition can occur between two energy states of a system. In the context of spectroscopy, selection rules determine which types of transitions are observable in a particular spectroscopic technique, such as infrared (IR) spectroscopy.</p> <p>IR spectroscopy is based on the absorption or emission of infrared radiation by molecules. It involves transitions between different vibrational energy levels of the molecules. The selection rule for IR activity is known as the electric dipole selection rule. According to this rule, for a molecule to exhibit an infrared absorption, the vibrational mode must result in a change in the molecular dipole moment.</p> <p>Now, let's consider the molecule B2H6, which is called diborane. Diborane consists of two boron atoms (B) and six hydrogen atoms (H). In this molecule, the B-B bond is symmetrical, and the H atoms are arranged symmetrically around the boron atoms. Due to this symmetry, some vibrational modes of B2H6 do not result in a net change in the dipole moment of the molecule and, therefore, do not produce infrared absorption. These modes are said to be \"IR inactive.\"</p> <p>However, there are vibrational modes in B2H6 that do result in a change in the dipole moment and satisfy the electric dipole selection rule. For example, the bending vibrations of the BH3 groups and the stretching vibrations of the B-H bonds involve changes in dipole moment and are therefore IR active. These modes can be observed in the infrared spectrum of B2H6.</p> <p>In summary, the selection rule for IR activity requires a change in the molecular dipole moment for a vibrational mode to be observable in the infrared spectrum. B2H6 exhibits IR activity for certain vibrational modes that involve changes in the dipole moment, such as the bending and stretching vibrations</p> <p></p> <p>Correction :- Beneze - pi to anti pi and sigma to anti sigma</p> <p>Acetone:- n to anti pi , pi to anti pi , n to anti sigma , sigma to anti sigma</p> <p></p> <p></p> <p></p> <p></p> <p></p> </li> <li> <p>Thermodynamics</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> </li>"},{"location":"1_Core/Math_1/","title":"Math 1","text":"<p>This course focuses on the essential role of vector calculus in science and engineering, highlighting its application in modeling dynamic problems through differential and integral equations. Students will explore functions of several variables, which are increasingly relevant in scientific contexts compared to single-variable functions, as well as the complex interactions of these variables.</p> <p>The course aims to deepen understanding of the derivatives and integrals of multivariable functions, with applications spanning probability, fluid dynamics, electrical sciences, and more. </p>"},{"location":"1_Core/Math_1/01_Polar_Coordinates_and_Conic_Sections/","title":"01 Polar Coordinates and Conic Sections","text":""},{"location":"1_Core/Math_1/01_Polar_Coordinates_and_Conic_Sections/#polar-coordinates","title":"Polar Coordinates","text":"<p>In polar coordinate system, we locate a point with reference to:</p> <ol> <li>pole a fixed point    (usually fixed at the origin)</li> <li>initial ray a fixed line, passing through the pole    (usually \\(+x\\) axis)</li> </ol> <p>Let </p> <ul> <li>\\(r\\) - directed distance of the point from pole<ul> <li>\\(r &gt; 0\\) forward</li> <li>\\(r &lt; 0\\) backward</li> </ul> </li> <li>\\(\\theta\\) - directed angle of radius vector from the initial ray<ul> <li>\\(\\theta &lt; 0\\) anti-clockwise</li> <li>\\(\\theta &gt; 0\\) clockwise</li> </ul> </li> <li>\\(P(r, \\theta)\\) - corresponding point</li> </ul> <p></p>"},{"location":"1_Core/Math_1/01_Polar_Coordinates_and_Conic_Sections/#circle-through-pole","title":"Circle Through Pole","text":"\\[ r = \\pm a, \\quad 0 \\le \\theta \\le 2 \\pi \\] <p>represents a circle with center @pole and radius \\(a\\). Sign can be either, because it is the same circle traversed in the opposite direction</p>"},{"location":"1_Core/Math_1/01_Polar_Coordinates_and_Conic_Sections/#straight-line-through-pole","title":"Straight line through pole","text":"\\[ \\theta = \\theta_0, \\quad - \\infty &lt; r &lt; \\infty \\]"},{"location":"1_Core/Math_1/01_Polar_Coordinates_and_Conic_Sections/#idk","title":"IDK","text":"\\(r\\) \\(\\theta\\) Diagram const const point const inequality arc inequality const straight line segment inequality inequality region"},{"location":"1_Core/Math_1/01_Polar_Coordinates_and_Conic_Sections/#cartesian-iff-polar","title":"Cartesian \\(\\iff\\) Polar","text":"<p>Consider the point \\(P(x, y) \\iff P(r, \\theta)\\)</p> \\[ \\begin{aligned} x &amp;= r \\cos\\theta \\\\ y &amp;= r \\sin\\theta \\\\ r^2 &amp;= x^2 + y^2 \\\\ \\theta &amp;= \\tan^{-1} \\left( \\frac y x \\right) \\end{aligned} \\]"},{"location":"1_Core/Math_1/01_Polar_Coordinates_and_Conic_Sections/#symmetry","title":"Symmetry","text":"<p>Let \\(r = f(\\theta)\\) be a polar curve</p>"},{"location":"1_Core/Math_1/01_Polar_Coordinates_and_Conic_Sections/#x-axis","title":"X-axis","text":"<p>\\(P(r, \\theta)\\) and \\(P'(r, - \\theta)\\) lie on same graph</p> Symmetry about Vary theta \\(P(r, \\theta)\\) lies on the same graph as or \\(P(r, \\theta)\\) lies on the same graph as X-axis \\(0 \\le \\theta \\le \\pi\\) \\(P'(r, -\\theta)\\) \\(P'(-r, \\pi -\\theta)\\) Y-axis \\(\\frac{-\\pi} 2 \\le \\theta \\le \\frac \\pi 2\\) \\(P'(-r, -\\theta)\\) \\(P'(r, \\pi -\\theta)\\) Origin \\(0 \\le \\theta \\le \\frac \\pi 2\\) \\(P'(-r, \\theta)\\) \\(P'(r, \\pi + \\theta)\\)"},{"location":"1_Core/Math_1/01_Polar_Coordinates_and_Conic_Sections/#shapes","title":"Shapes","text":""},{"location":"1_Core/Math_1/01_Polar_Coordinates_and_Conic_Sections/#limacon","title":"Limacon","text":"\\[ r = a \\pm b \\cos\\theta \\\\ \\text{ or } \\\\ r = a \\pm b \\sin\\theta \\] \\(\\frac a b\\) Type \\(&lt;1\\) inner loop \\(=1\\) cardioid \\(&gt;1\\) outer loop"},{"location":"1_Core/Math_1/01_Polar_Coordinates_and_Conic_Sections/#roses","title":"Roses","text":"\\[ \\begin{aligned} r &amp;= a \\cos(n\\theta) \\\\ &amp;\\text{ or } \\\\ r &amp;= a \\sin(n\\theta) \\\\ \\text{No of petals } N &amp;= \\begin{cases} n, &amp;  n = \\text{odd} \\\\ 2n, &amp; n = \\text{even} \\end{cases} \\\\ \\text{Axis of first petal } \\theta &amp;=  \\begin{cases} 0 &amp;  r = a \\textcolor{orange}{\\cos}(n \\theta) \\\\ \\dfrac \\pi {2n} &amp; r = a \\textcolor{orange}{\\sin} (n \\theta) \\end{cases} \\\\ \\text{Length of petals} &amp;= a \\\\ \\text{Angular Gap between axes of petals} &amp;= \\frac{2 \\pi}{N} \\end{aligned} \\]"},{"location":"1_Core/Math_1/01_Polar_Coordinates_and_Conic_Sections/#lemmiscates","title":"Lemmiscates","text":"\\[ r^2 = a \\cos\\theta \\\\ \\text{ or } \\\\ r^2 = a \\sin\\theta \\\\ \\]"},{"location":"1_Core/Math_1/01_Polar_Coordinates_and_Conic_Sections/#straight-line","title":"Straight Line","text":"\\[ r \\cos(\\theta-\\theta_0) = r_0 \\] <ul> <li>\\(P(r, \\theta)\\) is any point on given line</li> <li>\\(P_0(r_0, \\theta_0)\\) is foot of \\(\\perp\\)r from the pole</li> </ul>"},{"location":"1_Core/Math_1/01_Polar_Coordinates_and_Conic_Sections/#circle","title":"Circle","text":"\\[ r^2 + {r_0}^2 - 2 r r_0 \\cos(\\theta - \\theta_0) = a^2 \\\\ \\] <ul> <li>\\(P(r, \\theta)\\) is any point on circle</li> <li>\\(P_0(r_0, \\theta_0)\\) is center of circle</li> <li>\\(a\\) is radius</li> </ul>"},{"location":"1_Core/Math_1/01_Polar_Coordinates_and_Conic_Sections/#radius-passing-through-pole","title":"Radius passing through pole","text":"\\[ r_0 = a\\\\ r = 2a cos(\\theta - \\theta_0) \\]"},{"location":"1_Core/Math_1/01_Polar_Coordinates_and_Conic_Sections/#center-lies-on-axis","title":"Center lies on axis","text":"Center at \\(r\\) \\((a,0)\\) \\(2a \\cos \\theta\\) \\((-a,0)\\) \\(-2a \\cos \\theta\\) \\((a, \\frac \\pi 2)\\) \\(2a \\sin \\theta\\) \\((a, -\\frac \\pi 2)\\) \\(-2a \\sin \\theta\\)"},{"location":"1_Core/Math_1/01_Polar_Coordinates_and_Conic_Sections/#area-under-curve","title":"Area under curve","text":"<p>For a polar curve \\(r = f(\\theta), \\alpha \\le \\theta \\le \\beta\\)</p> \\[ A = \\frac12 \\int\\limits_{\\theta = \\alpha}^\\beta r^2 \\cdot d\\theta \\] <p>For area bounded by the curves \\(r_1 = f_1(\\theta), r_2 = f_2(\\theta), \\alpha \\le \\theta \\le \\beta\\) such that \\(r_1 &lt; r_2\\)</p> \\[ A = \\frac12 \\int\\limits_{\\theta = \\alpha}^\\beta {r_2}^2 - {r_1}^2 \\cdot d\\theta \\]"},{"location":"1_Core/Math_1/01_Polar_Coordinates_and_Conic_Sections/#length-of-curve","title":"Length of curve","text":"<p>For a curve \\(r = f(\\theta), \\alpha \\le \\theta \\le \\beta\\) traversed exactly once from \\(\\theta = \\alpha \\to \\beta\\)</p> \\[ L = \\int\\limits_{\\theta = \\alpha}^\\beta \\sqrt{ r^2 + (r')^2 } \\cdot d\\theta \\qquad \\left[ r' = \\frac{dr}{d \\theta} \\right] \\]"},{"location":"1_Core/Math_1/01_Polar_Coordinates_and_Conic_Sections/#conic-sections","title":"Conic Sections","text":"<p>Let</p> <ul> <li>\\(P(r, \\theta)\\) be any point on the conic section with focus at origin</li> <li>\\(e = \\dfrac{ \\text{Distance bw focii} }{ \\text{Distance bw vertices} }\\)</li> </ul> Directrix \\(r\\) \\(x = a\\) \\(\\frac{ke}{1 + e \\cos\\theta}\\) \\(x = -a\\) \\(\\frac{ke}{1 - e \\cos\\theta}\\) \\(y = a\\) \\(\\frac{ke}{1 + e \\sin\\theta}\\) \\(y = -a\\) \\(\\frac{ke}{1 - e \\sin\\theta}\\)"},{"location":"1_Core/Math_1/01_Polar_Coordinates_and_Conic_Sections/#shapes_1","title":"Shapes","text":"\\(e\\) Shape \\(0 &lt; e &lt; 1\\) Ellipse \\(e = 1\\) Parabola \\(e &gt; 1\\) Hyperbola <p>For ellipse,</p> \\[ k = a \\left[ \\frac 1 e - e \\right] \\]"},{"location":"1_Core/Math_1/02_Limits_and_Continuity/","title":"02 Limits and Continuity","text":""},{"location":"1_Core/Math_1/02_Limits_and_Continuity/#limits","title":"Limits","text":"<p>Let \\(f\\) be defined @ all points in some neighborhood of a point \\(x_0\\)</p> <p>Then \\(L = \\lim\\limits_{x \\to x_0} f(x)\\) is limit for \\(f(x)\\) when \\(x \\to x_0\\) if for a given \\(\\epsilon &gt; 0\\), there exists a \\(\\delta &gt; 0\\) such that \\(|x-x_0| &lt; \\delta \\implies |f(x)-L| &lt; \\epsilon\\)</p>"},{"location":"1_Core/Math_1/02_Limits_and_Continuity/#finding-delta","title":"Finding \\(\\delta\\)","text":"<ol> <li>Solve the inequality \\(f(x) - L &lt; \\epsilon\\) for \\(x\\)</li> <li>Find an interval \\((a, b)\\) such that \\(a \\le x_0 \\le b\\)</li> <li>Choose \\(\\delta = \\min (x_0-a, b - x_0)\\)</li> </ol> <p>This choice places the interval \\((x_0 - \\delta, x_0 + \\delta)\\) within \\((a, b)\\)</p>"},{"location":"1_Core/Math_1/02_Limits_and_Continuity/#one-sided-limits","title":"One-sided Limits","text":"<p>Let \\(f\\) be defined at all points in the neigborhood of \\(x_0\\) (in particular to right of \\(x_0\\)), then \\(f\\) is said to have the right-hand limit \\(L\\), when \\(x\\) approaches \\(x_0\\) from the right if the following conditions are satisfied:</p> <p>For a given \\(\\epsilon &gt; 0\\), there exists a \\(\\delta &gt; 0\\) such that</p> <ul> <li>\\(x_0 &lt; x &lt; x_0 + \\delta\\)</li> <li>\\(|f(x) - L| &lt; \\epsilon\\)</li> </ul> <p>The limit is represented as</p> \\[ L = \\lim_{x \\to {x_0}^+} f(x) = f({x_0}^+) \\] <p>Similarly, we define the left-hand limit</p> <p>While working on one-sided problms, we proceed as follows</p> \\[ \\begin{aligned} f({x_0}^+) &amp;= \\lim_{h \\to 0} f(x_0 + h), &amp; h &gt; 0 \\\\ f({x_0}^-) &amp;= \\lim_{h \\to 0} f(x_0 - h), &amp; h &gt; 0 \\end{aligned} \\]"},{"location":"1_Core/Math_1/02_Limits_and_Continuity/#continuity","title":"Continuity","text":"<p>A function \\(f(x)\\) is continuous @ a point \\(x_0\\) if the following conditions are satisfied</p> <ol> <li>\\(f(x_0)\\) exists</li> <li>\\(\\lim_{x \\to x_0} f(x)\\) (Both LHL and RHL) exists</li> <li>\\(\\lim_{x \\to x_0} f(x) = f(x_0)\\)</li> </ol>"},{"location":"1_Core/Math_1/02_Limits_and_Continuity/#note","title":"Note","text":"<p>If \\(f\\) and \\(g\\) are continuous functions in a domain \\(D\\), then the following functions are also continuous in all points of F</p> \\[ \\begin{aligned} f \\pm g \\\\ fg \\\\ \\frac f g \\\\ kf, &amp; (k \\text{=  const}) \\end{aligned} \\] <p>The following functions are known to be continuous in their domain of definition</p> <ol> <li>polynomial</li> <li>exponential</li> <li>trignometric</li> </ol>"},{"location":"1_Core/Math_1/03_Vector_Calculus/","title":"03 Vector Calculus","text":""},{"location":"1_Core/Math_1/03_Vector_Calculus/#vector-valued-functions","title":"Vector Valued Functions","text":"<p>The motion of a particle moving space is given by</p> \\[ \\vec r = x(t) \\cdot \\hat i + y(t) \\cdot \\hat j + z(t) \\cdot \\hat k, \\\\ a \\le t \\le b, \\quad a, b \\in R \\]"},{"location":"1_Core/Math_1/03_Vector_Calculus/#limits","title":"Limits","text":"<p>\\(\\vec r(t)\\) has a limit \\(\\vec L\\) as \\(t\\) approaches \\(t_0\\) if the following is satisfied For every \\(\\epsilon &gt; 0\\), there exists a \\(\\delta &gt; 0\\) such that \\(0&lt;|t - t_0|&lt; \\delta \\implies | \\vec r(t) - \\vec L | &lt; \\epsilon\\)</p> <p>The limit is denoted as</p> \\[ \\lim_{t \\to t_0} \\vec r(t) = \\vec L \\]"},{"location":"1_Core/Math_1/03_Vector_Calculus/#continuity","title":"Continuity","text":"<p>\\(r(t)\\) is continuous @ \\(t = t_0\\) if</p> <ol> <li>\\(\\vec r(t_0)\\) exists</li> <li>\\(\\lim_{t \\to t_0} \\vec r(t)\\) exists</li> <li>\\(\\lim_{t \\to t_0} \\vec r(t) = \\vec r(t_0)\\)</li> </ol>"},{"location":"1_Core/Math_1/03_Vector_Calculus/#derivative","title":"Derivative","text":"\\[ \\frac{dr}{dt} = \\lim_{\\Delta t \\to 0} \\frac{     \\vec r(t + \\Delta t) - \\vec r(t) }{\\Delta t} \\] Quantity Velocity \\(\\frac{d \\vec r}{d t}\\) Acceleration \\(\\frac{d \\vec V}{d t}\\) \\(\\frac{d^2 \\vec r}{d t^2}\\) Speed \\(\\vert\\vec V\\vert\\) Direction \\(\\frac{\\vec V}{\\vert\\vec V\\vert}\\)"},{"location":"1_Core/Math_1/03_Vector_Calculus/#note","title":"Note","text":"<p>Velocity = Speed \\(\\times\\) Direction</p> <p>The path of a particle is said to be smooth if</p> <ol> <li>\\(\\frac{d \\vec r}{d t} \\ne 0\\)</li> <li>\\(\\frac{d \\vec r}{d t}\\) is continuous</li> </ol> <p>If \\(\\vec u\\) is a vector of constant length, then \\(\\vec u \\cdot \\frac{d \\vec u}{d t} = 0\\) (circle, perpendicular, cos 90 = 0)</p> <p>The path of a particle is gievn by eliminating the parameter \\(t\\) from \\(x, y, z\\) eg: The path of a particle having \\(\\vec r(t) = \\cos t \\cdot \\hat i + \\sin t \\hat j, \\quad t \\in I\\)</p> \\[ \\begin{aligned} x^2 + y^2 &amp;= \\cos^2 t + \\sin^2 t \\\\ &amp;= 1 \\end{aligned} \\] <p>Therefore, this path is a circle with radius = 1</p>"},{"location":"1_Core/Math_1/03_Vector_Calculus/#angle-between-vectors","title":"Angle Between Vectors","text":"\\[ \\begin{aligned} \\cos \\theta &amp;= \\frac{     \\vec a \\cdot \\vec b }{     |\\vec a| |\\vec b| } \\\\ \\theta &amp;= \\cos^{-1} \\left( \\frac{     \\vec a \\cdot \\vec b }{     |\\vec a| |\\vec b| } \\right) \\end{aligned} \\]"},{"location":"1_Core/Math_1/03_Vector_Calculus/#arc-length","title":"Arc Length","text":"<p>If</p> <ul> <li>\\(\\vec r(t)\\) is a smooth curve, traversed exactly once from \\(t=a \\to b\\)</li> <li>\\(\\vec V\\) is the velocity vector</li> </ul> \\[ L = \\int\\limits_a^b  |\\vec V(t)| \\cdot dt \\] <p>Length is basically the integral of speed</p>"},{"location":"1_Core/Math_1/03_Vector_Calculus/#arc-length-parameter","title":"Arc Length Parameter","text":"<p>If \\(\\vec r(t) \\quad t \\ge t_0\\) is a smooth curve, then arc length parameter wrt base point @ \\(t=0\\) is</p> \\[ L = \\int\\limits_{\\tau = t_0}^t  |\\vec V(\\tau)| \\cdot d \\tau \\]"},{"location":"1_Core/Math_1/03_Vector_Calculus/#special-vectors","title":"Special Vectors","text":"Vector Symbol Unit Tangent Vector \\(\\hat T\\) \\(\\frac{ \\frac{d \\vec r}{dt} }{\\vert\\frac{d \\vec r}{dt}\\vert}\\) \\(\\frac{\\vec V}{\\vert \\vec V \\vert}\\) Principle Unit Normal Vector \\(\\hat N\\) \\(\\frac{ \\frac{d \\vec T}{dt} }{\\vert\\frac{d \\vec T}{dt}\\vert}\\) Curvature Rate of change in direction of curve, wrt arc length \\(k\\) \\(\\frac{d \\vec T}{d s}\\) \\(\\frac{1}{\\vert \\vec V \\vert} \\cdot \\vert\\frac{d \\hat T} {dt}\\vert\\) Radius of Curvature \\(\\rho\\) \\(\\frac{1}{k}\\) <p>Curvature @ any point on a</p> <ul> <li>straight line is 0</li> <li>smaller circle will be greater than that of a larger one</li> </ul>"},{"location":"1_Core/Math_1/03_Vector_Calculus/#components-of-vector","title":"Components of Vector","text":"<p>If \\(\\vec a = a_t \\cdot \\hat T + a_N \\cdot \\hat N\\), then</p> Component Symbol Tangential \\(a_T\\) \\(\\frac{d \\vertV \\vert}{dt}\\) Normal \\(a_N\\) \\(k \\vertV\\vert^2\\) \\(\\sqrt{\\vert\\vec a\\vert^2 - {a_T}^2}\\)"},{"location":"1_Core/Math_1/03_Vector_Calculus/#note_1","title":"Note","text":"<p>If speed is contant</p> <ul> <li>\\(a_T = 0\\)</li> <li>all acceleration wil be in direction of \\(\\hat N\\)</li> </ul> <p>\\(a_T\\) only exists when objects speed up / slow down</p>"},{"location":"1_Core/Math_1/04_Partial_Derivatives/","title":"04 Partial Derivatives","text":""},{"location":"1_Core/Math_1/04_Partial_Derivatives/#functions-of-several-variables","title":"Functions of Several Variables","text":"<p>Let \\(D\\) be the set of all \\(n\\) tuples of the form \\((x_1, x_2, \\dots , x_n)\\), where \\(x_1, x_2, \\dots, x_n\\) are real numbers. A function on \\(D\\) is a rule \\(f\\) that assigns a number \\(w = f(x_1, x_2, \\dots, x_n)\\) for each element in \\(D\\).</p> <p>If there exists only number \\(w\\) for each element in \\(D\\), then it is said to be a single-valued function. If more than one \\(w\\) exists, then it is said to be a many-valued function.</p> <p>While finding domain \\(D\\)</p> <ul> <li>we include all the points which make the function \\(f\\) well-defined</li> <li>neglect the values which make \\(w\\) a complex or undefined number</li> </ul>"},{"location":"1_Core/Math_1/04_Partial_Derivatives/#neighborhood","title":"Neighborhood","text":"<p>A neighborhood of a point \\(P_0(x_0, y_0)\\) is a circular disc, with centre @ \\(P_0\\) and radius \\(r\\), where \\(r\\) is a small +ve number.</p> <p>If</p> <ul> <li>\\(r= \\epsilon\\), \\(\\epsilon\\) neighborhood</li> <li>\\(r = \\delta\\), \\(\\delta\\) neighborhood</li> </ul> <p>In 3 dimensions, we replace circular disk with an open spherical ball with centre @ \\(P_0\\)</p>"},{"location":"1_Core/Math_1/04_Partial_Derivatives/#types-of-points","title":"Types of points","text":"<p>Let \\(S\\) be a non-empty set in the XY plane . A point \\(P_0(x_0, y_0)\\) is said to be</p> Point Condition Interior there exists a neighborhood of \\(P_0\\) which lies completely inside \\(S\\) Boundary every neighborhood of \\(P_0\\) contains points of \\(S\\) and points outside \\(S\\) Exterior there exists a neighborhood of \\(P_0\\) completely outside \\(S\\)"},{"location":"1_Core/Math_1/04_Partial_Derivatives/#types-of-sets","title":"Types of Sets","text":"Characteristic Open contains interior points only Closed contains interior and all boundary points Bounded lies completely inside an open disk of finite radius Unbounded cannot be enclosed inside open disk of finite radius <p>\\(XY\\) plane is both open and closed.</p>"},{"location":"1_Core/Math_1/04_Partial_Derivatives/#level","title":"Level","text":"<p>For a function \\(f(x, y)\\) and constant \\(c\\),</p> Equation Level Curve \\(f(x, y) = c\\) Level Surface \\(f(x, y, z) = c\\)"},{"location":"1_Core/Math_1/04_Partial_Derivatives/#limits","title":"Limits","text":"<p>Let \\(f\\) be a function defined at all points in the some neighborhood f \\((x_0, y_0)\\). We say that \\(f\\) has a limit \\(L\\), when the point \\((x, y)\\) approaches \\((x_0, y_0)\\) if for every \\(\\epsilon &gt; 0\\), there exists a \\(\\delta &gt; 0\\) such that</p> \\[ \\begin{aligned} 0 &lt; \\text{ D b/w } (x, y) \\text{ and } (x_0, y_0) &amp;&lt; \\delta \\\\ 0 &lt; \\sqrt{ (x-x_0)^2 + (y-y_0)^2 } &amp;&lt; \\delta \\\\ | f(x,y) - L | &amp;&lt; \\epsilon \\\\ \\implies L &amp;= \\lim_{(x, y) \\to (x_0, y_0)} f(x, y) \\end{aligned} \\] <p>Here, \\((x, y)\\) approaches \\((x_0, y_0)\\) in an infinite number of ways.</p>"},{"location":"1_Core/Math_1/04_Partial_Derivatives/#2-path-test","title":"2 Path Test","text":"<p>TO show that the limit of \\(f(x, y)\\) does not exist @ \\((x_0, y_0)\\), we find 2 different paths through which the value of limits are different.</p> <p>We choose the path as \\(y = mx^n\\) or \\(x = m y^n\\), where \\(m\\) and \\(n\\) are constants. The choice depends on the problem. We try to obtain a final limit in terms of \\(m\\)</p>"},{"location":"1_Core/Math_1/04_Partial_Derivatives/#continuity","title":"Continuity","text":"<p>A function \\(f(x, y)\\) is continuous at \\((x_0, y_0)\\) if</p> <ol> <li>\\(f(x_0, y_0)\\) exists</li> <li>\\(\\lim_{(x, y) \\to (x_0, y_0)} f(x,y)\\) exists</li> <li>\\(\\lim_{(x, y) \\to (x_0, y_0)} f(x,y) = f(x_0, y_0)\\)</li> </ol> <p>The following functions are continuous in their domain of definition</p> <ol> <li>Polynomial</li> <li>Exponential</li> <li>Circular</li> <li>Trignometric</li> </ol>"},{"location":"1_Core/Math_1/04_Partial_Derivatives/#partial-derivatives","title":"Partial Derivatives","text":"<p>Let \\(f(x,y)\\) be a function of 2 variables.</p> <p>Provided the limit exists, the partial derivative of \\(f\\) wrt \\(x\\) is denoted and defined by</p> \\[ \\begin{aligned} \\frac{\\partial f}{\\partial x} &amp;= \\lim_{\\Delta x \\to 0} \\frac{ f(x + \\Delta x, \\ y) - f(x, y) }{\\Delta x} \\\\ \\frac{\\partial f}{\\partial y} &amp;= \\lim_{\\Delta y \\to 0} \\frac{ f(x, \\ y + \\Delta y) - f(x, y) }{\\Delta y} \\end{aligned} \\] <p>We define higher order partial derivatives as</p> \\[ \\begin{aligned} f_x &amp;= \\frac{\\partial^2 f}{\\partial x^2} &amp;= \\frac{\\partial}{\\partial x}\\left[ \\frac{\\partial f}{\\partial x} \\right] \\\\ f_{xy} &amp;=\\frac{\\partial^2 f}{\\partial x \\partial y} &amp;= \\frac{\\partial}{\\partial x}\\left[ \\frac{\\partial f}{\\partial y} \\right] \\\\ f_{xy} &amp;= f_{yx} \\\\ f_{xx} &amp;= (f_x)_x \\end{aligned} \\]"},{"location":"1_Core/Math_1/04_Partial_Derivatives/#laplace-equation","title":"Laplace Equation","text":"<p>If \\(u\\) is a function</p> \\[ u_{xx} + u_{yy} + u_{zz} = 0 \\]"},{"location":"1_Core/Math_1/04_Partial_Derivatives/#chain-rule","title":"Chain Rule","text":"<p>If \\(w = f(x, y)\\) a function where \\(x, y\\) are themselves functions of</p> <ul> <li>an independent parameter \\(t\\)</li> </ul> \\[ \\frac{dw}{dt} = \\left( \\frac{\\partial w}{\\partial x} \\cdot \\frac{dx}{dt} \\right) + \\left( \\frac{\\partial w}{\\partial y} \\cdot \\frac{dy}{dt}  \\right) \\] <ul> <li>2 independent parameters \\(u, v\\)</li> </ul> \\[ \\begin{aligned} \\frac{\\partial w}{\\partial u} &amp;= \\left( \\frac{\\partial w}{\\partial x} \\cdot \\frac{\\partial x}{\\partial u} \\right) + \\left( \\frac{\\partial w}{\\partial y} \\cdot \\frac{\\partial y}{\\partial u} \\right) \\\\ \\frac{\\partial w}{\\partial v} &amp;= \\left( \\frac{\\partial w}{\\partial x} \\cdot \\frac{\\partial x}{\\partial v} \\right) + \\left( \\frac{\\partial w}{\\partial y} \\cdot \\frac{\\partial y}{\\partial v} \\right) \\end{aligned} \\]"},{"location":"1_Core/Math_1/04_Partial_Derivatives/#implicit-differentiation","title":"Implicit Differentiation","text":"<p>Let \\(y\\) be a function of \\(x\\), expressed as an implicit relation \\(f(x, y) = 0\\).</p> <p>Differentiating partially wrt \\(x\\)</p> \\[ \\begin{aligned} \\frac{\\partial f}{\\partial x} + \\left( \\frac{\\partial f}{\\partial y} \\cdot \\frac{dy}{dx} \\right) &amp;= 0 \\\\ \\implies \\frac{dy}{dx} &amp;= \\frac{-\\partial f / \\partial x}{\\partial f / \\partial y} \\\\ &amp;= \\frac{- f_x}{f_y} \\end{aligned} \\] <p>If \\(z\\) is a function of \\(x\\) and \\(y\\), given by an implicit relation \\(f(x,y,z) = 0\\)</p> \\[ \\begin{aligned} z_x &amp;= \\frac{-f_x}{f_z} \\\\ z_y &amp;= \\frac{-f_y}{f_z} \\end{aligned} \\]"},{"location":"1_Core/Math_1/04_Partial_Derivatives/#gradient-vector","title":"Gradient Vector","text":"<p>Let \\(f = f(x,y)\\) be a function. Then the gradient of \\(f\\)</p> \\[ \\begin{aligned} \\text{grad } f &amp;= \\nabla f \\\\ &amp;= f_x \\cdot \\hat i + f_y \\cdot \\hat j \\end{aligned} \\] <p>\\(\\nabla\\) is the vector differential operator</p> <p>\\(\\nabla f\\) acts along the normal at any point to the level curve of \\(f\\)</p>"},{"location":"1_Core/Math_1/04_Partial_Derivatives/#directional-derivative","title":"Directional Derivative","text":"<p>Let \\(f\\) be a function defined at all pionts in some neighborhood of \\(P_0(x_0, y_0)\\). Then, provided the limit exists, the directional derivative of \\(f\\) in the direction of \\(\\vec a = a_1 \\hat i + a_2 \\hat j\\) is given by</p> \\[ \\begin{aligned} \\text{DD} &amp;= (D_{\\hat u} f)_{P_0} \\\\ &amp;= \\lim_{s \\to 0} \\frac{f(x_0 + su_1, y_0 + su_2) - f(x_0, y_0)}{s} \\\\ &amp;= \\nabla f \\cdot \\hat u \\\\ \\nabla f &amp;= ( \\nabla f )_{P_0} \\\\ \\hat u &amp;= u_1 \\hat i + u_2 \\hat j, \\text{ unit vector in direction of } \\vec a \\\\ &amp;= \\frac{\\vec A}{|\\vec A|} \\end{aligned} \\]"},{"location":"1_Core/Math_1/04_Partial_Derivatives/#notes","title":"Notes","text":"Direction f DD \\(\\nabla f\\) increases more rapidly \\(\\vert  \\nabla f  \\vert\\) \\(- \\nabla f\\) decreases more rapidly \\(- \\vert  \\nabla f  \\vert\\) \\(\\perp \\text{to } (\\nabla f) \\text{ or } (-\\nabla f)\\) no change 0"},{"location":"1_Core/Math_1/04_Partial_Derivatives/#tangent-plane","title":"Tangent Plane","text":"<p>Let \\(f = f(x, y, z)\\). Then, the equation of the tangent plane passing through a point \\(P_0(x_0, y_0, z_0)\\) is given by</p> \\[ (x - x_0) {f_x}_{(P_0)} + (y - y_0) {f_y}_{(P_0)} + (z - z_0) {f_z}_{(P_0)} = 0 \\]"},{"location":"1_Core/Math_1/04_Partial_Derivatives/#normal-line","title":"Normal Line","text":"<p>The equations of normal line at \\(P_0\\) are given by</p> \\[ \\begin{aligned} x &amp;= x_0 + t {f_x}_{(P_0)} \\\\ y &amp;= y_0 + t {f_y}_{(P_0)} \\\\  z &amp;= z_0 + t {f_z}_{(P_0)} \\end{aligned}, \\quad t \\text{ is some parameter} \\]"},{"location":"1_Core/Math_1/04_Partial_Derivatives/#linearisation","title":"Linearisation","text":"<p>Let \\(f(x, y, z)\\) be a function and \\(P_0(x_0, y_0, z_0)\\) be any point in the domain of definition. Then, the linearisation of \\(f\\) about \\(P_0\\) is given by</p> \\[ L(x, y, z) = f(P_0) + (x - x_0){f_x}_{(P_0)} + (y - y_0){f_y}_{(P_0)} + (z - z_0){f_z}_{(P_0)} \\] <p>At all continuous points, \\(f\\) and \\(L\\) are the same.</p>"},{"location":"1_Core/Math_1/04_Partial_Derivatives/#extreme-values-of-a-function","title":"Extreme Values of a Function","text":"<p>Let \\(f(x,y)\\) be a function, and \\((a,b)\\) be a point.</p> <p>Absolute maximum is the point at which \\(f\\) is max; absolute minimum is the point at which \\(f\\) is minimum. They are obtained by evaluating \\(f\\) at all local minima/maxima and comparing the values.</p> Local Point Characteristic Maximum \\(f(a, b) &gt; f(x, y), \\quad \\forall (x, y)\\) in the neighborhood of \\((a,b)\\) Minimum \\(f(a, b) &lt; f(x, y), \\quad \\forall (x, y)\\) in the neighborhood of \\((a,b)\\) Saddle \\(f\\) increases in some directions and decreases in other directions at \\((a, b)\\)"},{"location":"1_Core/Math_1/04_Partial_Derivatives/#finding-local-points","title":"Finding local points","text":"<p>At point \\((a, b)\\)</p> \\[ \\begin{aligned} 1. &amp; f_x = 0 \\text{ and } f_y = 0 \\\\ 2. &amp; r = f_{xx}, s = f_{xy}, t = f_{yy}, \\\\ &amp; D = rt - s^2 \\end{aligned} \\] \\(D\\) \\(r\\) \\((a, b)\\) &gt; 0 &lt; 0 Maximum &gt; 0 &gt; 0 Minimum &lt; 0 - Saddle = 0 - Test Fails <p>Note: In the above table, we can replace \\(r\\) by \\(t\\) as well.</p>"},{"location":"1_Core/Math_1/04_Partial_Derivatives/#constrained-maxima-minima","title":"Constrained maxima, minima","text":"<p>We extremise a function \\(f(x, y, z)\\) subject to constraint/condition \\(\\phi(x, y, z) = 0\\). We then proceed as follows</p> <ol> <li>From Lagrange\u2019s function, \\(\\lambda =\\) Lagrange\u2019s multiplier constant</li> </ol> \\[ F(x, y, z) = f + \\lambda \\phi \\] <ol> <li>The extreme values are given by</li> </ol> \\[ F_x = F_y = F_z = 0 \\] <ol> <li>Solve the equations for \\(x, y, z, \\lambda\\)</li> </ol> <p>Note</p> <ol> <li>By Lagrange\u2019s method, we cannot find whether \\(f\\) has a maximum or minimum</li> <li>If \\(f\\) is to be extremised subject to constraints \\(\\phi_1 = \\phi_2 = 0\\), then the Lagrange\u2019s function becomes</li> </ol> \\[ F = f + \\lambda_1 \\phi_1 + \\lambda_2 \\phi_2 \\]"},{"location":"1_Core/Math_1/05_Multiple_Integrals/","title":"05 Multiple Integrals","text":""},{"location":"1_Core/Math_1/05_Multiple_Integrals/#double-integrals","title":"Double Integrals","text":"<p>represented by</p> \\[ I = \\iint f(x, y) \\ dy \\ dx \\] <p>The limits of outer integral will always be constants.</p> Direction of entry parallel to axis 1<sup>st</sup> Integral 2<sup>nd</sup> Integral \\(X\\) \\(x\\) \\(y\\) \\(Y\\) \\(y\\) \\(x\\)"},{"location":"1_Core/Math_1/05_Multiple_Integrals/#changing-order-of-integration","title":"Changing order of integration","text":"<ol> <li>Obtain the new limits</li> <li>Evaluate the integrals</li> </ol>"},{"location":"1_Core/Math_1/05_Multiple_Integrals/#cartesian-integral-iff-polar-integral","title":"Cartesian Integral \\(\\iff\\) Polar Integral","text":"<p>Let \\(f\\) be defined in a domain \\(R\\) in the \\(XY\\) plane. Then</p> \\[ \\begin{aligned} \\iint\\limits_{R} f(x, y) \\ dA &amp; = \\iint\\limits_{R'} f(r \\cos\\theta, r \\sin\\theta) \\cdot r \\ dr \\ d\\theta \\\\ \\text{where } x &amp;= r \\cos\\theta, y = r \\sin\\theta, dA = dx \\ dy \\end{aligned} \\] <p>Note: First integrate wrt to \\(r\\), then \\(\\theta\\)</p>"},{"location":"1_Core/Math_1/05_Multiple_Integrals/#triple-integrals","title":"Triple Integrals","text":"<p>represented by</p> \\[ I = \\iiint f(x, y, z) \\ dz \\ dy \\ dx \\]"},{"location":"1_Core/Math_1/06_Vector_Integrals/","title":"06 Vector Integrals","text":""},{"location":"1_Core/Math_1/06_Vector_Integrals/#line-integrals","title":"Line Integrals","text":"<p>Let \\(f(x, y, z)\\) be a function whose domain consists of a smooth curve \\(C: \\vec r(t) = x(t) \\hat i + y(t) \\hat j + z(t) \\vec k\\). Then, then line integral of \\(f\\) over \\(C\\) is given by</p> \\[ \\int\\limits_C f(x, y, z) \\ ds = \\int\\limits_C f(x, y, z) \\cdot |\\vec V| \\ dt \\] <p>because displacement s = \\(\\int\\) velocity = \\(\\int\\) speed x direction</p> <p>Note</p> <ol> <li>We aevaluate the integral by converting the integral in terms of a parameter \\(t\\), or writing in terms of any one variable \\(x\\) or \\(y\\) or \\(z\\) alone</li> <li>A curve is smooth if \\(\\frac{d \\vec r}{dt} \\ne 0\\) and \\(\\frac{d \\vec r}{dt}\\) is a constant</li> <li>A closed curve which doesn\u2019t cross itself is called a simple closed curve</li> <li>If \\(C\\) is a simple closed curve enclosing a region \\(R\\), then +ve direction is that direction through which one walks such that the enclosed region on their left</li> </ol>"},{"location":"1_Core/Math_1/06_Vector_Integrals/#work-done","title":"Work Done","text":"<p>The work done by a force field \\(\\vec F = M \\hat i + N \\hat j + P \\vec k\\) along curve \\(C: x \\hat i + y \\hat j + z \\hat k, a \\le t \\le b\\) is</p> \\[ \\begin{aligned} W &amp;= \\int\\limits_C \\vec F \\cdot d \\vec r \\\\ &amp;= \\int\\limits_C (M \\ dx + N \\ dy + P \\ dz) \\ dt \\end{aligned} \\] <p>The above integral is also referred to as the circulation of vector \\(\\vec F\\) in fluid flow problems.</p>"},{"location":"1_Core/Math_1/06_Vector_Integrals/#conservative-forced-field","title":"Conservative Forced Field","text":"<p>If the line integral is independent of the path of integration, then \\(\\vec F\\) is said to conservative/irrotational.</p> <p>A force \\(\\vec F = M \\hat i + N \\hat j + P \\vec k\\) is conservative</p> <ol> <li> \\[    \\begin{aligned}    M_y &amp;= N_x \\\\      P_y &amp;= N_z \\\\      P_x &amp;= M_z    \\end{aligned}    \\] </li> <li> <p>there exists a scalar potential function \\(\\phi(x, y, z)\\) such that</p> </li> </ol> \\[ \\begin{aligned} \\vec F &amp;= \\nabla \\phi \\\\ \\text{where } \\nabla \\phi &amp;= \\phi_x \\hat i + \\phi_y \\hat j + \\phi_z \\hat k \\end{aligned} \\] <ol> <li>If \\(C\\) is any path joining A and B</li> </ol> \\[ \\begin{aligned} W &amp;= \\int\\limits_C \\vec F \\cdot d \\vec r \\\\    &amp;= \\phi(B) - \\phi(A) \\end{aligned} \\]"},{"location":"1_Core/Math_1/06_Vector_Integrals/#greens-theorem-in-a-plane","title":"Green\u2019s Theorem in a Plane","text":"<p>Let \\(\\vec F = M \\hat i + N \\hat j\\) be a vector-valued function defined at all points in a region \\(R\\) in the \\(XY\\) plane, bounded by a simple closed curve C. Then, the counter-clockwise circulation of \\(\\vec F\\) or flux or tangential form of Green\u2019s theorem is given by</p> \\[ \\begin{aligned} \\oint\\limits_C \\vec F \\cdot d \\vec r &amp;= \\int\\limits_C M \\ dx + N \\ dy \\\\ &amp;= \\iint\\limits_R  \\left( \\frac{\\partial N}{\\partial x} - \\frac{\\partial M}{\\partial y} \\right) \\ dx \\ dy \\end{aligned} \\]"},{"location":"1_Core/Math_1/06_Vector_Integrals/#gauss-divergence-theorem","title":"Gauss Divergence Theorem","text":"<p>Let \\(F = M \\hat i + N \\hat j + P \\hat k\\) be a vector-valued function, defined at all points of closed surface \\(S\\), enclosing a volume \\(V\\). Then, the outward-drawn flux of \\(\\vec F\\) is given by</p> \\[ \\begin{aligned} \\iint_S \\vec F \\cdot \\vec n \\cdot ds &amp;= \\iiint (\\text{div } \\vec F) \\ dv \\\\ \\text{where } (\\text{div } \\vec F) &amp;= \\nabla \\cdot \\vec F \\\\ &amp;= \\frac{\\partial M}{\\partial x} + \\frac{\\partial N}{\\partial y} + \\frac{\\partial P}{\\partial z} \\\\ \\vec n &amp;= \\frac{\\nabla \\phi}{ |\\nabla \\phi| } \\\\ &amp;\\text{(unit outward-drawn normal vector to surface S)}\\\\ \\phi &amp;= \\phi(x, y, z) \\\\ &amp;\\text{(equation of surface S)} \\end{aligned} \\]"},{"location":"1_Core/Math_1/06_Vector_Integrals/#stokes-theorem","title":"Stoke\u2019s Theorem","text":"<p>If \\(\\vec F = M \\hat i + N \\hat j + P \\vec k\\) is defined on all points on an open surface bounded by a simple curve \\(C\\),</p> \\[ \\begin{aligned} \\int \\limits_C \\vec F \\cdot dr &amp;= \\iint \\limits_S (\\text{curl } \\vec F) \\cdot \\hat n \\cdot ds \\\\ \\text{where } (\\text{curl } \\vec F) &amp;= \\vec V \\times \\vec F \\\\ &amp;= \\begin{vmatrix} \\hat i &amp; \\hat j &amp; \\hat k \\\\ \\frac{\\partial}{\\partial x} &amp; \\frac{\\partial}{\\partial y} &amp; \\frac{\\partial}{\\partial z} \\\\ M &amp; N &amp; P \\end{vmatrix} \\end{aligned} \\]"},{"location":"1_Core/Math_1/07_Infinite_Series/","title":"07 Infinite Series","text":""},{"location":"1_Core/Math_1/07_Infinite_Series/#infinite-series","title":"Infinite Series","text":"<p>Let \\(\\set{a_n}_{n \\in \\mathbb{Z^+}}\\) be a sequence. Then \\(\\sum\\limits_{n = 1}^\\infty a_n = a_1 + a_2 + \\dots\\) is called a series.</p> <p>If the series has a finite number of terms, it is called a finite series; otherwise it is called an infinite series.</p> <p>A finite series is always convergent.</p> <p>A infinite series may/ may not be convergent</p> Series Type \\(1 + \\frac{x}{1!} + \\frac{x^2}{2!} + \\dots + \\frac{x^n}{n!} = e^x\\) Converges to \\(e^x\\) \\(1 + 1 + \\dots\\) Divergent \\(1 + \\frac12 + \\dots + \\frac1n\\) Divergent \\(1 - 1 + 1 - 1 +  \\dots\\) Neither convergent/divergentit is an alternating series which oscilates <p>If we are able to find the sum of a series, then the series converges to the sum \\(S_n = \\dfrac{a}{1 - r}\\)</p> <ul> <li>if sum is finite, then convergent series</li> <li>else, divergent series</li> </ul>"},{"location":"1_Core/Math_1/07_Infinite_Series/#series-of-ve-terms","title":"Series of +ve Terms","text":"<p>Consider series \\(\\sum\\limits_{n = 1}^\\infty a_n = a_1 + a_2 + \\dots + a_n\\). This series is a series of +ve terms as \\(a_n \\ge 0, \\forall n\\).</p> <p>We use the following tests.</p>"},{"location":"1_Core/Math_1/07_Infinite_Series/#ntextth-term-test","title":"\\(n^\\text{th}\\) Term Test","text":"\\[ \\lim\\limits_{n \\to \\infty} a_n =  \\begin{cases} \\ne 0 &amp; \\text{Divergent} \\\\ = 0 &amp; \\text{Test fails} \\end{cases} \\]"},{"location":"1_Core/Math_1/07_Infinite_Series/#important-results","title":"Important Results","text":"<ul> <li>Geometric sum \\(a + ar + ar^2 + \\dots\\)<ul> <li>converges to \\(\\dfrac{a}{1 - r}, |r| &lt; 1\\)</li> <li>diverges</li> </ul> </li> </ul> Converges Diverges Geometric Series \\(a + ar + ar^2 + \\dots\\) \\(\\vert  r \\vert  &lt; 1\\)converges to \\(\\dfrac{a}{1-r}\\) \\(\\vert  r  \\vert \\ge 1\\) p-series \\(\\sum\\limits_{n = 1}^\\infty \\dfrac{1}{n^p}\\) \\(p &gt; 1\\) \\(p \\le 1\\) \\[ \\begin{aligned} \\lim\\limits_{n \\to \\infty} \\frac{ \\ln \\vert n\\vert  }{n} &amp;= 0 \\quad (\\ln \\vert  n  \\vert \\text{ always } &lt; n, \\text{ so den reaches } \\infty \\text{ faster} ) \\\\ \\lim\\limits_{n \\to \\infty} x^{\\frac{1}{n}} &amp;= 1 \\\\ \\lim\\limits_{n \\to \\infty} n^{\\frac{1}{n}} &amp;= 1 \\\\ (x^0 = n^0 &amp;= 1) \\\\ \\lim\\limits_{n \\to \\infty} \\left( 1 + \\frac x n \\right)^n &amp;= e^x \\\\ \\lim\\limits_{n \\to \\infty} x^n &amp;= 0 \\text{ if } |x| &lt; 1 \\\\ \\lim\\limits_{n \\to \\infty} \\frac{x^n}{n!} &amp;= 0 \\\\ (n! &amp;&gt; x^n),  \\text{ when } n \\text{ is large so den reaches } \\infty \\text{ faster} \\end{aligned} \\]"},{"location":"1_Core/Math_1/07_Infinite_Series/#integral-test","title":"Integral Test","text":"<p>This test can be applied when \\(a_n = f(n)\\) is integrable</p> <p>Let</p> <ul> <li>\\(\\sum a_n\\) be a series of +ve terms</li> <li>\\(a_n = f(n)\\) where \\(f\\) is<ul> <li>continuous</li> <li>+ve</li> <li>decreasing function of \\(n\\), for some \\(n \\ge N\\)</li> </ul> </li> </ul> <p>Then by integral test, \\(\\int\\limits_N^\\infty f(x) \\ dx\\) and \\(\\sum\\limits_N^\\infty a_n\\) converge/diverge together</p> \\(I\\) Finite Converges(basically \\(S_n\\) is finite number) Infinite Diverges"},{"location":"1_Core/Math_1/07_Infinite_Series/#ratio-test","title":"Ratio Test","text":"<p>Used when series contains factorials like \\(n!, (2n)!\\)</p> <p>Let \\(\\sum a_n\\) be a series of +ve terms.</p> <p>Let \\(\\lim\\limits_{n \\to \\infty} \\dfrac{a_{n+1}}{a_n} = k\\)</p> \\(k\\) \\(&lt; 1\\) Converges \\(&gt; 1\\) Diverges \\(0, 1\\) Test Fails"},{"location":"1_Core/Math_1/07_Infinite_Series/#root-test","title":"Root Test","text":"<p>Used when series contains terms with exponents, such as \\(n^n, n^{n+1}, n^\\frac1n\\)</p> <p>Let \\(\\lim\\limits_{n \\to \\infty} (a_n)^\\frac1n = k\\)</p> \\(k\\) \\(&lt; 1\\) Converges \\(&gt; 1\\) Diverges \\(1\\) Test Fails"},{"location":"1_Core/Math_1/07_Infinite_Series/#limit-comparison-test","title":"Limit Comparison Test","text":"<p>Best used when \\(a_n\\) is a fraction of polynomial, ie \\(a_n = \\frac{P(n)}{Q(n)}\\), where \\(P, Q\\) are polynomials in terms of \\(n\\)</p> <p>Let</p> <ul> <li> <p>\\(\\sum a_n\\) be a series of +ve terms</p> </li> <li> <p>\\(\\sum b_n\\) be a known series (we know if it converges/diverges)</p> <ul> <li> <p>We choose \\(b_n = \\dfrac{1}{n^{q-p}}\\), where</p> </li> <li> <p>P = degree of numerator</p> </li> <li> <p>Q = degree of denominator</p> </li> <li> <p>If \\(b_n\\) is a p-series of the form \\(\\sum \\dfrac{1}{n^p}\\) |   \\(p\\)   |           | | :-----: | :-------: | |  \\(&gt; 1\\)  | converges | | \\(\\le 1\\) | diverges  |</p> </li> <li>\\(\\lim\\limits_{n \\to \\infty} \\frac{a_n}{b_n} = k\\)</li> </ul> </li> </ul> <p>Then</p> Given \\(k = c (\\ne 0)\\) both \\(\\sum a_n\\) and \\(\\sum b_n\\) converge \\(k = 0, \\sum b_n\\) converges \\(\\sum a_n\\) converges \\(k \\to \\infty, \\sum b_n\\) diverges \\(\\sum a_n\\) diverges"},{"location":"1_Core/Math_2/","title":"Math 2","text":"<p>This course introduces the fundamentals of Linear Algebra and the Theory of Complex Variable Functions, focusing on their applications across various fields. Key topics covered include:</p> <ul> <li>Matrix Operations: Essential for data manipulation and problem-solving in various disciplines.</li> <li>Complex Numbers: Fundamental for understanding advanced mathematical concepts.</li> <li>Complex Integrals: Important for analyzing functions in multiple dimensions.</li> </ul> <p>Students will learn how these techniques are applied in engineering, physics, computer science, and economics, providing a solid foundation for further studies in these areas.</p>"},{"location":"1_Core/Math_2/01_System_of_Linear_Equations/","title":"01 System of Linear Equations","text":""},{"location":"1_Core/Math_2/01_System_of_Linear_Equations/#elementary-row-operations","title":"Elementary Row Operations","text":"\\[ A = \\begin{bmatrix} 1 &amp; 2 &amp; -1 \\\\ -9 &amp; 6 &amp; 4 \\\\7 &amp; 3 &amp; -1 \\end{bmatrix}_{3 \\times 3} \\] <ul> <li> <p>Any 2 rows can be interchanged</p> <p>\\(R_1 \\iff R_2\\) - Any row can be multiplied/divided by any number other than 0</p> <p>\\(R_1 \\to 2R_1\\) - Any row can be added/subtracted to any row</p> <p>\\(R_1 \\to R_1 \\pm 2 R_2\\)</p> </li> </ul>"},{"location":"1_Core/Math_2/01_System_of_Linear_Equations/#ref","title":"REF","text":"<p>Reduced Echelon Form</p> <p>Upper \\(\\triangle\\)r matrix</p> <ul> <li> <p>1<sup>st</sup> non-zero elment in a row should be 1</p> <p>(called as leading one) - Leading one should occur to the right side of previous rows\u2019 leading one(s) - If there is any zero row, it should be the last row   otherwise, we need to interchange rows to ensure this rule</p> </li> </ul> <p>example</p> \\[ \\begin{bmatrix} 1 &amp; 4 &amp; 5 &amp; 3 \\\\ 0 &amp; 1 &amp; 2 &amp; 8 \\\\0 &amp; 0 &amp; 1 &amp; 5 \\end{bmatrix} \\quad \\begin{bmatrix} 1 &amp; 4 &amp; 3 &amp; 5 \\\\ 0 &amp; 1 &amp; 8 &amp; 2 \\\\0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} \\]"},{"location":"1_Core/Math_2/01_System_of_Linear_Equations/#rref","title":"RREF","text":"<p>diagonal matrix</p> <p>is the REF matrix where the elements of the columns of the leading ones (other than itself) are 0.</p> \\[ \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 3\\\\ 0 &amp; 1 &amp; 0 &amp; 8\\\\0 &amp; 0 &amp; 1 &amp; 5 \\end{bmatrix} \\quad \\begin{bmatrix} 1 &amp; 0 &amp; 5 &amp; 0\\\\ 0 &amp; 1 &amp; 8 &amp; 0\\\\0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} \\]"},{"location":"1_Core/Math_2/01_System_of_Linear_Equations/#rank","title":"Rank","text":"<p>no of non-zero rows of a matrix in REF/RREF</p>"},{"location":"1_Core/Math_2/01_System_of_Linear_Equations/#gauss-methods","title":"Gauss Methods","text":"Method Form Gauss Elimination REF Gauss Jordan RREF <ol> <li>Write equation in matrix form \\(AX = B\\), where<ul> <li>\\(A\\) is coefficients matrix</li> <li>\\(B\\) is constant matrix</li> <li>\\(X\\) is variable matrix</li> </ul> </li> </ol> <p>Converted augmented matrix = \\([A | B]\\) into REF</p> <ol> <li> <p>Cases</p> <p>\\(n\\) is the number of unknown variables</p> </li> </ol> Rank(A\\vert B) \\(\\ne\\) rank(A) no solutions \\(=\\) rank(A) \\(= n\\) unique solutions \\(=\\) rank(A) \\(&lt; n\\) infinite solutions <ol> <li> <p>Back Substitution</p> <p>Degree of freedom = no of vars - no of equations</p> </li> </ol>"},{"location":"1_Core/Math_2/01_System_of_Linear_Equations/#homogeneous-linear-system","title":"Homogeneous Linear System","text":"<p>There will always be a solution.</p> <p>If there is unique solution, it is always all 0s. This is called as trivial solution.</p>"},{"location":"1_Core/Math_2/01_System_of_Linear_Equations/#inverse-of-matrix","title":"Inverse of matrix","text":"<p>If \\(A\\) and \\(B\\) are 2 non-singular matrices such that \\(|A| \\ne 0\\), then \\(A^{-1} = B \\iff A\\cdot B = I\\)</p> <p>\\(I\\) is identity matrix</p> \\[ \\begin{aligned} I_{2 \\times 2} &amp;= \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix} \\\\ I_{3 \\times 3} &amp;= \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\0 &amp; 0 &amp; 1 \\end{bmatrix} \\end{aligned} \\] <p>To find inverse</p> <ul> <li>use row transformations to convert \\([A:I] \\to [I:B]\\)</li> <li>then \\(B = A^{-1}\\)</li> </ul> <p>If \\(A\\) is singular, inverse does not exist</p>"},{"location":"1_Core/Math_2/02_Vector_Spaces/","title":"02 Vector Spaces","text":""},{"location":"1_Core/Math_2/02_Vector_Spaces/#set","title":"Set","text":"<p>collection of well-defined elements</p>"},{"location":"1_Core/Math_2/02_Vector_Spaces/#vector-space","title":"Vector Space","text":"<p>A non-empty set \\(V\\) with binary operations \\(\\oplus\\) and \\(\\odot\\), which satisfies the following rules</p> Law Closure Law wrt Addition \\(\\forall u,v \\in V, \\quad \\vec u \\oplus \\vec v \\in V\\) Commutative Law wrt Addition \\(\\vec u \\oplus v = \\vec v \\oplus u\\) Associative Law wrt Addition \\((\\vec u \\oplus \\vec v) \\oplus \\vec w = \\vec u \\oplus (\\vec v \\oplus \\vec w)\\) Existence of additive identity For \\(\\vec u \\in V\\), there exists \\(\\vec 0 \\in V\\) such that\\(\\vec 0 \\oplus \\vec u = \\vec u \\oplus \\vec 0 = \\vec u\\)\\(\\vec 0\\) is not necessarily \\((0, 0)\\) Existence of additive inverse For \\(\\vec u \\in V\\), there exists \\(-u \\in V\\) such that\\(\\vec u \\oplus (- \\vec u) =   (-\\vec u) \\oplus \\vec u = \\vec 0\\) Closure Law wrt multiplication For any scalar \\(\\alpha\\) (any real no) and \\(\\vec u \\in V\\)\\(\\alpha \\odot \\vec u \\in V\\) Distributive Law (Right-Side) For \\(u, v \\in V\\) and scalar \\(\\alpha\\)\\(\\alpha \\odot (\\vec u \\oplus \\vec v) = (\\alpha \\odot \\vec u) \\oplus (\\alpha \\odot \\vec v)\\) Distributive Law (Left-Side) For \\(u \\in V\\) and scalars \\(\\alpha, \\beta\\)\\((\\alpha + \\beta) \\odot \\vec u = (\\alpha \\odot \\vec u ) \\oplus (\\beta \\odot \\vec u)\\) Distributive Law (Variation) For \\(u \\in V\\) and scalars \\(\\alpha, \\beta\\)\\((\\alpha \\beta) \\odot \\vec u = \\alpha \\odot (\\beta \\odot \\vec u)\\) Existence of unity For \\(u \\in V\\)\\(1 \\odot \\vec u = \\vec u\\)"},{"location":"1_Core/Math_2/02_Vector_Spaces/#known-vector-spaces","title":"Known Vector Spaces","text":"<ul> <li>Real numbers</li> <li>\\(R_2, R_3, R_n\\)</li> <li>matrices</li> <li>polynomials<ul> <li>form \\(ax^n + bx^{n-1} + \\dots + \\alpha, \\quad a, b \\in R, \\quad n \\in Z\\)</li> <li>\\(P_n\\) means degree of the polynomial \\(\\le n\\)</li> </ul> </li> <li>continuous functions</li> </ul>"},{"location":"1_Core/Math_2/02_Vector_Spaces/#subspace","title":"Subspace","text":"<p>Let \\(S \\subset V\\) vector space. Then, \\(S\\) is a subspace if</p> <ol> <li>\\(\\vec 0 \\in S\\)</li> <li>\\(\\forall u, v \\in S, \\quad u \\oplus v \\in S\\)</li> <li>\\(\\forall u \\in S, \\quad \\alpha \\odot u \\in S\\)</li> </ol> <p>Trick to identify is if sum of powers of multiplicative terms is 1 For eg, \\(x^a y^b + w^c z^d\\) is subspace if \\(a + b= 1, c + d = 1\\)</p>"},{"location":"1_Core/Math_2/02_Vector_Spaces/#polynomial","title":"Polynomial","text":"<p>\\(P_n\\) is a polynomial where degree \\(\\le n\\)</p> <p>For eg, even \\((1 + x)\\) is \\(P_3\\), as degree \\(= 1 \\le 3\\)</p>"},{"location":"1_Core/Math_2/03_Linear-Dependence%2C_Span%2C_Basis/","title":"03 Linear Dependence, Span, Basis","text":""},{"location":"1_Core/Math_2/03_Linear-Dependence%2C_Span%2C_Basis/#linearly-dependentindependent","title":"Linearly-Dependent/Independent","text":"<p>Let \\(\\alpha_1 u_1 + \\alpha_2 u_2 + \\dots + \\alpha_n u_n = \\vec 0\\)</p> Condition Conclusion \\(\\alpha_1 = \\alpha_2 = \\alpha_3 = \\dots = 0\\) Independent else Dependent"},{"location":"1_Core/Math_2/03_Linear-Dependence%2C_Span%2C_Basis/#working","title":"Working","text":"<ol> <li> <p>Column-wise</p> </li> <li> Condition Solution Conclusion \\(r(A) = n\\) unique \\((0, 0, \\dots)\\) independent else infinitely-many dependent </li> </ol>"},{"location":"1_Core/Math_2/03_Linear-Dependence%2C_Span%2C_Basis/#span","title":"Span","text":"<p>Let \\(\\vec v = \\alpha_1 u_1 + \\alpha_2 u_2 + \\dots + \\alpha_n u_n\\)</p>"},{"location":"1_Core/Math_2/03_Linear-Dependence%2C_Span%2C_Basis/#working_1","title":"Working","text":"<ol> <li>Column-wise</li> <li> Condition Solution Conclusion \\(r(A) = r(A:B) = n\\) unique span \\(r(A) = r(A:B) &lt; n\\) infinite span else not span </li> </ol>"},{"location":"1_Core/Math_2/03_Linear-Dependence%2C_Span%2C_Basis/#basis","title":"Basis","text":"<ol> <li>\\(S\\) is Linearly-independent \u2013&gt; row-wise working</li> <li>\\(S\\) spans \\(V\\) \u2013&gt; dim(\\(V\\)) = no of vectors in \\(S\\)</li> </ol> <p>Note: \\(\\vec 0\\) has no basis</p>"},{"location":"1_Core/Math_2/03_Linear-Dependence%2C_Span%2C_Basis/#dimension","title":"Dimension","text":"<p>no of unknowns</p> <p>no of vectors in its basis</p> <p>dim \\((\\vec 0)= 0\\)</p>"},{"location":"1_Core/Math_2/04_Linear_Transformations/","title":"04 Linear Transformations","text":""},{"location":"1_Core/Math_2/04_Linear_Transformations/#linear-transformations","title":"Linear Transformations","text":"<p>Consider a linear transformation</p> \\[ L:  \\underbrace{U}_{\\text{Domain}} \\to \\underbrace{W}_{\\text{Codomain}} \\]"},{"location":"1_Core/Math_2/04_Linear_Transformations/#properies","title":"Properies","text":"<ol> <li>\\(L(O_u) = O_w\\)</li> <li>\\(L(\\vec u \\oplus \\vec v) = L(\\vec u) + L(\\vec v)\\)</li> <li>\\(L(\\alpha \\odot u) = \\alpha \\cdot L(\\vec u)\\)</li> </ol>"},{"location":"1_Core/Math_2/04_Linear_Transformations/#tricks","title":"Tricks","text":"<p>A transformation is not Linear Transformation if</p> <ul> <li>Power \\(\\ne\\) 1 or 0</li> <li>there is modulus(absolute value)</li> <li>determinant</li> </ul>"},{"location":"1_Core/Math_2/04_Linear_Transformations/#kernel","title":"Kernel","text":"\\[ S = \\set{ \\vec u: L(\\vec u) = O_w } \\] <p>Set of all input values</p>"},{"location":"1_Core/Math_2/04_Linear_Transformations/#range","title":"Range","text":"\\[ S = \\set{ L(\\vec u) } \\] <p>Set of all output values</p>"},{"location":"1_Core/Math_2/04_Linear_Transformations/#properties","title":"Properties","text":"Property Condition One-one Kernel = \\(\\set{O_u}\\) Onto dim(range) = dim(codomain)"},{"location":"1_Core/Math_2/04_Linear_Transformations/#dimension-theorem","title":"Dimension Theorem","text":"\\[ \\text{ dim(range) + dim(kernel) = dim(U) } \\]"},{"location":"1_Core/Math_2/05_Eigen_Values%2C_Vectors/","title":"05 Eigen Values, Vectors","text":""},{"location":"1_Core/Math_2/05_Eigen_Values%2C_Vectors/#eigen-values","title":"Eigen Values","text":"<p>are the values of \\(\\lambda\\) that satisfy equation</p> \\[ | A - \\lambda I | = 0 \\]"},{"location":"1_Core/Math_2/05_Eigen_Values%2C_Vectors/#properties","title":"Properties","text":"<ol> <li>Eigen values of upper/lower \\(\\triangle\\)r matrix = diagonal elements</li> <li>No of eigen values = order of A</li> <li>Sum of eigen values = Sum of diagonal elements</li> <li>Product of eigen values = \\(|A|\\)</li> <li>If eigen values of \\(A = \\lambda\\), then</li> </ol> Matrix Eigen Value \\(A^{-1}\\) \\(\\frac{1}{\\lambda}\\) \\(A^n\\) \\(\\lambda^n\\) \\(A^T\\) \\(\\lambda\\)"},{"location":"1_Core/Math_2/05_Eigen_Values%2C_Vectors/#eigen-vectors","title":"Eigen Vectors","text":"<p>are the values of \\(X\\) that satisfies equation</p> \\[ (A - \\lambda I) X = 0 \\] <p>Eigen vector(s) of \\(A\\) = eigen vector(s) of \\(A^{-1}, A^n, A^T\\)</p>"},{"location":"1_Core/Math_2/05_Eigen_Values%2C_Vectors/#working","title":"Working","text":"Scenario Method Repeating eigen values back substitution else Cramer\u2019s rule for 2 independent rows"},{"location":"1_Core/Math_2/06_Intro_to_Complex_Calculus/","title":"06 Intro to Complex Calculus","text":""},{"location":"1_Core/Math_2/06_Intro_to_Complex_Calculus/#complex-numbers","title":"Complex Numbers","text":"\\[ z = x + iy \\] <p>Make sure that all calculations are in radian</p>"},{"location":"1_Core/Math_2/06_Intro_to_Complex_Calculus/#properties","title":"Properties","text":"\\[ \\begin{aligned} |z| &amp;= \\sqrt{x^2 + y^2} \\\\ |z_1 \\cdot z_2| &amp;= |z_1| \\cdot |z_2| \\\\ \\left| \\frac{z_1}{z_2} \\right| &amp;= \\frac{ |z_1| }{ |z_2| } \\\\ \\bar z &amp;= x - iy \\\\ |\\bar z| &amp;= |z| \\\\ \\bar{ |z| }^2 &amp;= z \\cdot \\bar z \\\\ \\frac{z + \\bar z}{2} &amp;= \\text{Re}(z) \\\\ \\frac{z - \\bar z}{2i} &amp;= \\text{Im}(z) \\\\ \\overline{z_1 \\pm z_2} &amp;= \\bar z_1 \\pm \\bar z_2 \\\\ \\overline{z_1 \\cdot z_2} &amp;= \\bar z_1 \\cdot \\bar z_2 \\\\ \\overline{\\left( \\frac{z_1}{z_2} \\right)} &amp;= \\frac{\\bar z_1}{\\bar z_2} \\end{aligned} \\]"},{"location":"1_Core/Math_2/06_Intro_to_Complex_Calculus/#circles","title":"Circles","text":"\\(\\vert  z  \\vert = r\\) circle with radius \\(r\\) @ \\((0, 0)\\) \\(\\vert  z-z_0  \\vert = r\\) circle with radius \\(r\\) @ \\(z_0\\)"},{"location":"1_Core/Math_2/06_Intro_to_Complex_Calculus/#triangle-inequality","title":"Triangle Inequality","text":"Upper Bound Lower Bound \\(\\vert  z_1 \\pm z_2 \\vert  \\le \\vert  z_1 \\vert  + \\vert  z_2 \\vert\\) \\(\\vert  z_1 \\pm z_2 \\vert  \\ge \\text{abs} (\\vert  z_1 \\vert  - \\vert  z_2  \\vert )\\) <p>abs refers to absolute value</p>"},{"location":"1_Core/Math_2/06_Intro_to_Complex_Calculus/#argument","title":"Argument","text":"\\[ \\begin{aligned} \\text{arg } z &amp;= \\left| \\frac{y}{x} \\right| \\\\ \\text{Arg } z &amp;= \\text{Principle Value of arg } z\\\\ \\text{arg}(z_1 \\cdot z_2) &amp;= \\text{arg}(z_1) + \\text{arg}(z_2) \\\\ \\text{arg}\\left( \\frac{z_1}{z_2} \\right) &amp;= {\\text{arg}(z_1)} - {\\text{arg}(z_2)} \\end{aligned} \\]"},{"location":"1_Core/Math_2/06_Intro_to_Complex_Calculus/#polar-form","title":"Polar Form","text":"\\[ \\begin{aligned} z &amp;= r \\cdot e^{i \\theta} \\\\ &amp;= r (\\cos \\theta + i \\sin \\theta) \\end{aligned} \\]"},{"location":"1_Core/Math_2/06_Intro_to_Complex_Calculus/#root","title":"Root","text":"\\[ \\begin{aligned} c &amp;= (r \\cdot e^{i\\theta})^{\\frac{1}{n}} \\\\ &amp;= r^{\\frac{1}{n}} \\cdot e^{\\frac{i\\theta}{n}} \\\\ &amp;= r^{\\frac{1}{n}} \\Bigg(     \\cos \\left(\\frac{\\theta}{n}\\right) + i \\sin \\left(\\frac{\\theta}{n}\\right) \\Bigg) \\\\ r &amp;= |z| \\\\ \\frac{\\theta}{n} &amp;= \\frac{\\text{Arg }z + 2k\\pi}{n}, k \\in [0, n) \\\\ e^{i(n\\theta)} &amp;= \\cos(n\\theta) + i \\sin(n\\theta) \\\\ e^{-i(n\\theta)} &amp;= \\cos(n\\theta) - i \\sin(n\\theta) \\end{aligned} \\]"},{"location":"1_Core/Math_2/07_Complex_Regions/","title":"07 Complex Regions","text":""},{"location":"1_Core/Math_2/07_Complex_Regions/#connected-set","title":"Connected Set","text":"<p>A set where any 2 points can be joined without leaving the set</p> <p>Refer to Types of Sets</p> <ul> <li>open' = closed</li> <li>closed' = open</li> </ul>"},{"location":"1_Core/Math_2/07_Complex_Regions/#domain","title":"Domain","text":"<p>a set that is both open and connected.</p>"},{"location":"1_Core/Math_2/07_Complex_Regions/#limitaccumulation-point","title":"Limit/Accumulation Point","text":"<p>Deleted neighborhood of \\(z_0\\) contains atleast one point of \\(S\\)</p> <p>closed set has all limit points</p> <p>all interior points and boundary points are limit points</p>"},{"location":"1_Core/Math_2/07_Complex_Regions/#properties-of-functions","title":"Properties of Functions","text":""},{"location":"1_Core/Math_2/07_Complex_Regions/#differentiable","title":"Differentiable","text":"<p>Consider derivative equation</p> \\[ f'(z) = \\lim_{\\Delta z \\to 0} \\frac{ f(z + \\Delta z) - f(z) }{ \\Delta z } \\] <p>A function is said to differentiable if \\(f'(z)\\) is unique</p>"},{"location":"1_Core/Math_2/07_Complex_Regions/#analytic","title":"Analytic","text":"<p>Differentiable @ \\(z_0\\) and its neighborhood</p>"},{"location":"1_Core/Math_2/07_Complex_Regions/#entire","title":"Entire","text":"<p>Analytic Everywhere</p>"},{"location":"1_Core/Math_2/07_Complex_Regions/#harmonic","title":"Harmonic","text":"<p>\\(u\\) is harmonic if it satisfied Laplace equation, ie</p> \\[ u_{xx} + u_{yy} = 0 \\] <p>If \\(f(z) = u+iv\\), then</p> <ul> <li>\\(f(z)\\) is analytic<ul> <li>Put \\(y = 0, x = z \\to f(z) = f(x)\\) for shortcut</li> </ul> </li> <li>real and imaginary parts are harmonic</li> <li>\\(v\\) is harmonic conjugate of \\(u\\)</li> </ul>"},{"location":"1_Core/Math_2/07_Complex_Regions/#hyperbolic-function","title":"Hyperbolic Function","text":"\\[ \\begin{aligned} \\cos(ix) &amp;= \\cosh(x) &amp; \\sin(ix) &amp;= i \\sinh(x) \\\\ \\cosh(x) &amp;= \\frac{e^x + e^{-x}}{2} &amp; \\sinh(x) &amp;= \\frac{e^x - e^{-x}}{2} \\\\ \\sinh'(x) &amp;= \\cosh(x) &amp; \\cosh'(x) &amp;= \\sinh(x) \\end{aligned} \\] \\[ \\cosh^2(x) - \\sinh^2(x) = 1 \\]"},{"location":"1_Core/Math_2/07_Complex_Regions/#cr-equation","title":"CR Equation","text":"<p>Consider \\(f(z) = u + iv\\)</p> Rectangular Polar \\(u_x = v_y\\) \\(u_r = \\frac{1}{r} v_\\theta\\) \\(u_y = - v_x\\) \\(u_\\theta = -r v_r\\) Continuous \\(u_x, u_y, v_x, v_y\\) \\(u_r, u_\\theta, v_r, v_\\theta\\) \\(f'(z)\\) \\(u_x + i v_x\\) \\((u_r + i v_r) e^{-i \\theta}\\)"},{"location":"1_Core/Math_2/08_Elementary%2C_Exponential_Functions/","title":"08 Elementary, Exponential Functions","text":""},{"location":"1_Core/Math_2/08_Elementary%2C_Exponential_Functions/#ez","title":"\\(e^z\\)","text":"\\[ \\begin{aligned} |e^z| &amp;= e^x \\\\ e^{z + (2k \\pi) i} &amp;= e^z \\end{aligned} \\]"},{"location":"1_Core/Math_2/08_Elementary%2C_Exponential_Functions/#log-z","title":"\\(\\log z\\)","text":"\\[ \\begin{aligned} \\log z &amp;= \\ln r + i \\theta \\\\ &amp;= \\ln r + i[\\text{Arg}(z) + 2k\\pi] \\\\ \\text{Log} z &amp;= \\ln r + i[\\text{Arg}(z)] \\end{aligned} \\]"},{"location":"1_Core/Math_2/08_Elementary%2C_Exponential_Functions/#complex-exponents","title":"Complex Exponents","text":"\\[ \\begin{aligned} z &amp;= e^{\\log z} \\\\ z_c &amp;= e^{c \\log z} \\\\ PV(z^c) &amp;= e^{c \\text{ Log} z} \\end{aligned} \\]"},{"location":"1_Core/Math_2/09_Complex_Integrals/","title":"09 Complex Integrals","text":""},{"location":"1_Core/Math_2/09_Complex_Integrals/#line-integral","title":"Line Integral","text":"<p>For \\(\\int f(z) \\ dz\\), put \\(z = r \\cdot e^{i \\theta}\\)</p>"},{"location":"1_Core/Math_2/09_Complex_Integrals/#ml-inequality","title":"ML Inequality","text":"<p>maximum value / upper bound of integral</p> \\[ \\left| \\int_C f(z) \\ dz \\right| \\le M \\times L \\] <p>where</p> <ul> <li>\\(M =\\) max value of \\(f(z)\\)</li> <li>\\(L =\\) length of contour \\(C\\)</li> </ul>"},{"location":"1_Core/Math_2/09_Complex_Integrals/#theorems","title":"Theorems","text":"Theorem Cauchy-Goursat Cauchy-Integral Cauchy-Integral for derivatives Cauchy Residue Condition \\(f(z)\\) is analytic inside/on \\(C\\) - \\(f(z)\\) is analytic inside/on \\(C\\)- \\(z_0\\) is a point inside \\(C\\) - \\(f(z)\\) is analytic inside/on \\(C\\)- \\(z_0\\) is a point inside \\(C\\) Identity \\(\\int_C f(z) \\ dz = 0\\) \\(\\int_C \\frac{f(z)}{z-z_0} dz = 2 \\pi i \\cdot f(z_0)\\) \\(\\int_C \\frac{f(z)}{(z-z_0)^{n+1}} dz = \\frac{2 \\pi i}{n!} \\times f^{(n)}(z_0)\\) $\\int_C f(z)  dz = 2 \\pi i \\times \\ [\\text{Sum of residues at poles lying inside/on } C]$ add for multiple points \u274c \u2705 \u2705 Theorem Condition Identity add for multiple points Cauchy-Goursat \\(f(z)\\) is analytic inside/on \\(C\\) \\(\\int_C f(z) \\ dz = 0\\) \u274c Cauchy-Integral - \\(f(z)\\) is analytic inside/on \\(C\\)- \\(z_0\\) is a point inside \\(C\\) \\(\\int_C \\frac{f(z)}{z-z_0} dz = 2 \\pi i \\cdot f(z_0)\\) \u2705 Cauchy-Integral for derivatives - \\(f(z)\\) is analytic inside/on \\(C\\)- \\(z_0\\) is a point inside \\(C\\) \\(\\int_C \\frac{f(z)}{(z-z_0)^{n+1}} dz = \\frac{2 \\pi i}{n!} \\times f^{(n)}(z_0)\\) \u2705 Cauchy Residue \\(\\int_C f(z) \\ dz = 2 \\pi i \\times (\\sum R)\\) \u274c <p>\\(\\sum R =\\) Sum of residues at poles lying inside/on \\(C\\)</p>"},{"location":"1_Core/Math_2/09_Complex_Integrals/#residue","title":"Residue","text":"Type \\(R\\) Simple Pole \\(\\lim_{z \\to z_0} (z-z_0) f(z)\\) Pole of order \\(m\\) \\(\\dfrac{1}{m-1} \\times \\dfrac{d^{m-1}}{dz^{m-1}} [(z-z_0)^m f(z)]_{z = z_0}\\) \\(\\dfrac{P(z_0)}{\\textcolor{orange}{Q}(z_0)}, P(z_0) \\ne 0, Q(z_0) = 0\\) \\(\\dfrac{P(z_0)}{\\textcolor{orange}{Q'}(z_0)}\\)"},{"location":"1_Core/Math_2/09_Complex_Integrals/#laurents-series","title":"Laurent\u2019s Series","text":"\\[ \\begin{aligned} f(z) &amp;= \\sum_0^\\infty a_n (z-z_0)^n + \\underbrace{     \\sum_1^\\infty \\frac{b_n}{(z - z_0)^n} }_\\text{Principal Part} \\\\ a_n &amp;= \\frac{1}{2 \\pi i} \\times \\int \\frac{f(z)}{(z-z_0)^{     \\textcolor{orange}{n}+1 }} \\\\ b_n &amp;= \\frac{1}{2 \\pi i} \\times \\int \\frac{f(z)}{(z-z_0)^{     \\textcolor{orange}{-n}+1 }}  \\end{aligned} \\] <p>The following equation is only valid if \\(0 &lt; |z| &lt; 1\\)</p> \\[ \\begin{aligned} (1+z)^{-1} &amp;= 1 - z + z^2 - z^3 + \\dots \\\\ (1-z)^{-1} &amp;= 1 + z + z^2 + z^3 + \\dots \\\\(1+z)^{-2} &amp;= 1 - 2z + 3z^2 - 4z^3 + \\dots \\\\(1-z)^{-2} &amp;= 1 + 2z + 3z^2 + 4z^3 + \\dots \\end{aligned} \\]"},{"location":"1_Core/Math_2/09_Complex_Integrals/#singular-points","title":"Singular Points","text":"<p>Take all \\(n\\) points \\((\\pm n\\pi, \\pm 2n\\pi, \\dots)\\)</p>"},{"location":"1_Core/Math_2/09_Complex_Integrals/#isolated-point","title":"Isolated Point","text":"<p>No other singular point in close neighborhood</p>"},{"location":"1_Core/Math_2/09_Complex_Integrals/#poles","title":"Poles","text":"<p>isolated points are poles too</p> <p>poles of order \\(m=1\\) are simple poles</p>"},{"location":"1_Core/Physics_Lab/","title":"Lab","text":"<p>This course offers a hands-on approach to core-level physics through practical experiments designed for first-year engineering students. It emphasizes the fundamental principles of Mechanics, Waves, and Optics, allowing students to gain practical experience with the equipment and techniques used in scientific experimentation.</p> <p>Here is the demonstration of every experiment in the physics lab. </p> <ol> <li>Collisions - I</li> <li>Collisions - II</li> <li>Collisions - III</li> <li>Atwood's Machine</li> <li>Diffraction for Single Slit</li> <li>Diffraction for Double Slit</li> <li>Conservation of Energy</li> <li>Fine Strucutre</li> <li>Friction</li> <li>Photoelectric Effect</li> <li>Rotational Inertia</li> <li>Vibration</li> </ol>"},{"location":"1_Core/Probability_%26_Statistics/","title":"Probability &amp; Statistics","text":"<p>This course explores probability theory and statistics, emphasizing their relevance to real-world phenomena and applications. Students will engage with both computational and theoretical aspects of probability, gaining skills in handling data sets and understanding statistical inference.</p> <p>The course highlights the diverse applications of probability theory across fields such as mathematical statistical physics, mathematical biology, theoretical computer science, and actuarial science, which evaluates financial risks in insurance and finance. By linking theoretical concepts with practical applications, students will develop a comprehensive understanding of how probability and statistics inform decision-making in various disciplines.</p>"},{"location":"1_Core/Probability_%26_Statistics/#references","title":"References","text":"<ul> <li> Statistics Review | Chris Mack | University of Texas</li> <li> MIT RES.6-012 Introduction to Probability</li> <li> 6.041 Probabilistic Systems Analysis and Applied Probability</li> <li> MIT 18.650 Statistics for Applications, Fall 2016</li> <li> Stanford CS109 Introduction to Probability for Computer Scientists</li> <li> A Student's Guide to Bayesian Statistics | Ben Lambert</li> <li> Statistics Courses | Penn State University</li> </ul>"},{"location":"1_Core/Probability_%26_Statistics/01_Intro/","title":"Introduction","text":""},{"location":"1_Core/Probability_%26_Statistics/01_Intro/#goals","title":"Goals","text":"<ol> <li>Summary statistics: Describe/summarize a large set of data with a few \u2018statistics\u2019</li> <li>Statistical inference: Use sample data to infer population characteristics</li> </ol>"},{"location":"1_Core/Probability_%26_Statistics/01_Intro/#probability-vs-statistics","title":"Probability vs Statistics","text":"<ul> <li>Probability: Predict behavior of sample given known knowledge of population</li> <li>Statistics: Infer properties of population given knowledge of sample</li> </ul> <p>The two are tied together by sampling distribution</p>"},{"location":"1_Core/Probability_%26_Statistics/01_Intro/#approaches","title":"Approaches","text":"Frequentist Bayesian Probability Limiting case of repeated measurements Subjective, based degree of certainty in the event Data Random variable Constant Model parameters Unknown constant Unknown random variable Basis Weak law of large numbersAssumes IID Limitations Not optimal for rare events Intervals Confidence IntervalsWith large number of repeated samples, \\(\\alpha \\%\\) of such calculated confidence intervals would include the true value of the parameter Credible IntervalsEstimated parameter has a \\(95 \\%\\) probability of falling within the given interval Statistics Use prior belief to systematically update knowledge after experiment, through Bayes theorem"},{"location":"1_Core/Probability_%26_Statistics/01_Intro/#formulae","title":"Formulae","text":"\\[ \\begin{aligned} P(S) &amp;=1 \\\\ 0 \\le P(A) &amp;\\le 1 \\\\ P(A') &amp;= 1 - P(A) \\\\ P(A \\cup B) &amp;= P(A) + P(B) - P(A \\cap B) \\\\ P(A \\cup B \\cup C) &amp;= P(A) + P(B) + P(C) - P(A \\cap B) - P(B \\cap C) - P(A \\cap C) + P(A \\cap B \\cap C) \\\\ P(A \\cap B') &amp;= P(A) - P(A \\cap B) \\\\ &amp;= P(A \\cup B) - P(B) \\end{aligned} \\]"},{"location":"1_Core/Probability_%26_Statistics/01_Intro/#cases","title":"Cases","text":"Case Property Mutually-Exclusive \\(P(A \\cap B) = 0\\) Mutually-Exhaustive \\(P(A \\cup B) = 1\\) Independent \\(P(A \\cap B) = P(A) \\cdot P(B)\\) <p>2 events are independent if one event does not affect the occurance of the other</p>"},{"location":"1_Core/Probability_%26_Statistics/01_Intro/#no-of-ways","title":"No of ways","text":"When to use No of ways of selection Product Rule there are \\(k\\) elements, and each have different ways of selection \\(n_1 \\times n_2 \\times \\dots \\times n_k\\) Permutation some sort of ordering \\(nP_r = \\frac{n!}{(n-r)!}\\) Combination \\(nP_r = \\frac{n!}{r!(n-r)!}\\) Indistinguishable Objects there are \\(k\\) objects, such that \\(x_1 + x_2 + \\dots + x_k = n\\), where \\(x_1, x_2, \\dots\\) are the no of elements of that type \\(\\frac{n!}{x_1 ! \\times x_2 ! \\times \\dots \\times x_k !}\\)"},{"location":"1_Core/Probability_%26_Statistics/01_Intro/#conditional-probability","title":"Conditional Probability","text":"<p>Probability of A given B is the probability of A occuring given that A has already occured</p> \\[ P(A|B) = \\frac{P(A \\cap B)}{P(B)} \\quad P(B) \\ne 0 \\]"},{"location":"1_Core/Probability_%26_Statistics/02_Bayes_Theorem/","title":"02 Bayes Theorem","text":""},{"location":"1_Core/Probability_%26_Statistics/02_Bayes_Theorem/#bayes-theorem","title":"Bayes\u2019 Theorem","text":"<p>It determines the probability of an event with uncertain knowledge.  </p> \\[ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} \\] <p>where - \\(P(A|B)\\) = posterior, - \\(P(B|A)\\) = likelihood, - \\(P(A)\\) = prior probability - \\(P(B)\\) = marginal probability </p>"},{"location":"1_Core/Probability_%26_Statistics/02_Bayes_Theorem/#general-formula","title":"General Formula","text":"\\[ \\begin{aligned} P(A_i|B) &amp;= \\frac{P(A_i \\land B)}{P(B)} \\\\ &amp;= \\frac{P(B | A_i) \\cdot P(A_i)}{\\sum\\limits_{j=1}^{n} P(B|A_j) \\cdot P(A_j)} \\\\ \\end{aligned} \\] <p>where \\(A_1, A_2, \\dots, A_n\\) are all mutually exclusive events</p>"},{"location":"1_Core/Probability_%26_Statistics/02_Bayes_Theorem/#conditions","title":"Conditions","text":"<ol> <li>Events must be disjoint (no overlapping)</li> <li>Events must be exhaustive: they combine to include all possibilities</li> </ol>"},{"location":"1_Core/Probability_%26_Statistics/02_Bayes_Theorem/#phrases","title":"Phrases","text":"<ul> <li>\u201cout of\u201d</li> <li>\u201cof those who\u201d</li> </ul>"},{"location":"1_Core/Probability_%26_Statistics/02_Bayes_Theorem/#_1","title":"02 Bayes Theorem","text":""},{"location":"1_Core/Probability_%26_Statistics/02_Random_Variables/","title":"Random Variables","text":""},{"location":"1_Core/Probability_%26_Statistics/02_Random_Variables/#types-of-random-numbers","title":"Types of Random Numbers","text":"Can be produced by computers Easy to implement Truly Random \u274c Quasi-Random \u2705 \u274c Pseudo-Random \u2705 \u2705"},{"location":"1_Core/Probability_%26_Statistics/02_Random_Variables/#random-distribution-functions","title":"Random Distribution Functions","text":"PDF Probability Density Function CDF Cumulative Density Function"},{"location":"1_Core/Probability_%26_Statistics/02_Random_Variables/#central-limit-theorem","title":"Central Limit Theorem","text":"<p>PDF of sample mean with sample size \\(n&gt;30\\) tends to normal distribution, regardless of what the underlying distribution is</p> \\[ \\bar x \\sim N \\left(\\mu, \\frac{\\sigma^2}{n} \\right) \\] <p>Interpretation: Given a sufficiently large sample</p> <ol> <li>Mean of sample means \\(\\approx\\) normal-distribution</li> <li>Mean of sample means \\(\\approx\\) population mean</li> <li>Variance of sample means \\(\\approx\\) Population Variance/Sample Size</li> </ol>"},{"location":"1_Core/Probability_%26_Statistics/02_Random_Variables/#moment-generating-function","title":"Moment-Generating Function","text":"<p>For a random variable \\(x\\) $$ \\begin{aligned} M_x(t) &amp;= E[ e^{tx} ] \\ \\implies \\underbrace{\\dfrac{d^{(k)} M_x}{dt^{(k)}} (0)}{\\text{k th derivative } } &amp;= \\underbrace{E(x^k)} \\ \\implies M_x(t) &amp;= \\sum_{k=0}^\\infty \\dfrac{t^k}{k!} m_k, &amp; m_k = E(x^k) \\ t &amp;\\in R, k \\in Z \\end{aligned} $$ Note: Does not exist for all distributions (for eg: Log-Normal) $$ \\begin{aligned} x, y \\text{ have same dist} &amp;\\iff M_x(t) = M_y(t) \\ x, y \\text{ have same dist} &amp;\\implies {m_k}_x = {m_k}_y &amp; \\text{(Converse not necessarily true)} \\end{aligned} $$ For a sequence of random variables $x_1, x_2, \\dots, $}</p> \\[ M_{X_i}(t) \\to M_X(t) \\implies P(X_i \\le x) \\to P(X \\le x) \\]"},{"location":"1_Core/Probability_%26_Statistics/02_Random_Variables/#large-of-large-numbers","title":"Large of Large Numbers","text":"<p>Consider iid rv \\(x_1, \\dots, x_n\\) with mean and variance \\(\\mu, \\sigma^2\\) $$ x = \\dfrac{\\sum x_i}{n} , n \\to \\infty \\implies E(x) \\to \\mu_x $$</p> <ul> <li>This is how casinos\u2019 make money for blackjack, as they have a higher expected value compared to the player</li> <li>But does not apply for Poker, as the casino makes money from round fees, since Poker is played against players, not the casino</li> </ul> \\[ P(\\vert X-\\mu \\vert \\ge \\epsilon) \\le \\dfrac{\\sigma^2}{n \\epsilon^2} \\]"},{"location":"1_Core/Probability_%26_Statistics/02_Random_Variables/#averaging-distributions","title":"Averaging Distributions","text":"<p>Given \\(n\\) identically-distributed RVs with variance \\(\\sigma^2\\) and correlation \\(\\rho\\), the variance of the mean is $$ {\\sigma^2}' = \\rho \\sigma^2 + (1 - \\rho)\\dfrac{\\sigma^2}{n} $$</p>"},{"location":"1_Core/Probability_%26_Statistics/03_Discrete_Random_Variables/","title":"03 Discrete Random Variables","text":""},{"location":"1_Core/Probability_%26_Statistics/03_Discrete_Random_Variables/#discrete-random-variables","title":"Discrete Random Variables","text":"<p>takes finite/countably-infinite no of values</p>"},{"location":"1_Core/Probability_%26_Statistics/03_Discrete_Random_Variables/#pdf","title":"PDF","text":"\\[ \\begin{aligned} f(x) &amp;= P(X = x) \\\\ f(x) &amp;\\ge 0 \\\\ \\sum f(x) &amp;= 1 \\end{aligned} \\]"},{"location":"1_Core/Probability_%26_Statistics/03_Discrete_Random_Variables/#cdf","title":"CDF","text":"\\[ \\begin{aligned} F(x) &amp;= P(X \\le x) \\\\ &amp;= \\sum\\limits_0^x f(x) \\\\ P(a \\le X \\le b) &amp;= \\sum\\limits_a^b f(x) \\end{aligned} \\]"},{"location":"1_Core/Probability_%26_Statistics/03_Discrete_Random_Variables/#terms","title":"Terms","text":"Notation Formula \\(E(x)\\) \\(\\mu\\) \\(\\sum x \\cdot f(x)\\) \\(E(x^2)\\) \\(\\sum x^2 \\cdot f(x)\\) \\(V(x)\\) \\(\\sigma^2\\) \\(E(x^2) - [E(x)]^2\\) \\(\\text{SD}(x)\\) \\(\\sigma\\) \\(\\sqrt {V(x)}\\) Normalised Variable \\(z\\) \\(\\dfrac{x - E(x)}{\\text{SD}}\\) \\[ \\begin{aligned} E(k) &amp;= k &amp; E(kx) &amp;= k \\cdot E(x) &amp; E(z) &amp;= 0\\\\ V(k) &amp;= 0 &amp; V(kx) &amp;= k^2 \\cdot V(x) &amp; V(z) &amp;= 1 \\end{aligned} \\]"},{"location":"1_Core/Probability_%26_Statistics/03_Discrete_Random_Variables/#distributions","title":"Distributions","text":"Distribution \\(f(x)\\) \\(\\mu\\) \\(V(x)\\) Bernoulli - 2 outcomes- independent &amp; identical trial \\(p\\) \\(p(1-p)\\) Binomial \\(n\\) indepedent Bernoulli events w/ replacement \\(nC_x \\cdot p^x \\cdot (1-p)^{n-x}\\) \\(np\\) \\(np(1-p)\\) Hypergeometric \\(n\\) dependent Bernoulli trials without replacement \\(f(x) = \\frac{MC_x \\times (N-M) C_{(n-x)} }{NC_n}\\) \\(\\text{max}\\Big(0, n- (N-m) \\Big) \\le x \\le \\text{min}(n, M)\\) \\(n \\left(\\dfrac M N \\right)\\) \\(\\left( \\dfrac{N-n}{N-1} \\right) \\cdot n \\cdot \\dfrac M N \\left( 1 - \\dfrac M N \\right)\\) Negative Binomial \\(p=\\) Probability of success after \\((r-1)\\) failures \\(f_x(x) = \\begin{cases} \\begin{pmatrix} x-1\\\\ r-1 \\end{pmatrix} p^r q^{x-r}, &amp; x= r, r+1, \\dots  \\\\ 0, &amp; \\text{o.w.} \\end{cases}\\) \\(\\dfrac{rq}{p}\\) \\(\\dfrac{rq}{p^2}\\) Geometric Negative binomial dist with \\(r=1\\)No of failures before first success Poisson discrete phenomenon in continuous intervalPoisson dist can simulate binomial dist with small value of \\(p\\) \\(\\dfrac {e^{-\\mu} \\times \\mu^x}{x!}\\) \\(\\alpha t\\) \\(\\alpha t\\)"},{"location":"1_Core/Probability_%26_Statistics/03_Discrete_Random_Variables/#rate-parameter-alpha","title":"Rate Parameter \\((\\alpha)\\)","text":"<p>occurences per unit interval</p> <p>\\(\\alpha = \\dfrac 1 \\beta\\)</p> <p>(\\(\\beta\\) will be discussed in next topic)</p>"},{"location":"1_Core/Probability_%26_Statistics/04_Continuous_Random_Variables/","title":"Continuous Random Variable","text":"<p>takes value on a continuum of scale, ie, can take any decimal value</p>"},{"location":"1_Core/Probability_%26_Statistics/04_Continuous_Random_Variables/#pdf","title":"PDF","text":"\\[ \\begin{aligned} f(x) &amp;\\ge 0 \\\\ \\int f(x) \\ \\mathrm{d} x &amp;= 1 \\end{aligned} \\]"},{"location":"1_Core/Probability_%26_Statistics/04_Continuous_Random_Variables/#cdf","title":"CDF","text":"\\[ \\begin{aligned} F(x) &amp;= P(X \\le x) \\\\ &amp;= \\int\\limits_{- \\infty}^x f(x) \\ \\mathrm{d} x \\\\ P(a \\le X \\le b) &amp;= P(a &lt; x &lt; b) \\\\ &amp;= \\int\\limits_a^b f(x) \\ \\mathrm{d} x \\end{aligned} \\]"},{"location":"1_Core/Probability_%26_Statistics/04_Continuous_Random_Variables/#terms","title":"Terms","text":"Formula \\(E(x)\\) \\(\\int x \\cdot f(x) \\ \\mathrm{d} x\\) \\(E(x^2)\\) \\(\\int x^2 \\cdot f(x) \\ \\mathrm{d} x\\) <p>(others are the same as discrete)</p>"},{"location":"1_Core/Probability_%26_Statistics/04_Continuous_Random_Variables/#distributions","title":"Distributions","text":"Distribution Comment \\(f(x)\\) \\(\\mu\\) \\(\\sigma^2(x)\\) Skewness Kurtosis Modality Symmetry Diagram Uniform \\(\\begin{cases} \\frac 1 {B-A} &amp; A \\le x \\le B \\\\ 0 &amp; \\text{elsewhere} \\end{cases}\\) \\(\\dfrac {B+A} 2\\) \\(\\dfrac 1 {12} (B-A)^2\\) \u2705 Normal/Gaussian/Bell-Curve/\\(z\\) \\(\\dfrac {1}{\\sigma \\sqrt{2\\pi}} \\exp \\left\\{ \\dfrac {-1}{2} \\left(\\dfrac{x-\\mu}{\\sigma} \\right)^2 \\right\\}\\) \\(\\begin{aligned} P(x&lt;k) &amp;= P \\left(z&lt;\\frac{k-\\mu}{\\text{SD}} \\right) \\\\ P(x&gt;k) &amp;= P(x &lt; -k) \\end{aligned}\\) 0 1 0 3 1 \u2705 Gumbel/Type 1 Extreme Value Normal distribution with skew and fatter tails \\(\\exp \\Bigg[ - \\exp \\left( \\dfrac{-(x-\\mu)}{\\sigma} \\right ) \\Bigg]\\) \\(\\mu + \\sigma \\gamma_e\\)\\(\\gamma_e \\approx 0.577\\) (Euler\u2019s constant) \\(\\dfrac{\\pi^2 \\sigma^2}{6}\\) Log-Normal Type of gumbel distribution Student\\(t\\) Tends to normal distribution for large dof \\(\\dfrac{\\bar x-\\mu}{s/\\sqrt{n}}\\) 0 &gt;1 \u2705 Binomial \\(\\to\\) Normal Approx \\(np \\ge 10\\) or \\(n(1-p) \\ge 10\\) Normal distribution \\(\\begin{aligned} x' &amp;= x \\pm 0.5 \\\\ z &amp;= \\frac{x' - \\mu}{\\text{SD}} \\end{aligned}\\) \\(np\\) \\(np(1-p)\\) Chi-Square\\(\\chi^2\\) PDF of \\(\\sum \\limits_i N_i(\\mu_i, \\sigma_i)^2\\), where \\(N_i\\) is independent of \\(N_j, \\ \\forall i \\ne j\\) \\(\\dfrac{(n-k)s^2}{\\sigma^2}\\)\\(\\lambda = \\sum_{i=1}^n \\mu_i^2\\) \\((n-k)+\\lambda\\) \\(2[(n-k) + 2 \\lambda]\\) Gamma time between \\(n\\) occurrences \\(\\dfrac{1}{B^\\alpha \\lceil\\alpha} \\cdot x^{\\alpha-1} \\cdot \\exp \\left(\\dfrac{-x}{\\beta} \\right)\\) \\(\\alpha \\beta\\) \\(\\alpha \\beta^2\\) Exponential time between successive/consecutive \\(\\lambda \\cdot \\exp(-\\lambda x)\\) \\(\\dfrac 1 \\lambda  = \\beta\\) \\(\\dfrac 1 {\\lambda^2} = \\beta^2\\) Power Law \\(L(x) \\cdot x^{-(\\alpha-1)}; x &gt; x_\\min\\) Pareto Power law with \\(\\alpha =1.16\\)Average value of those whose value is greater than \\(y\\) is \\(y\\) times the constant \\(\\lambda/(\\lambda-1)\\)\\(\\lambda\\) controls the thickness of tail \\(P(X &gt; x) = \\begin{cases} (x_m/x)^\\lambda, &amp; x \\ge x_m \\\\ 1, &amp; x &lt; x_m \\end{cases}\\)Top \\(q\\)th percentiles share = \\((q/100)^{(\\lambda-1)/\\lambda}\\) \\(1 - F(x) = \\bar F(x) = P(X&gt;x)\\) Size distributionSizes of citiesIncomeFamily namesPopularitySocial network patternsCrime per convictSizes of large earthquakesPower outages Zipf Pareto with \\(\\lambda=1\\) Empirical city sizeFirm sizeEquivalent to relationship of slope of -1 between log rank of city (based on city size) and log of population Laplacian/Double-Exponential Distribution of diff of two iid exponential vars \\(\\dfrac{1}{2b} \\exp \\left( \\dfrac{- \\vert x-\\mu \\vert}{b} \\right)\\) Logistic/Sigmoid \\(\\dfrac{1}{b} \\times \\dfrac{\\exp \\left(\\dfrac{-(x-\\mu)}{b} \\right)}{1+\\exp \\left(\\dfrac{- (x-\\mu )}{b} \\right)}\\) \\(F(x) = \\dfrac{1}{1+\\exp \\left(\\dfrac{-(x-\\mu)}{b} \\right)}\\) \\(0\\) \\(b\\dfrac{\\pi}{\\sqrt{3}}\\) \\(3+1.2\\) <ul> <li>DOF = Degrees of freedom</li> <li>\\(n\\) for sampling</li> <li> <p>\\(n-k-1\\) for regression</p> </li> <li> <p>\\(\\lambda\\) = mean no of occurances per unit time   \\(\\lambda = \\alpha\\text{(poisson)}\\)</p> </li> <li>\\(\\beta\\) = mean time b/w occurances   \\(\\beta = \\frac 1 \\lambda = \\frac 1 {\\alpha\\text{(poisson)}}\\)</li> <li>\\(\\alpha\\) = shape parameter   it is the average number of occurrences of an event</li> </ul> <p>Pareto Distribution</p>"},{"location":"1_Core/Probability_%26_Statistics/05_Joint_Distributions/","title":"05 Joint Distributions","text":"\\[ \\begin{aligned} f(x, y) &amp;= P(X=x, Y=y) \\\\ &amp;= P(x \\cap y) \\\\ F(x,y) &amp;= P(X \\le x, Y \\le y) \\\\ f(x, y) &amp;\\ge 0 \\\\ f(x|y) &amp;= \\frac{f(x, y)}{f(y)} \\end{aligned} \\] Discrete Continuous PDF \\(\\sum\\limits_x \\sum\\limits_y f(x, y) = 1\\) \\(\\int\\limits_x \\int\\limits_y f(x, y) \\ \\mathrm{d} y \\mathrm{d} x = 1\\) CDF \\(\\sum\\limits_0^x \\sum\\limits_0^y f(x, y)\\) \\(\\int\\limits_{- \\infty}^x \\int\\limits_{- \\infty}^y f(x, y) \\ \\mathrm{d} y \\mathrm{d} x\\) \\(f(x)\\) \\(\\sum\\limits_y f(x,y)\\) \\(f(x) = \\int\\limits_y f(x,y) \\ \\mathrm{d} y \\mathrm{d} x\\) \\(f(y)\\) \\(\\sum\\limits_x f(x,y)\\) \\(\\int\\limits_x f(x,y) \\ \\mathrm{d} y \\mathrm{d} x\\) \\(E(x, y)\\) \\(\\sum\\limits_x \\sum\\limits_y xy \\cdot f(x, y)\\) \\(\\int\\limits_x \\int\\limits_y xy \\cdot f(x, y) \\ \\mathrm{d} y \\mathrm{d} x\\) \\(E(x)\\) \\(\\sum\\limits_x x \\cdot f(x,y)\\) \\(\\int\\limits_x x \\cdot f(x,y) \\ \\mathrm{d} y \\mathrm{d} x\\) \\(E(y)\\) \\(\\sum\\limits_y y \\cdot f(x,y)\\) \\(\\int\\limits_y y \\cdot f(x,y) \\ \\mathrm{d} y \\mathrm{d} x\\)"},{"location":"1_Core/Probability_%26_Statistics/05_Joint_Distributions/#covariance","title":"Covariance","text":"\\[ \\begin{aligned} \\text{Cov} (x,y) &amp;= E(x,y) - E(x) \\cdot E(y) \\\\ &amp;=  \\begin{cases} &gt;0 &amp; \\text{directly-dependent} \\\\ 0 &amp; \\text{independent}\\\\&lt;0 &amp; \\text{inversely-dependent} \\end{cases} \\end{aligned} \\]"},{"location":"1_Core/Probability_%26_Statistics/05_Joint_Distributions/#independence","title":"Independence","text":"\\[ \\begin{aligned} f(x,y) &amp;= f(x) \\cdot f(y) \\\\ E(x, y) &amp;= E(x) \\cdot E(y) \\\\ \\text{Cov}(x,y) &amp;= 0 \\end{aligned} \\]"},{"location":"1_Core/Probability_%26_Statistics/06_Sampling_Distributions/","title":"Sampling","text":"<p>Used when it is not feasible to analyze the entire population</p> <p>Estimation: Using the sample to estimate population parameter(s)</p>"},{"location":"1_Core/Probability_%26_Statistics/06_Sampling_Distributions/#population-v-sample","title":"Population v Sample","text":"Property Population Sample Definition comprises of all units pertaining to a particular characteristic under study is a part of a population, which is selected such that it is representative of the entire population Size \\(N\\) \\(n\\) Mean \\(\\mu\\) \\(\\bar x = \\dfrac {\\sum_i^n x_i}{n}\\) Variance \\(\\sigma^2\\) \\(s^2 = \\dfrac {\\sum_i^n (x_i-\\bar x)^2}{n \\textcolor{hotpink}{-1}}\\) Standard Deviation \\(\\sigma\\) \\(s\\)"},{"location":"1_Core/Probability_%26_Statistics/06_Sampling_Distributions/#relations","title":"Relations","text":"\\[ \\begin{aligned} \\mathbb E(\\bar x) &amp;= \\mu \\\\ \\mathbb E[s^2_x] &amp;= \\sigma^2_x \\\\ \\\\ s^2_{\\bar x} &amp;= \\frac{\\sigma^2_x}{n} , s_{\\bar x} = \\frac{\\sigma_x}{\\sqrt n} \\\\ z_\\text{sample} &amp;= \\frac{\\bar x - \\mu_x}{\\sigma_x/\\sqrt n } \\end{aligned} \\]"},{"location":"1_Core/Probability_%26_Statistics/06_Sampling_Distributions/#bessels-correction","title":"Bessel\u2019s Correction","text":"<p>$$ \\begin{aligned} \\text{Var}(x) &amp;= E[(x)^2] - (E[x])^2 \\ \\implies E[(x)^2] &amp;= \\sigma^2 + \\mu^2 \\ \\ \\text{Var}(\\bar x) &amp;= E[(\\bar x)^2] - (E[\\bar x])^2 \\ \\implies E[(\\bar x)^2] &amp;= \\dfrac{\\sigma^2}{n} + \\mu^2 \\ \\ \\implies \\sigma^2 &amp;= s^2_\\text{uncorrected} + \\text{Bias} \\ &amp;= s^2_\\text{uncorrected} + \\dfrac{\\sigma^2}{n} \\</p> <p>\\implies \\sigma^2 &amp;= s^2_\\text{uncorrected} \\times \\dfrac{n}{\\text{DOF}} \\ &amp;= s^2_\\text{uncorrected} \\times \\underbrace{\\dfrac{n}{n-1} }_{\\mathclap{\\text{Bessel's Correction}}} \\end{aligned} $$</p> <p>Reasoning</p> <ul> <li>Degrees of freedom: We lose a degree of freedom when estimating \\(\\bar x\\)</li> <li>Bias correction: While sampling with small sample size, less probable elements don\u2019t show up which gives us an underestimated sample dispersion</li> </ul>"},{"location":"1_Core/Probability_%26_Statistics/06_Sampling_Distributions/#sample-vs-population-standard-deviation","title":"Sample vs Population Standard Deviation","text":""},{"location":"1_Core/Probability_%26_Statistics/06_Sampling_Distributions/#for-different-distributions","title":"For Different Distributions","text":"<p>Higher the skew of population distribution, larger the sample size required to approximate the sample size to the population</p>"},{"location":"1_Core/Probability_%26_Statistics/06_Sampling_Distributions/#for-the-different-population-size","title":"For the different population size","text":"<p>Sample vs Population SD does not depend on population size</p>"},{"location":"1_Core/Probability_%26_Statistics/06_Sampling_Distributions/#interval-estimation","title":"Interval Estimation","text":"<p>Confidence % \\(= 1- \\alpha\\)</p> <p>Most common is \\(95\\%\\) confidence interval estimate</p> \\[ \\begin{aligned} 1 - \\alpha &amp;= 0.95 \\\\ \\alpha &amp;= 0.05 \\\\ \\alpha/\\small 2 &amp;= 0.025 \\end{aligned} \\]"},{"location":"1_Core/Probability_%26_Statistics/06_Sampling_Distributions/#population-mean","title":"Population mean","text":"\\(\\sigma^2\\) \\(n\\) statistic \\(\\mu\\) known any \\(z = \\dfrac {\\bar x - \\mu}{\\sigma / \\sqrt n}\\) \\(\\bar x \\pm z_{\\alpha/\\small 2} \\cdot \\dfrac \\sigma {\\sqrt n}\\) unknown \\(&gt;30\\) \\(z = \\dfrac {\\bar x - \\mu}{s/ \\sqrt n}\\) \\(\\bar x \\pm z_{\\alpha/\\small 2} \\cdot \\dfrac s {\\sqrt n}\\) unknown \\(\\le 30\\) \\(t = \\dfrac {\\bar x - \\mu}{s / \\sqrt n}\\) \\(\\bar x \\pm t_{\\small n-1, \\alpha/\\small 2} \\cdot \\dfrac s {\\sqrt n} \\\\(n-1) \\to \\text{deg of freedom}\\) \\[ \\begin{aligned} n &amp;= \\left( \\frac{z_{\\alpha/\\small 2} \\cdot \\sigma}{w} \\right)^2 \\\\ &amp;= \\left( \\frac{z_{\\alpha/\\small 2} \\cdot s}{w} \\right)^2 \\end{aligned} \\] <p>where</p> <ul> <li>\\(n\\) is sample size</li> <li>\\(w\\) is distance from \\(\\mu\\) = \\(\\frac{\\text{interval width}}{2}\\)</li> </ul>"},{"location":"1_Core/Probability_%26_Statistics/06_Sampling_Distributions/#proportion","title":"Proportion","text":"\\[ \\begin{aligned} p &amp;= \\hat p \\pm z_{\\alpha/\\small2} \\sqrt {\\frac{\\hat p \\hat q}{n}} \\\\ \\hat p &amp;= \\frac x n = \\frac{\\text{Favorable no of cases}}{\\text{Total no of cases}} \\\\ \\hat q &amp;= 1 - \\hat p \\end{aligned} \\]"},{"location":"1_Core/Probability_%26_Statistics/06_Sampling_Distributions/#population-variance-sd","title":"Population Variance / SD","text":"\\[ \\begin{aligned} \\sigma^2 &amp;= \\left[ \\frac{(n-1)s^2}{\\chi^2_{(n-1), (\\alpha/\\small 2)}}, \\frac{(n-1)s^2}{\\chi^2_{(n-1), (1-\\alpha/\\small 2)}} \\right] \\\\ \\sigma &amp;= \\sqrt {\\sigma^2} \\end{aligned} \\]"},{"location":"1_Core/Probability_%26_Statistics/06_Sampling_Distributions/#inequalities","title":"Inequalities","text":"<p>Let \\(x\\) be a random variable such that \\(x_i \\in [a, b]\\)</p> <p>Consider</p> <ul> <li>sample size \\(n\\)</li> <li>\\(\\epsilon &gt; 0\\)</li> </ul>"},{"location":"1_Core/Probability_%26_Statistics/06_Sampling_Distributions/#hoeffdings-inequality","title":"Hoeffding\u2019s Inequality","text":"\\[ \\begin{aligned} P (\\vert \\hat \\mu \u2212 \\mu \\vert &gt; \\epsilon) &amp; \\le 2 \\exp \\left[ \\dfrac{-2 n \\epsilon^2}{(b-a)^2} \\right] \\\\ \\sum_{b}^B P (\\vert \\hat \\mu_b \u2212 \\mu_b \\vert &gt; \\epsilon) &amp; \\le 2 \\exp \\left[ \\dfrac{-2 n \\epsilon^2}{(b-a)^2} \\right] \\times B \\end{aligned} \\] <p>where</p> <ul> <li>\\(\\mu\\) is any parameter and \\(\\hat \\mu\\) is its estimate</li> <li>\\(n&gt;0\\)</li> <li>\\(\\epsilon &gt; 0\\)</li> <li>\\(B=\\) no of \u2018bins\u2019</li> </ul> <p>Notes</p> <ul> <li>We want low \\(P (\\vert \\hat \\mu \u2212 \\mu \\vert &gt; \\epsilon)\\)</li> <li>Even though \\(P (\\vert \\hat \\mu \u2212 \\mu \\vert &gt; \\epsilon)\\) will depend on \\(\\mu\\), the bound is independent of \\(\\mu\\)</li> </ul>"},{"location":"1_Core/Probability_%26_Statistics/06_Sampling_Distributions/#vapnik-chervonenkis-inequality","title":"Vapnik-Chervonenkis Inequality","text":"\\[ P (\\vert \\bar x \u2212 \\mu \\vert &gt; \\epsilon) \\le 4 \\cdot m_h(2n) \\cdot \\exp \\left[ \\dfrac{-1}{8} n \\epsilon^2 \\right] \\] <p>Where \\(m_h(n) = 2^n\\)</p>"},{"location":"1_Core/Probability_%26_Statistics/07_Testing_of_Hypothesis/","title":"Testing of Hypothesis","text":"<p>\\(\\alpha\\)</p> <ul> <li>level of significance</li> <li>size of critical region</li> </ul> <p>Confidence level = \\((1-\\alpha) \\times 100 \\%\\)</p> <p>The entire distribution is divided into 2 regions</p> <ol> <li>Critical Region    Region of rejection of \\(H_0\\)    it is decided based on \\(H_1\\)</li> <li>Acceptance Region    Region of acceptance of \\(H_0\\)</li> </ol>"},{"location":"1_Core/Probability_%26_Statistics/07_Testing_of_Hypothesis/#population-mean","title":"Population Mean","text":"\\[ \\begin{aligned} H_0: \\mu &amp;= \\mu_0 &amp; &amp;\\text{(Null Hypothesis)} \\\\ H_1: \\mu &amp;&lt; \\mu_0, \\mu \\ne \\mu_0, \\mu &gt; \\mu_0 &amp; &amp;\\text{(Alternative Hypothesis)} \\\\ \\end{aligned} \\] \\(\\sigma^2\\) \\(n\\) Test Statistic/Probability Distribution known any \\(z_c = \\frac{\\bar x - \\mu_0}{\\sigma/\\sqrt n}\\) unknown \\(&gt;30\\) \\(z_c = \\frac{\\bar x - \\mu_0}{s/ \\sqrt n}\\) unknown \\(\\le 30\\) \\(t_c = \\frac{\\bar x - \\mu_0}{s / \\sqrt n}\\)"},{"location":"1_Core/Probability_%26_Statistics/07_Testing_of_Hypothesis/#critical-region","title":"Critical Region","text":"Left-Tailed Two-Tailed Right-Tailed \\(H_1\\) \\(\\mu &lt; \\mu_0\\) \\(\\mu \\ne \\mu_0\\) \\(\\mu &gt; \\mu_0\\) p-value \\(F(z_c)\\) \\(\\alpha(t-\\text{dist})\\) \\(2[ F(-z_c) ]\\) \\(2 \\alpha(t-\\text{dist})\\) \\(F(-z_c)\\) \\(\\alpha(t-\\text{dist})\\) Cases Accept \\(H_1\\) if \\(\\begin{aligned} z_c &amp; \\le -z_\\alpha \\\\ t_c &amp;\\le -t_{(n-1), \\alpha} \\\\ p &amp;\\le \\alpha \\end{aligned}\\)else accept \\(H_0\\) Accept \\(H_1\\) if \\(\\begin{aligned} z_c \\le -z_{\\alpha/2} &amp;\\text{ or } z_c \\ge +z_{\\alpha/2}\\\\ t_c \\le -t_{(n-1), (\\alpha/2)} &amp;\\text{ or } t_c \\ge +t_{(n-1), (\\alpha/2)} \\\\ p &amp;\\le \\alpha \\end{aligned}\\)else accept \\(H_0\\) Accept \\(H_1\\) if \\(\\begin{aligned} z_c &amp;\\ge +z_\\alpha \\\\ t_c &amp;\\ge +t_{(n-1), \\alpha} \\\\ p &amp;\\le \\alpha \\end{aligned}\\)else accept \\(H_0\\)"},{"location":"1_Core/Probability_%26_Statistics/07_Testing_of_Hypothesis/#proportion","title":"Proportion","text":"\\[ \\begin{aligned} H_0: p &amp;= p_0 &amp; &amp;\\text{(Null Hypothesis)} \\\\ H_1: p &amp;&lt; p_0, p \\ne p_0, p &gt; p_0 &amp; &amp;\\text{(Alternative Hypothesis)} \\\\ z_c &amp;= \\frac{\\hat p - p_0}{     \\sqrt{ \\frac{p_0(1-p_0)}{n} } } &amp; &amp; \\hat p = \\frac x n = \\text{Estimated value of } p\\\\ \\end{aligned} \\]"},{"location":"1_Core/Probability_%26_Statistics/07_Testing_of_Hypothesis/#critical-region_1","title":"Critical Region","text":"Left-Tailed Two-Tailed Right-Tailed \\(H_1\\) \\(p &lt; p_0\\) \\(p \\ne p_0\\) \\(p &gt; p_0\\) p-value \\(F(z_c)\\) \\(2[ F(-z_c) ]\\) \\(F(-z_c)\\) Cases Accept \\(H_1\\) if \\(\\begin{aligned}z_c &amp;\\le -z_\\alpha \\\\ p &amp;\\le \\alpha \\end{aligned}\\)else accept \\(H_0\\) Accept \\(H_1\\) if \\(\\begin{aligned} z_c \\le -z_{\\alpha/2} &amp;\\text{ or } z_c \\ge +z_{\\alpha/2} \\\\ p &amp;\\le \\alpha \\end{aligned}\\)else accept \\(H_0\\) Accept \\(H_1\\) if \\(\\begin{aligned} z_c &amp;\\ge +z_\\alpha \\\\ p &amp;\\le \\alpha \\end{aligned}\\)else accept \\(H_0\\)"},{"location":"1_Core/Probability_%26_Statistics/07_Testing_of_Hypothesis/#variancesd","title":"Variance/SD","text":"\\[ \\begin{aligned} H_0: \\sigma^2 &amp;= \\sigma^2_0 &amp; &amp;\\text{(Null Hypothesis)} \\\\ H_1: \\sigma^2 &amp;&lt; \\sigma^2_0, \\sigma^2 \\ne \\sigma^2_0, \\sigma^2 &gt; \\sigma^2_0 &amp; &amp;\\text{(Alternative Hypothesis)} \\\\ \\chi_c^2 &amp;= (n-1) \\frac{s^2}{\\sigma_0^2} \\end{aligned} \\]"},{"location":"1_Core/Probability_%26_Statistics/07_Testing_of_Hypothesis/#critical-region_2","title":"Critical Region","text":"Left-Tailed Two-Tailed Right-Tailed \\(H_1\\) \\(p &lt; p_0\\) \\(p \\ne p_0\\) \\(p &gt; p_0\\) p-value 1 - \\(\\alpha\\)(table) 1 - \\(\\alpha\\)(table) 1 - \\(\\alpha\\)(table) Cases Accept \\(H_1\\) if \\(\\begin{aligned}\\chi_c^2 &amp;\\le \\chi^2_{(n-1), (1-\\alpha)}  \\\\ p &amp;\\le \\alpha \\end{aligned}\\)else accept \\(H_0\\) Accept \\(H_1\\) if \\(\\begin{aligned}\\chi_c^2 \\le \\chi^2_{(n-1), (1-\\alpha/2)} &amp;\\text{ or } \\chi_c^2 \\ge \\chi^2_{(n-1), (\\alpha/2)} \\\\ p &amp;\\le \\alpha \\end{aligned}\\)else accept \\(H_0\\) Accept \\(H_1\\) if \\(\\begin{aligned}\\chi_c^2 &amp;\\ge \\chi^2_{(n-1), \\alpha}  \\\\ p &amp;\\le \\alpha \\end{aligned}\\)else accept \\(H_0\\)"},{"location":"1_Core/Probability_%26_Statistics/07_Testing_of_Hypothesis/#errors","title":"Errors","text":"\\(H_0\\) is true \\(H_0\\) is false \\(H_0\\) is incorrect Reject \\(H_0\\) Type 1 Error = \\(\\alpha\\) Correct Type 3 ErrorRight answer to the wrong question Accept \\(H_0\\) Correct Type 2 Error = \\(\\beta\\) <p>Type 1 error is alright, but Type 2 error is dangerous</p> <ul> <li>\\(\\alpha\\) = P(reject \\(H_0\\) | \\(H_0\\) is true)</li> <li>\\(\\beta\\) = P(accept \\(H_0\\) | \\(H_0\\) is false)</li> </ul>"},{"location":"1_Core/Probability_%26_Statistics/07_Testing_of_Hypothesis/#power-of-test","title":"Power of Test","text":"\\[ \\text{Power of Test} = 1 - \\beta \\] <p>Greater the power of test, the better means that we can more accurately detect when \\(H_0\\) is false</p>"},{"location":"1_Core/Probability_%26_Statistics/08_Regression/","title":"Regression","text":"<p>used to predict for the dependent variable on the basis of past information available on dependent and independent variables.</p> <p>The estimated regression line is given by</p> \\[ \\begin{aligned} \\hat y &amp;= b_0 + b_1 x \\\\ b_1 &amp;= \\frac{     n \\ \\sum (xy) - \\sum x \\sum y }{     n \\ \\sum x^2 - \\Big( \\sum x \\Big)^2 } \\\\ b_0 &amp;= \\bar y - b_1 \\bar x \\\\ \\bar x &amp;= \\frac{\\sum x} n \\\\ \\bar y &amp;= \\frac{\\sum y} n \\end{aligned} \\] Term Meaning \\(y\\) dependent variable \\(x\\) independent variable \\(b_0\\) y-intercept \\(b_1\\) slope \\(\\hat y\\) estimated value \\(\\bar x\\) mean of \\(x\\) \\(\\bar y\\) mean of \\(y\\)"},{"location":"1_Core/Probability_%26_Statistics/08_Regression/#correlation","title":"Correlation","text":"<p>gives the degree of linear relationship between 2 vars</p> <p>Properties</p> <ul> <li>Dimensionless</li> <li>Symmetric: \\(r(x, y)=r(y, x)\\)</li> <li>\\(r \\in [-1, +1]\\)</li> </ul> \\[ r(x, y) = \\dfrac{\\text{cov}(x, y)}{\\sigma_x \\sigma_y} \\]"},{"location":"1_Core/Probability_%26_Statistics/08_Regression/#pearsons-correlation","title":"Pearson\u2019s Correlation","text":"<p>Also called product moment correlation $$ \\begin{aligned} r &amp;= \\dfrac{1}{n-1} \\sum_{i=1}^n z_{xi} z_{yi} \\ &amp;= \\dfrac{     \\sum (x_i - \\bar x)(y_i - \\bar y) }{ \\sqrt{\\sum (x_i - \\bar x)^2 \\sum (y_i - \\bar y)^2} } \\ &amp;= \\dfrac{     n \\sum(xy) - \\sum x \\sum y }{     n     \\sqrt{\\sum (x^2) - \\big(\\sum x \\big)^2 }     \\sqrt{ \\sum (y^2) - \\big(\\sum y \\big)^2 } } \\end{aligned} $$</p> <p>Measures whether 2 vars are above/below mean at the same time</p>"},{"location":"1_Core/Probability_%26_Statistics/08_Regression/#modified-correlation","title":"Modified Correlation","text":"<p>Setting the center as origin \\(\\implies \\bar x=\\bar y=0\\)</p> <ul> <li>Contributes +vely if both vars are positive</li> <li>Contributes +vely if both vars are negative</li> <li>Contributes -vely if both vars are opposing sign</li> </ul> \\[ r_0 = \\dfrac{     \\sum x_i y_i }{ \\sqrt{\\sum (x_i)^2 \\sum (y_i)^2} } \\] <p>Useful for comparing time-series, returns, etc</p> Type Correlation Strength Weak \\(\\vert  r  \\vert \\le 0.5\\) Moderate \\(0.5 &lt; \\vert  r  \\vert &lt; 0.8\\) Strong \\(\\vert  r  \\vert \\ge 0.8\\) Direction Directly \\(r &gt; 0\\) Inversely \\(r &lt; 0\\)"},{"location":"1_Core/Probability_%26_Statistics/08_Regression/#coefficient-of-determination","title":"Coefficient of Determination","text":"<p>\\(R^2\\) value is used for non-linear regression. It shows how well data fits within the regression.</p> <p>It has a range of \\([0, 1]\\). Higher the better.</p> \\[ \\begin{aligned} R^2 &amp;= 1 - \\frac{ \\text{SS}_{res} }{ \\text{SS}_{tot} } \\\\ \\text{SS}_\\text{res} &amp;= \\sum\\limits_{i=1}^n (y_i - \\hat y)^2 \\\\ \\text{SS}_\\text{tot} &amp;= \\sum\\limits_{i=1}^n (y_i - \\bar y)^2 \\\\ \\bar y &amp;= \\frac{1}{n} \\sum\\limits_{i=1}^n y_i \\end{aligned} \\] <p>where</p> Symbol Meaning \\(\\text{SS}_\\text{res}\\) Residual sum of squares \\(\\text{SS}_\\text{tot}\\) Total sum of squaresProportional to variance of the data"},{"location":"1_Core/Probability_%26_Statistics/09_Distribution_Tests/","title":"Distribution Tests","text":""},{"location":"1_Core/Probability_%26_Statistics/09_Distribution_Tests/#normality","title":"Normality","text":"<ul> <li>Histogram with Kernel Density Estimation</li> <li>Q-Q Plots</li> <li>Moment tests</li> </ul>"},{"location":"1_Core/Probability_%26_Statistics/09_Distribution_Tests/#jaque-bera-test","title":"Jaque-Bera Test","text":"<p>Tests for skewness and kurtosis combined $$ \\left[ \\dfrac{\\mu_3}{\\text{SE}(\\mu_3)} \\right]^2 +  \\left[ \\dfrac{\\mu_4}{\\text{SE}(\\mu_4')} \\right]^2 \\sim \\chi^2_2 $$</p>"},{"location":"1_Core/Probability_%26_Statistics/09_Distribution_Tests/#shapiro-wilk-test","title":"Shapiro-Wilk Test","text":"<p>\\(H_0:\\)\u00a0Sample \\(x\\)\u00a0comes from normal-distribution</p> <p>Characteristics of test</p> <ul> <li>Defined for \\(n \\ge 3\\)</li> <li>Best power for a given significance compared to other popular tests</li> </ul> <p>Limitations</p> <ul> <li>This test is sample-size biased</li> <li>Small sample size doesn't have enough information to conclude with high certainty</li> <li>For a large dataset, even a small departure from normality will trigger a rejection</li> <li>hence normal Q-Q plot should be used to confirm test results</li> <li>Failure to reject \\(H_0\\), ie accepting \\(H_1\\) is not proof that the distribution is normal</li> <li>Rejecting \\(H_0\\) does not tell you how much the distribution differs from normal distribution</li> </ul> <p>Test statistic</p> <ul> <li>\\(w \\in (0, 1]\\)</li> <li>Very similar to correlation coefficient of a normal \\(Q-Q\\) plot</li> <li>\\(w\\) independent of location and scale of \\(x\\)</li> </ul> \\[ w = \\dfrac{(\\sum a_i x_i)^2}{\\sum (x_i - \\bar x)^2} \\] <p>where</p> <ul> <li>\\(x_i=\\) \\(i\\)th smallest value</li> <li>\\(a_i=\\)\u00a0Shapiro-Wilk Constant</li> </ul>"},{"location":"1_Core/Probability_%26_Statistics/09_Distribution_Tests/#note","title":"Note","text":"<ul> <li>All tests are very sensitive to outliers</li> <li>One outlier: distribution appears skewed</li> <li>Two symmetric outliers: distribution appears to have heavy tails</li> </ul>"},{"location":"1_Core/Probability_%26_Statistics/09_Distribution_Tests/#_1","title":"Distribution Tests","text":""},{"location":"1_Core/Probability_%26_Statistics/10_Comparing_Samples/","title":"Comparing Samples","text":""},{"location":"1_Core/Probability_%26_Statistics/10_Comparing_Samples/#t-distribution-dof","title":"\\(t\\) Distribution DOF","text":"\\[ \\text{DOF} = \\dfrac{ \\Big[ \\sum_{i=1}^2 (s_i^2/n_i) \\Big]^2 }{ \\sum_{i=1}^2 \\dfrac{(s_i/n_i)^2}{n_i - 1} } \\]"},{"location":"1_Core/Probability_%26_Statistics/10_Comparing_Samples/#comparing-means","title":"Comparing Means","text":"<p>Using central-limit theorem, sampling distribution\u2019s mean is normally-distributed, else t-distributed</p>"},{"location":"1_Core/Probability_%26_Statistics/10_Comparing_Samples/#hat-sigma_1-ne-hat-sigma_2","title":"\\(\\hat \\sigma_1 \\ne \\hat \\sigma_2\\)","text":"<p>Given</p> <ul> <li>\\((\\bar x_1, s_1)\\)</li> <li>\\((\\bar x_2, s_2)\\)</li> </ul> \\[ \\begin{aligned} z \\text{ or } t &amp;= \\dfrac{(\\bar x_1 - \\bar x_2) - E[(\\bar x_1 - \\bar x_2)]}{\\sigma^2(\\bar x_1 - \\bar x_2)} \\\\ &amp;= \\dfrac{     (\\bar x_1 - \\bar x_2) - (\\hat \\mu_1 - \\hat \\mu_2) }{ \\sqrt{     \\sum_{i=1}^2 \\dfrac{s_i^2}{n_i} + 2 \\rho_{12} s_1 s_2 } } \\end{aligned} \\] <p>Simplification: Is \\(\\mu_1\\) and \\(\\mu_2\\) statistically different? \\(\\implies (\\hat \\mu_1 - \\hat \\mu_2)=0\\)</p>"},{"location":"1_Core/Probability_%26_Statistics/10_Comparing_Samples/#hat-sigma_1-hat-sigma_2","title":"\\(\\hat \\sigma_1 = \\hat \\sigma_2\\)","text":"<p>Pooled samples: If we are confident that the population variance are same, we can pool all data to make one estimate of the population variance</p> \\[ \\begin{aligned} s^2_{12} &amp;= \\dfrac{ (n_1-1) s^2_1 + (n_2-1) s^2_2 }{ (n_1-1) + (n_2-1) } \\end{aligned} \\]"},{"location":"1_Core/Probability_%26_Statistics/10_Comparing_Samples/#pairing","title":"Pairing","text":"<p>Matched Samples</p> <p>Compare samples before and after treatment $$ d_i = y_{i, T=1} - y_{i, T=0} $$ \\(T=\\) treatment variable $$ \\begin{aligned} z &amp;= \\dfrac{ \\bar d - \\hat \\mu_d }{ s_d/\\sqrt{n} } \\end{aligned} $$</p>"},{"location":"1_Core/Probability_%26_Statistics/10_Comparing_Samples/#inference","title":"Inference","text":"<ul> <li>If \\(z\\) or \\(t\\) within 95% 2-sided confidence interval centered around 0, then both series are similar</li> <li>Else, dissimilar</li> </ul>"},{"location":"1_Core/Probability_%26_Statistics/10_Comparing_Samples/#comparing-variances","title":"Comparing Variances","text":"<p>Assumes that the population distribution is Normal</p> <p>There is no central-limit theorem that can be applied here $$ \\begin{aligned} F &amp;= \\dfrac{s<sup>2_1/\\sigma</sup>2_1}{s<sup>2_2/\\sigma</sup>2_2} \\ &amp; \\sim F(n_1-1, n_2-1)  \\end{aligned} $$</p>"},{"location":"1_Core/Probability_%26_Statistics/10_Comparing_Samples/#correct-sampling","title":"Correct Sampling","text":"<ul> <li>Random sampling: When evaluating treatment, every subject must have equal probability of receiving treatment</li> <li>Equal sample sizes fore each treatment products optimal test</li> <li>Pairing can be used eliminate effect of uncontrolled variable</li> </ul>"},{"location":"1_Core/Probability_%26_Statistics/10_Comparing_Samples/#standard-error-of-mean","title":"Standard error of mean","text":"Error bars overlap Error bars contain both the sample means Inference \u2705 \u2705 Strong evidence that populations are not different \u2705 \u274c No strong evidence that populations are not different \u274c \u274c Strong evidence that populations are different"},{"location":"1_Core/Technical_Report_Writing/","title":"Technical Report Writing","text":"<p>This course has been divided into 2 sub-components.</p> Part Topics Professor A SentencesEffective writingPreparing questionnaries Dr. Shazi B Paragraph writingIEEE citations Dr. Sayantan <p>This course aims to enhance learners' understanding of the communication process, focusing on developing skills in writing technical reports and effectively presenting them to an audience. By the end of the course, students will be equipped with the tools necessary for clear and concise communication in technical contexts, enabling them to convey complex information effectively and confidently.</p> <p>For Compres, Part A + Introduction and Part B + Discussion Questions can be expected.</p>"},{"location":"1_Core/Technical_Report_Writing/01_Part_A/","title":"Part - A","text":""},{"location":"1_Core/Technical_Report_Writing/01_Part_A/#types-of-sentences","title":"Types of Sentences","text":""},{"location":"1_Core/Technical_Report_Writing/01_Part_A/#on-the-basis-of-structure","title":"On the basis of structure:","text":"Type of Sentence Structure Example Simple Sentence One independent clause \"She ran.\" Can have small conjunction between two words \"She ran and jumped.\" Compound Sentence Two or more independent clauses \"She ran, and he jumped.\" Joined by FANBOYS conjunction (For, And, Nor, Between, Or, Yet, So) \"She ran, and he jumped, but he fell.\" Complex Sentence One independent clause \"She ran to the store.\" One dependent clause (starts with some \"complex\" word) \"After she ran to the store, she went home.\" Compound-Complex Sentence Two or more independent clauses \"She ran to the store, and he jumped over the fence.\" One dependent clause \"After she ran to the store, she went home, but he stayed outside.\""},{"location":"1_Core/Technical_Report_Writing/01_Part_A/#mnemonic-for-remembering-classification-of-sentences-on-the-basis-of-form-scqrc","title":"Mnemonic for remembering classification of sentences on the basis of form: [SCQRC]","text":"<ol> <li>**S**tatement </li> <li>**C**ommand </li> <li>**Q**uestion </li> <li>**R**equest </li> <li>**C**ommand</li> </ol>"},{"location":"1_Core/Technical_Report_Writing/01_Part_A/#mnemonic-for-remembering-classification-of-sentences-on-the-basis-of-function-dniee","title":"Mnemonic for remembering classification of sentences on the basis of function: [DNIEE]","text":"<ol> <li>**D**escriptive </li> <li>**N**arrative </li> <li>**I**llocutionary </li> <li>**E**xpository </li> <li>**E**xclamatory</li> </ol>"},{"location":"1_Core/Technical_Report_Writing/01_Part_A/#effective-sentence-making","title":"Effective Sentence Making:","text":"<p>Check for the following - - Should not be too connotative (negative)  - Precise (mention names)  - Concise (difficult to approximate \u2192 estimate)  - Plainness (don't use complicated words)  - Avoid Cliches, Jargons, Foreign Words  - Avoid gender biased words (mankind \u2192 humanity) - Avoid too many nouns (Window Sash Installation Company \u2192 the company that installs window sashes)</p>"},{"location":"1_Core/Technical_Report_Writing/01_Part_A/#questionnaires","title":"Questionnaires:","text":"<ul> <li>Open Ended ( How would you describe this flavor of ice cream? ) </li> <li>Close Ended ( Do you think this ice cream is too rich in flavor? )</li> <li>Multiple Choice </li> <li>Checklist</li> <li>Ranking </li> <li>Short Answer </li> <li>Scale</li> </ul> <p>Misc points to remember:  - Double Barreled Question : How have teachers and students reacted to the new 30min lunch break?  - Leading Question : Do you think the new cafeteria lunch menu offers a better variety of healthy foods than the old one? - Minimum Number of Respondents is Total Size / 10</p>"},{"location":"1_Core/Technical_Report_Writing/02_Part_B/","title":"Part - B","text":""},{"location":"1_Core/Technical_Report_Writing/02_Part_B/#paragraph-writing","title":"Paragraph Writing","text":""},{"location":"1_Core/Technical_Report_Writing/02_Part_B/#types-of-pragraphs","title":"Types of Pragraphs:","text":"<ul> <li>Descriptive </li> <li>Example </li> <li>Process</li> <li>Opinion</li> <li>Narrative </li> </ul>"},{"location":"1_Core/Technical_Report_Writing/02_Part_B/#structure","title":"Structure:","text":"<ul> <li>Topic Sentence (What is the need to discuss the topic) </li> <li>Body </li> <li>Conclusion of Topic Sentence (Therefore we need it)</li> </ul>"},{"location":"1_Core/Technical_Report_Writing/02_Part_B/#abstract-writing","title":"Abstract Writing","text":"<ul> <li>Why we chose this topic?</li> <li>Problem Statement </li> <li>Solutions </li> <li>Impact and Outcome of the research. </li> </ul> <p>Thoughts should be expressed keeping [Approach \u2192 Method \u2192 Technique] order in mind. </p>"},{"location":"1_Core/Technical_Report_Writing/02_Part_B/#ieee-format","title":"IEEE Format:","text":"<ul> <li>Title in \" \" </li> <li>Journal must be underlined with Pencil </li> <li>Author Name is Eg. S Balamurugan</li> <li>Editor comes after journal </li> <li>\"Place : Publishing Press name\"</li> <li>Volume (vol), Issue (no), Page (pp), Year, DOI</li> </ul>"},{"location":"1_Core/Technical_Report_Writing/03_Sample_Report_%26_Slides/","title":"Sample Report and Slides","text":"<p>Observe the feedback from the professors, and improve on those points.</p> Topic Report Slides CyberSecurity and Data Leak Detection(Graded best of 2022-23 Sem 2) Report Slides The Problems of the Web and How to fix them Report <p>The CyberSecurity report has been provided by a junior. Thank you Sivaa! \u2728</p>"},{"location":"1_Core/Thermodynamics/","title":"Thermodynamics","text":"<p>Taught by Dr. Shashank Khurana, Asst HOD of Mechanical Eng Dept.</p> <p>This course provides a thorough exploration of thermodynamics, focusing on the principles governing energy, matter, and their interactions. Understanding these concepts is crucial for designing processes, devices, and systems that effectively utilize energy and matter in engineering applications.</p> <p>Key topics include the laws of conservation of energy, as well as the utilization of heat and work\u2014two fundamental forms of energy in transition. The course delves into the laws of thermodynamics as they apply to both control mass and control volume systems, highlighting the significance of irreversibility and availability in thermodynamic system design. Additionally, students will learn to use standard charts and tables of properties to aid in numerical problem-solving, reinforcing their grasp of the material through practical applications.</p>"},{"location":"1_Core/Thermodynamics/01_Intro/","title":"01 Intro","text":""},{"location":"1_Core/Thermodynamics/01_Intro/#pressure","title":"Pressure","text":"\\[ \\begin{aligned} P &amp;= \\frac F A \\\\ &amp;= \\rho g h \\\\ P_\\text{abs}  &amp;= P_\\text{atm} + P_\\text{gage} \\\\ &amp;= P_\\text{atm} - P_\\text{vac} \\\\ 1 \\text{ bar} &amp;= 100 \\text{ kPa} \\end{aligned} \\]"},{"location":"1_Core/Thermodynamics/01_Intro/#manometer","title":"Manometer","text":"<p>Denser fluid will be below</p> \\[ \\begin{aligned} P_\\text{a} &amp;= P_\\text{b} \\\\ P_{g_1} + \\rho_1 g h_1 &amp;= P_{g_2} + \\rho_2 g h_2 \\end{aligned} \\]"},{"location":"1_Core/Thermodynamics/01_Intro/#terms","title":"Terms","text":"Term Meaning Specific Gravity SG = \\(\\frac \\rho { \\rho_\\ce{H2O} }\\) Cycle Initial and final properties are the same Steady Flow Properties are independent of time Uniform Flow Properties are independent of location"},{"location":"1_Core/Thermodynamics/02_Pure_Substances%2C_Ideal_Gases/","title":"02 Pure Substances, Ideal Gases","text":""},{"location":"1_Core/Thermodynamics/02_Pure_Substances%2C_Ideal_Gases/#pure-substances","title":"Pure Substances","text":""},{"location":"1_Core/Thermodynamics/02_Pure_Substances%2C_Ideal_Gases/#graph","title":"Graph","text":""},{"location":"1_Core/Thermodynamics/02_Pure_Substances%2C_Ideal_Gases/#points","title":"Points","text":"Point Meaning \\(x\\) left(f) compressed/subcooled liquid N/A f saturated liquid 0 f-g saturated liquid-vapor mixture \\(0&lt;x&lt;1\\) g saturated vapor 1 right(g) superheated vapor N/A c critical point"},{"location":"1_Core/Thermodynamics/02_Pure_Substances%2C_Ideal_Gases/#terms","title":"Terms","text":"Term Meaning Formula \\(h_\\text{fg}\\) latent heat of vaporisation \\(h_g - h_f\\)(or use table) \\(\\Delta h\\) specific heat extracted \\(m \\times h_\\text{fg}\\) \\(x\\) Quality Fraction \\(\\frac{m_\\text{vap}}{m_\\text{tot}}\\)"},{"location":"1_Core/Thermodynamics/02_Pure_Substances%2C_Ideal_Gases/#compressed-liquid","title":"Compressed liquid","text":"<p>Properties are independent of pressure</p> <p>Therefore, at a given temperature</p> <ul> <li>\\(\\nu \\approx \\nu_f\\)</li> <li>\\(u \\approx u_f\\)</li> <li>\\(h \\approx h_f\\)</li> </ul>"},{"location":"1_Core/Thermodynamics/02_Pure_Substances%2C_Ideal_Gases/#ideal-gases","title":"Ideal Gases","text":"\\[ \\begin{aligned} PV  &amp;= mRT \\\\ &amp;= n R_u T \\\\ m &amp;= \\frac{PV}{RT} \\\\ \\rho &amp;= \\frac{P}{RT} \\\\ R &amp;= \\frac{R_u}{M}, M = \\text{Molar Mass (kg/mol)} \\\\ R_u &amp;= 0.8314 \\ \\mathrm{kJ/mol \\cdot K} \\\\ &amp;= 8.314  \\ \\mathrm{J/mol \\cdot K} \\\\ \\end{aligned} \\]"},{"location":"1_Core/Thermodynamics/03_Heat_and_Work/","title":"03 Heat and Work","text":""},{"location":"1_Core/Thermodynamics/03_Heat_and_Work/#addable-quantities","title":"Addable Quantities","text":"<ul> <li>mass</li> <li>volume</li> <li>U</li> <li>H</li> <li>\\(u\\) for closed system</li> </ul> <p>note that specific quanties like \\(h, u\\) can not be added</p>"},{"location":"1_Core/Thermodynamics/03_Heat_and_Work/#work","title":"Work","text":""},{"location":"1_Core/Thermodynamics/03_Heat_and_Work/#spring","title":"Spring","text":"\\[ \\begin{aligned} F &amp;= kx \\\\ W &amp;= \\frac{1}{2} k x^2 \\\\ &amp;= \\frac12 k ({x_2} ^2 - {x_1}^2) \\\\ \\end{aligned} \\]"},{"location":"1_Core/Thermodynamics/03_Heat_and_Work/#electric","title":"Electric","text":"\\[ \\begin{aligned} \\dot W &amp;= VI \\\\ W &amp;= VI \\Delta t \\end{aligned} \\]"},{"location":"1_Core/Thermodynamics/03_Heat_and_Work/#boundary-work","title":"Boundary Work","text":"<p>Note that temperature should be in \\(K\\) (Kelvin)</p> \\[ W_\\text{out, b} = \\int \\limits_{v_1}^{v_2} P \\cdot dv \\] Type Condition(s) \\(W_b\\) Isochoric \\(V = c\\) \\(0\\) Isobaric \\(P = c\\) \\(P_1(V_2 - V_1)\\) \\(mP_1(\\nu_2 - \\nu_1)\\) Isothermal \\(\\begin{aligned} T &amp;= c \\\\ PV &amp;= mRT \\\\ P_1 V_1 &amp;= P_2 V_2 \\end{aligned}\\) \\(P_i V_i \\ \\ln \\vert  \\frac{V_2}{V_1} \\vert  \\\\ P_i V_i \\ \\ln \\vert  \\frac{P_1}{P_2} \\vert\\) \\(mRT \\ \\ln \\vert  \\frac{V_2}{V_1} \\vert\\) \\(mRT \\ \\ln \\vert  \\frac{P_1}{P_2}  \\vert\\) Polytropic \\(\\begin{aligned} P V^n &amp;= c \\\\ P_1 (V_1)^n &amp;= P_2 (V_2)^n \\\\ \\frac{P_1}{P_2} &amp;= \\left( \\frac{V_2}{V_1} \\right)^n  \\end{aligned}\\) \\(\\frac{P_2 V_2 - P_1 V_1}{1-n}\\) \\(\\frac{mR(T_2 - T_1)}{1-n}\\)"},{"location":"1_Core/Thermodynamics/03_Heat_and_Work/#sign-convention","title":"Sign Convention","text":"Quantity Sign \\(Q_\\text{in}\\) + \\(Q_\\text{out}\\) - \\(W_\\text{in}\\) - \\(W_\\text{out}\\) + expansion + compression -"},{"location":"1_Core/Thermodynamics/04_Closed_Systems/","title":"04 Closed Systems","text":""},{"location":"1_Core/Thermodynamics/04_Closed_Systems/#first-law","title":"First Law","text":"\\[ \\begin{aligned} \\delta Q - \\delta W &amp;= dU \\\\ Q_\\text{net} - W_\\text{net} &amp;= \\Delta U \\\\ \\dot Q_\\text{net} - \\dot W_\\text{net} &amp;= \\frac{\\mathrm{d} E_\\text{sys}}{dt}  &amp; \\left(\\ne \\frac{\\Delta U}{\\Delta t} \\right) \\\\ \\text{For a cycle, } Q_\\text{net} &amp;= W_\\text{net} &amp; (\\Delta U = 0) \\end{aligned} \\]"},{"location":"1_Core/Thermodynamics/04_Closed_Systems/#specific-heat","title":"Specific Heat","text":"\\[ \\begin{aligned} \\Delta u &amp;= \\int \\limits_{T_1}^{T_2} C_V \\cdot \\mathrm{d} T \\\\ &amp;= u[T_2] - u[T_1] &amp; \\text{(A.7)}\\\\ \\text{For Solids and Liquids, } \\Delta u &amp;= C_V \\Delta T \\\\ \\text{For Insulated Rigid Tank, } \\Delta u &amp;= \\Delta U = 0 &amp; (Q_\\text{net} = W_\\text{net}  = 0) \\\\ \\Delta h &amp;= \\int \\limits_{T_1}^{T_2} C_P \\cdot \\mathrm{d} T \\\\ &amp;= h[T_2] - h[T_1] &amp; \\text{(A.8)} \\\\ C_V &amp;= C_P - R \\\\ C_P &amp;= \\sum_0^3 C_n \\theta^n  &amp; \\left( \\theta = \\frac{T[K]}{1000} \\right) \\\\ &amp;= C_0 + C_1 \\theta + C_2 \\theta^2 + C_3 \\theta^3 \\\\ \\end{aligned} \\]"},{"location":"1_Core/Thermodynamics/05_Control_Volume/","title":"05 Control Volume","text":""},{"location":"1_Core/Thermodynamics/05_Control_Volume/#flow-rates","title":"Flow Rates","text":"\\[ \\begin{aligned} \\dot V &amp;= vA \\\\ \\dot m &amp;= \\rho \\dot V = \\rho vA \\\\ &amp;= \\frac{\\dot V}{\\nu} = \\frac{vA}{\\nu} \\\\ \\Big( PV &amp;= mRT, m = PV, \\rho = \\frac{P}{RT} \\Big) \\end{aligned} \\]"},{"location":"1_Core/Thermodynamics/05_Control_Volume/#flow-work","title":"Flow Work","text":"<p>\\(W_\\text{f} = PV\\)</p> <p>For non-flowing fluid (fluid that remains inside tank), Flow work = 0</p>"},{"location":"1_Core/Thermodynamics/05_Control_Volume/#conservation-of-mass","title":"Conservation of Mass","text":"\\[ \\begin{aligned} \\sum m_\\text{in} - \\sum m_\\text{out} &amp;= \\Delta m \\\\ \\sum \\dot m_\\text{in} - \\sum \\dot m_\\text{out} &amp;= \\frac{\\mathrm{d} m}{\\mathrm{d} t} \\\\ \\end{aligned} \\]"},{"location":"1_Core/Thermodynamics/05_Control_Volume/#conservation-of-energy","title":"Conservation of Energy","text":"\\[ \\begin{aligned} E_\\text{in} - E_\\text{out} &amp;= \\Delta E_\\text{cv} \\\\ \\dot E_\\text{in} - \\dot E_\\text{out} &amp;= \\frac{\\mathrm{d} E_\\text{cv}}{\\mathrm{d} t} \\\\ \\dot Q_\\text{net} - \\dot W_\\text{net} + \\dot E_\\text{m, in} - \\dot E_\\text{m, out} &amp;= \\frac{\\mathrm{d} E_\\text{cv}}{\\mathrm{d} t} \\\\ \\dot E_\\text{in} &amp;= \\dot m \\left[ h + \\frac{v^2}{2000} + gz \\right] &amp; (h = u + P\\nu) \\\\ &amp;= \\dot m \\left[ u + \\frac{v^2}{2000} + gz \\right]  &amp; \\text{(non-flowing)} \\end{aligned} \\]"},{"location":"1_Core/Thermodynamics/05_Control_Volume/#steady-flow","title":"Steady Flow","text":"<p>Properties within the control volume remain constant with time</p>"},{"location":"1_Core/Thermodynamics/05_Control_Volume/#mass","title":"Mass","text":"\\[ \\begin{aligned} \\frac{\\mathrm{d} m_\\text{cv}}{\\mathrm{d} t} &amp;= 0 \\\\ \\sum \\dot m_\\text{in} &amp;= \\sum \\dot m_\\text{out} \\\\ \\dot m_1 &amp;= \\dot m_2 \\\\ \\rho_1 v_1 A_1 &amp;= \\rho_2 v_2 A_2 \\\\ \\frac{v_1 A_1}{\\nu_1} &amp;= \\frac{v_2 A_2}{\\nu_2} \\end{aligned} \\]"},{"location":"1_Core/Thermodynamics/05_Control_Volume/#energy","title":"Energy","text":"\\[ \\begin{aligned} \\frac{\\mathrm{d} E_\\text{cv}}{\\mathrm{d} t} &amp;= 0 \\\\ \\dot E_\\text{m, in} &amp;= \\dot E_\\text{m, out} \\\\ \\dot Q_\\text{net} - \\dot W_\\text{net} + \\dot E_\\text{m ,in} &amp;= \\dot E_\\text{m, out} \\end{aligned} \\]"},{"location":"1_Core/Thermodynamics/05_Control_Volume/#steady-flow-devices","title":"Steady Flow Devices","text":"Device \\(v\\) \\(P\\) \\(T\\) Work Nozzle inc dec Diffuser dec inc Turbinethermal \\(\\to\\) mechanical \\(\\dot W_\\text{in} = 0\\) Compressor inc inc \\(\\dot W_\\text{out} = 0\\) Throttling valve(isenthalpic) dec dec \\(\\dot W_\\text{in}  = \\dot W_\\text{out} = 0\\) \\(\\begin{aligned} h_1 &amp;= h_2 \\\\ u_1 + P_1 \\nu_1 &amp;= u_2 + P_2 \\nu_2 \\end{aligned}\\)"},{"location":"1_Core/Thermodynamics/05_Control_Volume/#unsteadytransient-flow","title":"Unsteady/Transient Flow","text":""},{"location":"1_Core/Thermodynamics/05_Control_Volume/#mass_1","title":"Mass","text":"\\[ \\begin{aligned} m_\\text{in} - m_\\text{out} &amp;= \\Delta m_\\text{cv} \\\\ &amp;= m_2 - m_1 \\\\ \\dot m_\\text{in} - \\dot m_\\text{out} &amp;= \\frac{\\mathrm{d} m_\\text{cv}}{\\mathrm{d} t} \\end{aligned} \\]"},{"location":"1_Core/Thermodynamics/05_Control_Volume/#energy_1","title":"Energy","text":"\\[ \\begin{aligned} E_\\text{in} - E_\\text{out} &amp;= \\Delta E_\\text{cv} \\\\ Q_\\text{net} - W_\\text{net} + E_\\text{m, in} - E_\\text{m, out} &amp;= \\Delta E_\\text{cv} \\\\ \\Delta E_\\text{cv} &amp;= m_2 e_2 - m_1 e_1 \\\\ e &amp;= h + \\frac{v^2}{2000} + gz \\end{aligned} \\]"},{"location":"1_Core/Thermodynamics/06_2nd_Law/","title":"06 2nd Law","text":""},{"location":"1_Core/Thermodynamics/06_2nd_Law/#terms","title":"Terms","text":"Term Meaning Formula \\(\\eta\\) Efficiency \\(\\frac{\\text{Desired Output}}{\\text{Input}}\\) COP Coefficient of Performance \\(\\frac{\\text{Desired Output}}{\\text{Input}}\\) \\(q\\) Calorific/Heating Value \\(\\frac Q m\\) Gravimetric mass terms"},{"location":"1_Core/Thermodynamics/06_2nd_Law/#devices","title":"Devices","text":"Device Purpose Heat Engine - Heat \\(\\to\\) Work- cycle $\\begin{aligned} \\eta_{\\small\\text{HE}} &amp;= \\frac{W_\\text{net, out}}{Q_\\text{H}} \\ &amp;= 1 - \\frac{Q_\\text{L}}{Q_\\text{H}} \\end{aligned}$ \\(\\eta_\\text{HE} &lt; 1\\)Kelvin-Plank Statement \\(\\begin{aligned} \\Delta U &amp;= 0 \\\\ Q_\\text{net} &amp;= W_\\text{net} \\\\ W_\\text{net, out} &amp;= Q_\\text{in} - Q_\\text{out} \\\\ &amp;= Q_\\text{H} - Q_\\text{L} \\end{aligned}\\) Refridgerator - maintain cool temp- Reverse HE $\\begin{aligned} \\text{COP}R &amp;= \\frac{Q\\text{L}}{Q_\\text{net, in}} \\ &amp;= \\frac{1}{ \\frac{Q_\\text{H}}{Q_\\text{L}} - 1 } \\end{aligned}$ \\(\\text{COP}_R\\) can be &gt; 1 Heat Pump - maintain warm temp- Reverse HE $\\begin{aligned} \\text{COP}{HP} &amp;= \\frac{Q\\text{H}}{W_\\text{net, in}} \\ &amp;= \\frac{1}{ 1 - \\frac{Q_\\text{L}}{Q_\\text{H}} } \\end{aligned}$ \\(\\begin{aligned} \\text{COP}_{HP} &amp;= \\text{COP}_{R} + 1 \\\\ \\text{COP}_{HP} &amp;&gt; \\text{COP}_{R} \\end{aligned}\\) <pre><code>flowchart LR\n\nsubgraph Heat Engine\ndirection LR\na([Warm]) --&gt;\n|Q&lt;sub&gt;H&lt;/sub&gt;| b[System] --&gt;\n|Q&lt;sub&gt;L&lt;/sub&gt;| c([Cool])\n\nb --&gt; |W&lt;sub&gt;net&lt;/sub&gt;| d[ ]\nend\n\nsubgraph Refridgerator/Heat Pump\ndirection LR\nr([Cool]) --&gt;\n|Q&lt;sub&gt;L&lt;/sub&gt;| q[System] --&gt;\n|Q&lt;sub&gt;H&lt;/sub&gt;| p([Warm])\ns[ ] --&gt; |W&lt;sub&gt;net&lt;/sub&gt;| q\nend</code></pre>"},{"location":"1_Core/Thermodynamics/06_2nd_Law/#carnot-cycle","title":"Carnot Cycle","text":"<p>For Heat Engine</p> <p>Adiabatic means polytropic process with**out** heat transfer</p> Transition Characteristic Constant Signs Work 1 - 2 Isothermal ExpansionHeat Absorbed \\(PV = c\\) $W_{12} &gt; 0 \\ Q_\\text{H} &gt; 0$ \\(P_1 V_1 \\ln \\vert  \\frac{V_2}{V_1} \\vert\\) \\(P_2 V_2 \\ln \\vert  \\frac{P_1}{P_2}  \\vert\\) 2 - 3 Adiabatic Expansion \\(PV^\\gamma = c\\) \\(W_{23} &gt; 0\\) \\(\\frac{P_3 V_3 - P_2 V_2}{1-n}\\) 3 - 4 Isothermal CompressionHeat Released \\(PV = c\\) $W_{34} &lt; 0 \\ Q_\\text{L} &lt; 0$ \\(P_3 V_3 \\ln \\vert  \\frac{V_4}{V_3} \\vert\\) \\(P_4 V_4 \\ln \\vert  \\frac{P_3}{P_4}  \\vert\\) 4 - 1 Adiabatic Compression \\(PV^\\gamma = c\\) \\(W_{41} &lt; 0\\) \\(\\frac{P_1 V_1 - P_4 V_4}{1-n}\\) \\[ \\begin{aligned} W_\\text{net, out} &amp;= W_{12} + W_{23} + W_{34} + W_{41} \\\\ \\eta &amp;= \\frac{W_\\text{net, out}}{Q_\\text{H}} \\\\ &amp;= 1 - \\frac{Q_\\text{L}}{Q_\\text{H}} \\\\ &amp;= 1 - \\frac{T_L}{T_H} \\end{aligned} \\] <p>Make sure of the signs when calculating \\(W_\\text{net, out}\\)</p>"},{"location":"1_Core/Thermodynamics/06_2nd_Law/#reverse-carnot-cycle","title":"Reverse Carnot Cycle","text":"<p>For Refridgerator, Heat Pump</p> <p>\\(Q_\\text{L} &gt; 0, Q_\\text{H} &lt; 0\\)</p> \\[ \\begin{aligned} W_\\text{net, in} &amp;= W_{12} + W_{23} + W_{34} + W_{41} \\\\ \\text{COP}_R &amp;= \\frac{Q_\\text{L}}{W_\\text{net, in}} \\\\ \\text{COP}_{HP} &amp;= \\frac{Q_\\text{H}}{W_\\text{net, in}} \\end{aligned} \\]"},{"location":"1_Core/Workshop/","title":"Workshop","text":"<p>This course has 2 components. One for weekly lecture classes and the other for weekly workshop sessions. You can find the last minute exam reviewal notes under Lecture, and demonstration videos for every experiment under Lab. </p> <p>This course aims to enhance both theoretical and practical knowledge of various manufacturing processes. It covers essential techniques and operations involved in machining, fitting, and joining processes. Key areas of focus include casting, metal forming, and welding and brazing, providing students with a comprehensive understanding of these foundational processes.</p> <p>Hands-on laboratory exercises will involve the use of metal cutting machines such as lathes, shapers, and planers, as well as drilling, milling, and grinding techniques. Through this blend of theoretical learning and practical application, students will gain valuable skills and insights applicable to the manufacturing industry.</p> <p>As of date these notes cover up to midsem for lecture, demonstration videos covers the entire coursework. </p>"},{"location":"1_Core/Workshop/lab/","title":"Lab","text":"<ol> <li>Vertical Milling</li> <li>Shaper Experiment</li> <li>Drilling &amp; Tapping</li> <li>Horizontal Milling</li> <li>Lathe</li> <li>Wood Working</li> <li>Gas Welding</li> <li>Arc Welding</li> <li>Foundry</li> <li>CNC Lathe Programming</li> <li>CNC Milling Programming</li> </ol>"},{"location":"1_Core/Workshop/lecture/","title":"Lecture","text":""},{"location":"1_Core/Workshop/lecture/#ch1-manufacturing-processes","title":"Ch1 Manufacturing Processes:","text":"<p>Casting : Material is given desired shape by melting. Machining : The process of removing unwated material from the surface of a material. Forming : The process which involves the deformation of a substance by going beyond its yeild strength to obtain desired shape. Powder Metallurgy : Fine powdered materials are pressed into desired shapes + heated and are placd in controlled environements to bond and get the finished product. Joining : Two or more pieced are joined together to produce the required shape. Two types are permenant joining and temporary joining.   </p>"},{"location":"1_Core/Workshop/lecture/#ch2-engineering-materials","title":"Ch2 Engineering Materials:","text":"<p>Loading : Tensile Loading (pull) + Compressive Loading (Push)</p> <p>Stress and Strain : - Stress : Sujected to external/resisting forces.     - Tensile     - Compressive     - Shear - Strain : Ratio of change in dimension to original dimension. </p> <p>Poisson's Ratio = \\(\\frac{\\text{Lateral Strain}}{\\text{Longitudinal Stress}}\\)</p> <p>Toughness vs Hardness vs Resilience | Property | Definition | Measurement | |------------|----------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------| | Toughness | Measure of Energy a material can absorb before it fractures. It is measured by the area under the stress-strain curve. | Area under the stress-strain curve | | Hardness | Property of a surface to resist abrasion or indentation. | - | | Resilience | Capacity of a material to absorb energy elastically. Upon removal of the load the energy stored is given off. | Triangular area under the elastic portion of the stress-strain curve. |</p> <p>Creep vs Fatigue :</p> <ul> <li>Creep: Time-dependent failure due to prolonged time under load. </li> <li>Fatigue: Unexpected and sudden failure that can occur under the yield point.</li> </ul>"},{"location":"1_Core/Workshop/lecture/#ch3-measurments-in-manufacturing","title":"Ch3 Measurments in Manufacturing:","text":"<p>Metrology : Science and process of ensuring the measurement meets specified degrees of both accuracy and precision. </p> <p>Measurement : Is the process of comparision of an unknown quanitity with a known quantity.</p> <p>Inspection : Examination of a component to determine wether it meets specified needs or not.</p> <p>Gauging : The proces of determining wheter the dimension is within specified limits or not. </p> <p>Testing : The process to know the perofmance of a product. </p> <p>Accuracy : The closeness of the measured value to the true value.</p> <p>Precision : The closeness of two or measured values to each other. </p> <p>Tolerance : The permissible deviation of a dimension from the desired size is known as tolerance. </p> <p>Surface Finish : The amount of geometric irregularities produced on the surface of a component during a manufacturing process.  </p>"},{"location":"1_Core/Workshop/lecture/#ch4-material-removal-process","title":"Ch4 Material Removal Process:","text":"Mechanism of Material Removal Description Depth of Cut D1-D2/2 Feed By how much distance the cutting tool must advance for each revolution of work. Cutting Speed The speed at which the workpiece moves with respect to the tool. S = Pi * D(mm) * N / 1000 Roughing Operation A large chunk to be removed without considering perfection. Finishing Operation Only a small portion is removed keeping final touches in middle. MRR 1000 * V * d * f Turning Excess material is removed by giving a depth of cut to its diameter. Facing Used to cut a flat surface perpendicular to the work piece's rotational axis. It is used to reduce the length of the workpiece. The length the workpiece travels is called the radius of the job. The depth of the cut is along the axis of the job. Knurling To produce regular patterns on the surface of metals. It is the process of pressing the metal hard enough to cause plastic deformation of metal into peaks and troughs. Low cutting speed and feed can be used w/ plenty of coolant. MRR is very low. Grooving Narrow grooves on Cylindrical Shapes, the diameter of the surface is slightly reduced. Cutting speed is slow. Depth of cut is given but no feed. Parting It is the operation of cutting a workpiece into 2 parts. The workpiece is rotated at a slow speed and the parting tool is fed perpendicular. NOTE: If a slow feed is used it will run for 2-3 revolutions without cutting and will suddenly bite the machine, this is undesired and is called hogging. Chamfering It is the operation of beveling (smoothening) the sharp edges of a workpiece to avoid injuries. It is used at an angle of 45 degrees."},{"location":"2_Core/Data_Structures_%26_Algorithms/","title":"Data Structures &amp; Algorithms","text":"Class Instructor Lecture Dr. Pranav M. Pawar Tutorial Dr. Pranav M. Pawar Practical Dr. Vijaykumar <p>This course covers essential knowledge and skills for understanding and implementing data structures, which are crucial for effective data organization and manipulation. This course emphasizes both the theoretical and practical aspects of data structures, enabling students to grasp the design and performance issues associated with various types.</p> <p>Students will explore a wide range of data structures, including linear and non-linear types, along with algorithms for data retrieval and modification. The course also introduces techniques for implementing these data structures on modern computers, providing students with the ability to analyze their performance in terms of time and space complexity. Through a combination of theoretical concepts and practical applications, students will be equipped to tackle real-world problems using efficient data management techniques.</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/01_Algorithms/","title":"01 Algorithms","text":""},{"location":"2_Core/Data_Structures_%26_Algorithms/01_Algorithms/#definitions","title":"Definitions","text":"Word Definition Algorithm step-by-step procedure to solve a problem in finite time Program implementation of algorithm in a programming language Data Structure Organization of data needed to solve the problem"},{"location":"2_Core/Data_Structures_%26_Algorithms/01_Algorithms/#algorithms","title":"Algorithms","text":"<pre><code>graph LR\ni[/Input/] --&gt;\nAlgorithm --&gt;\no[/Output/]</code></pre> <p>Algorithmic outputs always depend on the input.</p> <p>For example, binary search requires a sorted list as input.</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/01_Algorithms/#efficiency-of-algorithm","title":"Efficiency of Algorithm","text":""},{"location":"2_Core/Data_Structures_%26_Algorithms/01_Algorithms/#complexity","title":"Complexity","text":"<p>Determined by</p> <ol> <li>Space complexity (Space used)</li> <li>Time complexity (Running Time)</li> </ol> <p>This is more important</p> <p>Both of the above are defined as a function of input size</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/01_Algorithms/#cases","title":"Cases","text":"<ul> <li>Worst</li> <li>Average</li> <li>Best</li> <li>Amortized (Sequence opertions applied to input size \\(n\\)\u00a0averaged over time)</li> </ul>"},{"location":"2_Core/Data_Structures_%26_Algorithms/01_Algorithms/#experimentalemperical-analysis","title":"Experimental/Emperical Analysis","text":"<ol> <li>Write the program for the algorithm</li> <li>Run the program with different input sizes</li> <li>Measure running time</li> </ol> <p>For eg, in Java we can use <code>System.currentTimeMillis()</code> 4. Plot the result</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/01_Algorithms/#limitations","title":"Limitations","text":"<ol> <li>We need to implement and test</li> <li>Same programming language must be used to compare 2 algorithms</li> </ol> <p>Because our interpretation of the efficiency may vary with different programming languages 3. Only limited set of input is possible 4. Same h/w &amp; s/w should be used to compare 2 algorithms</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/01_Algorithms/#theoreticalmathematical-analysis","title":"Theoretical/Mathematical Analysis","text":"<ol> <li>Use pseudocode</li> <li>Determine the primitive operations</li> </ol> <p>We assume that each primitive operation takes 1 unit of time. 3. Define running time as a function of input size \\(n\\)</p> <pre><code>Algorithm arrayMax(a, n)\n    Input array A of n integers\n    Output maximum element of A\n\n    currentMax &lt;- A[0]\n    for i&lt;-1 to (n-1) do\n        if A[i] &gt; currentMax then\n            currentMax &lt;-A[i]\n\n    return currentMax\n</code></pre> Primitive Operations currentMax &lt;- A[0] 2- getting A[0] from memory- assignment for i \\(\\leftarrow\\) 1 to (n-1) \\(n+1\\)comparison A[i] &gt; currentMax \\(2(n-1)\\) currentMax &lt;-A[i] \\(2(n-1)\\) {increment counter i} \\(2(n-1)\\)1. add2. assign return currentMax 1 Total \\(7n-2\\)"},{"location":"2_Core/Data_Structures_%26_Algorithms/01_Algorithms/#advantage","title":"Advantage","text":"<ul> <li>acknowledges all possible inputs</li> <li>evaluate speed of algorithm independent of hardware/software used</li> </ul>"},{"location":"2_Core/Data_Structures_%26_Algorithms/01_Algorithms/#pseudocode","title":"Pseudocode","text":"<p>Simple representation of your program</p> <ul> <li>High-level description of algorithm</li> <li>more structured than english, but less detailed than a program</li> <li>hides program design issues</li> </ul> <p>All mathematical formatting like \\(n^2\\) (subscript) is allowed</p> Notation Meaning if \u2026 then \u2026 [else \u2026] Control Flow while \u2026 do (more are there, please complete this Thahir) \\(\\leftarrow\\) Assignment = Equality (like ==) &gt;, &lt;, \u2026 Comparison <p>eg</p> <pre><code>Algorithm arrayMax(A, n)\n    Input array A of n integers\n    Output maximum element of A\n\n    currentMax &lt;- A[0]\n    for i&lt;-1 to (n-1) do\n        if A[i] &gt; currentMax then\n            currentMax &lt;-A[i]\n\n    return currentMax\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/01_Algorithms/#notations","title":"Notations","text":"<p>All notations take the worst-case scenario</p> <p>Let \\(f(n)\\) be the algorithm for which we are finding the notation</p> Notation Purpose Condition \\(O(\\ g(n) \\ )\\) Upper Bound \\(f(n) \\le c \\cdot g(n)\\) \\(o(\\ g(n) \\ )\\) Strict Upper Bound \\(f(n) &lt; c \\cdot g(n)\\) \\(\\Omega(\\ g(n) \\ )\\) Lower Bound \\(f(n) \\ge c \\cdot g(n)\\) \\(\\omega(\\ g(n) \\ )\\) Strict Lower Bound \\(f(n) &gt; c \\cdot g(n)\\) \\(\\Theta(\\ g(n) \\ )\\) Tight 2-Sided Bounds \\(c_1 \\cdot g(n) \\le f(n) \\le c_2 \\cdot g(n)\\)"},{"location":"2_Core/Data_Structures_%26_Algorithms/01_Algorithms/#big-oh-notation","title":"Big Oh notation","text":"<p>Most commonly-used notation</p> <p>we neglect the constant factors</p> <p>examples:</p> \\[ O(1) &lt; O(\\log n) &lt; O(n) &lt; O(n \\log n) &lt; O(n^2 \\log n) &lt; O(n^2) &lt; \\dots &lt; O(2^n), O(e^n) \\] <pre><code>// O(n)\nfor(int i = 0; i&lt;n; i++)\n\nfor(int i = 0; i&lt;n; i--)\n\n// O(n^2)\nfor(int i = 0; i&lt;n; i++)\n  for(int i = 0; i&lt;n; j++)\n\n// O( n(n+1)/2 ) = O(n^2)\nfor(int i = 0; i&lt;n; i++)\n  for(int j = 0; j&lt;i; j++)\n\n// O(log_2 n)\nfor(int i = 1; i&lt;n; i*=2)\n\nfor(int i = 1; i&lt;n; i/=2)\n\n// O(log_b n)\nfor(int i = 1; i&lt;n; i*=b)\n\nfor(int i = 1; i&lt;n; i/=b)\n\n// O(n log_2 n)\nfor(int i = 0; i&lt;n; i++)\n    for(int i = 0; i&lt;n; i/=2)\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/01_Algorithms/#math-required","title":"Math Required","text":"<ol> <li> <p>Summations</p> </li> <li> <p>Log formulae</p> </li> </ol> \\[ \\begin{aligned} \\log xy  &amp;= \\log x + \\log y \\\\    \\log \\left( \\frac{x}{y} \\right) &amp;= \\log x - \\log y \\\\   \\log x^n &amp;= n \\log x \\end{aligned} \\] <ol> <li> <p>Proof Techniques</p> </li> <li> <p>Basic Probability</p> </li> </ol>"},{"location":"2_Core/Data_Structures_%26_Algorithms/02_Data_Structures/","title":"02 Data Structures","text":"Stack Queue Circular Queue Principle LIFO (Last in first out) FIFO (First in First out) FIFO Operations Push, Pop Enqueue, Dequeue Enqueue, Dequeue Insertion \\(t = t+1\\) \\(R = R+1\\) \\(R = (R+1) \\% n\\) Deletion \\(t = t-1\\) \\(F = F+1\\) \\(F = (F+1) \\% n\\) Size(not capacity) \\(t+ 1\\) \\((R - F)\\) \\([n - F+R)] \\% n\\) Overflow \\(t=n-1\\) \\(R=n\\) size \\(= n-1\\) Underflow \\(t=-1\\) \\(F=n\\) size \\(= 0\\) Time Complexity \\(O(1)\\) Space Complexity \\(O(1 \\times \\text{element size})\\) <p>Queue and CQ implementation is different in this course. What we studied in 12<sup>th</sup> grade is actually better, but we have to follow the textbook.</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/03_Stacks/","title":"03 Stacks","text":"<p>Data is stored one over the other</p> <p>\\(t\\) is a variable that refers to the top. Initial value is -1 (stack empty)</p> Operation Return Type Function push(element) void inserts element at top position pop() element removes topmost element and returns the removed element top() element returns the topmost element size() int returns no of elements isEmpty() boolean checks if empty <pre><code>Algorithm push(element)\n        if t = n-1\n            overflow\n        t = t + 1\n        a[t] = element\n\nAlgorithm pop()\n        if t = -1\n            underflow\n        t = t - 1\n        return a[t]\n\nAlgorithm size()\n    return (t+1)\n\nAlgorithm top()\n    return a[t]\n\nAlgorithm isEmpty()\n    if t = -1\n        return true\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/03_Stacks/#applications","title":"Applications","text":"<ol> <li>browsing history</li> <li>undo sequence</li> <li>chain of method calls in JVM (java virtual machine)</li> <li>Evaluation and conversion of expressions (infix, post-fix, pre-fix)</li> </ol>"},{"location":"2_Core/Data_Structures_%26_Algorithms/03_Stacks/#infix-to-postfix","title":"Infix \\(\\to\\) PostFix","text":"Token Stack Output a a + + a c ac - - ac+"},{"location":"2_Core/Data_Structures_%26_Algorithms/03_Stacks/#rules","title":"Rules","text":"Input Output HIN \\(\\leftarrow\\) LCIN HOUT, ALL OUT LIN \\(\\leftarrow\\) HCIN No change"},{"location":"2_Core/Data_Structures_%26_Algorithms/03_Stacks/#priority","title":"Priority","text":"Arithmetic Logical \\(()\\) NOT ^ AND \\(*/\\) OR \\(+-\\) <p>If what is coming in and what is already in have the same priority, then the one inside is considered as the higher priority</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/03_Stacks/#postfix-to-infix","title":"PostFix \\(\\to\\) Infix","text":"<p>Simple rules</p> Token Stack Action 3 3 Push 3 2 3, 2 Push 2 5 3, 2, 5 Push 5 ^ 3, 32 Pop \\(2, 5\\); Push \\(2^5\\) + 35 Pop \\(3, 32\\); Push \\(3+32\\)"},{"location":"2_Core/Data_Structures_%26_Algorithms/03_Stacks/#balancing-of-symbols","title":"Balancing of Symbols","text":"<p>We\u2019re basically checking if all the brackets are matched</p> <pre><code>while there are symbols in the expression do\n    if symbol is variable\n        do nothing\n    if symbol is opening\n        push it to the stack\n    if symbol is closing symbol\n        if stack is empty\n            invalid\n        else\n            valid\n\nif stack is empty   // once evaluation of expression is over\n    valid\nelse\n    invalid\n</code></pre> Token Stack Reason"},{"location":"2_Core/Data_Structures_%26_Algorithms/04_Queues/","title":"04 Queues","text":"<p>Front and Rear are two variables</p> Operation Return Type Function enqueue(element) void inserts element at rear position dequeue() element removes frontmost element and returns the removed element front() element returns the frontmost element rear() element returns the rearmost element size() int returns no of elements isEmpty() boolean checks if empty"},{"location":"2_Core/Data_Structures_%26_Algorithms/04_Queues/#college-implementation","title":"College Implementation","text":"<ul> <li>\\(f\\) shows the index of the first element</li> <li>\\(r\\) shows the free-space available to insert the next element   (index immediately after the last inserted element)</li> </ul>"},{"location":"2_Core/Data_Structures_%26_Algorithms/04_Queues/#linear-queue","title":"Linear Queue","text":"<p>Textbook</p> <pre><code>Algorithm enqueue(o)\n    if r=N then\n        return Error\n    else\n    Q[r]\u2190 o\n    r \u2190 r+1\n\nAlgorithm dequeue()\n    if f=r then\n        return Error\n    else\n        e \u2190 Q[f]\n        Q[f] \u2190 null\n        f \u2190 f+1\n        return e\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/04_Queues/#circular-queue","title":"Circular Queue","text":"<pre><code>Algorithm size()\n    return (n-f+r) mod n\n\nAlgorithm isEmpty()\n    if f = r\n        return true\n    else\n        return false\n\nAlgorithm front()\n    if isEmpty() then\n        return Error\n    else\n        return Q[f]\n\nAlgorithm dequeue()\n    if isEmpty() then\n        return Error\n\n    o \u2190 Q[f]\n    Q[f] \u2190 null\n    f \u2190 (f+1) % n\n\n    return o\n\nAlgorithm enqueue(o)\n    if size()= n-1 then\n        return Error\n    Q[r] \u2190 o\n    r \u2190 (r+1) % n\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/04_Queues/#questions","title":"Questions","text":"Operation \\(A[0]\\) \\(A[1]\\) \\(A[2]\\) F R Exception Output"},{"location":"2_Core/Data_Structures_%26_Algorithms/04_Queues/#applications","title":"Applications","text":"<ul> <li>buffers</li> <li>multi-threading priority</li> <li>data transfer priority</li> </ul>"},{"location":"2_Core/Data_Structures_%26_Algorithms/05_Lists/","title":"05 Lists","text":""},{"location":"2_Core/Data_Structures_%26_Algorithms/05_Lists/#linkedlist","title":"LinkedList","text":"<p>better than arrays in some aspects, because</p> <ul> <li>you can add/delete elements in runtime</li> <li>capacity can be modified in runtime</li> </ul>"},{"location":"2_Core/Data_Structures_%26_Algorithms/05_Lists/#logical-address","title":"Logical Address","text":"<p>index in the array, visible to user</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/05_Lists/#physical-address","title":"Physical Address","text":"<p>address in the memory</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/05_Lists/#singly-linkedlist","title":"Singly LinkedList","text":"<p>consists of</p> <ul> <li>data</li> <li>pointer to the next location</li> </ul> \\[ \\fbox{D} \\fbox{P} \\quad \\fbox{D} \\fbox{P} \\quad \\fbox{D} \\fbox{/} \\] <p>In the above diagram</p> <ul> <li>D = data</li> <li>P = pointer</li> <li>/ = null pointer</li> </ul>"},{"location":"2_Core/Data_Structures_%26_Algorithms/05_Lists/#inserting-at-tail","title":"Inserting at tail","text":"<ol> <li>allocate a new node</li> <li>enter element</li> <li>set node point to null</li> <li>make previous node point to current node</li> </ol>"},{"location":"2_Core/Data_Structures_%26_Algorithms/05_Lists/#removing-at-tail","title":"Removing at tail","text":"<ol> <li>set 2<sup>nd</sup> last node point to null</li> <li>de-allocate the memory (Java takes care of this automatically)</li> </ol>"},{"location":"2_Core/Data_Structures_%26_Algorithms/05_Lists/#implementation","title":"Implementation","text":"<pre><code>class Node\n{\n  int d; // data\n  Node p; // pointer\n\n  Node()\n  {\n    d = 0;\n    p = null;\n  }\n\n  Node(int data, Node ptr)\n  {\n    d = data;\n    p = ptr;\n  }\n  void setLink(Node ptr)\n  {\n    p = ptr;\n  }\n  void setData(int data)\n  {\n    d = data;\n  }\n  Node getLink()\n  {\n    return p;\n  }\n  int getData()\n  {\n    return d;\n  }\n}\n\nclass LinkedList\n{\n  static Node start;\n  static Node end;\n  static int size;\n\n  LinkedList()\n  {\n    start = null;\n    end = null;\n    size = 0;\n  }\n\n  int getSize()\n  {\n    return size;\n  }\n\n  boolean isEmpty()\n  {\n    return (getSize() == 0);\n  }\n\n  void insertAtStart(int val)\n  {\n    Node n = new Node(val, null);\n\n    if(size == 0) // inserting for the first time\n    {\n      end = n;\n    }\n    else\n    {\n      n.setLink(start); // set the link to the previous start\n    }\n\n    start = n; // this is the new start\n    size++;\n  }\n\n  void insertAtEnd(int val)\n  {\n    Node n = new Node(val, null);\n    if(size == 0)\n    {\n      start = n;\n    }\n    else\n    {\n      end.setLink(n);\n    }\n\n    end = n;\n    size++;\n  }\n\n  void insertAtIndex(int val, int index)\n  {\n    Node n = new Node(val, null);\n\n    // traversal\n    Node cur = start; // current node\n\n    int i = 0;\n    while(n != null)\n    {\n      if(i == index)\n      {\n        n.p = cur.p;\n        cur.p = n;\n        break;\n      }\n      else\n      {\n        cur = cur.getLink();\n        i++;\n      }\n\n      size++;\n    }\n\n    void deleteAtIndex(int index)\n    {\n      Node n = start;\n      int i = 0;\n\n      while(n!=null)\n      {\n        if(i == index-1)\n        {\n          n.p = ?????????????\n        }\n        else\n        {\n          n = n.getLink();\n          i++;\n        }\n      }\n\n      size++;\n    }\n    public void display()\n    {\n      Node n = start;\n      while(n!=null)\n      {\n        System.out.println(n.data);\n        n = \n      }\n    }\n  }\n}\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/05_Lists/#stacked-ll","title":"Stacked LL","text":"<p>implementing stack using linked list, rather than arrays</p> <pre><code>class StackedLL\n{\n  Node top;\n  StackedLL()\n  {\n    top = null;\n  }\n  void push(int data)\n  {\n    insertAtEnd(data);\n  }\n  void pop()\n  {\n        deleteAtEnd();\n  }\n}\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/05_Lists/#queued-ll","title":"Queued LL","text":"<pre><code>class QueuedLL\n{\n    Node f, r; \n  StackedLL()\n  {\n    f = null;\n    r = null;\n  }\n  void enqueue(int data)\n  {\n    insertAtEnd(data);\n  }\n  void pop()\n  {\n        deleteAtStart();\n  }\n}\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/05_Lists/#double-linked-list","title":"Double Linked List","text":"<p>DLL Practicals </p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/05_Lists/#circular-linked-list","title":"Circular Linked List","text":"<p>Used for dynamic circular queues to schedule tasks in OS</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/05_Lists/#single","title":"Single","text":"<p>Tail points to head</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/05_Lists/#double","title":"Double","text":"<p><code>TailFrontPointer</code> points to head</p> <p><code>HeadBackPointer</code> points to tail</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/06_Searching/","title":"06 Searching","text":"<p>returns</p> <ul> <li>index, if found</li> <li>\\(-1\\), if not found</li> </ul> <p>Space Complexity = \\(O(1)\\)</p> Linear Search Binary Search Working Go through each element of array and compare Divide the array into half each time Worst-Case Time Complexity \\(O(n)\\) \\(O(\\log_2 n)\\) Average-Case Time Complexity \\(O(n)\\) \\(O(\\log_2 n)\\) Best-Case Time Complexity \\(O(1)\\) \\(O(1)\\) Requires Sorted List \u274c \u2705 <p>Search Practicals</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/07_Sorting/","title":"07 Sorting","text":""},{"location":"2_Core/Data_Structures_%26_Algorithms/07_Sorting/#algorithms","title":"Algorithms","text":"Algorithm Working In-Place Worst Avg Best Bubble elements swapped with bubble \u2705 \\(O(n^2)\\) \\(O(n^2)\\) \\(O(n)\\) Selection swap current element with smallest \u2705 \\(O(n^2)\\) \\(O(n^2)\\) \\(O(n)\\) Merge Recursive Divide-Conquer \u274c \\(O(n \\log_2 n)\\) \\(O(n \\log_2 n)\\) \\(O(n \\log_2 n)\\) Quick Recursive Divide-Conquer Partition array around pivot \u2705 \\(O(n^2)\\) \\(O(n \\log_2 n)\\) \\(O(n \\log_2 n)\\) Insertion key compared with previous elements \u2705 \\(O(n^2)\\) \\(O(n^2)\\) \\(O(n)\\) Bucket bucket of pointers to linked lists \u274c \\(O(n^2)\\) \\(O(n+k)\\) \\(O(n + k)\\) Radix tuple-based \u2705 \\(O \\Big( (T(n) \\Big)\\) Heap Max-heap \u2705 \\(O(n \\log_2 n)\\)"},{"location":"2_Core/Data_Structures_%26_Algorithms/07_Sorting/#bubble-sort","title":"Bubble Sort","text":"<ul> <li>The \\(j\\) loop acts as a controller for no of times the inner loop runs</li> <li>The \\(i^{th}\\) element acts as the value stored in the \u2018bubble\u2019</li> </ul> <pre><code>for(int j=1; j&lt;=n-1; j++)\n    for(int i=0; i&lt;n-1; i++)\n        if(a[i+1]&lt;a[i])\n        {\n            t = a[i+1];\n            a[i+1] = a[i];\n            a[i] = t;\n        }\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/07_Sorting/#selection-sort","title":"Selection Sort","text":"<p>useful when memory write is a costly operation</p> <pre><code>for (j=0; j&lt;n-1; j++)\n{\n    m = a[j];\n    pos = j;\n\n    for(i=j+1; i&lt;n; i++)\n        if(a[i]&lt;m)\n        {\n            m = a[i];\n            pos = i;\n        }\n\n    a[pos] = a[j];\n    a[j] = m;\n}\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/07_Sorting/#merge-sort","title":"Merge Sort","text":"<ol> <li>Divide the list</li> <li>Recursively sort the divisions</li> <li>Merge the divisions</li> </ol> <p>Let \\(p, r, q\\) denote left, right, and middle indices</p> <pre><code>Algorithm mergeSort(A, p, r)\n    if p &lt; r\n    q \u2190 floor((p + r)/2)\n\n    mergeSort (A, p, q) // first half\n    mergeSort (A, q+1, r) // second half\n\n    mergeAsc(A, p, q, r) // or mergeDesc(A, p, q, r)\n\nAlgorithm mergeAsc(A, p, q, r)\n  n1 \u2190 q-p+1\n  n2 \u2190 r-q\n\n  Let L[0\u2026(n1-1)] // int[] l = new int[n1]\n  Let R[0\u2026(n2-1)] // int[] r = new int[n2]\n\n  for i \u2190 0 to n1-1\n      L[i] \u2190 A[p+i]\n  for i \u2190 0 to n2-1\n      R[i] \u2190 A[q+i+1]\n\n  i \u2190 0\n  j \u2190 0\n  k \u2190 p\n\n  while i&lt;n1 and j&lt;n2\n      if L[i] &lt;= R[j] // &gt;= for mergeDesc\n          A[k] \u2190 L[i]\n          i \u2190 i+1\n      else\n          A[k] \u2190 R[j]\n          j \u2190 j+1\n      k \u2190 k+1\n\n  while i&lt;n1\n      A[k] \u2190 L[i]\n      i \u2190 i+1\n      k \u2190 k+1\n\n  while j&lt; n2\n      A[k] \u2190 R[j]\n      j \u2190 j+1\n      k \u2190 k+1\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/07_Sorting/#quick-sort","title":"Quick Sort","text":"<p>After each iteration, the pivot element will move to its correct position</p> <pre><code>Algorithm quickSort(a, p, r)\n    if(p&lt;r)\n        q = partition(a, p, r)\n        quickSort(a, p, q - 1)  // Before pivot\n        quickSort(a, q + 1, r) // After pivot\n        // the reason q is left out is cuz it is already placed in its correct position\n\nAlgorithm partition(a, p, r)\n    pivot = a[r] // assuming last element as pivot\n    i = p - 1\n\n    for j=p to r-1\n        if a[j] &lt;= pivot\n            i = i+1\n            swap a[i] and a[j]\n\n    swap a[i+1] with pivot\n    return i+1 // this is the new position of the pivot element\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/07_Sorting/#randomized-quick-sort","title":"Randomized Quick Sort","text":"<p>The randomness reduces the worst-case complexity</p> <pre><code>Algorithm randomizedQuickSort(a, p, r)\n    if(p&lt;r)\n        q = randomizedPartition(a, p, r)\n\n        randomizedQuickSort(arr, p, q - 1)  // Before pivot\n            randomizedQuickSort(arr, q + 1, r) // After pivot\n\nAlgorithm randomizedPartition(a, p, r)\n    i = random(p, r)\n    exchange a[r] with a[i]\n    return partition(a, p, r)\n    // same as regular quick sort partition\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/07_Sorting/#insertion-sort","title":"Insertion Sort","text":"<pre><code>Algorithm insertionSort(a, n)\n    for i = 1 to n // important\n        key &lt;- a[i]\n        j &lt;- i-1\n        while j&gt;=0 and a[j] &gt; key\n            a[j+1] &lt;- a[j]\n            j &lt;- j-1\n        a[j+1] &lt;- key\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/07_Sorting/#bucket-sort","title":"Bucket Sort","text":"<p>Complexity goes down, as now you\u2019ll only be sorting subarrays.</p> <pre><code>function bucketSort(array, k) is\n  buckets \u2190 new array of k empty lists\n  M \u2190 the maximum key value in the array\n\n  for i = 1 to length(array) do\n    insert array[i] into buckets[floor(k \u00d7 array[i] / M)]\n\n  for i = 1 to k do\n    insertionSort(buckets[i])\n\n  return the concatenation of buckets[1], ...., buckets[k]\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/07_Sorting/#textbook","title":"Textbook","text":"<pre><code>Algorithm bucketSort(a, n)\n    buckets &lt;- array of linked lists\n    max &lt;- maximum key value in the array\n\n    for each entry e in S do\n        k &lt;- key of e\n        remove e from s\n        insert e at the end of bucket[k]\n\n    for i&lt;-0 to n-1 do\n        for each entry e in bucket[i] do\n            remove e from b[i]\n            insert e at the end of S\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/07_Sorting/#radix-sort","title":"Radix Sort","text":"<p>Lexicographical sort</p> <p>Tuple-based sorting for multi-dimensional element.</p> <pre><code>Algorithm radixSort(s)\n    INPUT sequence s of d-tuples\n    OUTPUT sequence s sorted lexicographically\n\n    bucketSort(S, Comparator)\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/07_Sorting/#heap-sort","title":"Heap Sort","text":"<ol> <li>Build a heap (\\(n\\) steps)</li> <li>Repeat (\\(n\\) times)</li> <li>Remove the root node and place in the last index</li> <li>Rebuild max-heap</li> </ol> <pre><code>void heapSort(int[] a)\n{\n  buildHeap(a, a.length);\n\n  for(int i = n-1; i&gt;=0; i--)\n  {\n    swap(a[0], a[i]);\n    maxHeapify(a, i, 0)\n  }\n}\n</code></pre> Sort Direction Heap Type Ascending Max-Heap Descending Min-Heap"},{"location":"2_Core/Data_Structures_%26_Algorithms/08_Hashing/","title":"08 Hashing","text":"<p>Data structure to store key-element pairs. Each key-element pair is called an item.</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/08_Hashing/#benefits","title":"Benefits","text":"<ol> <li>data encryption</li> <li>Search optimization, by reducing the search space</li> </ol>"},{"location":"2_Core/Data_Structures_%26_Algorithms/08_Hashing/#parts","title":"Parts","text":"<ol> <li>Hash Function</li> <li>Bucket/Array (called dictionary/table)</li> </ol>"},{"location":"2_Core/Data_Structures_%26_Algorithms/08_Hashing/#types","title":"Types","text":"Hashing Hash Function Use Component \\(\\sum x_n\\) Polynomial \\(\\sum x_{n} a^{n}\\) Unique code Division \\(k \\% n\\) Reduce code size \\(k=\\) key (element)\\(n =\\) prime (unique code) MAD \\((a k + b) \\% n\\)"},{"location":"2_Core/Data_Structures_%26_Algorithms/08_Hashing/#finding-remainder-in-calculator","title":"Finding Remainder in calculator","text":"<p>Mode &gt; Bases &gt; Decimal</p> <p>Remainder = Dividend - (Divisor * Quotient) = Dividend - (Divisor * \\(\\frac{\\text{Dividend}}{\\text{Divisor}}\\))</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/08_Hashing/#reason","title":"Reason","text":"<p>Dividend = (Divisor * Quotient) + Remainder</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/08_Hashing/#example","title":"Example","text":"\\[ \\begin{aligned} &amp;  10 \\% 3 \\\\ &amp;= 10 - \\left( 3 * \\frac{10}{3} \\right) \\\\ &amp;= 10 - ( 3 * 3 ) \\\\ &amp;= 10 - 9 \\\\ &amp;= 1 \\end{aligned} \\]"},{"location":"2_Core/Data_Structures_%26_Algorithms/09_Collision_Handling/","title":"09 Collision Handling","text":""},{"location":"2_Core/Data_Structures_%26_Algorithms/09_Collision_Handling/#collision-handling","title":"Collision Handling","text":"<p>eliminates collisions in hashing</p> Collision Search Complexity Disadvantage Separate Chaining array with \\(N\\) buckets pointing to linked lists \\(O(n)\\) memory wastage Linear Probing \\((i + j )\\% N\\) \\(O(1)\\) clustering Quadratic Probing \\((i + j^2) \\% N\\) some elements may not be able to stored Double Hashing <p>Problem with probing is the possibility of full bucket</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/09_Collision_Handling/#linear-probing-search","title":"Linear Probing Search","text":"<ol> <li> <p>Compute \\(i = h(k)\\)</p> </li> <li> <p>Start at array cell \\(a[i]\\)</p> </li> <li> <p>Probe consecutive locations until</p> </li> </ol> return case present not present empty cell <p>something more</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/09_Collision_Handling/#double-hashing","title":"Double Hashing","text":"<p>use a secondary hash function \\(d(k)\\)</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/09_Collision_Handling/#hashing","title":"Hashing","text":"\\[ \\begin{aligned} h(k) &amp;= k \\% N \\\\ d(k) &amp;= q - k \\% q \\\\ \\end{aligned} \\] <p>where</p> <ol> <li>\\(q\\) is prime and \\(q &lt; N\\)</li> <li>\\(N\\) is the no of elements</li> </ol>"},{"location":"2_Core/Data_Structures_%26_Algorithms/09_Collision_Handling/#bucket-placement","title":"Bucket Placement","text":"\\[ \\begin{aligned} \\text{index} = \\Big( i+ jd(k) \\Big) \\% N \\\\ i &amp;= h(k) \\\\ j &amp;= 0, 1,\\dots \\end{aligned} \\]"},{"location":"2_Core/Data_Structures_%26_Algorithms/09_Collision_Handling/#load-factor","title":"Load Factor","text":"\\[ \\lambda = \\frac{n}{N} \\] <ul> <li>\\(n\\) is the no of keys</li> <li>\\(N\\) is the no of buckets</li> </ul> <p>Load Factor should preferably be \\(\\lambda &lt; 0.75\\), or atleast \\(\\lambda &lt; 1\\)</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/09_Collision_Handling/#rehashing","title":"Rehashing","text":""},{"location":"2_Core/Data_Structures_%26_Algorithms/10_Trees/","title":"10 Trees","text":"<p>Hierarchical data structure</p> <p>Very useful for organization of data</p> <p>Used for computing spaced used by a directory\u2019s files and sub-directories.</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/10_Trees/#properties","title":"Properties","text":"<p>Refer to Trees in Discrete Structures </p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/10_Trees/#subtrees","title":"Subtrees","text":"<p>Tree consisting of a node, and maybe even descendants</p> <pre><code>flowchart TB\nA --&gt; B &amp; C &amp; sd\nsubgraph sd[Sub Tree]\n    D[\" \"]\nend</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/10_Trees/#height","title":"Height","text":"<p>maxDepth + 1</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/10_Trees/#tree-adt","title":"Tree ADT","text":""},{"location":"2_Core/Data_Structures_%26_Algorithms/10_Trees/#tree-using-linked-list","title":"Tree using Linked List","text":""},{"location":"2_Core/Data_Structures_%26_Algorithms/10_Trees/#bst-using-linked-list","title":"BST using Linked List","text":""},{"location":"2_Core/Data_Structures_%26_Algorithms/10_Trees/#bst-using-arrays","title":"BST using Arrays","text":"<pre><code>Algo\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/11_Binary_Trees/","title":"11 Binary Trees","text":""},{"location":"2_Core/Data_Structures_%26_Algorithms/11_Binary_Trees/#binary-tree","title":"Binary Tree","text":"<pre><code>flowchart TB\n\nsubgraph ct[Complete Tree]\n    direction TB\n    p --&gt; q &amp; r\n    q --&gt; s &amp; t\n    r --&gt; u &amp; v\nend\n\nsubgraph pt[Proper Tree]\n    direction TB\n    a --&gt; b &amp; c\n    b --&gt; d &amp; e\nend</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/11_Binary_Trees/#binary-search-tree","title":"Binary Search Tree","text":"<p>Inorder traversal goes through elements in ascending order</p> <p>Operations Complexity: \\(O(\\log_2 n)\\)</p> <pre><code>flowchart TB\n6 --&gt; 5 &amp; 7\n\n7 --&gt; 0[\" \"] &amp; 8\n8 --&gt; -1[\" \"] &amp; 9\n\n5 --&gt; 3 &amp; 4\n3 --&gt; 1 &amp; 2</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/11_Binary_Trees/#insertion","title":"Insertion","text":"<p>If the element already exists in BST, then traverse left once and then right once</p> <pre><code>flowchart LR\n\nBefore -.-&gt; After\n\nsubgraph Before\ndirection TB\nz[4] --&gt; x[2] &amp; y[3]\nx --&gt; w[1] &amp; l[\" \"]\nend\n\nsubgraph After\ndirection TB\n4 --&gt; 2 &amp; 3\n2 --&gt; 1 &amp; k[\" \"]\n1 --&gt; -1[\" \"] &amp; a[2]\nend</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/11_Binary_Trees/#deletion","title":"Deletion","text":"<pre><code>Algorithm delete(w, v)\n    find(w, v)\n\n    if isExternal()\n        remove w\n    if isInternal\n        Find smallest descendant d of w\n        Replace w with d\n        remove d in the leaf node\n</code></pre> <p>For internal node, you can replace \\(w\\) with the</p> <ul> <li>smallest element from right subtree</li> <li>largest element from left subtree</li> </ul>"},{"location":"2_Core/Data_Structures_%26_Algorithms/11_Binary_Trees/#depth","title":"Depth","text":"<p>Calculating depth is \\(O\\Big(1 + \\text{depth(v)} \\Big)\\)</p> <pre><code>Algorith depth(v)\n    if isRoot(v)\n        return 0\n    else\n        return 1 + depth( parent(v) )\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/11_Binary_Trees/#height","title":"Height","text":"<p>Height is kinda like the reverse of depth</p> <pre><code>Algorith depth(v)\n    if isExternal(v)\n        return 0\n    else\n        h = 0\n        for each w children(v) do\n            h = \n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/11_Binary_Trees/#properties","title":"Properties","text":"Notation Meaning \\(n\\) no of nodes \\(e\\) no of external nodes \\(i\\) no of internal nodes \\(h\\) height \\[ \\begin{aligned} e &amp;= i+1 \\\\ n &amp;= 2e - 1 \\\\ h &amp;\\le i \\\\ h &amp;\\le (n-1)/2 \\end{aligned} \\] \\[ \\begin{aligned} h+1 &amp;\\le e \\le 2^h \\\\ h &amp;\\le i \\le 2^h - 1 \\\\ 2h+1 &amp;\\le i \\le 2^{h+1} - 1 \\\\ one more \\end{aligned} \\] \\[ \\begin{aligned} h &amp;\\ge \\log_2 e \\\\ h &amp;\\ge \\log_2(n_1) -1 \\\\ i &amp;\\le 2^i \\end{aligned} \\]"},{"location":"2_Core/Data_Structures_%26_Algorithms/11_Binary_Trees/#expressions","title":"Expressions","text":"<pre><code>Algorithm printExpression(v)\n    if isInternal(v)\n        print(\"(\")\n        printExpression(leftChild(v))\n    print(v.element())\n    if isExternal(v)\n        printExpression(rightChild(v))\n        print(\")\")\n\nAlgorithm evaluateExpression()\n    something\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/11_Binary_Trees/#euler-tour-traversal","title":"Euler Tour Traversal","text":"Node Type Traversed Internal Thrice External Once"},{"location":"2_Core/Data_Structures_%26_Algorithms/11_Binary_Trees/#application","title":"Application","text":"<p>Computing number of descendants of a node</p> <pre><code>Algo countDescendants(t, v)\n    count = t.counter\n    somethign\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/12_Heap/","title":"12 Heap","text":"<p>Based on complete binary trees</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/12_Heap/#types","title":"Types","text":"Type Property Min-Heap \\(v \\le\\) descendants Max-Heap \\(v \\ge\\) descendants"},{"location":"2_Core/Data_Structures_%26_Algorithms/12_Heap/#implementation","title":"Implementation","text":""},{"location":"2_Core/Data_Structures_%26_Algorithms/12_Heap/#array","title":"Array","text":"<ol> <li>Root = 0</li> <li>Left Child = \\(2i+1\\)</li> </ol> <p>Consider a node with index \\(i\\)</p> Node Value Root of entire tree 0 Left Child \\(2i+1\\) Right Child \\(2i+2\\) Parent \\((i-1)/2\\)"},{"location":"2_Core/Data_Structures_%26_Algorithms/12_Heap/#linked-list","title":"Linked List","text":"\\[ \\fbox{l} \\fbox{data} \\fbox{r} \\fbox{parent} \\notag \\]"},{"location":"2_Core/Data_Structures_%26_Algorithms/12_Heap/#insertion","title":"Insertion","text":"<ol> <li>Find insertion point</li> <li>Store there</li> <li>Verify heap property</li> <li>if not satisfied, up bubbling</li> </ol>"},{"location":"2_Core/Data_Structures_%26_Algorithms/12_Heap/#deletion","title":"Deletion","text":"<ol> <li>Remove the root element (We cannot remove a particular element)</li> <li>Replace node with the last node of the subtree</li> <li>Verify Heap property</li> <li>if not satisfied, down bubbling</li> </ol>"},{"location":"2_Core/Data_Structures_%26_Algorithms/12_Heap/#heapification","title":"Heapification","text":"<p>up/down heap bubbling</p> <ol> <li>compare 2 elements</li> <li>swap if condition is not satisfied</li> </ol> <pre><code>void maxHeapify(int[] a, int n, int i)\n{\n  int largest = i, // assuming root is the largest\n        l = (2*i) + 1,\n        r = (2*i) + 2;\n\n  if(l&lt;n &amp;&amp; a[l]&gt;a[largest])\n    largest = l;\n\n  if(r&lt;n &amp;&amp; a[r]&gt;a[largest])\n    largest = r;\n\n  if(largest != i)\n    // swap root with largest node\n    swap(a[i], a[largest])\n}\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/12_Heap/#applications","title":"Applications","text":"<ol> <li>Heap Sort</li> <li>Order Statistics</li> <li>Priority Queue</li> </ol>"},{"location":"2_Core/Data_Structures_%26_Algorithms/12_Heap/#priority-queue","title":"Priority Queue","text":"<ul> <li>Max-Heap for max-priority queue</li> <li>Min-Heap for min-priority queue</li> </ul>"},{"location":"2_Core/Data_Structures_%26_Algorithms/13_Graphs/","title":"13 Graphs","text":"<p>Graphs in Discrete Structures </p> <p>Graphs without parallel edges and self loops are called as simple graphs.</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/13_Graphs/#representations","title":"Representations","text":""},{"location":"2_Core/Data_Structures_%26_Algorithms/13_Graphs/#adjacency-matrix-array","title":"Adjacency Matrix (Array)","text":"<p>\\(O(n^2)\\)</p> 9 7 40 60 9 1 0 1 0 7 1 1 1 1 40 0 0 1 1 6 0 1 0 1"},{"location":"2_Core/Data_Structures_%26_Algorithms/13_Graphs/#adjacency-list-linked-list","title":"Adjacency List (Linked List)","text":"<p>More efficient, as \\(O(n)\\)</p> <p>(diagram)</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/13_Graphs/#applications","title":"Applications","text":"<ol> <li>Networks<ul> <li>Computer Networks</li> <li>Transportation</li> </ul> </li> <li>Computer Vision    Pixels</li> </ol>"},{"location":"2_Core/Data_Structures_%26_Algorithms/13_Graphs/#connected-graphcomponents","title":"Connected Graph/Components","text":""},{"location":"2_Core/Data_Structures_%26_Algorithms/13_Graphs/#traversals","title":"Traversals","text":"BFS DFS Breadth First Depth First Queue Stack"},{"location":"2_Core/Data_Structures_%26_Algorithms/13_Graphs/#trick-to-remember","title":"Trick to remember","text":"<p>If the person is a Queue-t, they\u2019ll take your breadth away.</p> <p>If it is a stack, it has a depth associated with it.</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/13_Graphs/#single-source-shortest-path","title":"Single Source Shortest Path","text":"<p>Path from a single start point to every other point in the graph</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/13_Graphs/#dijkstras-algorithm","title":"Dijkstra\u2019s algorithm","text":"<p>each step has connected components</p> <p>Time complexity: \\(O(v^2)\\)</p> <p>If you use minimum priority queue, it\u2019ll be \\(O(v+e \\log v)\\)</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/13_Graphs/#disavantages","title":"Disavantages","text":"<p>It requires</p> <ol> <li>Simple graph</li> <li>Connected graph</li> <li>Positive Weights only</li> </ol>"},{"location":"2_Core/Data_Structures_%26_Algorithms/13_Graphs/#minimum-spanning-tree","title":"Minimum Spanning Tree","text":"<p>Refer Discrete Structures</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/13_Graphs/#prims-algorithm","title":"Prim\u2019s Algorithm","text":"<p>Refer Discrete Structures</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/13_Graphs/#kruskals-algorithm","title":"Kruskal\u2019s Algorithm","text":"<p>Refer Discrete Structures</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/14_Tries/","title":"14 Tries","text":"<p>advanced form of tree, mainly used for Text Processing</p> Property of Try Depends on Height 1 + length of longest string Width no of strings"},{"location":"2_Core/Data_Structures_%26_Algorithms/14_Tries/#types","title":"Types","text":"<p>Consider S = {cat, curry, bat, bees, catalyst}</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/14_Tries/#standard","title":"Standard","text":"<p>Stored using regular usual representation of tree</p> <pre><code>flowchart TB\n\n0(( )) --- 1((b)) &amp; 2((c))\n\n1 --- 3((a)) &amp; 4((e))\n\n3 --- 5((t))\n\n4 --- 6((e)) --- 7((s))\n2 --- 8((a)) &amp; 9((u))\n\n8 --- 10((t)) --- 11((a)) --- 12((l)) --- 13((y)) --- 14((s)) --- 15((t))\n\n9 --- 16((u)) --- 17((r)) --- 18((r)) --- 19((y))</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/14_Tries/#compressed","title":"Compressed","text":"<p>Stored using compact representation</p> <pre><code>flowchart TB\n1(( )) --- 2((b)) &amp; 3((c))\n\n2 --- 4((at)) &amp; 5((ees))\n\n3 --- 6((at)) &amp; 7((urry))\n\n6 --- 8((alyst))</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/14_Tries/#suffix","title":"Suffix","text":"<p>It is a compressed trie of suffixes for every character of a single word. Used for testing DNA sequencing \\((ATCG)\\).</p> <p>Consider <code>BANANA</code></p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/14_Tries/#generation","title":"Generation","text":"<pre><code>flowchart TB\n\nroot2(( )) --&gt;\nA &amp; NA &amp; BANANA \n\nA --&gt; NA1[NA] --&gt; NA2[NA]\nNA --&gt; NA3[NA]</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/14_Tries/#addressing","title":"Addressing","text":"<pre><code>flowchart TB\nsomething</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/14_Tries/#encoding-trie","title":"Encoding Trie","text":"<p>Left is 0. Right is 1.</p> <p>Leaves store characters.</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/14_Tries/#huffman-tree","title":"Huffman Tree","text":"<p>Uses huffman encoding</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/14_Tries/#aim","title":"Aim","text":"<p>Assign the minimum key to the character with the maximum frequency.</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/14_Tries/#steps","title":"Steps","text":"<ol> <li> <p>Calculate frequency of each character for input string</p> </li> <li> <p>Build tree, by grouping based on minimum frequencies</p> </li> <li> <p>Generate key/code of tree, taking left as 0 and right as 1</p> </li> <li> <p>Calculate the average no of code/key using</p> </li> </ol> \\[ \\text{Average Code Size} = \\frac{ \\sum\\limits_i \\text{Frequency}_i * \\text{No of Bits}_i }{ \\sum\\limits_i \\text{Frequency}_i } \\]"},{"location":"2_Core/Data_Structures_%26_Algorithms/14_Tries/#example","title":"Example","text":"<p>Consider input string <code>abracadabra</code></p> Letter Frequency a 5 b 2 c 1 d 1 r 2 <pre><code>flowchart TB\n11 --- |0| 1[a]\n11 --- |1| 6\n\n6 --- |0| 2\n6 --- |1| 4\n\n2 --- |0| c\n2 --- |1| d\n\n4 --- |0| b\n4 --- |1| r</code></pre> Letter Code a 0 b 110 c 100 d 101 r 111 <p>Average code size = 2.09</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/14_Tries/#compact-representation","title":"Compact Representation","text":"<p>Each string is added into an array of strings.</p> <p>Each node of the tree contains <code>(string_index, substring_start_index, substring_end_index)</code></p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/15_AVL/","title":"15 AVL","text":"<p>Balanced BST that reduce the worst-case time complexity from linear to logarithmic.</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/15_AVL/#balance-factor","title":"Balance Factor","text":"\\[ BF = \\text{Height of left subtree} - \\text{Height of right subtree} \\] <p>Leaves are always balanced, as they have a balanced factor of 0.</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/15_AVL/#balanced-tree","title":"Balanced Tree","text":"<p>Tree with balanced factor \\(\\{ -1, 0, +1 \\}\\)</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/15_AVL/#unbalanced-oversettextrotationlongrightarrow-balanced","title":"Unbalanced \\(\\overset{\\text{rotation}}{\\longrightarrow}\\) Balanced","text":"<p>Rotation Mechanism</p> Unbalanced Type of Rotation LL RR RR LL LR LR RL RL <pre><code>flowchart\n\nsubgraph Balanced\ndirection TB\np((2)) --- q((1)) &amp; r((3))\nend\nsubgraph LR Unbalanced\ndirection TB\na((3)) --- b((1)) &amp; c(( ))\n\nb --- d(( )) &amp; e((2))\nend\nsubgraph LL Unbalanced\ndirection TB\nf((3)) --- g((2)) &amp; h(( ))\n\ng --- i((1)) &amp; j(( ))\nend\n\nsubgraph RR Unbalanced\ndirection TB\nk((3)) --- l(( )) &amp; m((4))\n\nm --- n(( )) &amp; o((5))\nend\n\nsubgraph RL Unbalanced\ndirection TB\ns((3)) --- t(( )) &amp; u((5))\n\nu --- v((4)) &amp; w(( ))\nend</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/15_AVL/#time-complexity","title":"Time Complexity","text":"Operation Compexity Restructure \\(O(1)\\) Search \\(O(\\log_2 n)\\) Insertion \\(O(\\log_2 n)\\) Deletion \\(O(\\log_2 n)\\)"},{"location":"2_Core/Data_Structures_%26_Algorithms/16_B_Tree/","title":"16 B Tree","text":"<p>Generalized and ordered BST, where each node contains children as linked list, rather than just elements.</p> <p>It is used for data storage and access in hard disks.</p> <p></p> Feature Formula Order \\(n\\) Max No of Children \\(n\\) Max No of Keys \\(n-1\\) Middle element \\(\\left \\lceil \\dfrac{n}{2} \\right \\rceil\\)<sup>th</sup> element <p>There is no minimum for B Tree.</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/16_B_Tree/#direction","title":"Direction","text":"<p>It is grown in an upward direction, because</p> <ul> <li>insertion occurs only in the leaf nodes</li> <li>ensure balanced tree (as it will be hard to balance once the B Tree is already built)</li> </ul>"},{"location":"2_Core/Data_Structures_%26_Algorithms/16_B_Tree/#limitations","title":"Limitations","text":"<p>It has high space complexity, as many locations are empty.</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/16_B_Tree/#complexity","title":"Complexity","text":"Operation Compexity Restructure \\(O(1)\\) Search \\(O(\\log_2 n)\\) Insertion \\(O(\\log_2 n)\\) Deletion \\(O(\\log_2 n)\\)"},{"location":"2_Core/Data_Structures_%26_Algorithms/17_Tensors/","title":"Tensors","text":"<p>(Not for exam)</p> <p>Tensors are \\(n\\)-dimensional arrays, which keep track of the gradient of each element in the array.</p> <p>They are optimized for parallel computing and GPU-utilization, but more memory-intensive than regular arrays.</p> <p>In lazy mode, the operations are not executed until required</p> <p>Each tensor has</p> <ul> <li>ID</li> <li>List of inputs</li> <li>operation performed</li> <li>cached_data_output</li> </ul> <p>Tracking gradients is expensive, so</p> <pre><code>x = ndl.Tensor(\n    [1],\n  dtype = \"float32\"\n)\n\nsum = 0\nfor i in range(100):\n  sum += (x**2).detach()\n</code></pre> <p>Not using <code>detach()</code> will result in tracking the inputs and operations performed unnecessarily</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/17_Tensors/#broadcasting","title":"Broadcasting","text":"<p>Efficient, as it does not copy any data</p> <p>Rather than repeating the same value multiple times for matrix multiplication</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/00_Practice_Lab/","title":"00 Practice Lab","text":""},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/00_Practice_Lab/#java-file-handling","title":"Java File Handling","text":"<pre><code>import java.io.File;\nimport java.io.FileNotFoundException;\nimport java.io.PrintWriter;\nimport java.util.Scanner;\npublic class f {\n  public static void main(String[] args) throws\n  FileNotFoundException {\n    //Scanner method of reading files (Since JDK 4)\n    //Open file for reading contents\n    System.out.println( &amp; quot; Your file should be placed at: &amp; quot; +\n      System.getProperty( &amp; quot; user.dir &amp; quot;));\n    Scanner readMyFile = new Scanner(new File(args[0]));\n    //Open file for writing content\n    System.out.println( &amp; quot; Output file will be created at: &amp; quot; +\n      System.getProperty( &amp; quot; user.dir &amp; quot;));\n    PrintWriter writeToMyFile = new PrintWriter(new File(args[1]));\n    while (readMyFile.hasNext()) {\n      // Read the content of input file\n      // Read 3 integers\n      int a = readMyFile.nextInt();\n      int b = readMyFile.nextInt();\n      int c = readMyFile.nextInt();\n      //Read the string\n      String name = readMyFile.next(); //not\n      readMyFile.nextLine()\n      //Read a float\n      float f = readMyFile.nextFloat();\n      //Display the content of input file\n      System.out.printf( &amp; quot; % d % d % d % s % f % n &amp; quot;, a, b, c, name, f);\n      //You can also use System.out.print to display one data\n      type at a time.\n      /*\n      * %n is a new line character appropriate to the\n      platform running the application.\n      * You should always use %n, rather than \\n.\n      */\n      //Write data to file\n      int result = (a * a) + (b * b) + (c * c);\n      System.out.format( &amp; quot;\n        (a * a + b * b + c * c) = % d % s % f % n &amp; quot;,\n        result, name, f);\n      writeToMyFile.format( &amp; quot;\n        (a * a + b * b + c * c) = % d % s %\n        f % n &amp; quot;, result, name, f);\n      /*\n\n      6\n\n      * Again writeToMyFile.print can be use to write one\n      data type at a type.\n      */\n    }\n    readMyFile.close();\n    writeToMyFile.close();\n  }\n}\n/* now try to read inputs for these fields from keyboard */\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/00_Practice_Lab/#programming-language","title":"Programming Language","text":"<p>Both the below programs were made using Java.</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/00_Practice_Lab/#question-1","title":"Question 1","text":"<p>Write a program in your favorite programming language (C/C++/JAVA) to determine if a given Input Number is PERFECT or DEFICIENT or ABUNDANT. Assume that the input number is in the range 1 \u2013 32768 (inclusive at both sides).</p> <p>A number (consider only positive integers) is perfect if it is equal to the sum of its proper divisors. For example, 6 is a perfect number, because its proper divisors are 1, 2, and 3(note that we do not include the number itself), and 1+2+3=6.</p> <p>A number is de\ufb01cient if the sum of its proper divisors is less than the number. For example, 8 is de\ufb01cient, because its proper divisors are 1, 2, and 4, and 1 + 2 + 4 = 7, which is less than 8.</p> <p>A number is abundant if the sum of its proper divisors is greater than the number. For example, 12 is abundant, because 1 + 2 + 3 + 4 + 6 = 16, which is greater than 12.</p> <p>Write a program that prompts the user for a number, then determines whether the number is perfect, de\ufb01cient, or abundant. Your program should continue to prompt the user for numbers until a 0 is provided as input. An example session:</p> <pre><code>Enter an integer (0 to quit): 7\n7 is deficient.\nEnter an integer (0 to quit): 12\n12 is abundant.\nEnter an integer (0 to quit): 6\n6 is perfect.\nEnter an integer (0 to quit): 0\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/00_Practice_Lab/#algorithm","title":"Algorithm","text":"<ol> <li>Input number</li> <li>Find factors and their sum</li> <li> <p>Check the various cases. If sum of factors is</p> </li> <li> <p>\\(=\\) number, then perfect</p> </li> <li>\\(&lt;\\) number, then deficient</li> <li>\\(&gt;\\) number, then abundant</li> <li>Print the result</li> </ol> <pre><code>pseudocode\n</code></pre> <p>Time complexity is \\(O(n)\\), because of the <code>for</code> loop.</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/00_Practice_Lab/#code","title":"Code","text":"<pre><code>import java.util.Scanner;\n\nclass q\n{\n  public static void checker(int num)\n  {\n    int factorSum = 0;\n\n    for(int i = 1; i&lt;num; i++)\n      if(num % i == 0)\n        factorSum += i;\n\n    String text = \"\";\n    if (factorSum == num)\n      text = \"Perfect\";\n    else if (factorSum &lt; num)\n      text = \"Deficient\";\n    else if (factorSum &gt; num)\n      text = \"Abundant\";\n    System.out.println( num + \" is \" + text + \" number\");\n  }\n  public static void main( String args[] )\n  {\n    Scanner inp = new Scanner( System.in );\n\n    checker(7);\n    checker(12);\n    checker(6);\n\n    System.out.println(\"\\nInput a number of your wish:\");\n    int input = inp.nextInt();\n    checker(input);\n  }\n}\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/00_Practice_Lab/#inputoutput","title":"Input/Output","text":"<pre><code>7 is Deficient number\n12 is Abundant number\n6 is Perfect number\n\nInput a number of your wish:\n18\n18 is Abundant number\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/00_Practice_Lab/#question-2","title":"Question 2","text":"<p>Write a program that inputs two fractions in the form a/b and c/d, and outputs their sum in the form p/q cancelled down to its simplest form. Here, you can read the values of a,b,c,d as input from keyboard and show the output in the simplest form. i.e. numerator / denominator.</p> <pre><code>Input:\u00a05/6\u00a01/10 Output: 14/15\nInput:\u00a02/3\u00a04/6 Output: 4/3\nInput: 1/2\u00a03/4 Output: 5/4\nInput: 1/2\u00a01/2 Output: 1/1\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/00_Practice_Lab/#algorithm_1","title":"Algorithm","text":"<ol> <li>Input numbers</li> <li>Obtain the numerator and denominator by cross-multiplication</li> <li>Simplify the numerator and denominator</li> <li>Print the result</li> </ol> <pre><code>pseudocode\n</code></pre> <p>Time Complexity is \\(O(n)\\), because of the <code>for</code> loop.</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/00_Practice_Lab/#code_1","title":"Code","text":"<pre><code>import java.util.Scanner;\n\nclass q02 {\n  public static void checker(int a, int b, int c, int d)\n  {\n    int p = a*d + b*c,\n      q = b*d;\n\n    int pSim = p,\n      qSim = q;\n\n    for(int i = Math.min(p, q); i&gt;=2; i--)\n      if(p%i == 0 &amp;&amp; q%i == 0)\n      {\n        pSim = p/i;\n        qSim = q/i;\n        break;\n      }\n\n    System.out.println(\"\\n\" + pSim + \"/\" + qSim);\n  }\n  public static void main(String[] args) {\n    Scanner inp = new Scanner( System.in );\n    int a, b, c, d;\n    System.out.println(\"\\nEnter your values\");\n    System.out.println(\"a\"); a = inp.nextInt();\n    System.out.println(\"b\"); b = inp.nextInt();\n    System.out.println(\"c\"); c = inp.nextInt();\n    System.out.println(\"d\"); d = inp.nextInt();\n\n    checker(a, b, c, d);\n  }\n}\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/00_Practice_Lab/#inputoutput_1","title":"Input/Output","text":"<pre><code>Enter your values\na\n5\nb\n6\nc\n1\nd\n10\n\n14/15\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/01_Stacks/","title":"01 Stacks","text":""},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/01_Stacks/#question","title":"Question","text":"<p>Write a C/C++/JAVA program to perform the following actions on a STACK implemented using arrays / array of structures:</p> <ol> <li>Implement PUSH operation in a STACK for N (N &gt;= 5) STUDENT RECORDS. Each STUDENT RECORD should store &lt;IDNO, NAME, DOB,CGPA&gt;. You have to read each STUDENT record from an input file \u201cstudentin.dat\u201d stored locally in your directory and the PUSH it into the stack, one at a time. (You can use vi editor to create input data file. Make sure that the input data file contains at least 5 records).</li> <li>Implement the POP operation for the STACK in LIFO order and display all the records on the standard output (screen). Also, write the output results into an external file \u201cstudentout.dat\u201d.</li> </ol>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/01_Stacks/#algorithm","title":"Algorithm","text":""},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/01_Stacks/#pseudocode","title":"Pseudocode","text":"<pre><code>Algorithm push\n        INPUT read student records from file\n        OUTPUT student records to stack\n\n        while inputFile has records\n      if top = n\n        overflow\n      else\n        top &lt;- top + 1\n        studentArray[top] &lt;- record\n\nAlgorithm pop\n        INPUT student records from stack\n        OUTPUT write student records to file\n\n        while studentArray has records\n        print record\n        write to outputFile\n        top &lt;- top - 1\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/01_Stacks/#time-complexity","title":"Time Complexity","text":"Algorithm Complexity push \\(O(n)\\) pop \\(O(n)\\)"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/01_Stacks/#source-code","title":"Source Code","text":"<pre><code>// Ahmed Thahir 2020A7PS0198U\n\npackage Programs;\n\nimport java.io.File;\nimport java.io.FileNotFoundException;\nimport java.io.PrintWriter;\nimport java.util.Scanner;\n\npublic class p01 \n{\n  static int top = -1;\n  static int capacity = 10;\n  static String[] students = new String[capacity];\n\n  public static void push() throws FileNotFoundException\n  {\n    String inputFile = \"h:\\\\\nMy Drive\\\\\nNotes\\\\\nSem 4\\\\\n02 DSA\\\\\nPracticals\\\\\nPrograms\\\\\n\"\n      + \"studentin.dat\";\n    Scanner readMyFile = new Scanner( new File(inputFile) );\n\n    while (readMyFile.hasNext()) \n    {\n      if(top != capacity)\n      {\n        top = top + 1;\n        students[top] = readMyFile.nextLine();\n      }\n    }\n\n    readMyFile.close();\n  }\n\n  public static void pop() throws FileNotFoundException\n  {\n    String outputFile = \"h:\\\\\nMy Drive\\\\\nNotes\\\\\nSem 4\\\\\n02 DSA\\\\\nPracticals\\\\\nPrograms\\\\\n\"\n      + \"studentout.dat\";\n    PrintWriter writeToMyFile = new PrintWriter( new File(outputFile) );\n\n    while(top != -1)\n    {\n      System.out.println(students[top]);\n      writeToMyFile.format(\"%s \\n\", students[top]);\n\n      top = top-1;\n    }\n\n    writeToMyFile.close();\n  }\n\n  public static void main(String[] args) throws FileNotFoundException\n  {\n    push();\n    pop();\n  }\n}\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/01_Stacks/#test-cases","title":"Test Cases","text":""},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/01_Stacks/#input","title":"Input","text":"<pre><code>2021A7PS001 AAAA 1/1/2000 7.50\n2021A7PS002 BBBB 2/1/2000 9.20\n2021A7PS003 CCCC 3/1/2000 9.60\n2021A7PS004 DDDD 4/1/2000 8.75\n2021A7PS005 EEEE 5/1/2000 9.25\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/01_Stacks/#output","title":"Output","text":"<pre><code>2021A7PS005 EEEE 5/1/2000 9.25\n2021A7PS004 DDDD 4/1/2000 8.75\n2021A7PS003 CCCC 3/1/2000 9.60\n2021A7PS002 BBBB 2/1/2000 9.20\n2021A7PS001 AAAA 1/1/2000 7.50\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/02_Circular_Queues/","title":"02 Circular Queues","text":""},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/02_Circular_Queues/#question","title":"Question","text":"<p>Write a C/C++ program to perform the following actions on a QUEUE implemented using arrays / array of structures / array-lists (which is viewed circularly):</p> <ol> <li>Implement ENQUEUE(o) operation in a QUEUE for N (N &gt;= 5) STUDENT RECORDS. Each STUDENT RECORD should store <code>&lt;IDNO, NAME, DOB, CGPA&gt;</code>. You have to read each STUDENT record from an input file \u201cstudentin.dat\u201d stored locally in your directory and insert it into the queue, one at a time. (You can use vi editor to create input data file. Make sure that the input data file contains at least 5 records).</li> <li>Implement the DEQUEUE() operation for the QUEUE in FIFO order and display all the records on the standard output (screen display). Also, write the output results into an external file \u201cstudentout.dat\u201d.</li> <li>Display the student names (NAME field) whose CGPA is less than 9.</li> </ol>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/02_Circular_Queues/#algorithm","title":"Algorithm","text":""},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/02_Circular_Queues/#pseudocode","title":"Pseudocode","text":"<pre><code>Algorithm enqueue\n        INPUT read student records from file\n        OUTPUT student records to queue\n\n        while inputFile has records\n            if (F = 0 and R = n-1) or (F=R+1)\n                overflow\n            else if (F = -1 and R = -1)\n                F &lt;- 0\n                R &lt;- 0\n            else if (R = n-1)\n                R &lt;- 0\n            else\n                R &lt;- R+1\n\n            a[R] &lt;- element\n\nAlgorithm dequeue\n        INPUT student records from queue\n        OUTPUT write student records to file\n\n        while studentArray has records\n                record &lt;- a[F]\n                print record\n                write to outputFile\n\n                F &lt;- F+1\n\nAlgorithm displayNames\n        INPUT student records from queue\n        OUTPUT write student records to file\n\n        while studentArray has records\n                record &lt;- a[F]\n\n                if(cgpa &lt; 9.0)\n                    print name\n\n                F &lt;- F+1\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/02_Circular_Queues/#time-complexity","title":"Time Complexity","text":"Algorithm Complexity enqueue \\(O(n)\\) dequeue \\(O(n)\\) displayNames \\(O(n)\\)"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/02_Circular_Queues/#source-code","title":"Source Code","text":"<pre><code>// Ahmed Thahir 2020A7PS0198U\n\npackage Programs;\nimport java.io.File;\nimport java.io.FileNotFoundException;\nimport java.io.PrintWriter;\nimport java.util.Scanner;\n\npublic class p02\n{\n  static int f, r, n = 10;\n  static String[] students = new String[n];\n\n  public static void enqueue() throws FileNotFoundException\n  {\n    f = -1;\n    r = -1;\n\n    String inputFile = \"h:\\\\\nMy Drive\\\\\nNotes\\\\\nSem 4\\\\\n02 DSA\\\\\nPracticals\\\\\nPrograms\\\\\n\"\n      + \"studentin.dat\";\n    Scanner readMyFile = new Scanner( new File(inputFile) );\n\n    while (readMyFile.hasNext()) \n    {\n      if( (f == 0 &amp;&amp; r == n-1) || f == r+1 )\n      {\n        // overflow\n      }\n      else if (f == -1 &amp;&amp; r == -1)\n      {\n        f = 0;\n        r = 0;\n      }\n      else if (r == n-1)\n        r = 0;\n      else\n        r++;\n\n      students[r] = readMyFile.nextLine();\n    }\n\n    readMyFile.close();\n  }\n\n  public static void dequeue() throws FileNotFoundException\n  {\n    String outputFile = \"h:\\\\\nMy Drive\\\\\nNotes\\\\\nSem 4\\\\\n02 DSA\\\\\nPracticals\\\\\nPrograms\\\\\n\"\n      + \"studentout.dat\";\n    PrintWriter writeToMyFile = new PrintWriter( new File(outputFile) );\n\n    while( !(f == -1 &amp;&amp; r == -1) )\n    {\n      System.out.println(students[f]);\n      writeToMyFile.format(\"%s \\n\", students[f]);\n\n      if (F == n-1)\n                F = 0;\n      else if (F == R)\n      {\n        F = -1;\n        R = -1;\n      }\n      else\n                F = F+1;\n    }\n\n    writeToMyFile.close();\n  }\n\n  public static void displayNames() throws FileNotFoundException\n  {\n    enqueue();\n\n    while( !(f == -1 &amp;&amp; r == -1) )\n    {\n      String[] strArray = students[f].split(\" \");  \n      float cgpa = Float.parseFloat( strArray[strArray.length -1] );\n      String name;\n\n      if(cgpa &lt; 9f)\n      {\n        name = strArray[1];\n        System.out.println(name);\n      }\n\n      if (F == n-1)\n                F = 0;\n      else if (F == R)\n      {\n        F = -1;\n        R = -1;\n      }\n      else\n                F = F+1;\n    }\n  }\n\n  public static void main(String[] args) throws FileNotFoundException\n  {\n    enqueue();\n    dequeue();\n    displayNames();\n  }\n}\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/02_Circular_Queues/#test-cases","title":"Test Cases","text":""},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/02_Circular_Queues/#input","title":"Input","text":"<pre><code>2021A7PS001 AAAA 1/1/2000 7.50\n2021A7PS002 BBBB 2/1/2000 9.20\n2021A7PS003 CCCC 3/1/2000 9.60\n2021A7PS004 DDDD 4/1/2000 8.75\n2021A7PS005 EEEE 5/1/2000 9.25\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/02_Circular_Queues/#output","title":"Output","text":"<pre><code>2021A7PS001 AAAA 1/1/2000 7.50\n2021A7PS002 BBBB 2/1/2000 9.20\n2021A7PS003 CCCC 3/1/2000 9.60\n2021A7PS004 DDDD 4/1/2000 8.75\n2021A7PS005 EEEE 5/1/2000 9.25\nAAAA\nDDDD\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/03_DLL/","title":"03 DLL","text":""},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/03_DLL/#question","title":"Question","text":"<p>Write a C/C++/JAVA program to perform the following actions on a DOUBLY LINKED LIST:</p> <ol> <li>Implement insertLast(o) operation in a DLL for N (say N= 5) RECORDS of DATA SET (Student Record). You have to read each record of data set from an input file \u201cstudentin.dat\u201d stored locally in your directory and the insert it into the DLL, one at a time. (You can use vi/joe editor to create input data file. Make sure that the input data file contains at least 5 records).</li> <li>Implement the remove(p) operation for the DLL for any one record, by interactively asking for its position p from standard input (keyboard).</li> <li>Traverse the List in forward direction (beginning to end) and display all records on the standard output (display).</li> <li>Traverse the List in reverse direction (end to beginning) and display all records on the standard output (display).</li> </ol>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/03_DLL/#algorithm","title":"Algorithm","text":""},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/03_DLL/#pseudocode","title":"Pseudocode","text":"<pre><code>Algorithm insertLast(d)\n    INPUT read student records from file\n    OUTPUT student records to DLL\n\n    if size = 0\n        start = inserted element\n    else\n        inserted element's back link = existing element\n        existing element's front link = inserted element\n\n    end = n\n    size = size + 1\n\nAlgorithm remove(p)\n    INPUT student records of DLL\n    INPUT position of element to be removed\n    OUTPUT deleted element\n\n    if size = 0\n        empty list\n    else if p &gt; size\n        position out of bounds\n    else\n        current element = start\n        while current element is not null\n        if current element index = p\n            Print deleted element\n            previous element's front link = next element\n            next element's back link = previous element\n\n            current element's back and front link = null\n        else\n            current element = current element's front link\n            increment i\n\nAlgorithm traverseForward\n    INPUT student records of DLL\n    OUTPUT forward traversed list\n\n    if size = 0\n        List Empty\n    else\n        current element = start\n        while current element is not null\n            Print current element\n            current element = current element's front link\n\nAlgorithm traverseBackward\n    INPUT student records of DLL\n    OUTPUT backward traversed list\n\n    if size = 0\n        List Empty\n    else\n        current element = end\n        while current element is not null\n            Print current element\n            current element = current element's back link\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/03_DLL/#time-complexity","title":"Time Complexity","text":"Algorithm Complexity insertLast \\(O(n)\\) remove \\(O(n)\\) traverseForward \\(O(n)\\) traverseBackward \\(O(n)\\)"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/03_DLL/#extra-note","title":"Extra Note","text":"<p>I was initially thinking of assigning an index for each node, but that will increase the amount of steps for each process, because the processor has to update the index of elements any time a change happens to the list.</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/03_DLL/#source-code","title":"Source Code","text":"<pre><code>// Ahmed Thahir 2020A7PS0198U\n\npackage Programs;\nimport java.io.File;\nimport java.io.FileNotFoundException;\nimport java.util.Scanner;\n\nclass Node\n{\n  Node bp; // back pointer\n  String d; // data\n  Node fp; // front pointer\n\n  Node()\n  {\n    bp = null;\n    fp = null;\n  }\n\n  Node(String val)\n  {\n    this();\n    d = val;\n  }\n\n  Node(Node bptr, String data, Node fptr)\n  {\n    bp = bptr;\n    d = data;\n    fp = fptr;\n  }\n\n  void setBp(Node ptr)\n  {\n    bp = ptr;\n  }\n\n  void setFp(Node ptr)\n  {\n    fp = ptr;\n  }\n\n  void setData(String data)\n  {\n    d = data;\n  }\n\n  Node getBp()\n  {\n    return bp;\n  }\n\n  String getData()\n  {\n    return d;\n  }\n\n  Node getFp()\n  {\n    return fp;\n  }\n}\n\nclass DLL // Double Linked List\n{\n  static Node start;\n  static Node end;\n  static int size;\n\n  DLL()\n  {\n    start = null;\n    end = null;\n    size = 0;\n  }\n\n  public static void insertLast(String d)\n  {\n    Node n = new Node(d);\n    if(size == 0)\n    {\n      start = n;\n    }\n    else\n    {\n      n.setBp(end); // link new node's back to the existing node\n      end.setFp(n); // link existing node's front to the new node\n    }\n\n    end = n;\n    size++;\n  }\n\n  public static void remove(int pos)\n  {\n    int index = pos-1;\n\n    if(size == 0)\n    {\n      System.out.println(\"Empty\");\n    }\n    else if (index &gt;= size)\n    {\n      System.out.println(\"Index out of bounds\");\n    }\n    else\n    {\n      Node n = start;\n      int i = 0;\n      while( n!=null )\n      {\n        if(i == index)\n        {\n          System.out.println( \"Deleting: \" + n.getData() );\n\n          // link the previous and next one with each other\n          Node prev = n.getBp();\n          Node next = n.getFp();\n          prev.setFp(next);\n          next.setBp(prev);\n\n          // unlink the deleted node\n          n.setBp(null);\n          n.setFp(null);\n\n          break;\n        }\n        else\n        {\n          n = n.getFp();\n          i++;\n        }\n      }\n    }\n  }\n\n  public static void traverseForward()\n  {\n    if(size == 0)\n    {\n      System.out.println(\"List Empty\");\n    }\n    else\n    {\n      System.out.println(\"\\n\" + \"Traversing Forward\");\n\n      Node n = start;\n      while( n!=null )\n      {\n        System.out.println( n.getData() );\n        n = n.getFp();\n      }\n    }\n  }\n\n  public static void traverseBackward()\n  {\n    if(size == 0)\n    {\n      System.out.println(\"List Empty\");\n    }\n    else\n    {\n      System.out.println(\"\\n\" + \"Traversing Backward\");\n\n      Node n = end;\n      while( n!=null )\n      {\n        System.out.println( n.getData() );\n        n = n.getBp();\n      }\n    }\n  }\n}\n\npublic class p03\n{\n  static String rel = \"h:\\\\\nMy Drive\\\\\nNotes\\\\\nSem 4\\\\\n02 DSA\\\\\nPracticals\\\\\nPrograms\\\\\n\";\n  static String inputFile = rel +\n    \"studentin.dat\";\n  static String outputFile = rel +\n    \"studentout.dat\";\n\n  DLL students = new DLL();\n\n  public static void main(String[] args) throws FileNotFoundException\n  {\n    Scanner readMyFile = new Scanner( new File(inputFile) );\n    System.out.println(\"Reading from File\");\n    while (readMyFile.hasNext()) \n    {\n      String o = readMyFile.nextLine();\n      System.out.println(o);\n      DLL.insertLast(o);\n    }\n    readMyFile.close();\n\n    System.out.println(\"\\n\" + \"Enter position (1, 2, ...) to remove\");\n    Scanner inp = new Scanner(System.in);\n    int p = inp.nextInt();\n    inp.close();\n    DLL.remove(p);\n\n    DLL.traverseForward();\n    DLL.traverseBackward();\n  }\n}\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/03_DLL/#test-cases","title":"Test Cases","text":""},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/03_DLL/#input","title":"Input","text":"<pre><code>2021A7PS001 AAAA 1/1/2000 7.50\n2021A7PS002 BBBB 2/1/2000 9.20\n2021A7PS003 CCCC 3/1/2000 9.60\n2021A7PS004 DDDD 4/1/2000 8.75\n2021A7PS005 EEEE 5/1/2000 9.25\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/03_DLL/#output","title":"Output","text":"<pre><code>Reading from File\n2021A7PS001 AAAA 1/1/2000 7.50\n2021A7PS002 BBBB 2/1/2000 9.20\n2021A7PS003 CCCC 3/1/2000 9.60\n2021A7PS004 DDDD 4/1/2000 8.75\n2021A7PS005 EEEE 5/1/2000 9.25\n\nEnter position (1, 2, ...) to remove\n2\nDeleting: 2021A7PS002 BBBB 2/1/2000 9.20\n\nTraversing Forward\n2021A7PS001 AAAA 1/1/2000 7.50\n2021A7PS003 CCCC 3/1/2000 9.60\n2021A7PS004 DDDD 4/1/2000 8.75\n2021A7PS005 EEEE 5/1/2000 9.25\n\nTraversing Backward\n2021A7PS005 EEEE 5/1/2000 9.25\n2021A7PS004 DDDD 4/1/2000 8.75\n2021A7PS003 CCCC 3/1/2000 9.60\n2021A7PS001 AAAA 1/1/2000 7.50\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/04_Search/","title":"04 Search","text":""},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/04_Search/#question","title":"Question","text":"<p>Write a C/C++/JAVA program to perform the following: 1. Initialize 10000 unique, positive and consecutive integers whose values are in the range \\([0-9999]\\), and store them in serial/ascending order in an array <code>A[10000]</code>. i.e. you are already initializing the array in the sorted order using an iterative statement. 2. Implement the Linear Search and Binary Search algorithms and DISPLAY the <code>&lt;position in the array, search_time&gt;</code> for the following test cases: \\(5000, 9997, 50000\\)</p> <p>(assume index of the first element in array is 0)</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/04_Search/#algorithm","title":"Algorithm","text":""},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/04_Search/#pseudocode","title":"Pseudocode","text":"<pre><code>Algorithm linearSearch()\n    OUTPUT position of data\n\n    pos &lt;- -1\n    for i&lt;-0 to (n-1) do\n        if a[i] = data\n            pos = i\n    return pos\n\nAlgorith binarySearch()\n    OUTPUT position of searched element\n\n    pos &lt;- -1\n    f &lt;- 0\n    l &lt;- n-1\n\n    while f &lt;= l\n        if a[m] &lt; data\n            f = m+1\n        else if a[m] &gt; data\n        l = m-1\n    else\n        pos = i\n  return pos\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/04_Search/#time-complexity","title":"Time Complexity","text":"Algorithm Complexity linearSearch \\(O(n)\\) binarySearch \\(O(\\log_2 n)\\)"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/04_Search/#source-code","title":"Source Code","text":"<pre><code>// Ahmed Thahir 2020A7PS0198U\npackage Programs;\nclass p04\n{\n    static int n = 10000;\n    static int n1 = 5000,\n        n2 = 9997,\n        n3 = 50000;\n    static int[] a = new int[n];\n\n    static float linearSearchTime, binarySearchTime;\n\n    public static void initialize()\n    {\n        for(int i = 0; i&lt;n; i++)\n        a[i] = i;\n    }\n\n    public static int linearSearch(int d)\n    {\n        int pos = -1;\n        long startTime = System.nanoTime();\n\n        for(int i = 0; i&lt;n; i++)\n            if(a[i] == d)\n            {\n                pos = i;\n                break;\n            }\n\n        long endTime = System.nanoTime();\n        linearSearchTime = (endTime - startTime)/1000f;\n        return pos;\n    }\n\n    public static int binarySearch(int d)\n    {\n        int pos = -1;\n        long startTime = System.nanoTime();\n\n        int f = 0, l = n-1, m;\n        while(f &lt;= l)\n        {\n            m = (f+l)/2;\n\n            if(a[m] &lt; d)\n            f = m + 1;\n            else if ( a[m] &gt; d)\n            l = m - 1;\n            else // a[m] == d\n            {\n                pos = m;\n                break;\n            }\n        }\n\n        long endTime = System.nanoTime();\n        binarySearchTime = (endTime - startTime)/1000f;\n\n        return pos;\n    }\n\n    public static void linearSearchDisplay()\n    {\n        System.out.println( \n        \"Linear Search \\n\" +\n        \"Input \\t Index \\t Search Time(microsec) \\n\" +\n        n1 + \"\\t\" + linearSearch(n1) + \"\\t\" + linearSearchTime + \"\\n\" +\n        n2 + \"\\t\" + linearSearch(n2) + \"\\t\" + linearSearchTime + \"\\n\" +\n        n3 + \"\\t\" + linearSearch(n3) + \"\\t\" + linearSearchTime + \"\\n\"\n        );\n    }\n\n    public static void binarySearchDisplay()\n    {\n        System.out.println(\n        \"Binary Search \\n\" +\n        \"Input \\t Index \\t Search Time(microsec) \\n\" +\n        n1 + \"\\t\" + binarySearch(n1) + \"\\t\" + binarySearchTime + \"\\n\" +\n        n2 + \"\\t\" + binarySearch(n2) + \"\\t\" + binarySearchTime + \"\\n\" +\n        n3 + \"\\t\" + binarySearch(n3) + \"\\t\" + binarySearchTime + \"\\n\"\n        );\n    }\n\n    public static void main(String[] args)\n    {\n        initialize();\n        linearSearchDisplay();\n        binarySearchDisplay();\n    }\n}\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/04_Search/#test-cases","title":"Test Cases","text":"<pre><code>Linear Search \nInput   Index   Search Time(microsec) \n5000    5000    108.2\n9997    9997    221.8\n50000   -1      220.7\n\nBinary Search \nInput   Index   Search Time(microsec) \n5000    5000    2.1\n9997    9997    1.1\n50000   -1      0.7\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/05_Merge_Sort/","title":"05 Merge Sort","text":""},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/05_Merge_Sort/#question","title":"Question","text":"<p>IMPLEMENTATION of MERGE-SORT (use Recursive Version) using key field (descending order)</p> <ol> <li>Initialize 10000 positive, integer, Random Numbers whose values are in the range [0-9999] and store them in in an input array A[10000]. i.e. you are initializing the array with random numbers using an iterative statement. (you can use built-in random number generator function. assume that duplicate values are permitted).</li> <li>Implement the Merge-Sort Algorithm (recursive version) to sort in descending order. Measure the time to do sorting: use built-in timer function.</li> <li>Store the output in a text file: <code>mergeout.txt</code></li> <li>Display the first 7 records and last 7 records of the output file. (use unix commands <code>head -7 mergeout.txt</code> <code>tail -7 mergeout.txt</code>)</li> </ol>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/05_Merge_Sort/#algorithm","title":"Algorithm","text":""},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/05_Merge_Sort/#pseudocode","title":"Pseudocode","text":"<pre><code>Algorithm mergeSort(A, p, r)\n    if p &lt; r\n    q \u2190 floor( (p + r)/2 )\n\n    mergeSort (A, p, q)\n    mergeSort (A, q+1, r)\n\n    mergeAsc(A, p, q, r) // or mergeDesc(A, p, q, r)\n\nAlgorithm mergeDesc(A, p, q, r)\n  n1 \u2190 q-p+1\n  n2 \u2190 r-q\n\n  Let L[0\u2026(n1-1)]\n  Let R[0\u2026(n2-1)]\n\n  for i \u2190 0 to n1-1\n      L[i] \u2190 A[p+i]\n  for i \u2190 0 to n2-1\n      R[i] \u2190 A[q+1+i]\n\n  i \u2190 0\n  j \u2190 0\n  k \u2190 p\n\n  while i&lt;n1 and j&lt;n2\n      if L[i] &gt;= R[j]\n          A[k] \u2190 L[i]\n          i \u2190 i+1\n      else\n          A[k] \u2190 R[j]\n          j \u2190 j+1\n      k \u2190 k+1\n\n  while i&lt;n1\n      A[k] \u2190 L[i]\n      i \u2190 i+1\n      k \u2190 k+1\n\n  while j&lt; n2\n      A[k] \u2190 R[j]\n      j \u2190 j+1\n      k \u2190 k+1\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/05_Merge_Sort/#time-complexity","title":"Time Complexity","text":"Algorithm Complexity mergeSort \\(O(n \\log_2 n)\\)"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/05_Merge_Sort/#source-code","title":"Source Code","text":"<pre><code>// Ahmed Thahir 2020A7PS0198U\nimport java.io.File;\nimport java.io.FileNotFoundException;\nimport java.io.PrintWriter;\n\nclass p05\n{\n    static int n = 10000;\n    static int[] a = new int[n];\n    static String outputFile = \"h:\\\\\nMy Drive\\\\\nNotes\\\\\nSem 4\\\\\n02 DSA\\\\\nPracticals\\\\\n\"\n      + \"mergeout.txt\";\n\n    static float sortTime;\n\n    public static void initialize()\n    {\n        for(int i = 0; i&lt;n; i++)\n            a[i] = (int) ( Math.random() * n );\n    }\n\n    public static void mergeSort(int[] a, int p, int r)\n    {\n        if(p&lt;r)\n        {\n            int q = (p+r)/2;\n            // automatically floor, cuz java doesn't typecast into decimal\n\n            mergeSort(a, p, q);\n            mergeSort(a, q+1, r);\n\n            mergeDesc(a, p, q, r);\n        }\n    }\n\n    public static void mergeDesc(int[] a, int p, int q, int r)\n    {\n        int n1 = (q-p) + 1,\n        n2 = r-q;\n\n        int[] L = new int[n1],\n        R = new int[n2];\n\n        for (int i = 0; i &lt; n1; i++)\n        L[i] = a[p+i];\n        for (int j = 0; j &lt; n2; j++)\n        R[j] = a[q+1+j];\n\n        int i = 0,\n        j = 0,\n        k = p;\n\n        while (i&lt;n1 &amp;&amp; j&lt;n2)\n        {\n            if(L[i] &gt;= R[j])\n            // will be &lt;= for ascending\n            {\n                a[k] = L[i];\n                i++;\n            }\n            else\n            {\n                a[k] = R[j];\n                j++;\n            }\n            k++;\n        }\n        while(i&lt;n1)\n        {\n            a[k] = L[i];\n            i++;\n            k++;\n        }\n        while(j&lt;n2)\n        {\n            a[k] = R[j];\n            j++;\n            k++;\n        }\n    }\n\n    public static void display()\n    {\n        int z = 7;\n\n        System.out.println(\"Head\");\n        for(int i = 0; i&lt;z; i++)\n            System.out.println(a[i]);\n\n        System.out.println(\"\\nTail\");\n        for(int i = n-z; i&lt;n; i++)\n            System.out.println(a[i]);\n    }\n    public static void write() throws FileNotFoundException\n    {\n        PrintWriter writeToMyFile = new PrintWriter( new File(outputFile) );\n        for(int i = 0; i&lt;n; i++)\n            writeToMyFile.format(\"%s\\n\", a[i]);\n    }\n    public static void main(String[] args) throws FileNotFoundException\n    {       \n        initialize();\n\n        long startTime = System.nanoTime();\n\n        mergeSort(a, 0, n-1);\n\n        long endTime = System.nanoTime();\n        sortTime = (endTime - startTime)/1000f;\n        System.out.println(\"Sort Time: \" + sortTime + \" microsec\\n\");\n\n        display();\n        write();\n    }\n}\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/05_Merge_Sort/#test-cases","title":"Test Cases","text":"<pre><code>Sort Time: 3771.6 microsec\n\nHead\n9998\n9998\n9997\n9996\n9996\n9993\n9987\n\nTail\n6\n4\n4\n3\n1\n0\n0\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/06_Hashing/","title":"06 Hashing","text":""},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/06_Hashing/#question","title":"Question","text":"<p>Write an algorithm and C/C++/JAVA program to perform the following.</p> <p>It is required to store various strings in a HASH TABLE. The hash function is defined as follows:</p> <p>Read in strings from an input text file (source.txt) and calculate hash value for each string using the hash function given below. You can permit collisions, in case if they occur [i.e. one or more strings can map to the same hash value; you can store them in the same sub-list corresponding to the computed hash value]. </p> <p>Assume that the input string has English alphabets (upper case and lower case) and digits. Note the range of ASCII values for A-Z is 65-90, a-z is 97-122 and digits 0-9 is 48-57.</p> <p>HASH FUNCTION for an input string is defined as follows:</p> \\[ \\Bigg( \\left( \\sum \\text{alphabets' ASCII} + 2 \\sum \\text{digits' ASCII} \\right) * 17 + 5 \\Bigg) \\% 6 \\] <p>Note: MOD(%) denotes modulus operator (i.e. remainder after division)</p> <p>Example Input String : Az9 Hash Value</p> \\[ \\begin{aligned} &amp;= \\Big((65 + 122 + 2*57) *17 + 5 \\Big) \\% 6 \\\\ &amp;= (301*17 +5) \\% 6 \\\\ &amp;= 5122 \\% 6 \\\\ &amp;= 4 \\end{aligned} \\] <ol> <li>Compute the hash values for each of the following twenty input strings and display the values. Note: you can read each input string from a text file (one string in each line).</li> <li>Display the contents of the Hash Table showing the elements of each sub-list.</li> </ol>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/06_Hashing/#algorithm","title":"Algorithm","text":""},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/06_Hashing/#pseudocode","title":"Pseudocode","text":""},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/06_Hashing/#time-complexity","title":"Time Complexity","text":"Algorithm Complexity sum \\(O(n)\\) hash \\(O(1)\\) displaySublists \\(O(n)\\)"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/06_Hashing/#source-code","title":"Source Code","text":"<pre><code>// Ahmed Thahir 2020A7PS0198U\n\nimport java.io.File;\nimport java.io.FileNotFoundException;\nimport java.util.Scanner;\n\npublic class p06\n{\n    static String[] subList = new String[6];\n\n    public static void init()\n    {\n        for(int i=0; i&lt;6; i++)\n        subList[i] = \"\";\n    }\n\n    public static int sum(String str, char c)\n    {\n        int sum = 0;\n        int n = str.length();\n        if(c == 'c')\n        {\n            for (int i = 0; i &lt; n; i++)\n            {\n                if( (str.charAt(i) &gt;= 65 &amp;&amp; str.charAt(i) &lt;= 90) || (str.charAt(i) &gt;= 97 &amp;&amp; str.charAt(i) &lt;= 122) )\n                sum += str.charAt(i);\n            }\n        }\n        else if (c == 'n')\n        {\n            for (int i = 0; i &lt; n; i++)\n            {\n                if( (str.charAt(i) &gt;= 48) &amp;&amp; (str.charAt(i) &lt;= 57) )\n                sum += str.charAt(i);\n            }\n        }\n\n        return sum;\n    }\n    public static int hash(String record, int csum, int nsum)\n    {\n        int hash = ( (csum + 2*nsum) * 17 + 5 ) % 6;\n        return hash;\n    }\n\n    public static void input() throws FileNotFoundException\n    {\n        String inputFile = \"source.txt\";\n        Scanner readMyFile = new Scanner( new File(inputFile) );\n\n        while (readMyFile.hasNext()) \n        {\n            String record = readMyFile.nextLine();\n            int csum = sum(record, 'c');\n            int nsum = sum(record, 'n');\n\n            int hash = hash(record, csum, nsum);\n            subList[hash] += record + \" \";\n\n            System.out.println(\"The hash value of \" + record + \" is \" + hash);\n        }\n\n        readMyFile.close();\n    }\n\n    public static void displaySubsets()\n    {\n        System.out.println(\"\\n\\n\");\n        for(int i = 0; i&lt;6; i++)\n        {\n            System.out.println(\"The subset of \" + i + \" is \" + subList[i]);\n        }\n    }\n\n    public static void main(String[] args) throws FileNotFoundException\n    {\n        init();\n        input();\n        displaySubsets();\n    }\n}\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/06_Hashing/#test-cases","title":"Test Cases","text":""},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/06_Hashing/#input","title":"Input","text":"<pre><code>M2y\nN3x\nF4w\nO5v\nD2u\nA2t\nK5y\nM6z\nN7a\nY3w\nb2Y\ne3X\nf4W\nc5V\nd2U\na2T\nJ5Y\nm6Z\nn7A\ny3W\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/06_Hashing/#output","title":"Output","text":"<pre><code>The Hash value of M2y is 1\nThe Hash value of N3x is 5\nThe Hash value of F4w is 0\nThe Hash value of O5v is 2\nThe Hash value of D2u is 2\nThe Hash value of A2t is 0\nThe Hash value of K5y is 3\nThe Hash value of M6z is 4\nThe Hash value of N7a is 2\nThe Hash value of Y3w is 1\nThe Hash value of b2Y is 0\nThe Hash value of e3X is 2\nThe Hash value of f4W is 0\nThe Hash value of c5V is 2\nThe Hash value of d2U is 2\nThe Hash value of a2T is 0\nThe Hash value of J5Y is 0\nThe Hash value of m6Z is 4\nThe Hash value of n7A is 2\nThe Hash value of y3W is 1\n\nThe Subset of 0 : F4w A2t b2Y f4W a2T J5Y\nThe Subset of 1 : M2y Y3w y3W\nThe Subset of 2 : O5v D2u N7a e3X c5V d2U n7A\nThe Subset of 3 : K5y\nThe Subset of 4 : M6z m6Z\nThe Subset of 5 : N3x\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/07_BST/","title":"07 BST","text":""},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/07_BST/#question","title":"Question","text":"<p>Write an algorithm and C/C++/JAVA program for the following problem:</p> <ol> <li>Create a Binary Search Tree (Ordered Binary Tree) to store <code>&lt;IDNo, Name, CGPA&gt;</code> for \\(n\\) students (say \\(n=20\\) record at least)<ul> <li>Read from a text file</li> <li>Copy each record into nodes in the tree</li> <li>Assume IDNO is the primary key.</li> </ul> </li> <li>Perform INORDER Traversal of the above tree and show output.</li> <li>Perform PREORDER Traversal of the above tree and show output.</li> <li>Perform POSTORDER Traversal of the above tree and show output.</li> </ol>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/07_BST/#algorithm","title":"Algorithm","text":""},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/07_BST/#pseudocode","title":"Pseudocode","text":""},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/07_BST/#time-complexity","title":"Time Complexity","text":"Algorithm Complexity insert() \\(O(\\log n)\\) inFix() \\(O(n)\\) preFix \\(O(n)\\) postFix \\(O(n)\\)"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/07_BST/#source-code","title":"Source Code","text":"<pre><code>// Ahmed Thahir 2020A7PS0198U\n\nimport java.util.*;\nimport java.io.*;\n\nclass Node { \n    String key; \n    String name;\n    float CGPA;\n    Node left, right; \n\n    public Node(String data, String n, float c){ \n        key = data; \n        name = n;\n        CGPA = c;\n        left = right = null; \n    } \n}\n\nclass BST\n{ \n    Node root; \n\n    BST(){ \n        root = null; \n    } \n\n    void insert(String key, String name, float c)  { \n        root = insertRecursive(root, key, name, c); \n    } \n\n    Node insertRecursive(Node root, String key, String name, float c) { \n        if (root == null) { \n            root = new Node(key, name, c);\n        } \n        else if (key.compareTo(root.key)&lt;0)\n            root.left = insertRecursive(root.left, key, name, c);\n        else if (key.compareTo(root.key)&gt;0)    \n            root.right = insertRecursive(root.right, key, name , c);\n\n    return root; \n    } \n\n\n    void inFix() { \n        inFixRecursive(root); \n    } \n\n    void inFixRecursive(Node root) { \n        if (root != null) { \n            inFixRecursive(root.left); \n            System.out.print(root.key + \" \"); \n            System.out.print(root.name + \" \"); \n            System.out.println(root.CGPA + \" \"); \n            inFixRecursive(root.right); \n        } \n    } \n\n    void postFix() { \n        postFixRecursive(root); \n    } \n\n    void postFixRecursive(Node root) { \n        if (root != null) { \n            postFixRecursive(root.left); \n            postFixRecursive(root.right); \n            System.out.print(root.key + \" \"); \n            System.out.print(root.name + \" \"); \n            System.out.println(root.CGPA + \" \"); \n        } \n    }\n\n    void preFix() { \n        preFixRecursive(root); \n    } \n\n    void preFixRecursive(Node root) { \n        if (root != null) { \n            System.out.print(root.key + \" \"); \n            System.out.print(root.name + \" \"); \n            System.out.println(root.CGPA + \" \"); \n            preFixRecursive(root.left); \n            preFixRecursive(root.right); \n        } \n    }\n}\nclass p07\n{\n    public static void main(String[] args) throws FileNotFoundException\n    {\n        Scanner readMyFile = new Scanner(new File(\"input.txt\"));\n        BST bst = new BST(); \n\n        while(readMyFile.hasNext())\n        {\n            String key = readMyFile.next();\n            String name = readMyFile.next();\n            float CGPA = readMyFile.nextFloat();\n            bst.insert(key, name, CGPA); \n        }\n        System.out.println(\"InFix traversal:\"); \n        bst.inFix(); \n        System.out.println(\"\\n\\nPreFix traversal:\"); \n        bst.preFix(); \n        System.out.println(\"\\n\\nPostFix traversal:\"); \n        bst.postFix();\n    } \n}\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/07_BST/#test-cases","title":"Test Cases","text":""},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/07_BST/#input","title":"Input","text":"<pre><code>2019A7PS096U AA 7.6\n2019A7PS103U BB 7.5\n2019A7PS107U CC 7.4\n2019A7PS140U DD 7.3\n2019A3PS135U EE 8.5\n2019A3PS410U FF 8.4\n2019A7PS001U GG 8.3\n2019A7PS003U HH 8.2\n2019A7PS023U II 8.1\n2019A7PS034U JJ 8.0\n2019A7PS042U KK 7.9\n2019A7PS054U LL 7.8\n2019A7PS091U MM 7.7\n2019A7PS281U NN 9.1\n2019A7PS424U OO 9.0\n2019A3PS019U PP 8.9\n2019A3PS080U QQ 8.8\n2019A7PS153U RR 7.2\n2019A7PS209U SS 7.1\n2019A3PS113U TT 8.6\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/07_BST/#output","title":"Output","text":"<p><code>: InFix traversal: 2019A3PS019U PP 8.9  2019A3PS080U QQ 8.8  2019A3PS113U TT 8.6  2019A3PS135U EE 8.5  2019A3PS410U FF 8.4  2019A7PS001U GG 8.3  2019A7PS003U HH 8.2  2019A7PS023U II 8.1  2019A7PS034U JJ 8.0  2019A7PS042U KK 7.9  2019A7PS054U LL 7.8  2019A7PS091U MM 7.7  2019A7PS096U AA 7.6  2019A7PS103U BB 7.5  2019A7PS107U CC 7.4  2019A7PS140U DD 7.3  2019A7PS153U RR 7.2  2019A7PS209U SS 7.1  2019A7PS281U NN 9.1  2019A7PS424U OO 9.0  PreFix traversal: 2019A7PS096U AA 7.6  2019A3PS135U EE 8.5  2019A3PS019U PP 8.9  2019A3PS080U QQ 8.8  2019A3PS113U TT 8.6  2019A3PS410U FF 8.4  2019A7PS001U GG 8.3  2019A7PS003U HH 8.2  2019A7PS023U II 8.1  2019A7PS034U JJ 8.0  2019A7PS042U KK 7.9  2019A7PS054U LL 7.8  2019A7PS091U MM 7.7  2019A7PS103U BB 7.5  2019A7PS107U CC 7.4  2019A7PS140U DD 7.3  2019A7PS281U NN 9.1  2019A7PS153U RR 7.2  2019A7PS209U SS 7.1  2019A7PS424U OO 9.0  PostFix traversal: 2019A3PS113U TT 8.6  2019A3PS080U QQ 8.8  2019A3PS019U PP 8.9  2019A7PS091U MM 7.7  2019A7PS054U LL 7.8  2019A7PS042U KK 7.9  2019A7PS034U JJ 8.0  2019A7PS023U II 8.1  2019A7PS003U HH 8.2  2019A7PS001U GG 8.3  2019A3PS410U FF 8.4  2019A3PS135U EE 8.5  2019A7PS209U SS 7.1  2019A7PS153U RR 7.2  2019A7PS424U OO 9.0  2019A7PS281U NN 9.1  2019A7PS140U DD 7.3  2019A7PS107U CC 7.4  2019A7PS103U BB 7.5  2019A7PS096U AA 7.6</code></p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/08_Heaps/","title":"08 Heaps","text":"<p>Heap Sort Practicals covers all the concepts required for this.</p>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/09_Heap_Sort/","title":"09 Heap Sort","text":""},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/09_Heap_Sort/#code","title":"Code","text":"<pre><code>class Heap\n{\n    String[] nodes = new String[100];\n    int size = 0;\n\n    Heap()\n    {\n        nodes[0] = \"\";\n    }\n\n    int parent(int pos)\n    {\n        return pos/2;\n    }\n    int lc(int pos)\n    {\n        return 2*pos;\n    }\n    int rc(int pos)\n    {\n        return 2*pos + 1;\n    }\n\n    void swap(int a, int b)\n    {\n        String t = nodes[a];\n        nodes[a] = nodes[b];\n        nodes[b] = t;\n    }\n\n    void insert(String data)\n    {\n        size++;\n\n        nodes[size] = data;\n\n        int cur = size;\n        while(\n            nodes[cur].compareTo(nodes[parent(cur)]) &gt; 0\n        )\n        {\n            swap(cur, parent(cur));\n            cur = parent(cur);\n        }\n    }\n\n    void max_heapify(int size, int root)\n    {\n        int largest = root,\n                l = lc(root),\n                r = rc(root);\n\n        if(\n            l&lt;size &amp;&amp; nodes[l].compareTo(nodes[largest])&gt;0\n        )\n            largest = l;\n        if(\n            r&lt;size &amp;&amp; nodes[r].compareTo(nodes[largest])&gt;0\n        )\n            largest = r;\n\n        if(root != largest)\n        {\n            swap(root, largest);\n            max_heapify(size, largest);\n        }\n    }\n\n    void sort()\n    {\n        // build heap\n        for(int i = size; i&gt;=0; i--)\n        {\n            swap(i, 0);\n            max_heapify(i, 0);\n        }\n    }\n\n    void display()\n    {\n        for(int i=0; i&lt;=size; i++)\n        {\n            if(nodes[i].length() &gt; 0)\n                System.out.print(nodes[i] + \" \");\n        }\n    }\n}\n\nclass p06\n{\n    public static void main(String[] args)\n    {\n        Heap heap = new Heap();\n\n        heap.insert(\"CC\");\n        heap.insert(\"DF\");\n        heap.insert(\"MM\");\n        heap.insert(\"AB\");\n        heap.insert(\"ZX\");\n        heap.insert(\"PQ\");\n        heap.insert(\"LR\");\n        heap.display();\n\n        System.out.println();\n        heap.sort();\n        heap.display();\n    }\n}\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/09_Heap_Sort/#input","title":"Input","text":"<pre><code>RR\nBB\nYY\nGG\nNN\nQQ\nMM\nPP\nBB\nAA\nKT\nUV\nVV\nGG\nQQ\nMN\nPQ\nRS\nTU\nYM\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/09_Heap_Sort/#output","title":"Output","text":"<pre><code>YY YM VV UV RS TU RR QQ NN PQ KT GG BB QQ GG MM MN BB PP AA\nAA BB BB GG GG KT MM MN NN PP PQ QQ QQ RR RS TU UV VV YM YY\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/10_DFS/","title":"10 DFS","text":""},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/10_DFS/#just-the-graph-building-outgoing-nodes-part","title":"Just the graph building + outgoing nodes part","text":"<pre><code>import java.util.ArrayList;\n\nclass Graph\n{\n    int size;\n    ArrayList adj[];\n\n    Graph(int size)\n    {\n        this.size = size;\n\n        adj = new ArrayList[size];\n\n        for(int i = 0; i&lt;size; i++)\n        {\n            adj[i] = new ArrayList&lt;Integer&gt;();\n        }\n    }\n\n    void insert(int u, int v)\n    {\n        int ul = u-65;\n        int vl = v-65;\n        adj[ul].add(vl);\n    }\n\n    void outgoing(char from)\n    {\n        System.out.println(\"Nodes outgoing from \" + from);\n\n        int u = (char)(from) - 65;\n        for(int i=0; i&lt;adj[u].size(); i++)\n        {\n            int v = (int) adj[u].get(i);\n            char ch = (char) (v + 65);\n            System.out.println(ch);\n        }\n    }\n}\n\nclass p06\n{\n    public static void main(String[] args)\n    {\n        Graph graph = new Graph(8);\n\n        graph.insert('A', 'B');\n        graph.insert('A', 'E');\n        graph.insert('B', 'C');\n        graph.insert('D', 'E');\n        graph.insert('A', 'D');\n\n        graph.outgoing('A');\n    }\n}\n</code></pre>"},{"location":"2_Core/Data_Structures_%26_Algorithms/Practicals/10_DFS/#output","title":"Output","text":"<pre><code>Nodes outgoing from A\nB\nE\nD\n</code></pre>"},{"location":"2_Core/Database_Systems/","title":"Database Systems","text":"Class Instructor Lecture Sapna Sadhwani Tutorial Dr.Tamizharasan Periyasamy Practical Sapna Sadhwani <p>The scope of the course is to introduce the basic concepts and implementation issues of a Database System. This course is intended to give students a solid background in databases, with a focus on relational database management systems. </p> <p>Topics include data modeling, database design theory, data definition and manipulation languages, storage and indexing techniques, query processing and optimization, concurrency control and crash recovery. In addition to these traditional topics, this course covers a sample of advanced database topics such as distributed databases and spatial-temporal databases. The emphasis is to learn the concepts through rigorous mathematical foundation and implementation details.</p> <p>Unfortunately, query optimization (a very important concept) was skipped for my batch due to time contraints.</p>"},{"location":"2_Core/Database_Systems/01_Theory/","title":"01 Theory","text":""},{"location":"2_Core/Database_Systems/01_Theory/#parts-of-table","title":"Parts of Table","text":"<p>Rows - Records/Tuples</p> <p>Columns - Fields/Attributes</p>"},{"location":"2_Core/Database_Systems/01_Theory/#drawbacks-of-file-system-spreadheets","title":"Drawbacks of File System (Spreadheets)","text":"<ol> <li>Data redundancy and inconsistency</li> <li>duplication of information in different files</li> <li>different file formats</li> <li>Difficulty in accessing data</li> <li>Data isolation</li> <li>multiple files (scattered all over the place)</li> <li>Integrity problems    Data validation and constraints is difficult</li> <li>Collaboration is difficult</li> <li>Uncontrolled Access</li> <li>Security Problems</li> </ol>"},{"location":"2_Core/Database_Systems/01_Theory/#abstraction","title":"Abstraction","text":"<p>Data hiding</p> Level of Abstraction Physical Level describes how record is stored Logical Level describes what data is stored, and the relationship between data View Level describes information hiding"},{"location":"2_Core/Database_Systems/01_Theory/#view-of-data","title":"View of data","text":""},{"location":"2_Core/Database_Systems/01_Theory/#idk","title":"IDK","text":"<p>Schema is the skeleton of the table (without data)</p> <p>Instances is the content of the table (similar to tuples)</p>"},{"location":"2_Core/Database_Systems/01_Theory/#entity-relationship-model","title":"Entity-Relationship Model","text":"<p>Entity is a table</p> <p>Relation is the primary key??</p>"},{"location":"2_Core/Database_Systems/02_ER/","title":"02 ER","text":""},{"location":"2_Core/Database_Systems/02_ER/#database-paradigms","title":"Database Paradigms","text":"<ul> <li>Relational (tables)</li> <li>Hierarchical (like tree)</li> <li>Network (interconnected)</li> <li>Object Oriented </li> <li>Object-Relational</li> <li>ER (Entity-Relationship)</li> <li>NoSQL</li> </ul>"},{"location":"2_Core/Database_Systems/02_ER/#er-model","title":"ER Model","text":"<p>A database can be modelled as</p> <ul> <li>entity set</li> <li>relationship sets</li> </ul>"},{"location":"2_Core/Database_Systems/02_ER/#terms","title":"Terms","text":"Term Meaning Example Entity unique object specific person, company Entity Set set of entities Attributes properties/features of an entity/relationship name, age Composite Attributes sub-attributes first name, last name Relationship association among several entites Relationship Sets set of relationships Degree of Relationship Set no of entity sets that participate in a relationship set Mapping Cardinalities Type of mapping One-OneOne-ManyMany-OneMany-Many"},{"location":"2_Core/Database_Systems/02_ER/#er-diagram","title":"ER Diagram","text":""},{"location":"2_Core/Database_Systems/02_ER/#symbols","title":"Symbols","text":"Shape Meaning Rectangle Entity Set Double Rectangle Weak Entity Setentity without a primary key Diamond Relationship Set Double Diamond Weak Relationship Setrelation connecting a weak entity with something else Dashed ellipse derived attribute Double ellipse multi-valued attribute Underline primary key attribute Triangle \u2018is-a\u2019 relation Lines - link attribute to entity set- link entity set to relationship set \\(\\to\\) one \\(-\\) many"},{"location":"2_Core/Database_Systems/02_ER/#idk","title":"IDK","text":"<p>Super key is any key that can uniquely identify a record</p> <p>We don\u2019t have to include foreign key for a relation</p> <p>it is implied that the primary keys of the connected entities are the foreign keys for the relation</p>"},{"location":"2_Core/Database_Systems/02_ER/#disjoint-is-a","title":"Disjoint \u2018is-a\u2019","text":"<p>can be either this or that</p>"},{"location":"2_Core/Database_Systems/03_SQL/","title":"03 SQL","text":""},{"location":"2_Core/Database_Systems/03_SQL/#sql","title":"SQL","text":"<p>is a non-procedural language</p>"},{"location":"2_Core/Database_Systems/03_SQL/#database","title":"Database","text":"<p>keywords are not case-sensitive</p> <pre><code>create database dbName;\nshow databases;\nuse dbName;\n</code></pre>"},{"location":"2_Core/Database_Systems/03_SQL/#ddl","title":"DDL","text":"<p>Data Definition Language</p> <p>work with structure</p> <pre><code>## Create Table\ncreate table tableName(\n  col1 dataType(size),\n  col2 dataType(size),\n  col3 dataType(size)\n);\n\ndrop table student;\n</code></pre>"},{"location":"2_Core/Database_Systems/03_SQL/#constraints","title":"Constraints","text":"<ol> <li>Primary Key</li> <li>Foreign key</li> <li>Cascading</li> <li>not null</li> </ol> <pre><code>create table tableName(\n  col1 dataType(size),\n  col2 dataType(size),\n  col3 dataType(size),\n  col4 dataType(size),\n\n  primary key(col1, col2),\n  foreign key(col3),\n  not null(col4),\n  on delete cascade(col1, col2, col3, col4)\n);\n\nALTER TABLE department ADD PRIMARY KEY (dept_name);\n\nalter table orders modify column purch_amt float(10,5);\n</code></pre>"},{"location":"2_Core/Database_Systems/03_SQL/#dml","title":"DML","text":"<p>Data Manipulation Language</p> <p>work with entries</p> <pre><code>## Display properties of table\ndescribe Students;\n\n## Insert\ninsert into Students values(1, \"Thahir\", \"Database Systems\");\n\n## Insert Multiple\ninsert into Students values\n(1, \"Thahir\", \"Database Systems\"),\n(2, \"Blah\", null),\n(3, \"Blah\", null);\n</code></pre>"},{"location":"2_Core/Database_Systems/03_SQL/#deletion","title":"<code>deletion</code>","text":"<pre><code>delete\nfrom instructor;\n\ntruncate instructor;\n\ndelete\nfrom instructor\nwhere deptName = \"Finance\";\n\ndelete\nfrom instructor\nwhere deptName in (\n    select deptName\n  from departments\n  where building = \"Watson\"\n);\n\ndelete\nfrom instructor\nwhere salary &lt; (\n    select avg(salary)\n  from instructor\n);\n</code></pre>"},{"location":"2_Core/Database_Systems/03_SQL/#update","title":"<code>update</code>","text":"<pre><code>update instructor\nset salary = salary * 1.03\nwhere salary &gt; 100000;\n\nupdate instructor\nset salary = case when salary &lt;=10000\n    then salary * 1.05\n    else salary * 1.03;\n\nupdate student\nset totCreds = (\n    select sum(credits)\n  from takes\n  where takes.cid = (\n    select course.cid\n    from course\n  )\n    and student.id = takes.id\n    and takes.grade != 'F'\n    and takes.grade is not null\n);\n\nupdate student\nset totCreds set null somethign is here\n</code></pre>"},{"location":"2_Core/Database_Systems/04_SQL_Select/","title":"04 SQL Select","text":""},{"location":"2_Core/Database_Systems/04_SQL_Select/#select","title":"<code>select</code>","text":"<pre><code>## Display values of table\nselect * from tableName;\nselect * from tableName where id = 11;\n\nselect name, salary/12 as monthlySalary from tableName;\n\n## unique\nselect distinct city from table;\nselect count(distinct city) from table;\n</code></pre>"},{"location":"2_Core/Database_Systems/04_SQL_Select/#subqueries","title":"Subqueries","text":"<pre><code>age + salary\nage - salary\nage * salary\nage / salary\n</code></pre>"},{"location":"2_Core/Database_Systems/04_SQL_Select/#clauses","title":"Clauses","text":"<p>connectives</p>"},{"location":"2_Core/Database_Systems/04_SQL_Select/#logical","title":"logical","text":"<pre><code>and\nor\nnot\n</code></pre>"},{"location":"2_Core/Database_Systems/04_SQL_Select/#as","title":"<code>as</code>","text":"<pre><code>select name, courseId\n    from instructors as i, teaches as t\n</code></pre>"},{"location":"2_Core/Database_Systems/04_SQL_Select/#where","title":"<code>where</code>","text":"<pre><code>where id = 11;\nwhere Student.instructor = Teacher.name;\n</code></pre>"},{"location":"2_Core/Database_Systems/04_SQL_Select/#like","title":"<code>like</code>","text":"<p>for string operations</p> <pre><code>where name like \"a%\"; ## no character/any number of characters\nwhere name like \"a_\"; ## 1 character\n\n## we can create our own escape characters\nwhere name like \"100\\%\" escape '\\';\nwhere name like \"100&amp;%\" escape '&amp;';\n</code></pre>"},{"location":"2_Core/Database_Systems/04_SQL_Select/#having","title":"<code>having</code>","text":"<p>for group by clause</p> <pre><code>group by age having name like \"a%\";\n</code></pre>"},{"location":"2_Core/Database_Systems/04_SQL_Select/#between","title":"<code>between</code>","text":"<p>inclusive on both sides</p> <pre><code>select * from Student\n    where age between 15 and 20 ## range is [15, 20]\n</code></pre>"},{"location":"2_Core/Database_Systems/04_SQL_Select/#operations","title":"Operations","text":""},{"location":"2_Core/Database_Systems/04_SQL_Select/#merge","title":"Merge","text":"<pre><code>select * from students, players\n    where students.id = players.id;\n\nselect name, courseId\n    from instructors as i, teaches as t\n    where i.id = t.id;\n\nselect * from Student\n    where (age, name) = (15, \"Thahir\")\n</code></pre>"},{"location":"2_Core/Database_Systems/04_SQL_Select/#cartesian-product","title":"Cartesian Product","text":"<pre><code>select * from students, teachers;\n</code></pre> <p>For every record of <code>students</code>, there will be every possible combination with <code>teachers</code></p> <pre><code>select * from teachers, students;\n</code></pre> <p>For every record of <code>teachers</code>, there will be every possible combination with <code>students</code></p>"},{"location":"2_Core/Database_Systems/04_SQL_Select/#ordering","title":"Ordering","text":"<pre><code>select name from instructor order by name asc;\nselect name from instructor order by name desc;\n</code></pre> <p>if 2 people have the same name, the 2<sup>nd</sup> condition (here, age) will be given priority</p> <pre><code>select name from instructor order by name desc, age desc;\n</code></pre>"},{"location":"2_Core/Database_Systems/04_SQL_Select/#calculus","title":"Calculus","text":"<p>TRC = Tuple Relation Calculus</p> <p>DRC = Domain Relation Calculus</p>"},{"location":"2_Core/Database_Systems/04_SQL_Select/#semantic-representation","title":"Semantic Representation","text":"<pre><code>select A1, A2, ..., Am\n    from R1, R2, ..., Rn\n    where P\n</code></pre>"},{"location":"2_Core/Database_Systems/04_SQL_Select/#mathematical-representation","title":"Mathematical Representation","text":"\\[ \\Pi_{A_1, A_2, \\dots, A_m} \\bigg( \\sigma_{P} \\Big( R_1 \\times R_2 \\times \\dots \\times R_n \\Big) \\bigg) \\] Symbol Meaning \\(\\Pi\\) Projection \\(A_1, A_2, \\dots, A_m\\) Attributes (Columns) \\(\\sigma\\) Selection \\(P\\) Predicate (<code>where</code> condition) \\(R_1, R_2, \\dots, R_n\\) Relations"},{"location":"2_Core/Database_Systems/04_SQL_Select/#set-operations","title":"Set Operations","text":"Operation Meaning <code>union</code> a or b \\(A \\cup B\\) <code>intersect</code> a and b \\(A \\cap B\\) <code>except</code> a but not b \\(A \\cap B'\\) <pre><code>select cno from courses\n    where age\n\n(select id from Student where age &gt;= 15)\nexcept\n(select id from Student where age &lt; 20);\n\n## equivalent of \nselect * from Student where age between 15 and 20;\n</code></pre>"},{"location":"2_Core/Database_Systems/04_SQL_Select/#logic","title":"Logic","text":"<ul> <li>True (1)</li> <li>False (0)</li> <li>Unknown (X - don\u2019t care)</li> </ul> <p>Any comparison with <code>null</code> gives unknown for eg,</p> <ul> <li><code>age &gt; null</code></li> <li><code>age &lt;&gt; null</code></li> <li><code>age = null</code></li> </ul>"},{"location":"2_Core/Database_Systems/04_SQL_Select/#aggregate-functions","title":"Aggregate Functions","text":"<pre><code>count(*)\ncount(city)\ncount(distinct city)\n\nmax(salary)\nmin(salary)\n\nsum(salary)\navg(salary)\n</code></pre> <pre><code>select sum(salary)\n    from Teachers;\n\nselect dept, sum(salary)\n    from Teachers\n    where age &gt; 25\n    group by dept\n    having avg(salary) &gt; 45000;\n</code></pre> <p>The grouping attribute must be displayed as well, otherwise it won\u2019t make sence when viewing the table.</p> <p><code>count(*)</code> is the only aggregate function that does not ignore <code>null</code>, because some other fields might be filled. But, if there are only <code>null</code> values in the entire table, then even <code>count(*)</code> will return 0.</p>"},{"location":"2_Core/Database_Systems/04_SQL_Select/#predicate-order","title":"Predicate Order","text":"<ol> <li>where (before grouping)</li> <li>having (after grouping)</li> </ol>"},{"location":"2_Core/Database_Systems/04_SQL_Select/#subqueries_1","title":"Subqueries","text":"Clause Meaning <code>in</code> exact match <code>some</code> like <code>or</code> gate <code>all</code> like <code>and</code> gate <code>exists</code> less strict version of <code>in</code> <code>not exists</code> 0 <code>unique</code> at most once (0/1)"},{"location":"2_Core/Database_Systems/04_SQL_Select/#in","title":"<code>in</code>","text":"<pre><code>select count(distinct cid) from instructor\n    where semester = \"Fall\" and year = 2009 and\n        cid in (\n            select cid from instructor where semester = \"Spring\" and year = 2010\n        );\n\n## equivalent to\nselect distinct i1.cid\n    from instructor i1, instructor i2\n    where i1.semester = \"Fall\" and\n    i1.year = 2009 and\n    i2.semester = \"Spring\" and\n    i2.year = 2010 and\n    i1.cid = i2.cid;\n</code></pre>"},{"location":"2_Core/Database_Systems/04_SQL_Select/#some","title":"<code>some</code>","text":"<pre><code>select distinct name from instructor\n    where age &gt; some (select age from instructor);\n\n    where age not &gt; some (select age from instructor);\n</code></pre>"},{"location":"2_Core/Database_Systems/04_SQL_Select/#all","title":"<code>all</code>","text":"<p>not all = not in</p> <pre><code>select distinct name from instructor\n    where age &gt; all (select age from instructor);\n\n    where age not &gt; all (select age from instructor);\n</code></pre>"},{"location":"2_Core/Database_Systems/04_SQL_Select/#exists","title":"<code>exists</code>","text":"<pre><code>select cid\nfrom section as s\nwhere semester = \"Fall\" and year = 2009\n    and exists\n    (\n        select * from section as T\n        where semester = \"Spring\" and year = 2010\n        and s.cid = t.cide\n    );\n</code></pre>"},{"location":"2_Core/Database_Systems/04_SQL_Select/#not-exists","title":"<code>not exists</code>","text":"<pre><code>select distinct s.id, s.name\nfrom student as s\nwhere not exists\n(\n    (select course_id from course where dept_name = \"Biology\")\n    except\n    (select t.course)\n)\n</code></pre>"},{"location":"2_Core/Database_Systems/04_SQL_Select/#unique","title":"<code>unique</code>","text":"<pre><code>select t.cid from course as t\nwhere unique(select r.cid from section as r where t.cid = r.)\n</code></pre>"},{"location":"2_Core/Database_Systems/04_SQL_Select/#from-subqueries","title":"<code>from</code> subqueries","text":"<pre><code>select deptName, avgSalary\nfrom (\n  select deptName, avg(salary) as avgSalary\n  from Instructors\n  group by deptName\n)\nwhere age &gt; 30;\n\n## equivalent to\n\nselect deptName, avg(salary) as avgSalary\nfrom Instructors\ngroup by deptName\nhaving age &gt; 30;\n</code></pre>"},{"location":"2_Core/Database_Systems/04_SQL_Select/#with-clause","title":"<code>with</code> clause","text":"<pre><code>with maxTable(year, budget) as (\n    select max(year), max(budget) from department\n) ## no semi-colon\nselect department.name\nfrom department, maxTable\nwhere department.budget = maxTable.budget;\n</code></pre>"},{"location":"2_Core/Database_Systems/04_SQL_Select/#scalar-subquery","title":"Scalar Subquery","text":"<pre><code>select deptName, (\n    select count(*)\n  from instructor\n  where department.deptName = instructor.deptName\n) as numInstructors\nfrom department;\n\n## equivalent\n\nselect department.deptName, count(*) as numInstructors\n    from instructor, department\n    where department.deptName = instructor.deptName\n    group by deptName;\n</code></pre>"},{"location":"2_Core/Database_Systems/04_SQL_Select/#views","title":"Views","text":"<p>Temporary table</p> <pre><code>create view view_name as\nselect *\nfrom students;\n</code></pre>"},{"location":"2_Core/Database_Systems/04_SQL_Select/#cte-with-clause","title":"CTE <code>with</code> Clause","text":"<p>Temporary view, which you only need once.</p> <p>Useful when you need just an extra column, and for Recursive CTE.</p> <pre><code>with table_with_dpr as\n(\n    select\n    *,\n    dob - today() as AGE\n)\nselect *\nwhere AGE &gt; 30;\n</code></pre>"},{"location":"2_Core/Database_Systems/04_SQL_Select/#recursive-cte","title":"Recursive CTE","text":"<pre><code>with recursive cte (id, name, parent_id) as\n(\n  select     id,\n             name,\n             parent_id\n  from       products\n  where      parent_id = 19\n  union all\n  select     p.id,\n             p.name,\n             p.parent_id\n  from       products p\n  inner join cte\n          on p.parent_id = cte.id\n)\nselect * from cte;\n</code></pre>"},{"location":"2_Core/Database_Systems/05_Intermediary/","title":"05 Intermediary","text":""},{"location":"2_Core/Database_Systems/05_Intermediary/#join","title":"Join","text":"<p>1<sup>st</sup> table\u2019s order is always followed.</p> Outer Inner Natural working uses <code>null</code> for missing values selects all rows from both tables as long as there is a match between the columns. only common tuples with only the left table Returns records that have matching values in both tables condition a column name in both the tables must be same common table repeated \u2705 \u2705 \u274c <pre><code>select *\nfrom t1 left outer join t2 on t1.roll = t2.rollNum;\n\nselect *\nfrom t1 inner join t2 on t1.roll = t2.rollNum;\n\nselect *\nfrom t1 natural join t2;\n</code></pre>"},{"location":"2_Core/Database_Systems/05_Intermediary/#outer-join","title":"Outer Join","text":"Left Outer Join Right Outer Join Full outer Join Left table\u2019s tuples will occur once Right table\u2019s tuples will occur once Returns all records from the left table, and the matched records from the right table Returns all records from the right table, and the matched records from the left table Returns all records when there is a match in either left or right table"},{"location":"2_Core/Database_Systems/06_Miscelaneous_SQL/","title":"06 Miscelaneous SQL","text":""},{"location":"2_Core/Database_Systems/06_Miscelaneous_SQL/#referential-integrity","title":"Referential Integrity","text":"<p>Ensuring that the tuples in foreign table are in the main tables as well.</p>"},{"location":"2_Core/Database_Systems/06_Miscelaneous_SQL/#cascading","title":"Cascading","text":""},{"location":"2_Core/Database_Systems/06_Miscelaneous_SQL/#check-clause","title":"Check Clause","text":""},{"location":"2_Core/Database_Systems/06_Miscelaneous_SQL/#complex","title":"Complex","text":"<p>However, subqueries in check clause is not supported. Hence, triggers are preferred</p>"},{"location":"2_Core/Database_Systems/06_Miscelaneous_SQL/#indexing","title":"Indexing","text":"<p>speeds up querying, by using the indexes instead of looking at all records</p> <pre><code>create table student(\n    id int(4)\n  primary key(id);\n);\n\ncreate index idIndex on student(id);\n\nselect id\nfrom student\nwhere age &gt; 10;\n</code></pre>"},{"location":"2_Core/Database_Systems/06_Miscelaneous_SQL/#user-defined-types","title":"User-Defined Types","text":"<pre><code>create type dollars as numeric(12, 2) final\n\ncreate table department(\n    budget dollars;\n)\n</code></pre>"},{"location":"2_Core/Database_Systems/06_Miscelaneous_SQL/#domains","title":"Domains","text":"<pre><code>create domain name char(20) not null\n</code></pre>"},{"location":"2_Core/Database_Systems/06_Miscelaneous_SQL/#large-object-types","title":"Large-Object Types","text":"<p>Photos, videos, files are stored as a large object.</p> blob clob binary large object character large object large collection of uninterpreted binary data character dat <p>(some point)</p>"},{"location":"2_Core/Database_Systems/06_Miscelaneous_SQL/#authorization","title":"Authorization","text":"<ul> <li>Read (select)</li> <li>References (allow to create foreign key)</li> <li>Insert</li> <li>Update</li> <li>Delete</li> <li>Index</li> <li>Resources</li> <li>Alteration</li> <li>Drop</li> </ul>"},{"location":"2_Core/Database_Systems/06_Miscelaneous_SQL/#granting","title":"Granting","text":"<pre><code>grant &lt;privilegeList&gt; on tableName/viewName to &lt;userList&gt;\n\ngrant select on instructor to user1, user2, user3\ngrant all privileges instructor to user1, user2, user3\n\ncreate view geo_view as (select * from instructor where deptName = \"Geology\");\ngrant select on geo_view to geo_staff;\n</code></pre>"},{"location":"2_Core/Database_Systems/06_Miscelaneous_SQL/#revoking","title":"Revoking","text":"<pre><code>revoke &lt;privilegeList&gt; on tableName/viewName from &lt;userList&gt;\n\nrevoke select on instructor from user1, user2, user3\nrevoke all privileges instructor from user1, user2, user3\n</code></pre>"},{"location":"2_Core/Database_Systems/06_Miscelaneous_SQL/#roles","title":"Roles","text":"<pre><code>create role roleName;\ngrant roleName to userName;\n\ncreate role instructor;\ngrant instructor to Sapna;\n</code></pre> <p>Priveleges can be granded/revoked from roles as well</p> <pre><code>grant select on takes to instructor;\n</code></pre>"},{"location":"2_Core/Database_Systems/06_Miscelaneous_SQL/#chain-of-roles","title":"Chain of roles","text":"<pre><code>create role dean;\ngrant instructor to dean;\ngrant dean to Kumar;\n</code></pre>"},{"location":"2_Core/Database_Systems/07_Relational_Model/","title":"07 Relational Model","text":"<p>Relations are unordered</p> <p>Database is a collection of relations</p>"},{"location":"2_Core/Database_Systems/07_Relational_Model/#keys","title":"Keys","text":"<p>a superkey is a key that is sufficient to uniquely identify a tuple of each possible relation</p>"},{"location":"2_Core/Database_Systems/07_Relational_Model/#schema-diagram","title":"Schema Diagram","text":"<p>shows the different relations</p> <p></p>"},{"location":"2_Core/Database_Systems/07_Relational_Model/#design","title":"Design","text":"<p>The logical schema depicts the structure of the database, showing the tables, columns, and relationships with other tables in the database, and is a direct mapping of the Entity-Relationship diagram. The physical schema is created by actually generating the tables, columns, and relationships in the relational database management software (RDBMS) i.e SQL queries to create the database tables and relationships define</p>"},{"location":"2_Core/Database_Systems/08_Relational_Algebra/","title":"08 Relational Algebra","text":""},{"location":"2_Core/Database_Systems/08_Relational_Algebra/#query-languages","title":"Query Languages","text":"<p>Procedural</p> <p>Non-Procedural</p>"},{"location":"2_Core/Database_Systems/08_Relational_Algebra/#relational-algebra","title":"Relational Algebra","text":"Operator Symbol select \\(\\sigma\\) project \\(\\pi\\) union \\(\\cup\\) set difference \\(-\\) cartesian product \\(\\times\\) rename \\(\\rho_x(E)\\) natural/inner join \\(\\Join\\) left outer join \u27d5 right outer join \u27d6 full outer join \u27d7 sum \\(_\\text{Semester} \\ g_\\text{sum(age)}({student})\\) average \\(_\\text{Semester} \\ g_\\text{avg(age)}({student})\\)"},{"location":"2_Core/Database_Systems/08_Relational_Algebra/#merging","title":"Merging","text":"<p>\\(\\sigma_{A=B} T_1 \\times T_2 (\\text{instructor})\\)</p>"},{"location":"2_Core/Database_Systems/08_Relational_Algebra/#insertion","title":"Insertion","text":"\\[ \\begin{aligned} &amp;\\text{account} \\leftarrow \\text{account } \\cup \\{ \\\\ &amp;\\text{(\u201cAhmed\", A-973, 1200)} \\\\ &amp;\\text{(\u201cThahir\", A-193, 1300)} \\\\ &amp;\\} \\end{aligned} \\]"},{"location":"2_Core/Database_Systems/08_Relational_Algebra/#update","title":"Update","text":"\\[ \\begin{aligned} &amp;\\text{account} \\leftarrow \\Pi (something) \\end{aligned} \\]"},{"location":"2_Core/Database_Systems/08_Relational_Algebra/#additional-operations","title":"Additional operations","text":"<p>Not exactly part of relational algebra, but </p> <ol> <li>Set Intersection</li> <li>Natural Join</li> <li>Division</li> <li>Assignment</li> </ol>"},{"location":"2_Core/Database_Systems/08_Relational_Algebra/#division","title":"Division","text":"<p>Find all guests names who have a booking with all tour agencies located in Dubai.</p> <ul> <li>Column - common to A&amp;B</li> <li>Tuples - records of A having the same records in B</li> </ul> \\[ \\begin{aligned} R \u00f7 S = &amp; \\\\ \\{ \\quad &amp; t[a_1,...,a_n] : \\quad t \\in R \\\\ &amp; \\land \\forall s \\in S \\Big( (t[a_1, \\dots ,a_n] \\cup s) \\in R \\Big) \\quad \\} \\end{aligned} \\]"},{"location":"2_Core/Database_Systems/08_Relational_Algebra/#view","title":"View","text":"\\[ \\begin{aligned} &amp;\\text{create view allCustomers as} \\\\ &amp;\\Pi_\\text{branchName, customerName} (     \\text{depositor$\\Join$account} ) \\\\ &amp;\\Pi_\\text{branchName} ( \\\\ &amp;\\sigma \\text{ something}\\\\ &amp;) \\end{aligned} \\]"},{"location":"2_Core/Database_Systems/09_Relational_Calculus/","title":"09 Relational Calculus","text":""},{"location":"2_Core/Database_Systems/09_Relational_Calculus/#tuple-relational-calculus-trc","title":"Tuple Relational Calculus (TRC)","text":"<p>Display loans over $1200</p> \\[ \\{ t | t \\in \\text{loan} \\land t[\\text{amount}] &gt; 1200 \\} \\] <p>Display loan number for every loan &gt; $1200</p> \\[ \\begin{aligned} \\{ t | \\exists s \\in \\text{loan} ( \\\\  &amp;&amp; t[\\text{loanNumber}] &amp;= s[\\text{loanNumber}] \\\\ &amp;&amp; \\land s[\\text{amount}] &amp;&gt; 1200 \\\\ ) \\} \\end{aligned} \\] <p>Names of customers having loan at Perry branch \u201d</p> \\[ \\begin{aligned} \\{ &amp; t | \\textcolor{purple}{ \\underbrace{     \\exists b \\in \\text{borrower} \\land \\exists l \\in \\text{loan} }_\\text{from}} ( \\\\  &amp; \\textcolor{hotpink}{ \\underbrace{t.cn = b.cn}_\\text{select}} , \\\\  &amp; \\textcolor{orange}{\\underbrace{l.bn = \\text{\u201cPerry\"} \\land l.ln = b.ln}_\\text{where} \\\\ } &amp;) \\} \\end{aligned} \\]"},{"location":"2_Core/Database_Systems/09_Relational_Calculus/#domain-relational-calculus-drc","title":"Domain Relational Calculus (DRC)","text":"<p>Display loans over $1200</p> \\[ \\{ \\textcolor{purple}{\\underbrace{&lt;l, b, a&gt;}_\\text{select}} | \\textcolor{hotpink}{\\underbrace{&lt;l, b, a&gt; \\in \\text{loan}}_\\text{from}} \\land \\textcolor{orange}{\\underbrace{a &gt; 1200}_\\text{where}} \\} \\] <p>Display names of customers having loan &gt; $1200</p> \\[ \\{ &lt;n&gt; | \\exists l, b, a something \\} \\]"},{"location":"2_Core/Database_Systems/10_Functions%2C_Triggers/","title":"10 Functions, Triggers","text":""},{"location":"2_Core/Database_Systems/10_Functions%2C_Triggers/#functions","title":"Functions","text":"<p>returns a single value</p> <pre><code>create function deptCountFunc(deptName varchar(30)) returns integer\nbegin\n    declare dCount integer;\n\n    select count(*) into dCount\n    from instructor\n    where instructor.deptName = deptCountFunc.deptName;\n\n    return dCount;\nend\n</code></pre>"},{"location":"2_Core/Database_Systems/10_Functions%2C_Triggers/#invokation","title":"Invokation","text":"<p>can be called within a query only</p> <pre><code>select deptName\nfrom instructor\nwhere deptCountFunc(\"Physics\") &gt; 5;\n</code></pre>"},{"location":"2_Core/Database_Systems/10_Functions%2C_Triggers/#table-function","title":"Table Function","text":"<p>function that returns a table</p> <pre><code>create function instructorOf(deptName char(20)) returns table (\n  id varchar(5),\n  name varchar(20),\n  deptName varchar(20),\n  salary numeric (10, 2)\n)\n    return table(\n    select id, name, deptName, salary\n    from instructor\n    where instructor.deptName = instructorOf.deptName;\n  );\n</code></pre> <pre><code>select name\nfrom instructorOf(\"Physics\");\n</code></pre>"},{"location":"2_Core/Database_Systems/10_Functions%2C_Triggers/#procedure","title":"Procedure","text":"<p>is like a void function that returns nothing</p> <pre><code>create procedure deptCountProc (in deptName varchar(20),\n                                out dCount integer)\nbegin\n    select count(*) into dCount\n    from instructor\n    where instructor.deptName = deptCountProc.deptName;\nend\n</code></pre>"},{"location":"2_Core/Database_Systems/10_Functions%2C_Triggers/#invokation_1","title":"Invokation","text":"<p>can be called anywhere</p> <ul> <li> <p>within a query, or</p> </li> <li> <p>outside everything else</p> </li> </ul> <pre><code>declare dCount integer;\ncall deptCountProc(\"Physics\", dCount);\n</code></pre>"},{"location":"2_Core/Database_Systems/10_Functions%2C_Triggers/#loops","title":"Loops","text":""},{"location":"2_Core/Database_Systems/10_Functions%2C_Triggers/#while","title":"<code>while</code>","text":"<pre><code>while &lt;booleanExpression&gt; do\n    ; statements\nend while\n</code></pre>"},{"location":"2_Core/Database_Systems/10_Functions%2C_Triggers/#repeat","title":"<code>repeat</code>","text":"<p>is like <code>do while</code> in CPP</p> <pre><code>repeat\n    ; statements\nuntil &lt;booleanExpression&gt;\nend repeat\n</code></pre>"},{"location":"2_Core/Database_Systems/10_Functions%2C_Triggers/#for","title":"<code>for</code>","text":"<p>Find the budget of all departments</p> <pre><code>declare totalBudget integer default 0;\nfor\n    i as\n        select budget from department\n    do\n        set totalBudget = totalBudget + i.budget\nend for\n</code></pre>"},{"location":"2_Core/Database_Systems/10_Functions%2C_Triggers/#triggers","title":"Triggers","text":"<p>statement that is executed automatically as a side effect of a modication of the database</p> <pre><code>show triggers;\n</code></pre>"},{"location":"2_Core/Database_Systems/10_Functions%2C_Triggers/#referencing","title":"Referencing","text":"<ul> <li><code>referencing old row as orow</code> - updates and deletes</li> <li><code>referencing new row as nrow</code> - updates and inserts</li> </ul> <pre><code>create trigger setnullTrigger\nbefore update of takes\nreferencing new row as nrow\nfor each row\nwhen(nrow.grade = \"\")\nbegin atomic\n  set nrow.grade = null\n  set nrow.attendance = 0\nend;\n</code></pre> <pre><code>create trigger creditsEarned\nafter update of takes on(grade)\nreferencing new row as nrow\nreferencing old row as orow\nfor each row\nwhen nrow.grade != 'F' and nrow.grade is not null\n and (orow.grade = 'F' or orow.grade is null)\nbegin atomic\n    update student\n    set totCred = totCred + (\n    select credits\n    from course\n    where course.cid = nrow.cid\n  )\n  where student.id = nrow.id;\nend;\n</code></pre> <p><code>begin atomic</code> means update everywhere</p>"},{"location":"2_Core/Database_Systems/11_Database_Design/","title":"11 Database Design","text":"<p>A good database ensures there is no redundancy or anomalies.</p> A Bad Table ID Name CourseID CourseName CourseCredits 198 Ahmed 212 DBMS 3 199 Jameel 212 DBMS 3 200 Azra 212 DBMS 3 201 Habi 212 DBMS 3 202 Azhar 213 OOPS 3"},{"location":"2_Core/Database_Systems/11_Database_Design/#bad-practices","title":"Bad Practices","text":"<p>Increase time and space complexity of database operations.</p>"},{"location":"2_Core/Database_Systems/11_Database_Design/#redundancy","title":"Redundancy","text":"<p>The same data is present in multiple places.</p> <p>Note: It may be intentional for data backup.</p>"},{"location":"2_Core/Database_Systems/11_Database_Design/#anomaly","title":"Anomaly","text":""},{"location":"2_Core/Database_Systems/11_Database_Design/#updation","title":"Updation","text":"<p>If <code>courseName</code> and <code>courseCredits</code> change for <code>212</code>, all the records have to be changed.</p>"},{"location":"2_Core/Database_Systems/11_Database_Design/#insertion","title":"Insertion","text":"<p>If a new course comes up, but there are no students, then <code>courseID</code> cannot be entered into the bad table, as primary key (ID) will be <code>null</code>.</p>"},{"location":"2_Core/Database_Systems/11_Database_Design/#deletion","title":"Deletion","text":"<p>If I delete <code>ID 202</code>, then I\u2019ll lose details about the <code>courseId 213</code></p>"},{"location":"2_Core/Database_Systems/11_Database_Design/#keys","title":"Keys","text":"Superkey Primary Key Candidate Key Attribute(s) that can uniquely identify all attributes in a table. Primary key is a minimal super key. Key that can be a primary key."},{"location":"2_Core/Database_Systems/11_Database_Design/#decomposition","title":"Decomposition","text":"<ol> <li>Atomize every table wrt an entity</li> <li>Connect those tables using relational tables with foreign key</li> </ol> <p>Prevents</p> <ol> <li>Redundancy</li> <li>Anomaly</li> </ol>"},{"location":"2_Core/Database_Systems/11_Database_Design/#normalization","title":"Normalization","text":"<p>It is the process of structuring a database, usually a relational database, in accordance with a series of so-called normal forms in order to reduce data redundancy and improve data integrity.</p> <p>Sometimes, it\u2019s not feasible to re-create an entire database design. In those cases, we will have to normalize tables into a more appropriate design.</p> Prime Non-Prime attributes in the candidate key other attribute"},{"location":"2_Core/Database_Systems/11_Database_Design/#candidate-key","title":"Candidate Key","text":"<p>A key/combination of keys that can help either directly/indirectly derive all attributes of a table.</p>"},{"location":"2_Core/Database_Systems/11_Database_Design/#functional-dependency","title":"Functional Dependency","text":"<p>Gives a unique tuple as the output</p> <p>idk what this means: A key can be a functional dependancy, but the vice-versa does not hold</p>"},{"location":"2_Core/Database_Systems/11_Database_Design/#partial-dependency","title":"Partial Dependency","text":"<p>Consider a key combination as (name, age)</p> <p>Query possible with just name</p> <p>Subset of candidate key can derive non-prime attributes</p> \\[ p \\to np \\]"},{"location":"2_Core/Database_Systems/11_Database_Design/#transitive-dependency","title":"Transitive Dependency","text":"<p>Non prime attributes gives non-prime</p> \\[ np \\to np \\]"},{"location":"2_Core/Database_Systems/11_Database_Design/#full-dependency","title":"Full Dependency","text":"<p>Query possible only with (name, age) combination</p> <p>Subset of candidate key cannot derive non-prime attributes</p> \\[ f(a, b) = y \\implies f(a) \\ne y, f(b) \\ne y \\]"},{"location":"2_Core/Database_Systems/11_Database_Design/#normal-forms","title":"Normal Forms","text":"1NF 2NF 3NF BCNF 4NF 5NF No Multi-Valued Attributes \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 No Partial Dependency \u2705 \u2705 \u2705 \u2705 \u2705 No Transitive Dependency \u2705 \u2705 \u2705 \u2705 LHS = Candidate/Super Key \u2705 \u2705 \u2705 No Multi-Attribute Dependency \u2705 \u2705 Lossless Decomposition \u2705"},{"location":"2_Core/Database_Systems/11_Database_Design/#1nf","title":"1NF","text":""},{"location":"2_Core/Database_Systems/11_Database_Design/#given","title":"Given","text":"sid sname course 01 A CC, CP, OOP 02 B CP, DB 03 C DB"},{"location":"2_Core/Database_Systems/11_Database_Design/#normalized","title":"Normalized","text":"<p>We can turn into 2 tables</p> sid sname 01 A 02 B 03 C sid course 01 CP 01 CC 01 OOP 02 CP 02 DB 03 DB"},{"location":"2_Core/Database_Systems/11_Database_Design/#finding-candidate-key","title":"Finding Candidate Key","text":"<ol> <li> <p>Find the keys with no incoming    these compulsorily have to be in the combination, because there is no other way to reach them</p> </li> <li> <p>Find the transitive closures of all</p> <ul> <li> <p>attributes</p> </li> <li> <p>combination of failure keys</p> </li> </ul> </li> <li> <p>List out all keys</p> </li> <li> <p>Candidate keys are only the keys that are </p> </li> </ol>"},{"location":"2_Core/Database_Systems/11_Database_Design/#3nf","title":"3NF","text":"<p>every functional dependency \\(A \\to B\\) contains</p> <ul> <li>superkey \\(A\\)</li> <li>prime attribute \\(B\\)</li> </ul>"},{"location":"2_Core/Database_Systems/11_Database_Design/#armstrongs-inference-rules","title":"Armstrong\u2019s Inference Rules","text":"<p>\\(\\to\\) means derives</p> Rule Condition Inference Reflexive \\(y \\subset x\\) \\(x \\to y\\) Augmentation \\(x \\to y\\) \\(xz \\to yz\\) Transitive \\(x \\to y, y \\to z\\) \\(x \\to z\\) Decomposition \\(x \\to yz\\) \\(x \\to y, x \\to z\\) Union \\(x \\to y, x \\to z\\) \\(x \\to yz\\) Psuedotransitivity \\(x \\to y, wy \\to z\\) \\(wx \\to z\\)"},{"location":"2_Core/Database_Systems/11_Database_Design/#canonical-minimal-cover","title":"Canonical Minimal Cover","text":"<p>Removing one/more functional dependencies when a set of functional dependencies are given, ensuring that 5NF is still maintained.</p>"},{"location":"2_Core/Database_Systems/12_Indexing/","title":"12 Indexing","text":""},{"location":"2_Core/Database_Systems/12_Indexing/#indexing","title":"Indexing","text":"<p>We are trying to improve searching performance</p>"},{"location":"2_Core/Database_Systems/12_Indexing/#b-tree-indexing","title":"B+ Tree Indexing","text":"<p>Internal nodes contain indices</p> <p>Leaf nodes contain values</p>"},{"location":"2_Core/Database_Systems/12_Indexing/#advantage","title":"Advantage","text":"<ol> <li>Dynamic</li> <li>Faster than other forms of indexing</li> </ol>"},{"location":"2_Core/Database_Systems/12_Indexing/#structure","title":"Structure","text":"<pre><code>flowchart TB\nx --- a[&lt; x] &amp; c[&amp;#8805 x]</code></pre>"},{"location":"2_Core/Database_Systems/12_Indexing/#formulae","title":"Formulae","text":"IDK Formula Order \\(n\\) Min No of Children/Pointers \\(\\left \\lceil \\dfrac{n}{2} \\right \\rceil\\) Max No of Children/Pointers \\(n\\) Min No of Keys \\(\\left \\lceil \\dfrac{n}{2} \\right \\rceil - 1\\) Max No of Keys \\(n-1\\) Middle element \\(\\left \\lceil \\dfrac{n}{2} \\right \\rceil\\)<sup>th</sup> element"},{"location":"2_Core/Database_Systems/12_Indexing/#insertion","title":"Insertion","text":"<p>If there are 2 middle elements, we push the bigger one to the top.</p>"},{"location":"2_Core/Database_Systems/12_Indexing/#deletion","title":"Deletion","text":"<p>Minimum element of the right subtree will go up; basically the element next to the deleted element in the bottom linked list</p> <p>If the key goes less than the minimum key, then we have to borrow the</p> <ul> <li>maximum element of the left side, or</li> <li>minimum element of the right side</li> </ul> <p>if that\u2019s not possible, merge upward</p>"},{"location":"2_Core/Database_Systems/12_Indexing/#hashing","title":"Hashing","text":"<p>Technique to store values in a more accessible form.</p> Hashing Type Technique Using Open Chaining Linked List Closed Linear Quadratic"},{"location":"2_Core/Database_Systems/12_Indexing/#load-factor","title":"Load Factor","text":"\\[ \\text{LF} = \\frac{R}{} \\] <p>where \\(R\\) means Records</p>"},{"location":"2_Core/Database_Systems/12_Indexing/#linear-hashing","title":"Linear Hashing","text":""},{"location":"2_Core/Database_Systems/12_Indexing/#extendible-hashing","title":"Extendible Hashing","text":"<p>This technique is used to minimize the re-hashing costs in normal hashing, which arise due to collisions.</p> Component Meaning Memory Hash Function Hash Table Main Bucket Bins that are formatted similar to B+ Tree Leaves Disk (Secondary) Bucket Size max number of elements in the buckets(will be given) Depth Number of MSB bits required to differentiate Buckets <p>If local depth of filled page \\(\\ge\\) global depth</p> <ol> <li>double the hash table size</li> <li>allocate space for an additional bin</li> <li>re-distribute filled page indices</li> </ol> Case Action Local \\(&lt;\\) Global Local \\(=\\) Global double the hash table size Local \\(&gt;\\) Global 1. allocate space for an additional bin2. re-distribute filled page indices <p></p> <p>The number of combinations that will be connected to each bucket will be the remaining combination of the unused bits. For eg, \\(01\\) bucket will have 2 unused bits, so there will be 4 combinations</p>"},{"location":"2_Core/Database_Systems/12_Indexing/#bitmap","title":"Bitmap","text":"<p>Very easy bro</p> <p>The no of records will be the number of bits.</p>"},{"location":"2_Core/Database_Systems/12_Indexing/#limitations","title":"Limitations","text":"<p>For large database, we need a lot of bits.</p>"},{"location":"2_Core/Database_Systems/13_Transactions/","title":"13 Transactions","text":"<p>single logical unit of work formed by a set of operations.</p>"},{"location":"2_Core/Database_Systems/13_Transactions/#operations","title":"Operations","text":"<ul> <li><code>read(a)</code></li> <li><code>write(a)</code></li> </ul> <p>where <code>a</code> is a resource.</p>"},{"location":"2_Core/Database_Systems/13_Transactions/#conflicts","title":"Conflicts","text":"<p>occurs when resources are in shareable mode.</p> <p>2 operations conflict if they are</p> <ul> <li>on the same object/resource</li> <li>by different transactions</li> <li>atleast 1 transaction is a write</li> </ul>"},{"location":"2_Core/Database_Systems/13_Transactions/#transaction-states","title":"Transaction States","text":"<ol> <li>active</li> <li>partially-commited</li> <li>committed</li> <li>failed</li> <li>aborted</li> <li>terminated</li> </ol> <pre><code>flowchart LR\n\na[Active]\npc[Partially-Committed]\nf[Failed]\nab[Aborted]\nt[Terminated]\n\na --&gt;\n|R/W| pc --&gt;\n|Permanently Store| Committed --&gt;\nt\n\na --&gt;\n|Failure|f --&gt;\n|Roll Back|ab --&gt; t\n\npc --&gt;|Failure| f</code></pre>"},{"location":"2_Core/Database_Systems/13_Transactions/#acid","title":"ACID","text":"<p>Good features of a database. Relational databases are ACID-compliant, but NoSQL aren\u2019t</p>"},{"location":"2_Core/Database_Systems/13_Transactions/#atomicity","title":"Atomicity","text":"<p>Transaction status should be binary - occured/not occured. No transaction must not occur partially.</p> <p>If any sub-steps of a transaction(Operation/Query) fails, the whole transaction must fail and the database must be in the same state as the original.</p>"},{"location":"2_Core/Database_Systems/13_Transactions/#consistency","title":"Consistency","text":"<p>Correct data is ensured through constraints.</p>"},{"location":"2_Core/Database_Systems/13_Transactions/#isolation","title":"Isolation","text":"<p>Concurrent-Execution Safe</p> <p>Simultaneous transactions must be considered as multiple sequential transactions.</p> <p>Transactions must be one-one.</p>"},{"location":"2_Core/Database_Systems/13_Transactions/#durability","title":"Durability","text":"<p>Committed transactions must be stored to a non-volatile memory, to prevent loss of data.</p>"},{"location":"2_Core/Database_Systems/13_Transactions/#commit","title":"Commit","text":"<p>Storing update into permanent memory.</p>"},{"location":"2_Core/Database_Systems/13_Transactions/#concurrency-problems","title":"Concurrency Problems","text":"<p>Conflicts that occur when simultaneous transactions occur.</p> Problem Description Solution Blind Write Dirty-read uncommited transactions are read - Serial Scheduling- values should be stored only after committing Unrepeatable read multiple reads of the same parameter without a commit Lost Update a later write committed first is preferred over the first write committed second, when multiple writes occur simultaneously Nothing really, just how it works Phantom Read - transaction 1 reads- some other transaction deletes- transaction 1 tries reading, but can\u2019t find"},{"location":"2_Core/Database_Systems/13_Transactions/#solutions","title":"Solutions","text":"<p>Before any write occurs, always do a read.</p>"},{"location":"2_Core/Database_Systems/13_Transactions/#types-of-schedules","title":"Types of Schedules","text":""},{"location":"2_Core/Database_Systems/13_Transactions/#serial","title":"Serial","text":"<p>Schedule where each transaction occurs one after the other.</p>"},{"location":"2_Core/Database_Systems/13_Transactions/#parallel","title":"Parallel","text":"<p>Multiple schedules happen at the same time?</p>"},{"location":"2_Core/Database_Systems/13_Transactions/#concurrentinterleaved","title":"Concurrent/Interleaved","text":"<p>Helps improve Throughput</p> <p>Schedule where each transaction overlapping over each other.</p>"},{"location":"2_Core/Database_Systems/13_Transactions/#serializable","title":"Serializable","text":"<p>Non-serial schedule that can be converted into serial schedule.</p>"},{"location":"2_Core/Database_Systems/13_Transactions/#conflict-serializable","title":"Conflict Serializable","text":"<p>Serializable schedule that is possible, only after removing conflicts.</p>"},{"location":"2_Core/Database_Systems/13_Transactions/#equivalent","title":"Equivalent","text":"<p>Represented as \\(S \\equiv S'\\)</p> <p>A serial and non-serial schedule which can be converted to each other.</p>"},{"location":"2_Core/Database_Systems/13_Transactions/#conflict-equivalent","title":"Conflict Equivalent","text":"<p>Conflict Serializable schedule, such that conflicting-pairs occur in the same way as initially.</p>"},{"location":"2_Core/Database_Systems/13_Transactions/#view-serializable","title":"View Serializable","text":"<p>2 schedules \\(S, S'\\) that meet the following criteria. The transaction that</p> <ol> <li>performs first read for a resource in \\(S\\) must do the same in \\(S'\\)</li> <li>reads resource written by another transaction in \\(S\\) must do the same in \\(S'\\)</li> <li>performs final write for a resource in \\(S\\) must do the same in \\(S'\\)</li> </ol>"},{"location":"2_Core/Database_Systems/13_Transactions/#eliminating-conflicts","title":"Eliminating Conflicts","text":""},{"location":"2_Core/Database_Systems/13_Transactions/#precedence-graph","title":"Precedence Graph","text":"<p>Shows which transaction is dependent on which other transactions(s).</p> <pre><code>flowchart RL\n\nt1((T1))\nt2((T2))\nt3((T3))\nt2 --&gt;|z| t1\nt2 --&gt;|y| t3\n\nt3 --&gt;|x| t1</code></pre>"},{"location":"2_Core/Database_Systems/13_Transactions/#interpretations","title":"Interpretations","text":""},{"location":"2_Core/Database_Systems/13_Transactions/#serializability","title":"Serializability","text":"Loop/Cycle Simple Graph Serializable? \u274c \u2705 <p>The above example is serializable.</p>"},{"location":"2_Core/Database_Systems/13_Transactions/#order","title":"Order","text":"<p>Order will start from the transaction with indegree = 0</p> <p>In above example, order will be \\(T_2 \\to T_3 \\to T_1\\)</p>"},{"location":"2_Core/Database_Systems/13_Transactions/#blind-write","title":"Blind Write","text":"<p>without even reading the value, you are writing.</p>"},{"location":"2_Core/Database_Systems/13_Transactions/#throughput","title":"Throughput","text":"<p>Work done per unit time.</p>"},{"location":"2_Core/Database_Systems/13_Transactions/#2-phase-locking-concurrency-control","title":"2 Phase Locking Concurrency Control","text":"Lock Mode Type Number Shareable Read Max Capacity (50, 100, \u2026) Exclusive Write 1"},{"location":"2_Core/Database_Systems/13_Transactions/#lock-manager","title":"Lock Manager","text":"<p>Manages locks on data items</p>"},{"location":"2_Core/Database_Systems/13_Transactions/#lock-table","title":"Lock Table","text":"<p>used by lock manager blah blah</p>"},{"location":"2_Core/Database_Systems/13_Transactions/#graph","title":"Graph","text":""},{"location":"2_Core/Database_Systems/13_Transactions/#types","title":"Types","text":"Hold lock throughout Strict Rigorous Read \u274c \u2705 Write \u2705 \u2705"},{"location":"2_Core/Database_Systems/13_Transactions/#well-formed-transaction","title":"Well Formed Transaction","text":"<ul> <li>Data item must be locked before reading/writing.</li> <li>Should not try to<ul> <li>unlock free resource</li> <li>lock already-locked resource</li> </ul> </li> </ul>"},{"location":"2_Core/Database_Systems/Practicals/00_Intro/","title":"00 Intro","text":""},{"location":"2_Core/Database_Systems/Practicals/00_Intro/#creating-connection","title":"Creating Connection","text":"<ol> <li>Open Workbench</li> <li>Click +</li> <li>Enter any connection name (i put my uni ID 2020A7PS0198U)</li> <li>Enter hostname as 172.16.100.8</li> <li>Enter username as collegeid (like 2020A7PS0198U)</li> </ol>"},{"location":"2_Core/Database_Systems/Practicals/00_Intro/#jdbc","title":"JDBC","text":""},{"location":"2_Core/Database_Systems/Practicals/00_Intro/#creation","title":"Creation","text":"<ol> <li>File &gt; New Project</li> <li>Java with Ant</li> <li>Next &gt; Finish</li> </ol>"},{"location":"2_Core/Database_Systems/Practicals/00_Intro/#code","title":"Code","text":"<pre><code>package jdbc;\nimport java.sql.*;\n\npublic class JavaApplication7 {\n    public static void main(String[] args) {\n        query(\"salesman\");\n        query(\"instructor\");\n        query(\"takes\");\n\n    query(\"salesman\", \"salesman_id &gt; 5003\");\n    }\n  public static void query(String table)\n  {\n    query(table, \"\");\n  }   \n    public static void query(String table, String where)\n    {\n        try\n        {\n            String url = \"jdbc:mysql://172.16.100.8/20200198db\",\n                user = \"2020A7PS0198U\",\n                password = \"a\",\n                query = \"select * from \" + table;\n                        if(where!=\"\")\n                            query += \" where \" + where;\n\n            System.out.println(query + \" \ud83d\ude0a\");\n\n            Class.forName(\"com.mysql.cj.jdbc.Driver\");\n            Connection con = DriverManager.getConnection(url, user, password);\n            Statement stmt = con.createStatement();\n            ResultSet rs = stmt.executeQuery(query); \n            while(rs.next())\n            {\n                String col1 = rs.getString(1),\n                    col2 = rs.getString(2);\n                System.out.println(col1 + \" \" + col2);\n            }\n\n            rs.close();\n            stmt.close();\n            con.close();\n\n            System.out.println(\"\");\n        }\n        catch(Exception e)\n        {\n            System.out.println(\"Something Happened \ud83e\udd23\");\n        }\n    }   \n}\n</code></pre>"},{"location":"2_Core/Database_Systems/Practicals/00_Intro/#output","title":"Output","text":"<pre><code>select * from salesman \ud83d\ude0a\n5001 James Hoog\n5002 Nail Knite\n5003 Lauson Hen\n5005 Pit Alex\n5006 Mc Lyon\n5007 Paul Adam\n\nselect * from instructor \ud83d\ude0a\n102 ABC\n103 DEF\n104 GHI\n\nselect * from takes \ud83d\ude0a\n198 CS F111\n199 Bio F111\n200 Mech F111\n201 111\n\nselect * from salesman where salesman_id &gt; 5003 \ud83d\ude0a\n5005 Pit Alex\n5006 Mc Lyon\n5007 Paul Adam\n</code></pre>"},{"location":"2_Core/Database_Systems/Practicals/00_Intro/#gui","title":"GUI","text":""},{"location":"2_Core/Database_Systems/Practicals/00_Intro/#steps","title":"Steps","text":""},{"location":"2_Core/Database_Systems/Practicals/00_Intro/#code_1","title":"Code","text":"<pre><code>private void jButton1ActionPerformed(java.awt.event.ActionEvent evt) {                                         \n  String salesman_id = jTextField1.getText(),\n  name = jTextField2.getText(),\n  city = jTextField3.getText(),\n  commission = jTextField4.getText();\n\n  String table = \"salesman\",\n  values = \"'\" + salesman_id + \"', '\" + name + \"', '\" + city + \"', \" + commission + \"'\";\n\n  insertQuery(table, values);\n}                                        \n\npublic void insertQuery(String table, String values)\n{\n  try\n  {\n    String url = \"jdbc:mysql://172.16.100.8/20200198db\",\n    user = \"2020A7PS0198U\",\n    password = \"a\",\n    query = \"insert into \" + table + \" values(\"  + values + \")\";\n\n    System.out.println(query);\n\n    Class.forName(\"com.mysql.cj.jdbc.Driver\");\n    Connection con = DriverManager.getConnection(url, user, password);\n    Statement stmt = con.createStatement();\n    stmt.executeUpdate(query);\n\n    stmt.close();\n    con.close();\n\n    //                        JOptionPane.showMessageDialog(this, query);\n  }\n  catch(Exception e)\n  {\n    System.out.println(\"Something Happened \ud83e\udd23\");\n  }\n}   \n</code></pre>"},{"location":"2_Core/Database_Systems/Practicals/01/","title":"01","text":""},{"location":"2_Core/Database_Systems/Practicals/01/#initialization","title":"Initialization","text":"<pre><code>use 20200198db;\nset foreign_key_checks = 0;\ndrop table if exists Students;\nset foreign_key_checks  = 1;\n</code></pre>"},{"location":"2_Core/Database_Systems/Practicals/01/#queries","title":"Queries","text":"<pre><code>CREATE TABLE Students\n(\n  ROLL_NO int,\n  NAME varchar(20),\n  SUBJECT varchar(20)\n);\n</code></pre> <pre><code>DESC Students;\n</code></pre> <pre><code>INSERT INTO Students VALUES (198, \"Thahir\", \"OOPS\");\n</code></pre> <pre><code>INSERT INTO Students (ROLL_NO, NAME, SUBJECT) VALUES\n(198, \"Ahmed\", \"DSA\"),\n(231, \"Ram\", \"DSA\");\n</code></pre> <pre><code>SELECT * FROM Students;\nSELECT ROLL_NO FROM Students;\nSELECT * FROM Students ORDER BY ROLL_NO;\nSELECT * FROM Students ORDER BY ROLL_NO desc;\n</code></pre> <pre><code>ALTER TABLE Students ADD (AGE int,COUNTRY varchar(40));\nALTER TABLE Students DROP COLUMN AGE;\nALTER TABLE Students MODIFY COUNTRY varchar(20);\n\nalter table table_name drop primary key, add primary key(k1, k2, k3);\n</code></pre> <pre><code>DELETE FROM Students WHERE NAME = 'Ram';\nDELETE FROM Students WHERE AGE = 20;\n</code></pre> <pre><code>DROP TABLE Students;\n</code></pre>"},{"location":"2_Core/Database_Systems/Practicals/02/","title":"02","text":""},{"location":"2_Core/Database_Systems/Practicals/02/#initialization","title":"Initialization","text":"<pre><code>use 20200198db;\nset foreign_key_checks = 0;\ndrop table if exists department, student, course, advisor, instructor, teaches, takes, prereq;\nset foreign_key_checks  = 1;\n</code></pre>"},{"location":"2_Core/Database_Systems/Practicals/02/#questions","title":"Questions","text":"<ol> <li>Create a table for the following.    department(dept_name, building, budget)</li> </ol> <pre><code>create table department(\n  dept_name varchar(20),\n  building int(4),\n  budget float(10)\n);\n</code></pre> <ol> <li> <p>Alter the above table to add primary key.</p> <pre><code>ALTER TABLE department ADD PRIMARY KEY (dept_name);\n</code></pre> </li> <li> <p>Create the following tables with primary key and foreign key constraints     | Table                                                       | FK                                                           |     | ----------------------------------------------------------- | ------------------------------------------------------------ |     | Student(SID, name, dept_name, total_credit)          | dept_name                                                    |     | course(course_id, title, dept_name , credits)        | dept_name                                                    |     | instructor(IID, name, dept_name, salary)             | dept_name                                                    |     | teaches(IID, course_id, sec_id, semester, year)      | course_id, instructor(IID)                                   |     | takes(SID, course_id, sec_id, semester, year, grade) |                                                              |     | advisor(SID, IID)                                    | IID references instructor (IID), SID references student (SID) |     | prereq(course_id, prereq_id)                  | course_id references course(course_id), prereq_id references course(course_id) |</p> <p><pre><code>create table student(\n  SID int(4), \n  name varchar(20), \n  dept_name varchar(20), \n  total_credit int(4),\n\n  PRIMARY KEY(SID),\n  FOREIGN KEY (dept_name) REFERENCES department(dept_name)\n);\n\ncreate table course(\n  course_id varchar(20),\n  title varchar(20),\n  dept_name varchar(20),\n  credits int(4),\n\n  PRIMARY KEY(course_id),\n  FOREIGN KEY (dept_name) REFERENCES department(dept_name)\n);\n\ncreate table instructor(\n  IID int(4),\n  name varchar(20),\n  dept_name varchar(20),\n  salary int(4),\n\n  PRIMARY KEY(IID),\n  FOREIGN KEY (dept_name) REFERENCES department(dept_name)\n);\n\ncreate table teaches(\n  IID int(4),\n  course_id varchar(20),\n  sec_id int(4),\n  semester int(4) check (semester between 1 and 2),\n  year int(4),\n\n  PRIMARY KEY(IID),\n  FOREIGN KEY (course_id) REFERENCES course(course_id) ON DELETE CASCADE,\n  FOREIGN KEY (IID) REFERENCES instructor(IID) ON DELETE CASCADE\n);\n\ncreate table takes(\n  SID int(4),\n  course_id varchar(20),\n  sec_id int(4),\n  semester int(4) check (semester between 1 and 2),\n  year int(4),\n  grade varchar(1),\n\n  PRIMARY KEY(SID)\n);\n\ncreate table advisor(\n  SID int(4),\n  IID int(4),\n\n  PRIMARY KEY (SID),\n  FOREIGN KEY (IID) REFERENCES instructor(IID) ON DELETE CASCADE,\n  FOREIGN KEY (SID) REFERENCES student(SID) ON DELETE CASCADE\n);\n\ncreate table prereq(\n  course_id varchar(20),\n  prereq_id int(4),\n\n  PRIMARY KEY (prereq_id, course_id),\n  FOREIGN KEY (course_id) REFERENCES course(course_id) ON DELETE CASCADE,\n  FOREIGN KEY (prereq_id) REFERENCES course(course_id) ON DELETE CASCADE\n);\n</code></pre> 4. Alter the Instructor table to set 10,0000/- as default salary</p> </li> </ol> <pre><code>alter table instructor ALTER salary SET DEFAULT 10000;\n</code></pre> <ol> <li>Insert some rows into all of the above tables</li> </ol> <pre><code>insert into department values\n(\"Computer\", 7, 20000),\n(\"Biotech\", 9, 50000),\n(\"Mechanical\", 3, 70000);\n\ninsert into student values\n(198, \"Thahir\", \"Computer\", 100),\n(199, \"Someone\", \"Biotech\", 90),\n(200, \"Blah\", \"Mechanical\", 50);\n\ninsert into course values\n(\"CS F111\", \"OOPS\", \"Computer\", 3),\n(\"Mech F111\", \"Thermodynamics\", \"Mechanical\", 3),\n(\"Bio F111\", \"Gen Bio\", \"Biotech\", 3);\n\ninsert into instructor values\n(102, \"ABC\", \"Computer\", 5000),\n(104, \"GHI\", \"Mechanical\", 6050.9),\n(103, \"DEF\", \"Biotech\", 7000);\n\ninsert into teaches values\n(102, \"CS F111\", 3, 2, 2020),\n(103, \"Bio F111\", 2, 1, 2020),\n(104, \"Mech F111\", 1, 2, 2019);\n\ninsert into takes values\n(198, \"CS F111\", 3, 2, 2020),\n(199, \"Bio F111\", 2, 1, 2020),\n(200, \"Mech F111\", 1, 2, 2019);\n\ninsert into advisor values\n(198, 102),\n(199, 103),\n(200, 104);\n\ninsert into prereq values\n(\"CS F111\", \"Bio F111\"),\n(\"Mech F111\", \"Bio F111\");\n</code></pre> <ol> <li>Insert a row into \u2018takes\u2019 table with the semester value as 3 or above and view the effect of constraints.</li> </ol> <pre><code>insert into takes values\n(201, 111, 3, 10, 2019, 'D');\n</code></pre> <ol> <li>Delete some rows to view the effect of foreign key constraints.</li> </ol> <pre><code>delete from student;\n</code></pre> <ol> <li>TRUNCATE and DROP tables.</li> </ol> <pre><code>drop table prereq;\nTRUNCATE TABLE advisor;\n</code></pre>"},{"location":"2_Core/Database_Systems/Practicals/03/","title":"03","text":""},{"location":"2_Core/Database_Systems/Practicals/03/#initialization","title":"Initialization","text":"<pre><code>use 20200198db;\nset foreign_key_checks = 0;\ndrop table if exists salesman, customer, orders;\nset foreign_key_checks  = 1;\n</code></pre>"},{"location":"2_Core/Database_Systems/Practicals/03/#tables","title":"Tables","text":"<p>Create the three tables given below and answer the following queries</p>"},{"location":"2_Core/Database_Systems/Practicals/03/#salesman","title":"<code>salesman</code>","text":"salesman_id name city commission 5001 James Hoog New York 0.15 5002 Nail Knite Paris 0.13 5005 Pit Alex London 0.11 5006 Mc Lyon Paris 0.14 5003 Lauson Hen 0.12 5007 Paul Adam Rome 0.13"},{"location":"2_Core/Database_Systems/Practicals/03/#customer","title":"<code>customer</code>","text":"customer_id cust_name city grade salesman_id 3002 Nick Rimando New York 100 5001 3005 Graham Zusi California 200 5002 3001 Brad Guzan London 5005 3004 Fabian Johns Paris 300 5006 3007 Brad Davis New York 200 5001 3009 Geoff Camero Berlin 100 5003 3008 Julian Green London 300 5002 3003 Jozy Altidor Moscow 200 5007"},{"location":"2_Core/Database_Systems/Practicals/03/#orders","title":"<code>orders</code>","text":"ord_no purch_amt ord_date customer_id salesman_id 70001 150.50 2012-10-05 3005 5002 70009 270.65 2012-09-10 3001 5005 70002 65.26 2012-10-05 3002 5001 70004 110.50 2012-08-17 3009 5003 70007 948.50 2012-09-10 3005 5002 70005 2400.6 2012-07-27 3007 5001 70008 5760.00 2012-09-10 3002 5001 70010 1983.43 2012-10-10 3004 5006 70003 2480.40 2012-10-10 3009 5003 70012 250.45 2012-06-27 3008 5002 70011 75.29 2012-08-17 3003 5007 70013 3045.60 2012-04-25 3002 5001 <pre><code>create table salesman(\n  salesman_id int(4),\n  name varchar(20),\n  city varchar(20),\n  commission float(10, 5),\n\n  PRIMARY KEY(salesman_id)\n);\n\ncreate table customer(\n  customer_id int(4),\n  cust_name varchar(20),\n  city varchar(20),\n  grade int(4),\n  salesman_id int(4),\n\n  PRIMARY KEY(customer_id),\n  FOREIGN KEY(salesman_id) REFERENCES salesman(salesman_id) on delete cascade\n);\n\ncreate table orders(\n  ord_no int(5),\n  purch_amt float(10, 5),\n  ord_date date,\n  customer_id int(4),\n  salesman_id int(4),\n\n  PRIMARY KEY(ord_no),\n  FOREIGN KEY(salesman_id) REFERENCES salesman(salesman_id) on delete cascade\n);\n\ninsert into salesman values\n  (5001, \"James Hoog\", \"New York\", 0.15),\n  (5002, \"Nail Knite\", \"Paris\", 0.13),\n  (5005, \"Pit Alex\", \"London\", 0.11),\n  (5006, \"Mc Lyon\", \"Paris\", 0.14),\n  (5003, \"Lauson Hen\", null, 0.12),\n  (5007, \"Paul Adam\", \"Rome\", 0.13);\n\ninsert into customer values \n    (3002, \"Nick Rimando\", \"New York\", 100, 5001),\n    (3005, \"Graham Zusi\", \"California\", 200, 5002),\n    (3001, \"Brad Gusan\", \"London\", null, 5005),\n    (3004, \"Fabian Johns\", \"Paris\", 300, 5006),\n    (3007, \"Brad Davis\", \"New York\", 200, 5001),\n    (3009, \"Geoff Camero\", \"Berlin\", 100, 5003),\n    (3008, \"Julian Green\", \"London\", 300, 5002),\n    (3003, \"Jozy Altidor\", \"Moscow\", 200, 5007);\n\ninsert into orders values\n  (70001, 150.5, \"2012-10-05\", 3005, 5002),\n  (70009, 270.65, \"2012-09-10\", 3001, 5005),\n  (70002, 65.26, \"2012-10-05\", 3002, 5001),\n  (70004, 110.5, \"2012-08-17\", 3009, 5003),\n  (70007, 948.5, \"2012-09-10\", 3005, 5002),\n  (70005, 2400.6, \"2012-07-27\", 3007, 5001),\n  (70008, 5760, \"2012-09-10\", 3002, 5001),\n  (70010, 1983.43, \"2012-10-10\", 3004, 5006),\n  (70003, 2480.4, \"2012-10-10\", 3009, 5003),\n  (70012, 250.45, \"2012-06-27\", 3008, 5002),\n  (70011, 75.29, \"2012-08-17\", 3003, 5007),\n  (70013, 3045.6, \"2012-04-25\", 3002, 5001);\n</code></pre>"},{"location":"2_Core/Database_Systems/Practicals/03/#questions","title":"Questions","text":"<ol> <li>Write a query to find those customers with their name and those salesmen with their name and city who lives in the same city.</li> </ol> <pre><code>select customer.city as City, cust_name as Customer, salesman.name as Salesperson\n from customer, salesman\n where customer.salesman_id = salesman.salesman_id\n         and customer.city = salesman.city;\n</code></pre> <ol> <li>Write a SQL statement to find the names of all customers along with the salesmen who works for them.</li> </ol> <pre><code>select cust_name as Customer, salesman.name as Salesperson\n from customer, salesman\n where customer.salesman_id = salesman.salesman_id;\n</code></pre> <ol> <li>Write a SQL statement to display all those orders by the customers not located in the same cities where their salesmen live.</li> </ol> <pre><code>select *\n from orders\n where (orders.customer_id, orders.salesman_id) in (\n    select customer.customer_id, salesman.salesman_id\n     from customer, salesman\n     where customer.city != salesman.city\n  );\n</code></pre> <ol> <li>Write a SQL statement that finds out each order number followed by the name of the customers who made the order.</li> </ol> <pre><code>select ord_no as \"Order Number\", cust_name as Customer\n from orders, customer\n where orders.customer_id = customer.customer_id;\n</code></pre> <ol> <li>Write a SQL statement that sorts out the customer and their grade who made an order. Each of the customers must have a grade and served by at least a salesman, who belongs to a city.</li> </ol> <pre><code>select cust_name as Customer, grade as Grade\n from customer\n where grade is not null\n     and salesman_id is not null\n order by grade desc;\n</code></pre> <ol> <li>Write a query that produces all customers with their name, city, salesman and commission, who served by a salesman and the salesman works at a rate of the commission within 12% to 14%.</li> </ol> <pre><code>select cust_name as Customer, customer.city, salesman.name, salesman.commission as commission\n from customer, salesman\n where customer.salesman_id = salesman.salesman_id\n     and salesman.commission between 0.12 and 0.14;\n</code></pre> <ol> <li>Write a SQL statement that produces all orders with the order number, customer name, commission rate and earned commission amount for those customers who carry their grade is 200 or more and served by an existing salesman.</li> </ol> <pre><code>select ord_no as \"Order Number\", cust_name as Customer, commission as \"Commission Rate\", (commission * purch_amt) as \"Earned Commission\"\n from orders, customer, salesman\n where orders.customer_id = customer.customer_id\n     and orders.salesman_id = salesman.salesman_id\n     and grade &gt; 200\n     and customer.salesman_id is not null;\n</code></pre> <ol> <li>Write a query to display all customers with a grade above 100.</li> </ol> <pre><code>select cust_name as Customer\nfrom customer\nwhere grade &gt; 100;\n</code></pre> <ol> <li>Write a query statement to display all customers in New York who have a grade value above 100.</li> </ol> <pre><code>select cust_name as Customer\nfrom customer\nwhere city = \"New York\"\n and grade &gt; 100;\n</code></pre> <ol> <li> <p>Write a SQL statement to display all the customers, who are either belongs to the city New York or not had a grade above 100.</p> <pre><code>select cust_name as Customer\nfrom customer\nwhere (city = \"New York\") or (grade &lt;= 100);\n</code></pre> </li> <li> <p>Write a SQL query to display those customers who are neither belongs to the city New York nor grade value is more than 100.</p> <pre><code>select cust_name as Customer\nfrom customer\nwhere not(city = \"New York\" or grade &gt; 100);\n</code></pre> </li> <li> <p>Write a SQL statement to display either those orders which are not issued on date 2012-09-10 and issued by the salesman whose ID is 5005 and below, or those orders which purchase amount is 1000.00 and below.</p> <pre><code>select *\nfrom orders\nwhere not(\n    ord_date &lt; 2012-09-10\n    and salesman_id in (select salesman_id from salesman where salesman_id &lt;= 5005)\n    )\n    or purch_amt &lt;= 1000;\n</code></pre> </li> </ol>"},{"location":"2_Core/Database_Systems/Practicals/04/","title":"04","text":"<p>Same tables as 03.</p>"},{"location":"2_Core/Database_Systems/Practicals/04/#queries","title":"Queries","text":"<ol> <li>Write a query to find salesmen with all information who lives in the city where any of the customers lives.</li> </ol> <pre><code>select salesman.name as Salesperson\nfrom salesman\nwhere salesman_id is not null\n and salesman.name is not null\n    and salesman.city is not null\n    and salesman.commission is not null\n and salesman.city i some(\n     select customer.city from customer\n  );\n</code></pre> <ol> <li>Write a query to display all the orders that had amounts that were greater than at least one of the orders on September 10<sup>th</sup> 2012.</li> </ol> <pre><code>select *\nfrom orders\nwhere orders.purch_amt &gt; some(\n  select purch_amt\n  from orders\n  where ord_date = 2012-09-10\n);\n</code></pre> <ol> <li>Write a query to display all orders with an amount smaller than any amount for a customer in London.</li> </ol> <pre><code>select *\nfrom orders\nwhere orders.purch_amt &lt; some (\n  select purch_amt\n  from orders\n  where orders.customer_id in (\n     select customer.customer_id\n    from customer\n    where city = \"London\"\n  )\n);\n</code></pre> <ol> <li>Write a query to display only those customers whose grade are, in fact, higher than every customer in New York.</li> </ol> <pre><code>select cust_name as Customer\nfrom customer\nwhere grade = (\n  select max(grade)\n  from customer\n  where city = \"New York\"\n);\n</code></pre> <ol> <li>Write a query to get all the information for those customers whose grade is not as the grade of customer who belongs to the city London.</li> </ol> <pre><code>select *\nfrom customer\nwhere grade not in (\n  select grade\n  from customer\n  where city = \"London\"\n);\n</code></pre> <ol> <li>Write a query to display all the orders from the orders table issued by the salesman 'Paul Adam'.</li> </ol> <pre><code>select *\nfrom orders\nwhere salesman_id = (\n select salesman_id\n  from salesman\n  where name = \"Paul Adam\"\n);\n</code></pre> <ol> <li>Write a query to display all the orders for the salesman who belongs to the city London.</li> </ol> <pre><code>select *\nfrom orders\nwhere salesman_id = (\n select salesman_id\n  from salesman\n  where city = \"London\"\n);\n</code></pre> <ol> <li>Write a query to display all the orders which values are greater than the average order value for 10<sup>th</sup> October 2012.</li> </ol> <pre><code>select *\nfrom orders\nwhere purch_amt &gt; (\n select avg(purch_amt)\n  from orders\n  where ord_date = \"2012-10-10\"\n);\n</code></pre> <ol> <li>Write a query to display the commission of all the salesmen servicing customers in Paris.</li> </ol> <pre><code>select distinct commission\nfrom salesman\nwhere city = \"Paris\";\n</code></pre> <ol> <li> <p>Write a query to display all customers with orders on October 5, 2012.</p> <pre><code>select cust_name as Customer\nfrom customer\nwhere customer.customer_id in (\n    select orders.customer_id\n  from orders\n  where ord_date = \"2012-10-05\"\n);\n</code></pre> </li> </ol>"},{"location":"2_Core/Database_Systems/Practicals/05/","title":"05","text":"<p>Use Joins</p> <ol> <li>Write a SQL statement to know which salesman are working for which customer.</li> </ol> <pre><code>select name as Salesperson, cust_name as Customer\nfrom salesman inner join customer\non salesman.salesman_id=customer.salesman_id;\n</code></pre> <ol> <li>Write a SQL statement to find the list of customers who appointed a salesman for their jobs who gets a commission from the company is more than 12%.</li> </ol> <pre><code>select customer.cust_name as Customer\nfrom customer natural join salesman\nwhere salesman.commission &gt; 0.12;\n</code></pre> <ol> <li>Write a SQL statement to find the list of customers who appointed a salesman for their jobs who does not live in the same city where their customer lives, and gets a commission is above 12%.</li> </ol> <pre><code>select distinct customer.cust_name as Customer\nfrom customer join salesman\nwhere salesman.city != customer.city\n  and salesman.commission &gt; 0.12;\n</code></pre> <ol> <li>Write a SQL statement to make a join on the tables salesman, customer and orders in such a form that the same column of each table will appear once and only the relational rows will come.</li> </ol> <pre><code>select *\nfrom salesman natural join customer natural join orders;\n</code></pre> <ol> <li>Write a SQL statement to make a list in ascending order for the customer who works either through a salesman or by own.</li> </ol> <pre><code>select customer.cust_name as Customer\nfrom customer\norder by customer.cust_name asc;\n</code></pre> <ol> <li>Write a SQL statement to make a cartesian product between salesman and customer i.e. each salesman will appear for all customer and vice-versa.</li> </ol> <pre><code>select *\nfrom customer left outer join salesman\non customer.salesman_id = salesman.salesman_id;\n</code></pre> <p>Create course, prereq tables &amp; insert the values as given in the following tables</p>"},{"location":"2_Core/Database_Systems/Practicals/05/#course","title":"<code>course</code>","text":"course_id title dept_name credits BIO-301 Genetics Biology 4 CS-190 Game Design Comp. Sci. 4 CS-315 Robotics Comp. Sci. 3"},{"location":"2_Core/Database_Systems/Practicals/05/#prereq","title":"<code>prereq</code>","text":"course_id prereq_id BIO-301 BIO-101 CS-190 CS-101 CS-347 CS-101 <pre><code>use 20200198db;\nset foreign_key_checks = 0;\ndrop table if exists course, prereq;\nset foreign_key_checks  = 1;\n\ncreate table course (\n  course_id varchar(20),\n  title varchar(20),\n  dept_name varchar(20),\n  credits int,\n\n  primary key(course_id)\n);\n\ncreate table prereq (\n  course_id varchar(20),\n  prereq_id varchar(20),\n\n  primary key(course_id, prereq_id)\n##   foreign key(course_id) references course(course_id),\n##   foreign key(prereq_id) references course(course_id)\n);\n\ninsert into course values\n(\"BIO-301\", \"Genetics\" , \"Biology\", 4),\n(\"CS-190\", \"Game Design\", \"Comp. Sci.\", 4),\n(\"CS-315\", \"Robotics\", \"Comp. Sci.\", 3);\n\ninsert into prereq values\n(\"BIO-301\", \"BIO-101\"),\n(\"CS-190\", \"CS-101\"),\n(\"CS-347\", \"CS-101\");\n</code></pre> <ol> <li>Perform left outer join to get the following output.</li> </ol> course_id title dept_name credits prereq_id BIO-301 Genetics Biology 4 BIO-101 CS-190 Game Design Comp. Sci. 4 CS-101 CS-315 Robotics Comp. Sci. 3 <code>null</code> <pre><code>select *\nfrom course\nnatural left outer join prereq;\n</code></pre> <ol> <li>Perform right outer join to get the following output.</li> </ol> course_id title dept_name credits prereq_id BIO-301 Genetics Biology 4 BIO-101 CS-190 Game Design Comp. Sci. 4 CS-101 CS-347 <code>null</code> <code>null</code> <code>null</code> CS-101 <pre><code>select course_id, title, dept_name, credits, prereq_id\nfrom course\nnatural right outer join prereq;\n</code></pre> <ol> <li>Perform full outer join to get the following output.</li> </ol> course_id title dept_name credits prere_id BIO-301 Genetics Biology 4 BIO-101 CS-190 Game Design Comp. Sci. 4 CS-101 CS-315 Robotics Comp. Sci. 3 <code>null</code> CS-347 <code>null</code> <code>null</code> <code>null</code> CS-101 <pre><code>select *\nfrom course\nfull join prereq;\n## only thing that worked\n</code></pre>"},{"location":"2_Core/Database_Systems/Practicals/06/","title":"06","text":"<p>Use the sample database named <code>sakila</code> and its tables in MySQL Workbench for this week\u2019s lab. Create views for the following queries. Name of the view should be in the format \u201cV1_YourStudentID\u201d for the first query. (For ex. V1_2019A7PS0001). You can view the list of tables in the sakila database using \u2018Show tables\u2019 as given in the following screenshot.</p> <pre><code>use sakila;\nshow tables;\n</code></pre> <ol> <li>Create a view to find all actors whose last name contain the letters GEN.</li> </ol> <pre><code>drop view if exists ActorGEN;\n\ncreate view ActorGEN as\nselect first_name as 'First Name', last_name as 'Last Name'\nfrom actor\nwhere last_name like \"%GEN%\";\n\nselect * from ActorGEN;\n</code></pre> <ol> <li>Create a view to display the country_id and country columns of the following  countries: Afghanistan, Bangladesh, and China using <code>in</code>.</li> </ol> <pre><code>drop view if exists countriesView;\n\ncreate view countriesView as\nselect country_id, country\nfrom country\nwhere country in (\"Afghanistan\", \"Bangladesh\", \"China\");\n\nselect * from countriesView;\n</code></pre> <ol> <li>Create a view to List each film and the number of actors who are listed for that  film. Use tables film_actor and film. Use inner join.</li> </ol> <pre><code>drop view if exists filmActors;\n\ncreate view filmActors as\nselect title as 'Film', count(actor_id) as 'Number of Actors'\nfrom film\n inner join film_actor\n on film.film_id = film_actor.film_id\ngroup by film.film_id;\n\nselect * from filmActors;\n</code></pre> <ol> <li>Create a view to List the last names of actors, as well as how many actors have  that last name.</li> </ol> <pre><code>drop view if exists actorLastNames;\n\ncreate view actorLastNames as\nselect last_name as 'Last Name', count(last_name) as 'No of People'\nfrom actor\ngroup by last_name;\n\nselect * from actorLastNames;\n</code></pre> <ol> <li>Create a view to List last names of actors and the number of actors who have  that last name, but only for names that are shared by at least two actors</li> </ol> <pre><code>drop view if exists actorLastNamesIDK;\n\ncreate view actorLastNamesIDK as\nselect last_name as 'Last Name', count(last_name) as 'No of People'\nfrom actor\ngroup by last_name\nhaving count(last_name) &gt;= 2;\n\nselect * from actorLastNamesIDK;\n</code></pre> <ol> <li>Create a view to display the first and last names, as well as the address, of each staff member. Use the tables staff and address. Use Join.</li> </ol> <pre><code>drop view if exists staffStuff;\n\ncreate view staffStuff as\nselect first_name as 'First Name', last_name as 'Last Name', \naddress as 'Address'\nfrom staff\n inner join address\n on staff.address_id = address.address_id;\n\nselect * from staffStuff;\n</code></pre> <ol> <li>Create a view to Use subqueries to display all actors who appear in the film  Alone Trip.</li> </ol> <pre><code>drop view if exists AloneTripActors;\n\ncreate view AloneTripActors as\nselect concat(first_name, \" \", last_name) as Actor\nfrom actor\nwhere actor.actor_id in (\n select film_actor.actor_id\n  from film_actor\n  where film_actor.film_id = (\n    select film.film_id\n    from film\n    where title = \"Alone Trip\"\n  )\n);\n\nselect * from AloneTripActors;\n</code></pre> <ol> <li>Create a view to Display the most frequently rented movies in descending order.</li> </ol> <pre><code>drop view if exists FreqRentedMovies;\n\ncreate view FreqRentedMovies as\nselect title as 'Film', count(rental_id) as 'Number of Rentals'\nfrom film\n inner join inventory\n on inventory.film_id = film.film_id\n inner join rental\n on rental.inventory_id = inventory.inventory_id\ngroup by title\norder by count(rental_id) desc;\n\nselect * from FreqRentedMovies;\n</code></pre> <ol> <li>Create a view to Write a query to display for each store its store ID, city, and country.</li> </ol> <pre><code>drop view if exists StoreView;\n\ncreate view StoreView as\nselect store_id as 'Store ID', city as City, country as Country\nfrom store\n inner join address\n     on address.address_id = store.address_id\n inner join city\n     on city.city_id = address.city_id\n inner join country\n     on country.country_id = city.country_id;\n\nselect * from StoreView;\n</code></pre> <ol> <li> <p>Drop any of the views you created.</p> <pre><code>drop view StoreView;\n</code></pre> </li> </ol>"},{"location":"2_Core/Database_Systems/Practicals/07/","title":"07","text":"<pre><code>use 20200198db;\nset foreign_key_checks = 0;\ndrop table if exists employee, dept, department, deptSalary;\ndrop procedure if exists updateSalary;\nset foreign_key_checks  = 1;\n\ncreate table dept(\n  dnumber integer,\n  dname varchar(20),\n\n  primary key(dnumber)\n);\n\ninsert into dept values\n(1, \"Payroll\"),\n(2, \"TechSupport\"),\n(3, \"Research\");\n\ncreate table employee(\n  id integer,\n  name varchar(20),\n  superId integer,\n  salary float(10, 4),\n  bdate date,\n  dno integer,\n\n  primary key(id),\n  foreign key(dno) references dept(dnumber)\n);\n\ninsert into employee values\n(1, \"john\", 3, 100000, \"1960-01-01\", 1),\n(2, \"mary\", 3, 50000, \"1964-12-01\", 3),\n(3, \"bob\", null, 80000, \"1974-02-07\", 3),\n(4, \"tom\", 1, 50000, \"1978-01-17\", 2),\n(5, \"bill\", null, null, \"1985-01-20\", 1);\n</code></pre> <pre><code>create table deptSalary as\nselect dnumber, 0 as totalSalary from dept;\n\ndelimiter \\\\\ncreate procedure updateSalary(IN paraml int)\nbegin\n    update deptSalary\n    set totalSalary = (\n        select sum(salary) from employee where dno = paraml\n    )\n    where dnumber = paraml;\nend; \\\\\ndelimiter ;\ncall updateSalary(1);\ncall updateSalary(2);\ncall updateSalary(3);\n\nselect * from deptSalary;\n\nselect * from employee;\ndrop function if exists giveRaise;\ndelimiter \\\\\ncreate function giveRaise(oldval double, amount double)\nreturns double\ndeterministic\nbegin\n    declare newval double;\n    set newval = oldval * (1+amount);\n    return newval;\nend \\\\\ndelimiter ;\n\nselect name, salary, giveRaise(salary, 0.1) as newsal from employee;\n</code></pre>"},{"location":"2_Core/Database_Systems/Practicals/08/","title":"08","text":"<pre><code>use 20200198db;\nset foreign_key_checks = 0;\n\nset foreign_key_checks  = 1;\n</code></pre> <pre><code>## To Print \u201cHello World!\u201d\ndrop procedure if exists HelloWorld;\ndelimiter //\nCREATE PROCEDURE HelloWorld()\nBEGIN\nSELECT \"Hello World!\";\nEND; //\ndelimiter ;\nCALL HelloWorld();\n</code></pre> <pre><code>## To Print \u201cHello (with given name)\u201d using function\ndrop function if exists HelloName;\nCREATE FUNCTION HelloName(name VARCHAR(100))\nRETURNS VARCHAR(120) DETERMINISTIC\nRETURN CONCAT(\"Hello \", name);\nSET @fn_res = HelloName(\"BPDC\");\n\nselect @fn_res;\n</code></pre> <pre><code>## function to compute square\ndrop function if exists compute_square_fn;\ndelimiter //\ncreate function compute_square_fn(number int)\nreturns int\nbegin\nreturn number * number;\nend //\ndelimiter ;\nselect compute_square_fn(3);\n</code></pre> <pre><code>## function to compute area of circle\ndrop function if exists compute_circle_area;\ndelimiter //\ncreate function compute_circle_area(radius int)\nreturns float DETERMINISTIC\nbegin\nreturn 3.14*radius * radius;\nend\n//\ndelimiter ;\nselect compute_circle_area(4);\n</code></pre> <pre><code>## Control Statements\ndrop table if exists t;\ncreate table t(s1 int);\nINSERT INTO t VALUES (17);\n</code></pre> <pre><code>drop procedure if exists p12;\ndelimiter //\nCREATE PROCEDURE p12 (IN parameter1 INT)\nBEGIN\nDECLARE variable1 INT;\nSET variable1 = parameter1 + 1;\nIF variable1 = 0 THEN\n\nINSERT INTO t VALUES (17);\nEND IF;\nIF parameter1 = 0 THEN\n\nUPDATE t SET s1 = s1 + 1 where s1 = 17;\nELSE\nUPDATE t SET s1 = s1 + 2 where s1 = 17;\nEND IF;\nEND; //\n\ndelimiter ;\nCALL p12(1);\n</code></pre> <pre><code>## switch case\ndrop procedure if exists p13;\ndelimiter //\nCREATE PROCEDURE p13 (IN parameter1 INT)\nBEGIN\nDECLARE variable1 INT;\nSET variable1 = parameter1 + 1;\nCASE variable1\nWHEN 0 THEN\nINSERT INTO t VALUES (17);\nWHEN 1 THEN\nINSERT INTO t VALUES (18);\nELSE\nINSERT INTO t VALUES (19);\n\nEND CASE;\nEND; //\ndelimiter ;\nCALL p13(0);\nselect * from t;\n</code></pre> <pre><code>## while loop\ndrop procedure if exists p14;\ndelimiter //\nCREATE PROCEDURE p14 ()\nBEGIN\nDECLARE v INT;\nSET v = 0;\nWHILE v &lt; 5 DO\n\nINSERT INTO t VALUES (v);\n\nSET v = v + 1;\nEND WHILE;\nEND; //\ndelimiter ;\nCALL p14();\nselect * from t;\n</code></pre> <pre><code>## do-while loop\ndrop procedure if exists p15;\ndelimiter //\nCREATE PROCEDURE p15 ()\nBEGIN\nDECLARE v INT;\nSET v = 5;\nREPEAT\n\nINSERT INTO t VALUES (v);\n\nSET v = v + 1;\nUNTIL v &gt;= 10\nEND REPEAT;\nEND; //\ndelimiter ;\nCALL p15();\nselect * from t;\n</code></pre> <pre><code>## loop ... end loop\ndrop procedure if exists p16;\ndelimiter //\nCREATE PROCEDURE p16 ()\nBEGIN\nDECLARE v INT;\nSET v = 10;\nloop_label: LOOP\n\nINSERT INTO t VALUES (v);\n\nSET v = v + 1;\nIF v &gt;= 15 THEN\n\nLEAVE loop_label;\nEND IF;\nEND LOOP;\n\nEND; //\ndelimiter ;\nCALL p16();\n\nselect * from t;\n</code></pre> <pre><code>## display even-valued rows\nSELECT * FROM t WHERE MOD(s1, 2) = 0;\n</code></pre> <pre><code>## display odd-valued rows\nSELECT * FROM t WHERE MOD(s1, 2) != 0;\n</code></pre> <pre><code>## Use \u201cCASE\u201d to insert or update\ndrop procedure if exists assignment;\ndelimiter //\nCREATE PROCEDURE assignment (IN parameter1 INT, in parameter2 INT)\nBEGIN\nCASE parameter2\nWHEN 1 THEN\nINSERT INTO t VALUES (parameter1/parameter2);\nWHEN 2 THEN\nINSERT INTO t VALUES (parameter1/parameter2);\nELSE\nINSERT INTO t VALUES (parameter1);\n\nEND CASE;\nEND; //\ndelimiter ;\nCALL assignment(10, 0);\nCALL assignment(10, 1);\nCALL assignment(10, 2);\nselect * from t;\n</code></pre>"},{"location":"2_Core/Database_Systems/Practicals/09/","title":"09","text":"<ol> <li>Trigger to update the total salary of a department when a new employee is hired</li> </ol> <pre><code>drop trigger if exists update_salary_on_insert;\n\ndelimiter \\\\\ncreate trigger update_salary_on_insert\nafter insert on employee\nfor each row\nbegin\n if new.dno is not null then\n     update deptSalary\n        set totalSalary = totalSalary + new.salary\n        where dnumber = new.dno;\n    end if;\nend \\\\\ndelimiter ;\n\nselect * from deptSalary;\ninsert into employee value(6, \"Lucy\", null, 90000, \"1981-01-01\", 1);\nselect * from deptSalary;\ninsert into employee values(7, \"George\", null, 45000, \"1971-11-11\", null);\nselect * from deptSalary;\n</code></pre> <ol> <li>Trigger to update the total salary of a department when an employee tuple is modified    (Adding/subtracting the difference is not safe, as there may be cases where the employee shifts to different department)</li> </ol> <pre><code>drop trigger if exists update_salary_on_update;\n\ndelimiter \\\\\ncreate trigger update_salary_on_update\nafter update on employee\nfor each row\nbegin\n if old.dno is not null then\n     update deptSalary\n        set totalSalary = totalSalary - old.salary\n        where dnumber = old.dno;\n    end if;\n    if new.dno is not null then\n     update deptSalary\n        set totalSalary = totalSalary + new.salary\n        where dnumber = new.dno;\n    end if;\nend \\\\\ndelimiter ;\n\nselect * from deptSalary;\nupdate employee set salary = 100000 where id = 6;\nselect * from deptSalary;\n</code></pre> <ol> <li>Trigger to update the total salary of a department when an employee tuple is deleted</li> </ol> <pre><code>drop trigger if exists update_salary_on_delete;\n\ndelimiter \\\\\ncreate trigger update_salary_on_delete\nbefore delete on employee\nfor each row\nbegin\n if old.dno is not null then\n     update deptSalary\n        set totalSalary = totalSalary - old.salary\n        where dnumber = old.dno;\n    end if;\nend \\\\\ndelimiter ;\n\nselect * from deptSalary;\ndelete from employee where id = 6;\nselect * from deptSalary;\ndelete from employee where id = 7;\nselect * from deptSalary;\n</code></pre>"},{"location":"2_Core/Digital_Design/","title":"Digital Design","text":"Class Instructor Lecture Dr. Swarnalatha Tutorial Dr. Vilas Gaidhane Practical Dr. Jagadish Nayak <p>This course provides a solid foundation in digital circuit design, introducing essential concepts like number systems, error detection and correction codes, and Boolean algebra. It covers the design and functionality of basic digital components, such as adders, comparators, decoders, multiplexers, and memory elements like RAM and ROM. Students will also explore advanced circuit elements, including latches, flip-flops, counters, and shift registers, with an emphasis on both synchronous and asynchronous sequential circuits. </p> <p>The course includes hands-on experience with Verilog for simulating digital designs, allowing students to implement and test circuits, including sequence detectors and counters. Additionally, it introduces state machine models (Moore and Mealy), and circuit design techniques for various applications, including Analog-to-Digital and Digital-to-Analog converters. Through both theoretical knowledge and practical Verilog programming, students gain skills to design and analyze digital circuits effectively.</p> <p>This course is a pre-requisite to understanding Microprocessors and Interfacing in the next semester.</p>"},{"location":"2_Core/Digital_Design/01_Intro/","title":"01 Intro","text":""},{"location":"2_Core/Digital_Design/01_Intro/#binary-constants","title":"Binary constants","text":"<p>0 = off/high-level voltage</p> <p>1 = on/low-level voltage</p>"},{"location":"2_Core/Digital_Design/01_Intro/#logic-system","title":"Logic system","text":"<p>There is 10% tolerance</p>"},{"location":"2_Core/Digital_Design/01_Intro/#positive","title":"Positive","text":"<ul> <li>0 =  0v ()</li> <li>1 = 5v (4.5v - 5v)</li> </ul>"},{"location":"2_Core/Digital_Design/01_Intro/#negative","title":"Negative","text":"<ul> <li>0 = 0v</li> <li>1 = </li> </ul>"},{"location":"2_Core/Digital_Design/01_Intro/#pulse-notations","title":"Pulse Notations","text":""},{"location":"2_Core/Digital_Design/01_Intro/#rise-time","title":"Rise time","text":"<p>Time from 10% to 90% V</p>"},{"location":"2_Core/Digital_Design/01_Intro/#fall-time","title":"Fall Time","text":"<p>Time from 90% to 10% V</p>"},{"location":"2_Core/Digital_Design/01_Intro/#pulse-width","title":"Pulse Width","text":"<p>Time from 50% V of one end to 50% V of the other end</p>"},{"location":"2_Core/Digital_Design/01_Intro/#duty-cycle","title":"Duty Cycle","text":"<p>T = time period = T<sub>on</sub> + T<sub>off</sub></p>"},{"location":"2_Core/Digital_Design/01_Intro/#verilog","title":"Verilog","text":"<p>Verifying logic</p> <p>Hardware Description language</p> <p>Execution of lines in program happens concurrently</p>"},{"location":"2_Core/Digital_Design/01_Intro/#ports","title":"Ports","text":"<p>Input, outputs, wires, registers</p>"},{"location":"2_Core/Digital_Design/01_Intro/#multiple-bits","title":"Multiple Bits","text":"<p>Create an array</p> <p>In verilog, arrays are numbered in reverse</p> <pre><code>input[3:0] a; // 4bit          3 2 1 0\ninput[7:0] b; // 8bit  7 6 5 4 3 2 1 0\n\na = 4'b0000;\nb = 8'b1010100;\n</code></pre>"},{"location":"2_Core/Digital_Design/01_Intro/#modelling","title":"Modelling","text":""},{"location":"2_Core/Digital_Design/01_Intro/#types","title":"Types","text":"Gate level Dataflow Behavior Modelling Structural low level medium level high level definining gates mathematical (arithmetic/boolean) operations describe the behavior of the circuit custom gates or inbuilt gate like and(), or(), nand() <code>assign</code> 2 structures procedures - <code>initial</code>, <code>always</code> 2types of module instantiation <code>or(y, a, b);</code><code>or(z, c, d);</code> <code>assign y = a|b;</code><code>assign z = c|d;</code> <code>if(a==0 &amp;b==0)</code><code>y = 0</code> <code>else</code><code>y = 1</code> <code>or g1(y, a, b);</code><code>or g2(z, c, d);</code> <code>initial</code> <code>always</code> runs statement only once, during the entire simulation run continuous infinite loop Starts at t = 0 t = 0 <pre><code>initial\n  begin\n    statement;\n  end\n\nalways\n  begin\n    statement;\n  end\n\n//behavioral\n//example\nalways\n  #5 A = ~A; // invert every 5 nanoseconds\n\nalways\n  begin\n    #2 a = 1;\n    #3 a = 0;\n  end\n</code></pre>"},{"location":"2_Core/Digital_Design/01_Intro/#examples","title":"Examples","text":"<ol> <li>y = a or b or c</li> </ol> <pre><code>module or_gate(A, B,C, y); // ports, initialisation\n  input A, B, C; // declaration\n  output y; // declaration\n\n  // description, always (output, input)\n  or(y, A, B, C); // gate level modelling\n  assign y = A|B|C; // data flow modelling\n  end module\n</code></pre> <ol> <li>p = s', q = I<sub>0</sub>S, r = I<sub>1</sub>S    y = q+r</li> </ol> <pre><code>module circuit_1(I0, I1, S, y);\n    input I0, I1, s;\n  output y;\n  wire p, q, r;\n\n  not #1 G1(p, s); // #1 = time delay of 1ns\n  and #2 G2(q, i0, p); // #2 = time delay of 2ns\n  and #2 g3(r, s, i1);\n  or #2 g4(y, q, r);\n  end module\n</code></pre> <ol> <li>p = s', q = I<sub>0</sub>S, r = I<sub>1</sub>S    y = q+r    Dataflow</li> </ol> <pre><code>module circuit_1(I0, I1, S, y);\n input I0, I1, s;\n  output y;\n  wire p, q, r;\n\n  assign #1 p = Ns;\n  assign #2 q = i0 &amp; p;\n  assign #2 r = i1 &amp; s;\n  assign #2 y = q | r;\n\n  end module\n</code></pre> <ol> <li>Behavioral     <pre><code>module or_behavior(a,b,z);\n  output reg z;\n  always @ * // (a or b) // sensitivity list\n    begin\n      if (a == 1'b0 &amp; b == 1'b0) // 1 bit binary\n                z = 1'b0;\n      else\n        z = 1'b1;\n    end\nendmodule\n\nalways @ * begin   \n  case(s)\n    2'b00: // statement\n    2'b01: // statement\n    2'b10: // statement\n    2'b11: // statement\n  endcase\nend\n</code></pre></li> </ol>"},{"location":"2_Core/Digital_Design/01_Intro/#breadboard","title":"Breadboard","text":""},{"location":"2_Core/Digital_Design/02_Gates/","title":"02 Gates","text":""},{"location":"2_Core/Digital_Design/02_Gates/#gates","title":"Gates","text":"a b a' \\(a \\cdot b\\) a + b a nand b a nor b a xor b a xnor b 0 0 1 0 0 1 1 0 1 0 1 0 1 1 0 1 0 1 0 0 0 1 1 0 1 0 1 1 1 1 0 0 0 1"},{"location":"2_Core/Digital_Design/02_Gates/#verilog-codes","title":"Verilog Codes","text":"<pre><code>module gates(y, a, b);\n  input a, b;\n  output y;\n  wire andg, org, notg, nandg, norg, xorg, xnorg;\n\n  assign andg = a &amp; b;\n  assign org = a | b;\n  assign notg = ~a;\n\n  assign nandg = ~ (a&amp;b);\n  assign norg = ~ (a|b);\n\n  assign xorg = a^b;\n  assign xnorg = ~(a^b);\n\nendmodule\n</code></pre>"},{"location":"2_Core/Digital_Design/02_Gates/#bubbled-gates","title":"Bubbled Gates","text":"<p>inputs to the gates are negated</p> Bubbled AND Bubbled OR Function \\(y = a' \\cdot b'\\) \\(y = a' + b'\\) Another name Negative AND Negative OR equivalent to NOR NAND"},{"location":"2_Core/Digital_Design/02_Gates/#verilog","title":"Verilog","text":"<pre><code>module gates(y, a, b);\n  input a, b;\n  output y;\n  wire band, bor;\n\n  assign band = ~a &amp; ~b;\n  assign bor = ~a | ~b;\n\nendmodule\n</code></pre>"},{"location":"2_Core/Digital_Design/02_Gates/#universal-gates","title":"Universal Gates","text":"<p>NAND &amp; NOR are called universal gates because we can implement all other gates with just these 2</p>"},{"location":"2_Core/Digital_Design/02_Gates/#for-not","title":"for NOT","text":"<p>for NAND and OR, just split a single input into 2 wires and pass it through the gate</p> <pre><code>module gates(y, z, a, b);\n  input a, b;\n  output y, z;\n\n  assign y = nand(a, b);\n  assign z = nor(a, b);\n\nendmodule\n</code></pre>"},{"location":"2_Core/Digital_Design/02_Gates/#for-or","title":"for OR","text":"<p>NOR</p> <ol> <li>use a gate</li> <li>complement the output</li> </ol> <p>NAND</p> <ol> <li>complement a, b individually </li> <li>Complement the output using another gate</li> </ol> <pre><code>module gates(y, a, b);\n  input a, b;\n  output y;\n  wire p, q;\n\n  assign p = nor(a, b); // (a+b)'\n  assign y = nor(p, p); // a+b\n\n  assign p = nand(a, a); // a'\n  assign q = nand(b, b); // b'\n  assign y = nand(p, q); // (a' . b')' = a + b\n\nendmodule\n</code></pre>"},{"location":"2_Core/Digital_Design/02_Gates/#for-and","title":"for AND","text":"<p>NAND</p> <ol> <li>use a gate</li> <li>complement the output using another gate</li> </ol> <p>NOR</p> <ol> <li>complement a, b invidually</li> <li>Complement the output using another gate</li> </ol> <pre><code>module gates(y, a, b);\n  input a, b;\n  output y;\n  wire p, q;\n\n  assign p = nand(a, b); // (a*b)'\n  assign y = nand(p, p); // a*b\n\n  assign p = nor(a, a); // a'\n  assign q = nor(b, b); // b'\n  assign y = nor(p, q); // (a' + b')' = a * b\n\nendmodule\n</code></pre>"},{"location":"2_Core/Digital_Design/03_Number_Systems/","title":"03 Number Systems","text":""},{"location":"2_Core/Digital_Design/03_Number_Systems/#number-systems","title":"Number Systems","text":"<p>\\(d_{n-1} \\dots d_2 \\ d_1 \\ d_0 . d_{-1} \\ d_{-2}\\)</p> <ul> <li>\\(d_{n-1} \\to d_0\\) = integer part</li> <li>\\(d_{-1} \\to \\dots\\) = fraction part</li> <li> <p>\\(d_{n-1}\\) = MSD (most significant digit)</p> </li> <li> <p>\\(d_0\\) = LSD (least significant digit)</p> </li> </ul> Decimal Binary Octal Hexa 10 2 8 16 Groups of 3bits Groups of 4bits Decimal Binary Octal Hexa 0 0 0 0 1 1 1 1 2 10 2 2 3 11 3 3 4 100 4 4 5 101 5 5 6 110 6 6 7 111 7 7 8 1000 10 8 9 1001 11 9 10 1010 12 A 11 1011 13 B 12 1100 14 C 13 1101 15 D 14 1110 16 E 15 1111 17 F"},{"location":"2_Core/Digital_Design/03_Number_Systems/#binary","title":"Binary","text":"<p>Bit = each digit</p> <p>Nibble = group of 4bits</p> <p>Byte = group of 8bits</p> Unsigned Signed Magnitude 1's comp 2's comp Range \\(+(2^n - 1)\\) \\(-(2^{n-1} - 1) \\longleftrightarrow +(2^{n-1} - 1)\\) \\(-(2^{n-1} - 1) \\longleftrightarrow +(2^{n-1} - 1)\\) \\(-(2^{n-1} - 1) \\longleftrightarrow +2^{n-1}\\) +ve regular same as unsigned same as unsigned same as unsigned -ve - invert MSD Bit-by-bit complement of unsigned 1. Bit-by-bit complement of unsigned2. Add 1 +ve MSD 0 0 0 0 -ve MSD - 1 1 1 <pre><code>// 1s comp\nmodule ones(a,y);\n  input[3:0] a; // 4bits (0 to 3)\n  output[3:0] y;\n  assign y = ~a;\nendmodule\n\n// 2s comp\nmodule twos(a,y);\n  input[3:0] a; // 4bits (0 to 3)\n  output[3:0] y;\n  assign y = (~a) + 1;\nendmodule\n</code></pre>"},{"location":"2_Core/Digital_Design/03_Number_Systems/#decimal","title":"Decimal","text":""},{"location":"2_Core/Digital_Design/03_Number_Systems/#octal","title":"Octal","text":""},{"location":"2_Core/Digital_Design/03_Number_Systems/#hexadecimal","title":"Hexadecimal","text":""},{"location":"2_Core/Digital_Design/04_Codes/","title":"04 Codes","text":""},{"location":"2_Core/Digital_Design/04_Codes/#bcd","title":"BCD","text":"<p>Binary Coded Decimal</p> <p>Each digit of decimal will be represented in 4bit binary</p> <p>To convert a number &gt; 9, we add 6 (tutorial)</p> <p>Eg: 33 = 0011 0011</p>"},{"location":"2_Core/Digital_Design/04_Codes/#gray-code","title":"Gray Code","text":"<p>Reflection/Unit distance code</p> <p>Mirror the halfway horizontally</p> <p>just a way to send information in a private method</p>"},{"location":"2_Core/Digital_Design/04_Codes/#1-bit","title":"1 Bit","text":"<p>0</p> <p>1</p>"},{"location":"2_Core/Digital_Design/04_Codes/#2-bit","title":"2 Bit","text":"<p>0 0 = 0</p> <p>0 1 = 1</p> <p>1 1 = 2</p> <p>1 0 = 3</p>"},{"location":"2_Core/Digital_Design/04_Codes/#3-bit","title":"3 Bit","text":"<p>0 0 0 = 0</p> <p>0 0 1</p> <p>0 1 1</p> <p>0 1 0</p> <p>1 1 0</p> <p>1 1 1</p> <p>1 0 1</p> <p>1 0 0</p>"},{"location":"2_Core/Digital_Design/04_Codes/#xor-gate-shortcut","title":"XOR gate Shortcut","text":"<p>odd one detector</p>"},{"location":"2_Core/Digital_Design/04_Codes/#binary-to-gray","title":"Binary to Gray","text":"<ol> <li>convert into binary</li> <li>do XOR of</li> <li>Bring 1<sup>st</sup> digit down as it is</li> <li>do XOR of adjacent elements</li> </ol>"},{"location":"2_Core/Digital_Design/04_Codes/#gray-to-binary","title":"Gray to Binary","text":"<ol> <li>bring 1<sup>st</sup> digit down as it is</li> <li>do XOR diagonally after that</li> </ol>"},{"location":"2_Core/Digital_Design/04_Codes/#error-detection-and-code-correction","title":"Error detection and code correction","text":"<p>Parity is a technique to convert codes with even no of 1s or odd no of 1s by adding an extra bit.</p> <p>Parity Bit is added in MSD position</p> <p>we add a 1, if the number violates the parity type</p>"},{"location":"2_Core/Digital_Design/04_Codes/#even-parity-generator","title":"Even parity generator","text":"<p>we need even no of 1s</p> <p>value of binary with even no doesn't get affected</p> <p>XOR gate</p> a b c even parity odd parity 0 0 0 0 1 0 0 1 1 0 0 1 0 1 0 0 1 1 0 1 1 0 0 1 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 0 <p>Eg</p> <ol> <li>_ 0 0 1 0 0 1 (even no of 1s)    0 0 0 1 0 0 1</li> <li>_ 0 0 1 0 1 1 (odd no of 1s)    1 0 0 1 0 1 1</li> </ol> <p>Circuit contains 2 XOR gates, or we can just do with one </p>"},{"location":"2_Core/Digital_Design/04_Codes/#even-parity-checker","title":"Even Parity Checker","text":"<p>Checking if the parity being sent along with the bits is correct or not</p> <p>Circuit contains 3 XOR gates</p> <p>If there is no error, c = 0 If there is error, then c = 1</p> P X Y Z CheckerC 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 1 1 1 1 0 0 0 1 1 0 0 1 0 1 0 1 0 0 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0"},{"location":"2_Core/Digital_Design/04_Codes/#odd-parity-generator","title":"Odd parity generator","text":"<p>Circuit contains 2 XOR gates and 1 final not gate</p> <p>or XNOR gate</p>"},{"location":"2_Core/Digital_Design/04_Codes/#odd-parity-checker","title":"Odd parity checker","text":""},{"location":"2_Core/Digital_Design/04_Codes/#disadvantage-of-odd-and-even-parity","title":"Disadvantage of odd and even parity","text":"<p>we cannot find out where the error is present</p> <p>eg: 1001 is correct even parity but if the other end receives 0101, it is still correct by even parity, but it's not the same value</p>"},{"location":"2_Core/Digital_Design/04_Codes/#hamming-code","title":"Hamming Code","text":"<p>\\(A(t, m)\\)</p> <p>eg: A (7, 4) means 7 total bits, 4 message bits; this means there are 3 parity bits</p> <p>given no \\(= m_1 m_2 \\dots m_m\\) (b = the bit)</p> <p>Advantage</p> <ol> <li>we can generate parity</li> <li>check parity</li> <li>if there is any error, we can correct</li> </ol>"},{"location":"2_Core/Digital_Design/04_Codes/#no-of-parities","title":"No of Parities","text":"<p>\\(2^p \\ge p + m + 1\\)</p> <ul> <li>p = no of parity bits</li> <li>m = no of message bits</li> </ul> <p>final message will have (m+p) no of bits</p>"},{"location":"2_Core/Digital_Design/04_Codes/#position-of-parity","title":"Position of parity","text":"<p>Parities are added in the place of 2 powers, ie, \\(2^0, 2^1, 2^2, 2^3, \\dots\\), ie</p> <ul> <li>1<sup>st</sup> position</li> <li>2<sup>nd</sup> position</li> <li>4<sup>th</sup> position</li> <li>8<sup>th</sup> position</li> <li>so on</li> </ul>"},{"location":"2_Core/Digital_Design/04_Codes/#example","title":"Example","text":"1 2 3 4 5 6 7 p1 p2 m1 p4 m2 m3 m4"},{"location":"2_Core/Digital_Design/04_Codes/#value-of-parity","title":"Value of Parity","text":"<p>For even parity</p> <p>For 3 parities, create a table of 3bit input and wherever there is one is the positions of the parities</p> <ul> <li>\\(P_1 \\to 1, 3, 5, 7\\)</li> <li>\\(P_2 \\to 2, 3, 6, 7\\)</li> <li>\\(P_3 \\to 4, 5, 6, 7\\)</li> </ul> <p>For 4 parities, create a table of 4bit input and wherever there is one is the positions of the parities</p> <ul> <li>\\(P_1 \\to 1, 3, 5, 7, 9\\)</li> <li>\\(P_2 \\to\\)</li> <li>\\(P_3 \\to\\)</li> <li> <p>\\(P_4 \\to\\)</p> </li> <li> <p>A (7, 4) (total, message) is received as 1 0 1 0 1 1 1. Determine the correct code when even parity exists.</p> </li> <li>1 0 0 1 1 0 1 0</li> </ul>"},{"location":"2_Core/Digital_Design/04_Codes/#verilog-for-hamming","title":"Verilog for Hamming","text":"<p>m1 = D0, m2 = D1,\\(m_n = D_{n-1}\\) </p> <pre><code>module hamming_code(t, m);\n  input[3:0] m;\n  output[6:0] t;\n  wire p1, p2, p3;\n  assign p1 = (m[0] ^ m[1] ^ m[3]); // 1 3 5 7\n  assign p2 = (m[0] ^ m[2] ^ m[3]); // 2 3 6 7\n  assign p3 = (m[1] ^ m[2] ^ m[3]); // 4 5 6 7\n\n  assign t = {p1, p2, m[0], p3, m[1], m[2], m[3]};\n</code></pre>"},{"location":"2_Core/Digital_Design/05_Boolean/","title":"05 Boolean","text":""},{"location":"2_Core/Digital_Design/05_Boolean/#boolean-laws","title":"Boolean Laws","text":"Law Formula Complementation \\(\\bar0 = 1\\)\\(\\bar1 = 0\\)\\(\\bar{\\bar{x}} = x\\) and \\(x \\cdot 1 = x\\)\\(x \\cdot 0 = 0\\)\\(x \\cdot x = x\\)\\(x \\cdot \\bar x = 0\\) or \\(x + 0 = x\\)\\(x + 1 = 1\\)\\(x + x = x\\)\\(x + \\bar x = 1\\) commutative \\(x + y = y + x\\)\\(xy = yx\\) Associative \\(x+(y+z) = (x+y)+z\\)\\(x(yz) = (xy) z\\) Distributive \\(x(y+z) = xy + xz\\)\\(x + yz = (x+y)(x+z)\\) Demorgan's \\(\\overline{x+y} = \\bar x \\cdot \\bar y\\)\\(\\overline{x \\cdot y} = \\bar x + \\bar y\\)"},{"location":"2_Core/Digital_Design/05_Boolean/#duality-principle","title":"Duality Principle","text":"<p>We can obtain the dual of any boolean expression by</p> <ol> <li>operators are interchanged</li> <li>and -&gt; or</li> <li>or -&gt; and</li> <li>identity elements are inverted</li> <li>1 -&gt; 0</li> <li>0 -&gt; 1</li> </ol>"},{"location":"2_Core/Digital_Design/05_Boolean/#boolean-functions","title":"Boolean Functions","text":""},{"location":"2_Core/Digital_Design/05_Boolean/#sop-sigma","title":"SOP (\\(\\Sigma\\))","text":"<p>Sum of Product</p> <p>Represented by NAND gate</p> \\[ \\begin{aligned} f(a,b,c) &amp;= ab + bc \\\\ g(a,b,c) &amp;= a'b + b'c \\end{aligned} \\]"},{"location":"2_Core/Digital_Design/05_Boolean/#pos-pi","title":"POS (\\(\\pi\\))","text":"<p>Product of Sum</p> <p>Represented by NOR gate</p> \\[ \\begin{aligned} f(a,b,c) &amp;= (a+b)(b+c) \\\\ g(a,b,c) &amp;= (a'+b)(b'+c) \\end{aligned} \\]"},{"location":"2_Core/Digital_Design/05_Boolean/#canonical-form","title":"Canonical Form","text":""},{"location":"2_Core/Digital_Design/05_Boolean/#literal","title":"Literal","text":"<p>Each variable within a term of a Boolean expression.</p>"},{"location":"2_Core/Digital_Design/05_Boolean/#minterms","title":"Minterms","text":"<p>SOP</p> \\[ m_0 + m_1 + m_2 + \\dots \\] <p>Minterm (0) is targeted \\(x' = 0, x = 1\\), Minterms are wherever the output is 1</p> <p>Denoted by m<sub>0,1,2</sub></p> <p>They are \\(2^n\\) possible combinations of AND terms, n = no of literals</p> <p>in AND terms, a literal is</p> <ul> <li> <p>primed if its value is 0 (complemented)</p> </li> <li> <p>unprimed if its value is 1</p> </li> </ul> <p>so that the AND of all literals are always 1</p>"},{"location":"2_Core/Digital_Design/05_Boolean/#2-variable-minterm","title":"2 variable minterm","text":"\\[ 2^2 = 4 \\] x y Minterm Notation 0 0 x'y' m<sub>0</sub> 0' 0' = 1 0 1 x'y m<sub>1</sub> 0' 1 = 1 1 0 xy' m<sub>2</sub> 1 0' = 1 1 1 xy m<sub>3</sub> 1 1 = 1"},{"location":"2_Core/Digital_Design/05_Boolean/#3-var-minterm","title":"3 var minterm","text":"\\[ 2^3 = 8 \\] 0 0 0 x'y'z' m<sub>0</sub> 0 0 1 x'y'z m<sub>1</sub> 0 1 0 x'yz' m<sub>2</sub> 0 1 1 x'yz m<sub>3</sub> 1 0 0 xy'z' m<sub>4</sub> 1 0 1 xy'z m<sub>5</sub> 1 1 0 xyz' m<sub>6</sub> 1 1 1 Xyz m<sub>7</sub>"},{"location":"2_Core/Digital_Design/05_Boolean/#maxterms","title":"Maxterms","text":"<p>POS</p> \\[ M_0 \\cdot M_1 \\cdot M_3 \\cdot \\dots \\] <p>Maxterm (1) is targeted \\(x' = 1, x = 0\\), maxterms are wherever the output is 0</p> <p>Denoted by M<sub>0,1,2</sub></p> <p>In maxterms, there are\\(2^n\\) of OR terms</p> <p>In OR terms, literal is</p> <ul> <li>Primed if value is 1</li> <li>unprimed if value is 0</li> </ul> <p>so that all the OR of all literals is 0</p> x y Maxterm Notation 0 0 x + y M<sub>0</sub> 0 + 0 = 0 0 1 x + y' M<sub>1</sub> 0 + 1' = 0 1 0 x' + y M<sub>2</sub> 1' + 0 = 0 1 1 x' + y' M<sub>3</sub> 1' + 1' = 0"},{"location":"2_Core/Digital_Design/05_Boolean/#3-var-minterm_1","title":"3 var minterm","text":""},{"location":"2_Core/Digital_Design/05_Boolean/#statements","title":"Statements","text":"<ul> <li>Any given functions can be expressed in canonical form without using truth table</li> <li>For sum of minterms<ul> <li>insert sum of missing literal and its complement</li> <li>AND operation b/w terms</li> <li>expand</li> </ul> </li> <li>For product of maxterms, insert product of missing literal and its complement with OR operation, and expand</li> </ul> <p>basically,</p> \\[ \\begin{aligned}  &amp; \\ xyz + xy \\\\ =&amp; \\ xyz + xy(z+z') \\end{aligned} \\]"},{"location":"2_Core/Digital_Design/05_Boolean/#k-map","title":"K-Map","text":"<p>Karnaugh map</p> <p>uses grey code, as only 1 bit changes from one place to the next</p> <p>pictorial form of truth table used to simplify Boolean functions</p> <p>made up of squares All the squares represent minterm, or all represent maxterm</p>"},{"location":"2_Core/Digital_Design/05_Boolean/#minterm","title":"Minterm","text":"<p>Result will be SOP</p>"},{"location":"2_Core/Digital_Design/05_Boolean/#maxterm","title":"Maxterm","text":"<p>Result will be POS</p>"},{"location":"2_Core/Digital_Design/05_Boolean/#2-var-kmap","title":"2 Var KMap","text":""},{"location":"2_Core/Digital_Design/05_Boolean/#3-var-kmap","title":"3 Var KMap","text":""},{"location":"2_Core/Digital_Design/05_Boolean/#4-var-kmap","title":"4 Var KMap","text":""},{"location":"2_Core/Digital_Design/05_Boolean/#simplification-of-boolean-using-kmap","title":"Simplification of Boolean using KMap","text":""},{"location":"2_Core/Digital_Design/05_Boolean/#minterm_1","title":"Minterm","text":"<ul> <li>Each square with a 1 is an implicant</li> <li>Combine adjacent implicants to form prime implicant</li> </ul>"},{"location":"2_Core/Digital_Design/05_Boolean/#rules-for-kmap-grouping","title":"Rules for KMap grouping","text":"<ol> <li>Group size can be in terms of \\(2^n\\)</li> <li>Try to always group in the max size</li> <li>In a group, there should be at least one minterm(1)/maxterm(0) which is not a part of any other group    Otherwise, it will be a redundant group</li> </ol>"},{"location":"2_Core/Digital_Design/05_Boolean/#dont-care-condition","title":"Don't care condition","text":"<p>represents undefined function</p> <p>The don't care terms are represented as \\(X\\)</p> <p>Consider the don't care terms as</p> <ul> <li>1 for maxterm</li> <li>0 for minterm</li> </ul> <p>for minterm KMap, you can consider the don't care terms as 1 for maxterm KMap, you can consider the don't care terms as 0</p> <p>Not all don't care terms are necessary to be grouped, but if inclusion leads to larger groups of minterms, then include them also to minimize the function</p> <p>When grouping, make sure that is at least one real minterm/maxterm in every group</p>"},{"location":"2_Core/Digital_Design/05_Boolean/#idk","title":"IDK","text":"<ul> <li>AOI - AND OR inverter - SOP</li> <li>OAI - OR AND inverter - POS</li> </ul>"},{"location":"2_Core/Digital_Design/06_Digital_Circuits/","title":"06 Digital Circuits","text":""},{"location":"2_Core/Digital_Design/06_Digital_Circuits/#digital-circuits","title":"Digital circuits","text":"<ul> <li>combinational circuits</li> <li>sequential circuits</li> </ul> Combinational Sequential output depends on input inputpresent state storage \u274c \u2705 memory \u274c \u2705 Feedback(recursive input) \u274c \u2705"},{"location":"2_Core/Digital_Design/06_Digital_Circuits/#applications-of-combinational-circuits","title":"Applications of combinational circuits","text":"<ol> <li>Arithmetic and logic functions</li> <li>adder</li> <li>subtractor</li> <li>comparator</li> <li>PLD (Programmable Logic Device)</li> <li>Data Transmission</li> <li>multiplexer</li> <li>de-multiplexer</li> <li>encoder</li> <li>decoder</li> <li>code conversion</li> <li>Binary</li> <li>BCD</li> <li>7-Segment</li> </ol>"},{"location":"2_Core/Digital_Design/06_Digital_Circuits/#sequential-logic-circuit","title":"Sequential Logic Circuit","text":"<p>output depends on present inputs and past outputs (feedback)</p> <p>sequential circuit will have storage elements to store the past outputs so that they can be fed back to the input</p> <p>therefore, sequential circuit can be expressed as a combinational circuit with storage and feedback element</p> <pre><code>flowchart LR\nInputs --&gt; c[Combinational Circuit] --&gt; Outputs --&gt; s[Storage Element] --&gt; c</code></pre>"},{"location":"2_Core/Digital_Design/06_Digital_Circuits/#states","title":"States","text":"<ul> <li> <p>Present state</p> </li> <li> <p>Next state</p> </li> </ul>"},{"location":"2_Core/Digital_Design/06_Digital_Circuits/#examples","title":"Examples","text":"<ol> <li>counters</li> <li>shift registers</li> <li>sequence detector</li> <li>function generator</li> </ol>"},{"location":"2_Core/Digital_Design/06_Digital_Circuits/#clock","title":"Clock","text":"<p>is a periodic square pulse</p> <p>has 3 features</p> <ol> <li>rising(+ve) edge \\(( \\to )\\) \\((\\uparrow)\\)    not ideal, but alright for trigger because it is only for a short duration, but requires power</li> <li>level(neutral) edge \u2014    worst for trigger because large power required and duration is for \\(t\\) seconds</li> <li>falling(-ve) edge \\((\\to)\\) with a bubble \\((\\downarrow)\\)    best for trigger because it is only for a short duration and requires least power</li> </ol> <p>always low logic design is the best as it requires the least power</p>"},{"location":"2_Core/Digital_Design/06_Digital_Circuits/#types-of-sequential-circuits","title":"Types of Sequential Circuits","text":"<ol> <li>+ve trigger/sensitive</li> <li>Level trigger/sensitive</li> <li>-ve trigger/sensitive</li> </ol>"},{"location":"2_Core/Digital_Design/06_Digital_Circuits/#storage-elements","title":"Storage elements","text":"Latches FlipFlops Clock \u274c \u2705 Sync Type Asynchronous Asynchronous/Synchronous Trigger Type Level Edge"},{"location":"2_Core/Digital_Design/07_AdderSubtractor/","title":"07 AdderSubtractor","text":""},{"location":"2_Core/Digital_Design/07_AdderSubtractor/#adders","title":"Adders","text":""},{"location":"2_Core/Digital_Design/07_AdderSubtractor/#half-adder","title":"Half-Adder","text":"<p>Combination circuit that performs arithmetic sum of 2 single bit binary</p> <p>Limitation: Adding carry is not possible</p> x y C S 0 0 0 0 0 1 0 1 1 0 0 1 1 1 1 0 <p>\\(S = x \\oplus y, C= xy\\)</p> <pre><code>module half_adder(s, c, a, b);\n    input a, b;\n  output s, c;\n\n  xor(s, a, b);\n  and(c, a, b);\nendmodule\n</code></pre>"},{"location":"2_Core/Digital_Design/07_AdderSubtractor/#full-adder","title":"Full Adder","text":"<p>Arithmetic sum of 3 bit binary</p> x y c S C 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 1 0 1 1 0 0 1 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 <ul> <li>\\(S = x \\oplus y \\oplus c\\)</li> <li>\\(C = xy + c(x \\oplus y)\\)</li> </ul> <pre><code>module fullAdder(sum, carry, a, b, c);\n  input a, b, c;\n  output sum, carry;\n  wire p, q, r;\n\n  xor(sum, a, b, c);\n\n  and(p, a, b)\n  xor(q, a, b);\n  and(r, q, c);\n  or(carry, p, r);\n</code></pre>"},{"location":"2_Core/Digital_Design/07_AdderSubtractor/#full-adder-using-half-adders","title":"Full adder using half adders","text":"<p>2 half adders</p> <p>We need the algebraic expressions as full adder, but using 2 half adders</p> <p>Verilog code in slide 12</p>"},{"location":"2_Core/Digital_Design/07_AdderSubtractor/#4-bit-binary-adder","title":"4 bit binary adder","text":"<p>also called as</p> <ul> <li>4 bit ripple adder</li> <li>4 bit parallel adder</li> </ul> <p>binary adder performs arithmetic sum of two binary nos</p> <p>to add two n bit binary nos, n no of full adders are required</p> C3 C2 C1 C0 \\(C_{in}\\) A3 A2 A1 A0 (+) B3 B2 B1 B0 S3 S2 S1 S0"},{"location":"2_Core/Digital_Design/07_AdderSubtractor/#diagram","title":"Diagram","text":""},{"location":"2_Core/Digital_Design/07_AdderSubtractor/#subtractors","title":"Subtractors","text":""},{"location":"2_Core/Digital_Design/07_AdderSubtractor/#half-subtractor","title":"Half Subtractor","text":"x y B D 0 0 0 0 0 1 1 1 1 0 0 1 1 1 0 0 <ul> <li>\\(D = x'y + xy' = x \\oplus y\\)</li> <li>\\(B = x'y\\)</li> </ul> <pre><code>module halfSub(D, B, x, y);\n    input x, y;\n  output D, B;\n\n    assign D = x ^ y;\n  assign B = ~x + y;\n\nendmodule\n</code></pre>"},{"location":"2_Core/Digital_Design/07_AdderSubtractor/#full-subtractor","title":"Full Subtractor","text":"x y b D B 0 0 0 0 0 0 0 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 1 1 1 1 <ul> <li>\\(D = x \\oplus y \\oplus b\\)</li> <li>\\(B = x'y + b(x \\oplus y)'\\)</li> </ul>"},{"location":"2_Core/Digital_Design/07_AdderSubtractor/#full-subtractor-using-half-subtractor","title":"Full Subtractor using half-subtractor","text":"<p>2 half-subtractors</p>"},{"location":"2_Core/Digital_Design/07_AdderSubtractor/#binary-subtraction","title":"Binary Subtraction","text":"<p>Practically, binary subtraction is performed only in 2\u2019s complement form. Therefore, subtractor circuit is not of much use.</p> <p>\\(A - B = A + \\underbrace{(-B)}_\\text{2's complement}\\)</p> <p>The complement of binary can be obtained using XOR gate</p>"},{"location":"2_Core/Digital_Design/07_AdderSubtractor/#parallel-subtractor","title":"Parallel subtractor","text":"<p>Binary subtractor using 2\u2019s complement</p>"},{"location":"2_Core/Digital_Design/07_AdderSubtractor/#diagram_1","title":"Diagram","text":""},{"location":"2_Core/Digital_Design/07_AdderSubtractor/#other","title":"Other","text":""},{"location":"2_Core/Digital_Design/07_AdderSubtractor/#4-bit-binary-parallel-addersubtractor","title":"4 bit binary parallel adder/subtractor","text":"<p>\\(V = c_2 \\oplus c_3\\)</p> <p>V denotes the overflow</p> <ul> <li>addition<ul> <li>M = C<sub>0</sub> = 0</li> <li>no change to the inputs by the XOR gate</li> <li>Eg: \\(B_0 \\oplus 0 = B_0\\)</li> <li>S = A + B + M = A + B + 0 = A + B</li> <li>V = 0 means no overflow</li> <li>V = 1 means overflow</li> </ul> </li> <li>subtraction<ul> <li>M = C<sub>0</sub> = 1</li> <li>the inputs will get complemented by the XOR gate</li> <li>Eg:\\(B_0 \\oplus 1 = {B_0}'\\)</li> <li>S = A + 1s comp of B + C = A + 1s comp of B + 1 = A + 2s comp of B</li> <li>V = 0 means no overflow</li> <li>V = 1 means overflow</li> </ul> </li> </ul>"},{"location":"2_Core/Digital_Design/07_AdderSubtractor/#bcd-adder","title":"BCD Adder","text":"<ul> <li>valid values are from 0-9</li> <li>10-15 are invalid (and hence will be don\u2019t care condition)</li> </ul> <p>we use 8 4 2 1 coding method</p> <p>max possible sum of 2 BCD digits\\(= \\underbrace{1}_\\text{carry} + 9 + 9 = 19\\)</p> <ul> <li>Sum &lt;= 9 without carry<ul> <li>no correction is needed</li> <li>2+3 = 5</li> </ul> </li> <li>Sum &gt; 9 without carry<ul> <li>then add 6</li> <li>5+7 = 12</li> </ul> </li> <li>Sum &lt;= 9 with carry<ul> <li>then add 6</li> <li>8 + 8</li> </ul> </li> </ul> <p>We need two 4bit parallel adders</p> <p>Whenever the output is undefined, we have to consider that case as don\u2019t care</p> <ul> <li>for eg, for BCD, we take 10-15 places as don\u2019t-care</li> </ul> <p>if z8, z4, z2, z1, and k are the outputs of the first adder, then:</p> Decimal k z8 z4 z2 z1 Corrected Binary Sum BCD Sum 0 0 0 0 0 0 No correction 0000 \u2026 No correction Same as binary 9 0 1 0 0 1 No correction 1001 10 0 1 0 1 0 + 0110 0001 0000 \u2026 + 0110 16 1 0 0 0 0 + 0110 0001 0110 17 1 0 0 0 1 + 0110 0001 0111 18 1 0 0 1 0 + 0110 0001 1000 19 1 0 0 1 1 + 0110 0001 1001 <p>\\(C = z_2 z_8 + z_4 z_8 + k\\)</p>"},{"location":"2_Core/Digital_Design/07_AdderSubtractor/#circuit","title":"Circuit","text":""},{"location":"2_Core/Digital_Design/08_Comparator/","title":"08 Comparator","text":""},{"location":"2_Core/Digital_Design/08_Comparator/#comparatormagnitude-checker","title":"Comparator/Magnitude checker","text":"<p>used to check if a binary number is less than/equal to/greater than another binary no</p>"},{"location":"2_Core/Digital_Design/08_Comparator/#1-bit-comparator","title":"1 bit comparator","text":"x y G E L 0 0 0 1 0 0 1 0 0 1 1 0 1 0 0 1 1 0 1 0 <p>\\(L = x'y, E = x \\odot y, G = xy'\\)</p>"},{"location":"2_Core/Digital_Design/08_Comparator/#2-bit-comparator","title":"2 Bit Comparator","text":"A_1 A_0 B_1 B_0 G E L 0 0 0 0 1 0 0 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 1 0 0 0 1 1 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 <ul> <li>\\(G = A_1 {B_1}' + A_0 A_1 {B_0}' + A_0 {B_0}' {B_1}'\\)</li> <li>\\(E = (A_1 \\odot B_1) (A_2 \\odot B_2)\\)</li> <li>\\(L = {A_1}' B_1  + {A_0}' {A_1}' B_0 + {A_0}' B_0 B_1\\)</li> </ul>"},{"location":"2_Core/Digital_Design/08_Comparator/#3-bit-comparator","title":"3 Bit Comparator","text":"<ul> <li>E<ul> <li>\\(x_2 = A_2 \\odot B_2 \\quad (A_2 = B_2)\\)</li> <li>when \\(A_2 = 0, B_2 = 0, A_2 = 1, B_2 = 1\\)</li> <li>\\(x_1 = A_1 \\odot B_1 \\quad (A_1 = B_1)\\)</li> <li>\\(x_0 = A_0 \\odot B_0 \\quad (A_0 = B_0)\\)</li> <li>\\(E = x_2 \\cdot x_1 \\cdot x_0 = (A_2 \\odot B_2) \\cdot (A_1 \\odot B_1) \\cdot (A_0 \\odot B_0)\\)</li> </ul> </li> <li>L<ul> <li>if \\(A_2 &lt; B_2\\)</li> <li>\\(A_2 = 0, B_2 = 1 \\implies {A_2}' B_2\\)</li> <li>if \\(A_2 = B_2, A_1 &lt; B_1\\)</li> <li>\\(x_2\\) and\\(A_1 = 0, B_1 = 1 \\implies {A_1}' B_1\\)</li> <li>\\(x_2 \\cdot {A_1}' B_1\\)</li> <li>if \\(A_2 = B_2, A_1 = B_1, A_0 &lt; B_0\\)</li> <li>\\(x_2, x_1\\) and\\(A_0 = 0, B_0 = 1 \\implies {A_0}' {B_0}\\)</li> <li>\\(x_2 \\cdot x_1 \\cdot {A_0} B_0\\)</li> <li>\\(\\therefore, L = {A_2}' B_2 + x_2 {A_1}' B_1 + x_2 x_1 {A_0}' B_0\\)</li> </ul> </li> <li>G<ul> <li>\\(G = A_2 {B_2}' + x_2 A_1 {B_1}' + x_2 x_1 A_0 {B_0}'\\)</li> </ul> </li> </ul>"},{"location":"2_Core/Digital_Design/08_Comparator/#4-bit-comparator","title":"4 bit comparator","text":"<p>\\(E = x_3 x_2 x_1 x_0\\)</p>"},{"location":"2_Core/Digital_Design/08_Comparator/#diagram","title":"Diagram","text":""},{"location":"2_Core/Digital_Design/09_DecoderEncoder/","title":"09 DecoderEncoder","text":""},{"location":"2_Core/Digital_Design/09_DecoderEncoder/#decoder","title":"Decoder","text":"<p>converts binary numbers into decimal</p> <ul> <li>from n input lines</li> <li>to \\(2^n\\) unique output lines</li> </ul> <p>Decoder maps the value of the input to the subscript ?? Idk the exact word</p>"},{"location":"2_Core/Digital_Design/09_DecoderEncoder/#applications","title":"Applications","text":"<ol> <li>7 segment displays (parkings, counters, etc)</li> <li>selection in memories</li> <li>de-compressing files</li> </ol>"},{"location":"2_Core/Digital_Design/09_DecoderEncoder/#active","title":"Active","text":"<p>output -not input- gets affected</p> Active High Active Low on output 1 0 off output 0 1 gate AND NAND"},{"location":"2_Core/Digital_Design/09_DecoderEncoder/#enabled","title":"Enabled","text":"<p>basically the switch for the decoder</p> <p>Apart from the regular inputs, there is another input called \u2018enabled\u2019, which takes values high/low</p> <p>Controls whether the circuit is on/off</p> <ul> <li>enabled high - e = 1 enables the decoder</li> <li>enabled low - e = 0 enables the decoder</li> </ul>"},{"location":"2_Core/Digital_Design/09_DecoderEncoder/#2-to-4-line-decoder","title":"2 to 4 line decoder","text":"<p>also called \u20181 of 4 decoder\u2019</p> <p>\\(n = 2; 2^n = 4\\)</p>"},{"location":"2_Core/Digital_Design/09_DecoderEncoder/#active-high","title":"Active High","text":"x y d0 d1 d2 d3 0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 1 0 1 1 0 0 0 1 <p>\\(d_0 = x'y', d_1 = x'y, d_2 = xy', d_3 = xy\\)</p>"},{"location":"2_Core/Digital_Design/09_DecoderEncoder/#active-low","title":"Active low","text":"x y d0 d1 d2 d3 0 0 0 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1 0 <p>(Everything will be complemented)</p> <p>\\(d_0 = (x'y')', d_1 = (x'y)', d_2 = (xy')', d_3 = (xy)'\\)</p> <p>we could also use OR gate? \\(d_0 = x+y, d_1 = x+y', d_2 = x'+y, d_3 = x'+y'\\)</p>"},{"location":"2_Core/Digital_Design/09_DecoderEncoder/#decoder-with-enabled","title":"Decoder with enabled","text":"<p>x means don\u2019t-care</p>"},{"location":"2_Core/Digital_Design/09_DecoderEncoder/#enabled-high-active-high","title":"Enabled High, Active High","text":"e x y d0 d1 d2 d3 0 x x 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0 1 0 0 1 1 0 0 0 1 0 1 1 1 0 0 0 1 <p>\\(d_0 = ex'y', d_1 = ex'y, d_2 = exy', d_3 = exy\\)</p>"},{"location":"2_Core/Digital_Design/09_DecoderEncoder/#enabled-low-active-high","title":"Enabled Low, Active High","text":"e x y d0 d1 d2 d3 1 x x 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 1 <p>\\(d_0 = e'x'y', d_1 = e'x'y, d_2 = e'xy', d_3 = e'xy\\)</p>"},{"location":"2_Core/Digital_Design/09_DecoderEncoder/#3-to-8-line-decoder","title":"3 to 8 line decoder","text":"<p>\\(n = 3, 2^n = 8\\)</p> <p>also called as</p> <ul> <li>\u20181 of 8\u2019 decoder</li> <li>binary to octal decoder (not a mistake - octal is a subspace of decimal)</li> </ul> x y z d0 d1 d2 d3 d4 d5 d6 d7 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 1 <p>\\(d_0 = x'y'z', d_1 = x'y'z, d_2 = x'yz', d_3 = x'yz, d_4 = xy'z', d_5 = xy'z, d_6 = xyz', d_7 = xyz\\)</p>"},{"location":"2_Core/Digital_Design/09_DecoderEncoder/#4-16-decoder-using-3-8-decoder","title":"4-16 decoder using 3-8 decoder","text":"<p>requires</p> <ol> <li> <p>two 3-8 decoders    \\(x,y,z\\) as inputs</p> </li> <li> <p>\\(w\\) as enabled</p> <ul> <li> <p>1 enabled low</p> </li> <li> <p>1 enabled high</p> </li> </ul> </li> </ol> w x y z Output 0 0 0 0 d0 0 0 0 1 d1 0 0 1 0 d2 0 0 1 1 d3 0 1 0 0 d4 0 1 0 1 d5 0 1 1 0 d6 0 1 1 1 d7 1 0 0 0 d8 1 0 0 1 d9 1 0 1 0 d10 1 0 1 1 d11 1 1 0 0 d12 1 1 0 1 d13 1 1 1 0 d14 1 1 1 1 d15"},{"location":"2_Core/Digital_Design/09_DecoderEncoder/#4-16-decoder-using-2-4-decoders","title":"4-16 decoder using 2-4 decoders","text":"\\[ 16/ \\textcolor{orange}4 = 4,  4/ \\textcolor{orange} 1 = 1 \\\\ req = 4 + 1 = 5 \\] <p>we need 5 decoders in total</p> <p>idk exactly</p> <p></p>"},{"location":"2_Core/Digital_Design/09_DecoderEncoder/#diagram","title":"Diagram","text":""},{"location":"2_Core/Digital_Design/09_DecoderEncoder/#combinational-logic-implementation-using-decoder","title":"Combinational Logic implementation using decoder","text":"<p>Example: full adder</p> <p>\\(S = \\sum(1,2,4,7), C = \\sum(3,5,6,7)\\)</p> <ul> <li>3 inputs</li> <li>3-8 decoder</li> <li>2 or gates</li> </ul> <p>refer the gates notes to use different gates</p>"},{"location":"2_Core/Digital_Design/09_DecoderEncoder/#encoder","title":"Encoder","text":"<p>converts decimal numbers into binary</p> <p>is a combination circuit that performs inverse operation of a decoder</p> <p>has \\(2^n\\) inputs and \\(n\\) outputs</p> <p>is used to minimize data (compress)</p> <p>only 1 input will be high</p>"},{"location":"2_Core/Digital_Design/09_DecoderEncoder/#4-2-encoder-active-high","title":"4-2 encoder, active high","text":"D3 D2 D1 D0 E1 E0 0 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 0 1 1 <p>\\(E_1 = D_2+D_3, E_0 = D_1 + D_3\\)</p>"},{"location":"2_Core/Digital_Design/09_DecoderEncoder/#8-3-encoder-active-high","title":"8-3 Encoder, Active High","text":"D7 D6 D5 D4 D3 D2 D1 D0 E2 E1 E0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1 1 1 <ul> <li>\\(E_2 = D_4 + D_5 + D_6 + D_7\\)</li> <li>\\(E_1 = D_3 + D_4 + D_6 + D_7\\)</li> <li>\\(E_0 = D_1 + D_3 + D_5 + D_7\\)</li> </ul>"},{"location":"2_Core/Digital_Design/09_DecoderEncoder/#valid-line","title":"Valid Line","text":"<ul> <li>\\(V=0\\)<ul> <li>output is invalid</li> <li>inputs are inactive</li> <li>the outputs are not inspected and hence the output will be don\u2019t care condition</li> </ul> </li> <li>\\(V=1\\)<ul> <li>output is valid</li> <li>at least 1 input is active</li> </ul> </li> </ul>"},{"location":"2_Core/Digital_Design/09_DecoderEncoder/#priority-encoder","title":"Priority Encoder","text":"<p>encoder that includes priority function</p> <p>helps the encoder give preference to the highest </p>"},{"location":"2_Core/Digital_Design/09_DecoderEncoder/#4-2-encoder","title":"4-2 encoder","text":"<p>order of preference will be\\(\\underbrace{D_3}_\\text{highest} &gt; D_2 &gt; D_1 &gt; \\underbrace{D_0}_\\text{lowest}\\)</p> D3 D2 D1 D0 E1 E0 V 0 0 0 0 X X 0 0 0 0 1 0 0 1 0 0 1 X 0 1 1 0 1 X X 1 0 1 1 X X X 1 1 1 <p>Because of don\u2019t care condition, we have to consider 0s as well for the equations (figure it out on your own)</p> <ul> <li>\\(E_1 = D_2{D_3}' + D_3\\)</li> <li>\\(E_0 = D_1{D_2}'{D_3}' + D_3\\)</li> <li>\\(V = D_0 + D_1 + D_2 + D_3\\)</li> </ul>"},{"location":"2_Core/Digital_Design/09_DecoderEncoder/#8-3-encoder","title":"8-3 Encoder","text":"<p>order of preference will be \\(\\underbrace{D_7}_\\text{highest} &gt; \\ldots &gt; \\underbrace{D_0}_\\text{lowest}\\)</p> D7 D6 D5 D4 D3 D2 D1 D0 E2 E1 E0 V 0 0 0 0 0 0 0 0 X X X 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 X 0 0 1 1 0 0 0 0 0 1 X X 0 1 0 1 0 0 0 0 1 X X X 0 1 1 1 0 0 0 1 X X X X 1 0 0 1 0 0 1 X X X X X 1 0 1 1 0 1 X X X X X X 1 1 0 1 1 X X X X X X X 1 1 1 1"},{"location":"2_Core/Digital_Design/10_MuxDemux/","title":"10 MuxDemux","text":""},{"location":"2_Core/Digital_Design/10_MuxDemux/#multiplexer-mux","title":"Multiplexer (MUX)","text":"<p>also called as data selector</p> <p>Combinational circuit</p> <p>Digital switch</p> <ul> <li>\\(2^n\\) inputs</li> <li>1 output</li> <li>\\(n\\) selection lines</li> </ul>"},{"location":"2_Core/Digital_Design/10_MuxDemux/#applications","title":"Applications","text":"<ol> <li>servers</li> <li>multiple devices are connected to just a single server</li> <li>telecommunication</li> </ol>"},{"location":"2_Core/Digital_Design/10_MuxDemux/#2-1-mux","title":"2-1 Mux","text":"<ul> <li>2 inputs </li> <li>1 output</li> <li>1 selection line</li> </ul> S0 M 0 \\(i_0\\) 1 \\(i_1\\) <p>\\(M = i_0 {s_0}' + i_1 s_0\\)</p>"},{"location":"2_Core/Digital_Design/10_MuxDemux/#4-1-mux","title":"4-1 Mux","text":"<ul> <li>4 inputs</li> <li>1 output</li> <li>2 selection lines </li> </ul> s0 s1 M 0 0 \\(i_0\\) 0 1 \\(i_1\\) 1 0 \\(i_2\\) 1 1 \\(i_3\\) <p>\\(Y = i_0 {s_0}' {s_1}' + i_1 {s_0}' s_1 + i_2 s_0 {s_1}' + i_3 s_0 s_1\\)</p>"},{"location":"2_Core/Digital_Design/10_MuxDemux/#8-1-mux","title":"8-1 Mux","text":"<ul> <li>8 inputs</li> <li>1 output</li> <li>3 selection lines</li> </ul>"},{"location":"2_Core/Digital_Design/10_MuxDemux/#16-1-mux","title":"16-1 Mux","text":"<ul> <li>16 inputs</li> <li>1 output</li> <li>4 selection lines</li> </ul>"},{"location":"2_Core/Digital_Design/10_MuxDemux/#simplifying-mux","title":"Simplifying mux","text":"<ol> <li>draw truth table</li> <li>Choose variable(s) as selection lines</li> <li>other variable(s) as mux i/p</li> <li>write the function in terms of the mux i/p    (easier than drawing kmap)</li> </ol>"},{"location":"2_Core/Digital_Design/10_MuxDemux/#building-mux-using-smaller","title":"Building Mux using smaller","text":"<ol> <li>divide \\(n_1\\) by \\(n_2\\)</li> <li>no of muxes = sum of quotients</li> </ol> <p>positions of s1 and s2 are important</p> <p>MSD will be the selection line for the last mux</p>"},{"location":"2_Core/Digital_Design/10_MuxDemux/#4x1-using-2-1","title":"4x1 using 2-1","text":"\\[ n_\\text{req} = 4 \\\\ n_\\text{available} = 2 \\\\ 4/2 = 2, 2/2 = 1 \\implies \\text{no of muxes} = 2 + 1 = 3 \\]"},{"location":"2_Core/Digital_Design/10_MuxDemux/#8x1-using-2-1","title":"8x1 using 2-1","text":"\\[ n_\\text{req} = 8 \\\\ n_\\text{available} = 2 \\\\ 8/2 = 4, 4/2 = 2, 2/2 = 1 \\implies \\text{no of muxes}= 4 + 2 + 1 = 7 \\]"},{"location":"2_Core/Digital_Design/10_MuxDemux/#8x1-using-4x1-and-2x1","title":"8x1 using 4x1 and 2x1","text":"<p>two 4x1 and one 2x1</p> <p>\\(8/4 = 2; 2/2 = 1\\)</p>"},{"location":"2_Core/Digital_Design/10_MuxDemux/#16x1-using-4x1","title":"16x1 using 4x1","text":"\\[ n_\\text{req} = 16 \\\\ n_\\text{available} = 4 \\\\ 16/4 = 4, 4/4 = 1 \\implies \\text{no of muxes}= 4 + 1 = 5 \\]"},{"location":"2_Core/Digital_Design/10_MuxDemux/#diagram","title":"Diagram","text":""},{"location":"2_Core/Digital_Design/10_MuxDemux/#de-multiplexer-de-mux","title":"De-multiplexer (De-mux)","text":"<p>it is a digital switch with</p> <ol> <li>1 input</li> <li>\\(n\\) selection lines</li> <li>determines which output is connected to the input</li> <li>\\(2^n\\) multiple outputs</li> </ol>"},{"location":"2_Core/Digital_Design/10_MuxDemux/#1-2","title":"1-2","text":"S0 D0 D1 0 i 0 1 0 i <p>\\(D_0 = i {S_0}', D_1 = iS_0\\)</p>"},{"location":"2_Core/Digital_Design/10_MuxDemux/#1-4","title":"1-4","text":"s0 s1 D0 D1 D2 D3 0 0 i 0 0 0 0 1 0 i 0 0 1 0 0 0 i 0 1 1 0 0 0 i <p>\\(D_0 = i {s_0}'{s_1}', D_1 = i{s_0}'s_1, D_2 = i s_0 {s_1}', D_3 = i s_0 s_1\\)</p>"},{"location":"2_Core/Digital_Design/10_MuxDemux/#diagram_1","title":"Diagram","text":""},{"location":"2_Core/Digital_Design/11_Latches/","title":"11 Latches","text":"<p>Latches are usually level triggered</p>"},{"location":"2_Core/Digital_Design/11_Latches/#sr-latch","title":"SR Latch","text":"<p>used to store 0s and 1s applied as 2 inputs called \u2018set\u2019 and \u2018reset\u2019</p> <p>has 2 stable states</p> <ol> <li>SET state - Q = 1, Q\u2019 = 0</li> <li>RESET state - Q = 0, Q\u2019 = 1</li> </ol> <p>can be constructed using</p> <ul> <li>2 cross-coupled NOR gates, or</li> <li>2 cross-coupled NAND gates</li> </ul> <p>NOR-based and NAND-based are reverse of each other</p>"},{"location":"2_Core/Digital_Design/11_Latches/#nor-based","title":"NOR-based","text":"<p>\\(Q_{n+1} = (r + Q'_n)', Q'_{n+1} = (s + Q_n)'\\)</p>"},{"location":"2_Core/Digital_Design/11_Latches/#simplified-truth-table","title":"Simplified Truth Table","text":"s r \\(Q_n\\) \\(Q_{n+1}\\) 0 0 X \\(Q_n\\) 0 1 X 0 (reset) 1 0 X 1 (set) 1 1 X invalid (0, X)"},{"location":"2_Core/Digital_Design/11_Latches/#nand-based","title":"NAND-Based","text":"<p>active-low input latch, because if any of the input is 0, output is 1</p> <p>\\(Q_{n+1} = (s \\cdot Q'n)', Q'_{n+1} = (r \\cdot Q_n)'\\)</p>"},{"location":"2_Core/Digital_Design/11_Latches/#simplified-truth-table_1","title":"Simplified Truth Table","text":"s r \\(Q_n\\) \\(Q_{n+1}\\) 0 0 X invalid (1, X) 0 1 X 1 (set) 1 0 X 0 (reset) 1 1 X \\(Q_n\\) (no change)"},{"location":"2_Core/Digital_Design/11_Latches/#with-enabled","title":"With Enabled","text":"<p>We use NAND SR latch with enabled input</p> <p>The truth table is similar to NOR Latch</p> e s r \\(Q_n\\) \\(Q_{n+1}\\) 0 X X X No change 1 0 0 X \\(Q_n\\) 1 0 1 X 0 (reset) 1 1 0 X 1 (set) 1 1 1 X invalid (0, X) <p>\\(Q_{n+1} = (s' \\cdot Q'n)', Q'_{n+1} = (r' \\cdot Q_n)'\\)</p>"},{"location":"2_Core/Digital_Design/11_Latches/#d-latch","title":"D-Latch","text":"<p>Also called as delay/transparent latch</p> <p>In SR latch, when the inputs are compliment of each other, then the output is either set state or reset state</p> <p>The complimentary input conditions can be achieved by adding an inverter to one of the inputs of SR Latch</p> <p>Now, the SR Latch has a single input called D</p> <p>\\(D \\to S, D' \\to R\\)</p> <p>used as a base for storage device in digital systems</p> e d \\(Q_n\\) \\(Q_{n+1}\\) 0 X X No change 1 0 X 0 1 1 X 1 <p>\\(Q_{n+1} = (d' \\cdot Q'n)', Q'_{n+1} = (d \\cdot Q_n)'\\) cuz \\(s = d \\implies s' = d' \\quad r = d' \\implies r' = d\\)????</p> <p>\\(Q_{n+1} = d\\), right?</p>"},{"location":"2_Core/Digital_Design/11_Latches/#diagram","title":"Diagram","text":""},{"location":"2_Core/Digital_Design/12_FlipFlops/","title":"12 FlipFlops","text":""},{"location":"2_Core/Digital_Design/12_FlipFlops/#flip-flops","title":"Flip Flops","text":"<p>flip fl==o==p; cl==o==ck</p> <p>in latches with enabled, it is observed that inputs are recognized only when enabled is 1. Therefore, it is possible to replace enabled with a momentary pulse called clock so that the inputs can be recognized only for a specified time. Such a device is called a flipflop.</p> <p>(c means clock)</p> <p>FF are usually edge triggered</p> <p>the below truth tables are for positive trigger</p>"},{"location":"2_Core/Digital_Design/12_FlipFlops/#sr-flip-flop","title":"SR Flip Flop","text":"<p>almost identical compared to SR Latch, just that there is a clock </p> c s r \\(Q_n\\) \\(Q_{n+1}\\) 0 X X X No change 1 0 0 X \\(Q_n\\) 1 0 1 X 0 (reset) 1 1 0 X 1 (set) 1 1 1 X invalid (0) s r \\(Q_n\\) \\(Q_{n+1}\\) 0 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0 X 1 1 1 X"},{"location":"2_Core/Digital_Design/12_FlipFlops/#expression","title":"Expression","text":"<p>\\(Q_{n+1} = S + R' Q_n\\) (simplified using KMap)</p>"},{"location":"2_Core/Digital_Design/12_FlipFlops/#d-flip-flop","title":"D-Flip Flop","text":"<p>also called as transparent flip flop</p> c d \\(Q_n\\) \\(Q_{n+1}\\) 0 X X No change 1 0 X 0 1 1 X 1 d \\(Q_n\\) \\(Q_{n+1}\\) 0 1 0 0 0 0 1 0 1 1 1 1"},{"location":"2_Core/Digital_Design/12_FlipFlops/#expression_1","title":"Expression","text":"<p>\\(Q_{n+1} = d\\) (simplified using KMap)</p>"},{"location":"2_Core/Digital_Design/12_FlipFlops/#jk-flip-flop","title":"JK Flip Flop","text":"<p>SR replaced with JK</p> <p>Output of </p> <ul> <li>\\(Q'\\) will be another input of first NAND gate</li> <li>\\(Q\\) will be another input of second NAND gate</li> </ul> c j k \\(Q_n\\) \\(Q_{n+1}\\) 0 X X X No change 1 0 0 X \\(Q_n\\) 1 0 1 X 0 (reset) 1 1 0 X 1 (set) 1 1 1 X \\(\\overline{Q}_n\\) (toggle condition) j k \\(Q_n\\) \\(Q_{n+1}\\) 0 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0 1 1 1 1 0 <p>\\(Q_{n+1} = j Q'_n + k' Q_n\\)</p>"},{"location":"2_Core/Digital_Design/12_FlipFlops/#t-flip-flop","title":"T Flip Flop","text":"<p>Toggle Flip Flop</p> <p>similar to XOR gate</p> <p>We only want 00 and 11</p> <p>Remove J and K, add T</p> c t \\(Q_n\\) \\(Q_{n+1}\\) 0 X X No change 1 0 X \\(Q_n\\) 1 1 X \\(\\overline{Q}_n\\) (toggle condition) t \\(Q_n\\) \\(Q_{n+1}\\) 0 0 0 0 1 1 1 0 1 1 1 0 <p>\\(Q_{n+1} = T \\oplus Qn\\)</p>"},{"location":"2_Core/Digital_Design/12_FlipFlops/#ff-with-preset-and-resetclear","title":"FF with Preset and Reset/clear","text":""},{"location":"2_Core/Digital_Design/12_FlipFlops/#active-high","title":"Active High","text":"P R FF Response 0 0 Normal FF 0 1 Q = 0 1 0 Q = 1 1 1 Not used"},{"location":"2_Core/Digital_Design/12_FlipFlops/#active-low","title":"Active Low","text":"P R FF Response 0 0 Not used 0 1 Q = 1 1 0 Q = 0 1 1 Normal FF"},{"location":"2_Core/Digital_Design/12_FlipFlops/#diagrams","title":"Diagrams","text":""},{"location":"2_Core/Digital_Design/12_FlipFlops/#verilog","title":"Verilog","text":"<pre><code>module srff(q,qbar, s, r, c);\n  input s, r, c;\n  output q, qbar;\n  wire nand1, nand2;\n\n  nand(nand1, s, c);\n  nand(nand2, r, c);\n\n  nand(q, nand1, qbar);\n  nand(qbar, nand2, q);\nendmodule\n\nmodule testbench;\n  reg s, r, c; // reg means storage\n  wire q, qbar;\n\n  initial begin\n    c = 1'b1;\n    repeat(2) #200 c = ~c;\n  end\n\n  initial begin\n    s = 1'b0;\n    repeat(8) #25 s = ~s;\n  end\n\n  initial begin\n    r = 1'b1;\n    repeat(13) #15 r = ~r;\n  end\nendmodule\n</code></pre> <p>Dlatch using</p>"},{"location":"2_Core/Digital_Design/12_FlipFlops/#blocking","title":"Blocking","text":"<pre><code>module dLatch(input d, c, output reg q, qbar);\n  always @ (d, c);\n    if(c) begin\n      #4 q = d;\n      #4 qbar = ~d;\n    end\nendmodule\n</code></pre>"},{"location":"2_Core/Digital_Design/12_FlipFlops/#non-blocking","title":"Non-Blocking","text":""},{"location":"2_Core/Digital_Design/12_FlipFlops/#timing-diagram","title":"Timing Diagram","text":"<p>basically the graph thingy</p> <p>you\u2019ll do it obviously</p> <p></p>"},{"location":"2_Core/Digital_Design/12_FlipFlops/#excitation-table","title":"Excitation Table","text":"<p>one FF var will be reverse of the other helpful for JK and SR</p> <p>helps us to perform flip-flop conversions</p> <p>In regular truth tables, (j,k, Qn) are inputs and Qn+1 is output in excitation table, Qn and Qn+1 are inputs and j and k are outputs</p>"},{"location":"2_Core/Digital_Design/12_FlipFlops/#jk-ff","title":"JK FF","text":"j k \\(Q_n\\) \\(Q_{n+1}\\) 0 0 X \\(Q_n\\) 0 1 X 0 1 0 X 1 1 1 X \\(\\overline{Q}_n\\) \\(Q_n\\) \\(Q_{n+1}\\) j k 0 0 0 X 0 1 1 X 1 0 X 1 1 1 X 0"},{"location":"2_Core/Digital_Design/12_FlipFlops/#t-ff","title":"T FF","text":"<p>similar to XOR gate</p> T \\(Q_n\\) \\(Q_{n+1}\\) 0 0 0 0 1 1 1 0 1 1 1 0 \\(Q_n\\) \\(Q_{n+1}\\) T 0 0 0 0 1 1 1 0 1 1 1 0"},{"location":"2_Core/Digital_Design/12_FlipFlops/#d-ff","title":"D FF","text":"D \\(Q_n\\) \\(Q_{n+1}\\) 0 0 0 0 1 0 1 0 1 1 1 1 \\(Q_n\\) \\(Q_{n+1}\\) D 0 0 0 0 1 1 1 0 0 1 1 1"},{"location":"2_Core/Digital_Design/12_FlipFlops/#sr-ff","title":"SR FF","text":"s r \\(Q_n\\) \\(Q_{n+1}\\) 0 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0 1 0 0 1 1 0 1 1 1 1 0 X (Invalid) 1 1 1 X (Invalid) \\(Q_n\\) \\(Q_{n+1}\\) S R 0 0 0 X 0 1 1 0 1 0 0 1 1 1 X 0"},{"location":"2_Core/Digital_Design/12_FlipFlops/#conversion","title":"Conversion","text":"<ol> <li>Identify source and destination FF</li> <li>Tables</li> <li>Draw Truth table for destination FF</li> <li>extend it (change X into 0 and 1)</li> <li>Draw excitation table of source FF</li> <li>Merge both the tables</li> <li>Draw KMap which provides expression for conversion with</li> <li>inputs as<ol> <li>source FF input vars</li> <li>\\(Q_n\\)</li> </ol> </li> <li>output as dest FF input vars</li> <li>Draw circuit according to KMap</li> </ol>"},{"location":"2_Core/Digital_Design/12_FlipFlops/#sr-using-d","title":"SR using D","text":"<p>\\(S = D, R = D'\\)</p>"},{"location":"2_Core/Digital_Design/12_FlipFlops/#sr-using-jk","title":"SR using JK","text":"<p>\\(S = j Q'_n, R = k Q_n\\)</p>"},{"location":"2_Core/Digital_Design/13_FSM/","title":"13 FSM","text":""},{"location":"2_Core/Digital_Design/13_FSM/#finite-state-machine-fsm","title":"Finite State Machine (FSM)","text":""},{"location":"2_Core/Digital_Design/13_FSM/#state","title":"State","text":"<p>Condition of a sequential circuit based on state variables</p> <p>Flip flop acts as state register, used to store values of states</p>"},{"location":"2_Core/Digital_Design/13_FSM/#types","title":"Types","text":"Moore Model Mealy Model Output is F of states only inputs and states i/p affects o/p? N may affect states required to implement F more fewer Synchronisation with clock Y N"},{"location":"2_Core/Digital_Design/13_FSM/#state-equations","title":"State Equations","text":"<p>set of equations that describe the next state as a function of present state and inputs \\(NS = f(PS, ip)\\)</p>"},{"location":"2_Core/Digital_Design/13_FSM/#variables","title":"Variables","text":"<p>inputs, outputs, state vars are functions of time</p> <ul> <li>inputs/outputs (only small)<ul> <li>NS \\(x(t), y(t)\\)</li> <li>PS \\(x(t+1), y(t+1)\\)</li> </ul> </li> <li>flip-flop outputs (capital or small)<ul> <li>NS \\(A(t), B(t),  a(t), b(t)\\)</li> <li>PS \\(A(t+1), B(t+1), a(t+1), b(t+1)\\)</li> </ul> </li> </ul>"},{"location":"2_Core/Digital_Design/13_FSM/#characteristic-equations","title":"Characteristic Equations","text":"<p>equation for the input and outputs of flipflops</p> Flip Flop Type \\(Q(t+1)\\) D D JK \\(jQ\u2019 + k\u2019Q\\) T \\(T \\oplus Q\\)"},{"location":"2_Core/Digital_Design/13_FSM/#state-table","title":"State Table","text":"<p>is the listing of next states and outputs(not necessary) for all combinations of input and present state</p> <p>total no of combinations = \\(2^{m+n}\\), where</p> <ul> <li>\\(m =\\) no of state vars</li> <li>\\(n =\\) no of input vars</li> </ul> PS Input NS Output"},{"location":"2_Core/Digital_Design/13_FSM/#state-diagram","title":"State Diagram","text":"<p>has circles representing all possible states</p> <p>do this last in the exam, if possible</p> Moore Mealy circle has states, output states arrow has input input, output"},{"location":"2_Core/Digital_Design/13_FSM/#state-reduction","title":"State Reduction","text":"<p>Designing of SC from a given state diagram</p> <p>It is possible to obtain the same input-output relation with another state diagram with fewer states. Reduction has many advantages</p> <ol> <li>fewer flipflops</li> <li>cost is minimized</li> <li>easier maintainance</li> </ol> <p>In design specification, the states will be represented as alphabets. It is necessary to assign binary codes for practical implication</p>"},{"location":"2_Core/Digital_Design/13_FSM/#state-assignment-techniques","title":"State Assignment Techniques","text":"<p>There are different ways of assigning codes into the states</p> <ol> <li>Binary code</li> <li>Grey code</li> <li>One-hot code</li> <li>In \\(n\\) bit code, only one bit is 1 and the remaining are 0s</li> <li>basically decoder output is fed in for assigning</li> </ol> <p>unused states will be X (don\u2019t care condition)</p>"},{"location":"2_Core/Digital_Design/13_FSM/#design","title":"Design","text":"<p>Sequential circuit can be designed using any flip flop</p> <p>the design starts with specification, which includes a word description, inputs and outputs of the circuit</p>"},{"location":"2_Core/Digital_Design/13_FSM/#steps","title":"Steps","text":"<ol> <li>state diagram</li> <li>state table</li> <li>if necessary, reduce the no of states and obtain the new state table</li> <li>assign codes to the states</li> <li>binary coded state table</li> <li>choose type of flip flop</li> <li>simplify flipflop input-output equation using kmap</li> <li>logic diagram</li> </ol> <p>design using </p> <ul> <li>d FF   next state can be obtained directly from the state table from the knowledge of present state and input</li> <li>jk or t FF   excitation table is formed to determine the inputs of FF for the next state output</li> </ul>"},{"location":"2_Core/Digital_Design/13_FSM/#sequence-detector","title":"Sequence Detector","text":"<p>There\u2019s 2 types:</p> <ol> <li>Overlapping</li> <li>non-overlapping</li> </ol> <p>eg: 1001 sequence detector</p>"},{"location":"2_Core/Digital_Design/13_FSM/#moore-non-overlapping","title":"Moore non-overlapping","text":"<pre><code>flowchart LR\n0((s0/0))\n1((s1/0))\n\n2((s2/0))\n3((s3/0))\n4((s4/1))\n\n0 --&gt;|1| 1 --&gt;|0| 2 --&gt;|0| 3 --&gt;|1| 4 --&gt; |1| 1\n\n0 --&gt;|0| 0\n1 --&gt;|1| 1\n2 --&gt;|1| 1\n3 --&gt;|0 &lt;br /&gt; reset - wrong sequence| 0\n4 --&gt; |0 &lt;br /&gt; reset for next 1001| 0</code></pre>"},{"location":"2_Core/Digital_Design/13_FSM/#moore-overlapping","title":"Moore Overlapping","text":"<pre><code>flowchart LR\n0((s0/0))\n1((s1/0))\n\n2((s2/0))\n3((s3/0))\n4((s4/1))\n\n0 --&gt;|1| 1 --&gt;|0| 2 --&gt;|0| 3 --&gt;|1| 4 --&gt;|0| 2\n\n1 --&gt;|1| 1\n2 --&gt;|1| 1\n3 --&gt;|0| 0\n4 --&gt;|1 &lt;br /&gt; restart for next 1001| 1</code></pre>"},{"location":"2_Core/Digital_Design/13_FSM/#mealy-non-overlapping","title":"Mealy non-overlapping","text":"<pre><code>flowchart LR\n0((s0))\n1((s1))\n2((s2))\n3((s3))\n\n0 --&gt;|1/0| 1 --&gt;|0/0| 2 --&gt;|0/0|3 --&gt;|1/1 &lt;br /&gt; Detected output| 0\n\n0 --&gt;|0/0| 0\n1 --&gt;|1/0| 1\n2 --&gt;|1/0| 1\n3 --&gt;|0/0 &lt;br /&gt; Reset - Wrong Sequence| 0</code></pre>"},{"location":"2_Core/Digital_Design/13_FSM/#mealy-overlapping","title":"Mealy Overlapping","text":"<pre><code>flowchart LR\n0((s0))\n1((s1))\n2((s2))\n3((s3))\n\n0 --&gt;|1/0| 1 --&gt;|0/0| 2 --&gt;|0/0|3 --&gt;|1/1 &lt;br /&gt; overlapping| 1\n\n0 --&gt;|0/0| 0\n1 --&gt;|1/0| 1\n2 --&gt;|1/0| 1\n3 --&gt;|0/0| 0</code></pre>"},{"location":"2_Core/Digital_Design/14_Registers/","title":"14 Registers","text":""},{"location":"2_Core/Digital_Design/14_Registers/#registers","title":"Registers","text":"<p>binary storage device consisting of group of flipflops</p> <ul> <li>each flipflop in a register can store 1 bit binary</li> <li>for \\(n\\) bits, we need \\(n\\) flipflops</li> </ul> <p>the frequency of the clocks are what determine the speed</p> <p>this is what we talk about in \u2018over-clocking\u2019</p>"},{"location":"2_Core/Digital_Design/14_Registers/#4-bit-register-using-d-ff","title":"4 bit register using D-FF","text":""},{"location":"2_Core/Digital_Design/14_Registers/#register-w-parallel-load-control","title":"Register w/ \\(\\parallel\\) load control","text":""},{"location":"2_Core/Digital_Design/14_Registers/#load-0","title":"Load = 0","text":"<p>\\(Q\\) is fed back to the D FF</p> <p>@ every clock pulse, output is re-written</p>"},{"location":"2_Core/Digital_Design/14_Registers/#load-1","title":"Load = 1","text":"<p>sets/overwrites the previous value with new inputted value</p>"},{"location":"2_Core/Digital_Design/14_Registers/#shift-register","title":"Shift Register","text":"<p>a register capable of shifting binary information from 1 flip flop to another</p> \\[ \\fbox 1 \\fbox 0 \\fbox 1 \\] <p>Input and output can be serial/parallel</p> <ol> <li>SISO</li> <li>SIPO</li> <li>PISO</li> <li>PIPO</li> </ol>"},{"location":"2_Core/Digital_Design/14_Registers/#shift-right","title":"Shift-Right","text":"clk i/p \\(Q_2\\) \\(Q_1\\) \\(Q_0\\) 0 0 0 0 0 1 0 0 0 0 2 1 1 0 0 3 1 1 1 0 4 - 0 1 1 5 - 0 0 1"},{"location":"2_Core/Digital_Design/14_Registers/#other-types","title":"Other Types","text":""},{"location":"2_Core/Digital_Design/14_Registers/#bidirectional-shift","title":"Bidirectional Shift","text":""},{"location":"2_Core/Digital_Design/14_Registers/#rotate-shift","title":"Rotate Shift","text":""},{"location":"2_Core/Digital_Design/14_Registers/#right","title":"Right","text":""},{"location":"2_Core/Digital_Design/14_Registers/#left","title":"Left","text":""},{"location":"2_Core/Digital_Design/14_Registers/#universal-shift-register","title":"Universal Shift Register","text":"<p>contains</p> <ul> <li>features<ul> <li>clock pulse</li> <li>reset/clear</li> <li>four 4 x 1 mux for modes</li> <li>No change</li> <li>Shift Left</li> <li>Shift Right</li> <li>Parallel load</li> <li>four d flipflops</li> </ul> </li> <li>inputs<ul> <li>serial input for shift-left</li> <li>serial input for shift-right</li> </ul> </li> </ul>"},{"location":"2_Core/Digital_Design/14_Registers/#ringshift-register-counter","title":"Ring/Shift-Register Counter","text":"<ul> <li>circular shift-register</li> <li>mod(n) counter</li> </ul> <p>in addition to regular SISO register </p> <ul> <li>the last FF output will be the first FF input</li> <li>only one FF is set (value = 1) at a time</li> <li>all others are cleared (value = 0)</li> </ul> clk pulse i/p \\(Q_3\\) \\(Q_2\\) \\(Q_1\\) \\(Q_0\\) 0 (initial stage) - 1 0 0 0 1 0 0 1 0 0 2 0 0 0 1 0 3 0 0 0 0 1 4 1 1 0 0 0 <p>if \\(n =\\) no of flipflops</p> <ul> <li>the no of distinguished states = mod(ring counter) = \\(n\\)</li> <li>to achieve a cycle, input is required \\(n\\) times</li> </ul> <pre><code>flowchart LR\n1000 --&gt; 0100 --&gt; 0010 --&gt; 0001 --&gt; 1000</code></pre>"},{"location":"2_Core/Digital_Design/14_Registers/#ring-counter-using-decoder","title":"Ring Counter using decoder","text":"<ol> <li>one 2 bit counter</li> <li>one \\(2 \\times 4\\) decoder</li> <li>four AND gates</li> </ol>"},{"location":"2_Core/Digital_Design/14_Registers/#johnson-counter","title":"Johnson Counter","text":"<p>other names</p> <ul> <li>Twisted Ring counter</li> <li>Switched Ring Tail counter</li> <li>mod(2n) counter</li> </ul> <p>same as ring counter, but</p> <ul> <li>first i/p \\(= Q'_0\\)</li> <li>any no of 0s/1s is possible</li> </ul> <p>if \\(n =\\) no of flipflops,</p>"},{"location":"2_Core/Digital_Design/14_Registers/#-no-of-states-2n","title":"- no of states = \\(2n\\)","text":"clk pulse i/p \\(Q_3\\) \\(Q_2\\) \\(Q_1\\) \\(Q_0\\) 0 - 0 0 0 0 1 1 1 0 0 0 2 1 1 1 0 0 3 1 1 1 1 0 4 1 1 1 1 1 5 0 0 1 1 1 6 0 0 0 1 1 7 0 0 0 0 1 8 0 0 0 0 0 <pre><code>flowchart LR\n0000 --&gt; 1000 --&gt; 1100 --&gt; 1110 --&gt;\n1111 --&gt; 0111 --&gt; 0011 --&gt; 0001 --&gt; 0000</code></pre>"},{"location":"2_Core/Digital_Design/14_Registers/#diagrams","title":"Diagrams","text":""},{"location":"2_Core/Digital_Design/15_Counters/","title":"15 Counters","text":""},{"location":"2_Core/Digital_Design/15_Counters/#counter","title":"Counter","text":"<p>is a register that goes through a prescribed sequence of states, upon the application of clock pulse</p> <p>mod of a counter = no of states</p> <p>Features</p> <ol> <li>active low clock</li> <li>active low reset</li> <li>FF: JK/T</li> </ol>"},{"location":"2_Core/Digital_Design/15_Counters/#binary-counter","title":"Binary Counter","text":"<p>follows a binary counting sequence (normal without any zigzag)</p> bits no of FF counting possibility no of states counter name \\(n\\) \\(n\\) \\(0 \\to 2^{n-1}\\) \\(2^n\\) \\(mod(2^n)\\) 2 2 \\(0 \\to 3\\) \\(4\\) mod 4 3 3 \\(0 \\to 7\\) \\(8\\) mod 8 <p>\\(mod(N)\\) counter that divides input frequency by \\(n\\) is called as \u2018divided-by-\\(n\\)\u2019 counter</p>"},{"location":"2_Core/Digital_Design/15_Counters/#types","title":"Types","text":""},{"location":"2_Core/Digital_Design/15_Counters/#counting-method","title":"Counting method","text":"<ol> <li>up-counter \\((0 \\to n)\\)</li> <li>down-counter \\((n \\to 0)\\)</li> </ol>"},{"location":"2_Core/Digital_Design/15_Counters/#types_1","title":"Types","text":"Asynchoronous Synchoronous clk pulse for flipflops different same inputs on Synonym Ripple steps for design directly multiple non-binary counter possible?zigzag - (0, 5, 6, 3, 7) N Y"},{"location":"2_Core/Digital_Design/15_Counters/#2-bit-ripple-w-neg-trigger","title":"2-bit ripple w/ neg trigger","text":"<p>ripple means asynchoronous</p> <p>storeable values \\(= 0, 1, 2, 3\\)</p> <p>\\(c_1 = Q_0\\)</p>"},{"location":"2_Core/Digital_Design/15_Counters/#upcounter","title":"upcounter","text":"clk \\(A_1\\) \\(A_0\\) 0 (initial cond) 0 0 1 0 1 2 1 0 3 1 1 4 0 0"},{"location":"2_Core/Digital_Design/15_Counters/#down-counter","title":"down counter","text":"clk \\(A_1\\) \\(A_0\\) 0 (initial cond) 0 0 1 1 1 2 1 0 3 0 1 4 0 0 <p>When m = 0, the circuit acts as an upcounter; m = 1 down counter??????????</p> <pre><code>flowchart\n\nsubgraph Up\n    p[00] --&gt; q[01] --&gt; r[10] --&gt; s[11] --&gt; p\nend\n\nsubgraph Down\n    a[00] --&gt;    d[11] --&gt;    c[10] --&gt; b[01] --&gt; a\nend</code></pre>"},{"location":"2_Core/Digital_Design/15_Counters/#idk","title":"IDK","text":""},{"location":"2_Core/Digital_Design/15_Counters/#for-2nd-ff","title":"For 2<sup>nd</sup> FF","text":"counter Trigger clk up Neg \\(Q\\) up Pos \\(Q'\\) down Neg \\(Q'\\) down Pos \\(Q\\) up/down Neg \\(M \\odot Q\\) M = 1 (up)M = 0 (down) up/down Pos \\(M \\oplus Q\\) M = 1 (up)M = 0 (down)(same)"},{"location":"2_Core/Digital_Design/15_Counters/#4-bit-ripple-w-neg-trigger","title":"4-bit ripple w/ neg trigger","text":""},{"location":"2_Core/Digital_Design/15_Counters/#upcounter_1","title":"upcounter","text":"clk \\(A_3\\) \\(A_2\\) \\(A_1\\) \\(A_0\\) 0 (initial cond) 0 0 0 0 1 0 0 0 1 2 0 0 1 0 3 0 0 1 1 \u2026 15 1 1 1 1 16 0 0 0 0"},{"location":"2_Core/Digital_Design/15_Counters/#bcd-ripple-counter","title":"BCD Ripple Counter","text":"<p>also called as decade</p> <p>requires</p> <ul> <li>4 FF</li> <li>NAND gate</li> <li>OR gate</li> </ul> clk \\(A_3\\) \\(A_2\\) \\(A_1\\) \\(A_0\\) 0 (initial cond) 0 0 0 0 1 0 0 0 1 2 0 0 1 0 3 0 0 1 1 \u2026 9 1 0 1 0 10 0 0 0 0 \u2026 X X X X"},{"location":"2_Core/Digital_Design/15_Counters/#4-decade-counter","title":"4 decade counter","text":"<p>also called as 4 digit BCD counter</p> <p>mod 10 counter</p> <p>contains four BCD counters</p>"},{"location":"2_Core/Digital_Design/15_Counters/#custom-mod-counters","title":"Custom mod counters","text":"<p>not all states are used</p> <ol> <li>Find no of FF    the required no of FF is the smallest \\(n\\) that satisfies    \\(N \\le 2^n\\), where<ul> <li>\\(N =\\) no of states</li> <li>\\(n =\\) no of FF</li> </ul> </li> <li>Write the counting sequence of the counter</li> <li>Draw the truth table</li> <li>If necessary, find the minimal expression for reset condition, using KMAP</li> </ol>"},{"location":"2_Core/Digital_Design/15_Counters/#design-of-mod6-asynchoronous-counter","title":"Design of mod6 asynchoronous counter","text":""},{"location":"2_Core/Digital_Design/15_Counters/#cascading-of-ripple-counter","title":"Cascading of Ripple Counter","text":"<p>If we have 2 counters mod M and mod N, then the resulting cascaded ripple counter will be mod(MxN)</p> <p>The MSB of the first(left) counter is connected to the clock of the second (right) counter</p>"},{"location":"2_Core/Digital_Design/15_Counters/#synchoronous-counter","title":"Synchoronous Counter","text":""},{"location":"2_Core/Digital_Design/15_Counters/#design","title":"Design","text":"<ol> <li> <p>identify the no of FF</p> </li> <li> <p>counting sequence and state diagram</p> </li> <li> <p>choice of FF, excitation table</p> </li> <li> <p>minimal expression for excitation (KMAP)</p> </li> <li> <p>kmap is drawn for J,K or T wrt the corresponding Present state variables</p> </li> <li> <p>Logic Diagram</p> </li> </ol>"},{"location":"2_Core/Digital_Design/15_Counters/#3-bit-upcounter","title":"3 Bit upcounter","text":""},{"location":"2_Core/Digital_Design/15_Counters/#3-bit-downcounter","title":"3 Bit downcounter","text":"\\(Q_3(t)\\) \\(Q_2(t)\\) \\(Q_1(t)\\) \\(Q_3(t+1)\\) \\(Q_2(t+1)\\) \\(Q_1(t+1)\\) J3 K3 J2 K2 J1 K1 0 0 0 1 1 1 1 X 1 X 1 X 1 1 1 1 1 0 X 0 X 0 X 1 1 1 0 1 X 0 X 1 1 X 1 0 1 1 X 0 0 X X 1 1 0 0 0 X 1 1 X 1 X 0 1 1 0 0 X X 0 X 1 0 1 0 0 0 X X 1 1 X 0 0 1 0 0 X 0 X X 1 PS\\(Q_3 Q_2 Q_1\\) NS\\(Q_3 Q_2 Q_1\\) \\(J_3\\) \\(K_3\\) \\(J_2\\) \\(K_2\\) \\(J_1\\) \\(K_1\\) 000 111 111 110 110 101 101 100 100 011 011 010 010 001 001 000 \\[ \\begin{aligned} J_1 &amp;= ? &amp; K_1 &amp;= ? \\\\ J_2 &amp;= ? &amp; K_2 &amp;= ? \\\\ J_3 &amp;= Q_2' Q_1' &amp; K_3 &amp;= Q_2' Q_1' \\end{aligned} \\]"},{"location":"2_Core/Digital_Design/15_Counters/#updown","title":"up/down","text":"<pre><code>flowchart LR\n\n000 --&gt; |1|001\n001 --&gt; |0|000</code></pre> <ul> <li>\\(m = 0 \\to\\) downcounter</li> <li>\\(m=1 \\to\\) upcounter</li> </ul> PS\\(Q_3 Q_2 Q_1\\) \\(m\\) NS\\(Q_3 Q_2 Q_1\\) \\(J_3\\) \\(K_3\\) \\(J_2\\) \\(K_2\\) \\(J_1\\) \\(K_1\\) 000 0 111 1 X 1 X 1 X 000 1 001 0 X 0 X 1 X 001 0 000 0 X 0 X X 1 001 1 010 0 X 1 X X 1 010 0 010 0 X X 1 1 X 010 1 011 0 X X 0 1 X 011 0 010 0 X X 0 X 1 011 1 100 1 X X 1 X 1 100 0 011 X 1 1 X 1 X 100 1 101 X 0 0 X 1 X 101 0 100 X 0 0 X X 1 101 1 110 X 0 1 X X 1 110 0 101 X 0 X 1 1 X 110 1 111 X 0 X 0 1 X 111 0 110 X 0 X 0 X 1 111 1 000 X 1 X 1 X 1 <p>4 variable KMAP</p> \\[ \\begin{aligned} J_1 &amp;= 1 &amp; K_1 &amp;= 1 \\\\ J_2 &amp;= Q_1 \\odot M &amp; K_2 &amp;= Q_1 \\odot M \\\\ J_3 &amp;= Q_2 \\odot Q_1 \\odot  m &amp; K_3 &amp;= Q_2 \\odot Q_1 \\odot  m \\\\ \\end{aligned} \\]"},{"location":"2_Core/Digital_Design/15_Counters/#bcd-upcounter","title":"BCD upcounter","text":"PS\\(Q_4 Q_3 Q_2 Q_1\\) NS\\(Q_4 Q_3 Q_2 Q_1\\) \\(J_4\\) \\(K_4\\) \\(J_3\\) \\(K_3\\) \\(J_2\\) \\(K_2\\) \\(J_1\\) \\(K_1\\) 0000 0001 0 X 0 X 0 X 1 X 0001 0010 0 X 0 X 1 X X 1 0010 0011 0 X 0 X X 0 1 X 0011 0100 0 X 1 X X 1 X 1 0100 0101 0 X X 0 0 X 1 X 0101 0110 0 X X 0 1 X X 1 0110 0111 0 X X 0 X 0 1 X 0111 1000 1 X X 1 X 1 X 1 1000 1001 X 0 0 X 0 X 1 X 1001 0000 X 1 0 X 0 X X 1 1010 - 1011 - 1100 - 1101 - 1110 - 1111 - <p>10-15 are unused states</p> \\[ \\begin{aligned} J_1 &amp;= 1 &amp; K_1 &amp;= 1 \\\\ J_2 &amp;= Q_4' Q_1 &amp; K_2 &amp;= Q_1 \\\\ J_3 &amp;= Q_2 Q_1 &amp; K_3 &amp;= Q_2 Q_1 \\\\ J_4 &amp;= Q_3 Q_2 Q_1 &amp; K_4 &amp;= Q_1 \\end{aligned} \\]"},{"location":"2_Core/Digital_Design/15_Counters/#special-conditions","title":"Special Conditions","text":""},{"location":"2_Core/Digital_Design/15_Counters/#lockoutdeadlock","title":"Lockout/Deadlock","text":"<p>Both PS and NS are unused states</p>"},{"location":"2_Core/Digital_Design/15_Counters/#self-start","title":"Self-Start","text":"<p>When a system is switched on and enters an unused state, and then after a few clock pulses, the system enters a used state</p>"},{"location":"2_Core/Digital_Design/15_Counters/#steps","title":"Steps","text":"<ol> <li>Fill up unused state as present state</li> <li>Fill up FF i/p based on the equation obtained from KMAP</li> <li>Fill up the next state (using excitation wrt present state and FF ip)</li> </ol>"},{"location":"2_Core/Digital_Design/15_Counters/#question","title":"Question","text":"<p>Does design of a synchoronous BCD counter using JK FF have lockout condition?</p> <p>unused states = {10, 11, 12, 13, 14, 15}</p> <p>= {1010, 1011, 1100, 1101, 1110, 1111}</p> Unused PS\\(Q_4 Q_3 Q_2 Q_1\\) \\(J_4\\) \\(K_4\\) \\(J_3\\) \\(K_3\\) \\(J_2\\) \\(K_2\\) \\(J_1\\) \\(K_1\\) NS\\(Q_4 Q_3 Q_2 Q_1\\) 10 1010 0 0 0 0 0 0 1 1 1011 11 11 1011 0 1 1 1 1 0 1 1 0100 4 12 1100 0 0 0 0 0 0 1 1 1101 13 13 1101 0 1 0 0 0 1 1 1 0110 6 14 1110 0 0 0 0 0 0 1 1 1111 15 1111 1 1 1 1 1 1 1 1 0000 <pre><code>flowchart LR\n\nsubgraph Used\n    0 --&gt; 1 --&gt; 2 --&gt; 3 --&gt; 4 --&gt; 5 --&gt; 6 --&gt; 7 --&gt; 8 --&gt; 9 --&gt; 0\nend\n\nsubgraph Unused\n    10 --&gt; 11 --&gt; 4\n    12 --&gt; 13 --&gt; 6\n    14 --&gt; 15 --&gt; 0\nend</code></pre> <p>From unused state, the counter goes to used state after a few clock pulses and counts in a normal way</p> <p>Hence, it is self-starting</p>"},{"location":"2_Core/Digital_Design/15_Counters/#mod-6-counter","title":"Mod 6 counter","text":"<pre><code>flowchart LR\n000 --&gt; 001 --&gt; 010 --&gt; 011 --&gt; 100 --&gt; 101 --&gt; 000</code></pre> PS\\(Q_3 Q_2 Q_1\\) NS\\(Q_3 Q_2 Q_1\\) \\(J_3\\) \\(K_3\\) \\(J_2\\) \\(K_2\\) \\(J_1\\) \\(K_1\\) 0 000 001 1 001 010 2 010 011 3 011 100 4 100 101 5 101 000 6 110 X X X X X X X 7 111 X X X X X X X <p>unused states will be represented as don\u2019t care in the KMAP</p> \\[ \\begin{aligned} J_1 &amp;= 1 &amp; K_1 &amp;= 1 \\\\ J_2 &amp;= Q_3' Q_1 &amp; K_2 &amp;= Q_1 \\\\ J_3 &amp;= Q_2 Q_1 &amp; K_3 &amp;= Q_1 \\\\ \\end{aligned} \\] Unused PS\\(Q_3 Q_2 Q_1\\) \\(J_3\\) \\(K_3\\) \\(J_2\\) \\(K_2\\) \\(J_1\\) \\(K_1\\) NS\\(Q_3 Q_2 Q_1\\) 6 110 0 0 0 0 1 1 111 7 7 111 1 1 0 1 1 1 000 0 <p>Self-start</p> <p>Design a type D counter that goes through the states \\(0, 1, 2, 4, 0, \\dots\\) , such that the unused states must always go to 0 on the own next clock pulse</p> <pre><code>flowchart LR\nsubgraph Used\n    0 --&gt; 1 --&gt; 2 --&gt; 4 --&gt; 0\nend\nsubgraph Unused\n    3 &amp; 5 &amp; 6 &amp; 7 --&gt; 0\nend</code></pre> <p>because of the given next states for the unused states, we cannot write it as don\u2019t care for them</p> PS\\(Q_3 Q_2 Q_1\\) NS\\(Q_3 Q_2 Q_1\\) \\(D_3\\) \\(D_2\\) \\(D_1\\) 0 000 001 0 0 1 1 001 010 0 1 0 2 010 100 1 0 0 3 011 000 0 0 0 4 100 000 0 0 0 5 101 000 0 0 0 6 110 000 0 0 0 7 111 000 0 0 0 \\[ D_1 = Q_3' Q_2' Q_1' \\\\ D_2 = Q_3' Q_2' Q_1 \\\\ D_3 = Q_3' Q_2 Q_1' \\] <p>Using a JK counter, count \\(3, 4, 6, 7, 3, \\dots\\)</p> PS\\(Q_3 Q_2 Q_1\\) NS\\(Q_3 Q_2 Q_1\\) \\(D_3\\) \\(D_2\\) \\(D_1\\) 0 000 - X X X 1 001 - X X X 2 010 - X X X 3 011 100 1 0 0 4 100 101 1 0 1 5 101 110 1 1 0 6 110 111 1 1 1 7 111 011 0 1 1 <p>Is Johnson counter lockout or self-start?</p> Unused PS\\(Q_4 Q_3 Q_2 Q_1\\) NS\\(Q_4 Q_3 Q_2 Q_1\\) 2 0010 1001 9 4 0100 1010 10 5 0101 0010 2 6 0110 1011 11 9 1001 0100 4 10 1010 1100 13 11 1011 0101 5 13 1101 0110 6 <p>Lockout condition</p>"},{"location":"2_Core/Digital_Design/15_Counters/#diagrams","title":"Diagrams","text":""},{"location":"2_Core/Digital_Design/16_Memory/","title":"16 Memory","text":""},{"location":"2_Core/Digital_Design/16_Memory/#memory","title":"Memory","text":"<p>storage device</p> <p>stores binary information</p>"},{"location":"2_Core/Digital_Design/16_Memory/#memory-cell","title":"Memory cell","text":"<p>Basic storage device that stores 1 bit</p> <p>works when enabled/select = 0, no read/write operations occur</p> Memory Enable Read/Write Enabled = 1 0 X No operations allowed 1 0 Write to selected word 1 1 Read from selected word"},{"location":"2_Core/Digital_Design/16_Memory/#memory-location","title":"Memory location","text":"<p>group of 8 cells to store a byte</p> <p>we can read/write</p>"},{"location":"2_Core/Digital_Design/16_Memory/#other-stuff","title":"Other stuff","text":"<p>16 bits = 1 word???</p>"},{"location":"2_Core/Digital_Design/16_Memory/#memory-address","title":"Memory Address","text":"<p>unique binary number for every memory location</p> <p>\\(n\\) bit binary address can generate \\(2^n\\) different binary addresses</p> <p>Hence, for \\(n=10\\), we can have 1024 addresses this is called as 1 kilo location</p> <p>Similarly, for \\(n=11\\), we can have \\(2 \\times 1024\\) addresses this is 2 kilo locations</p>"},{"location":"2_Core/Digital_Design/16_Memory/#1-kb-memory-unit","title":"1 KB Memory Unit","text":"\\[ \\underbrace{1 K}_\\text{Memory Storage} \\times \\underbrace{8}_\\text{Word Size/ Data Lines} \\] <ul> <li>Memory Storage = \\(1024 = 2^{10}\\)</li> <li>address lines \\(n = 10\\)</li> <li> <p>Data lines \\(m = 8\\)</p> <ul> <li>8 data input lines</li> <li>8 data output lines</li> </ul> </li> <li> <p>1 enabled line</p> </li> </ul> \\[ \\begin{aligned} \\text{Capactity} &amp;= m \\times 2^n \\text{ bits} \\\\ &amp;= \\frac{m \\times 2^n}{8 \\times 1024} \\text{ KiloBytes} \\end{aligned} \\]"},{"location":"2_Core/Digital_Design/16_Memory/#8k-x-16","title":"8K x 16","text":"<ol> <li> <p>16 data input lines, 16 data output lines</p> </li> <li> <p>13 address lines</p> </li> </ol> \\[ \\begin{aligned} 2^n &amp;= 8 \\times 2^{10} \\\\ &amp;= 2^3 \\times 2^{10} \\\\ &amp;= 2^{13} \\\\ \\implies n &amp;= 13 \\end{aligned} \\] <ol> <li>Capacity = 16KB</li> </ol> \\[ \\begin{aligned} \\text{Capactity} &amp;= m \\times 2^n \\text{ bits} \\\\    &amp;= \\frac{16 \\times 2^{13}}{8 \\times 1024} \\text{ KiloBytes} \\\\    &amp;= 16 \\text{ KB} \\end{aligned} \\]"},{"location":"2_Core/Digital_Design/16_Memory/#operations","title":"Operations","text":"Read Write Address line Address line Enabled line Enabled line Read control Write control Data line Data line"},{"location":"2_Core/Digital_Design/16_Memory/#types-of-semi-conductor-memory","title":"Types of Semi-conductor Memory","text":"<ol> <li>RAM</li> <li>ROM</li> </ol> RAM ROM Full form Random Access Memory Read-Only Memory Read Y Y Write Y N Types 1. Dynamic - Refreshing Logic2. Static 1. OTP - One time Programmable2. EP - Eraseable Programmable+ UV - Ultraviolet+ EE - Electrically Erasable Volatile? Y"},{"location":"2_Core/Digital_Design/16_Memory/#address-decoding","title":"Address Decoding","text":"<p>we need</p> <ul> <li>\\(n\\) and gates</li> <li>\\(n \\to 2^n\\) decoder</li> <li>something or gates</li> </ul>"},{"location":"2_Core/Digital_Design/16_Memory/#4-x-4-ram","title":"4 x 4 RAM","text":"<ul> <li>\\(n = 2\\) address lines</li> <li>\\(m = 4\\) data lines</li> </ul> <p>We need</p> <ul> <li>four and gates</li> <li>one 2x4 decoder</li> <li>four or gates</li> </ul> <p>The four OR gates acts as buffer -  gives high current level and helps in sending information for long distances</p>"},{"location":"2_Core/Digital_Design/16_Memory/#1k-ram","title":"1K RAM","text":"<ul> <li>\\(=2^{10}\\)</li> <li>\\(n=10\\) address lines</li> <li>\\(m=\\)</li> </ul>"},{"location":"2_Core/Digital_Design/16_Memory/#coincident-decoding","title":"Coincident Decoding","text":"<p>instead of using one large decoder, we will use two equal smaller decoders. This allows us to minimize the no of and gates</p> <p>one decoder acts as MSD rows(X), and the other acts as LSD columns (Y)</p> <p>data gets stored at \u2018crossing points\u2019</p> <p>eg: instead of one \\(10 \\times 2^{10}\\), we will use two \\(5 \\times 2^{32}\\)</p> <p>we will only need \\(2 \\times 32 = 64\\) and gates</p>"},{"location":"2_Core/Digital_Design/16_Memory/#address-multiplexing","title":"Address Multiplexing","text":"<p>RAS - Row Address Select, CAS - Column Address Select</p> <p>eg: instead of one \\(10 \\times 2^{10}\\), we will use two \\(5 \\times 2^{32}\\) decoders; we can further simplify this by sending</p>"},{"location":"2_Core/Digital_Design/16_Memory/#-one-5bit-address","title":"- one 5bit address","text":"<p>Instead of sending </p>"},{"location":"2_Core/Digital_Design/16_Memory/#rom","title":"ROM","text":"<ul> <li>\\(n\\) address lines</li> <li>\\(m\\) data output lines (no inputs)</li> </ul> <p>every cross point is considered to be a fuse point</p> <ul> <li>if fuse exists, it is logic 1   it is represented as \\(\\times\\) at crossing points</li> <li>else, it is logic 0</li> </ul>"},{"location":"2_Core/Digital_Design/16_Memory/#32-x-8-rom","title":"32 x 8 ROM","text":"<ul> <li>\\(2^5 \\times 8\\)</li> <li>\\(n = 5\\) address lines</li> <li>\\(m = 5\\) data output lines</li> </ul>"},{"location":"2_Core/Digital_Design/16_Memory/#comb-circuit-using-rom","title":"Comb circuit using ROM","text":"<ol> <li>a circuit inputs a 3 bit binary and outputs a binary equal to the square of input no</li> </ol> A2 A1 A0 B5 B4 B3 B2 B1 B0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0 1 1 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 1 1 1 0 1 0 0 1 0 0 1 1 1 1 1 0 0 0 1 <p>To minimize, we can take \\(B_0 = A_0, B_1 = 0\\)</p> <p>So instead of \\(8 \\times 6\\) ROM, we can minimize to \\(8 \\times 4\\) ROM</p>"},{"location":"2_Core/Digital_Design/16_Memory/#pld","title":"PLD","text":"<p>Programmable logic devices</p> <ol> <li>PROM  (Programmable ROM)</li> <li>PAL (Programmable Array Logic)</li> <li>PLA</li> </ol>"},{"location":"2_Core/Digital_Design/16_Memory/#pla","title":"PLA","text":"<p>Programmable Logic Array</p> <p>diagram is important</p> <p>here, X denotes a 0 or 1 - it just denotes a connection (different from ROM)</p> <p>used to implement a boolean function in SOP form</p> <p>it consists of</p> <ul> <li>\\(n\\) inputs   every input is provided with<ul> <li>buffer</li> <li>inverter</li> </ul> </li> <li>\\(k\\) AND gates   takes care of Product terms</li> <li>\\(m\\) OR gates   takes care of Sum terms</li> <li>\\(m\\) XOR gates   used to generate normal/complement of output; this is like adder/subtractor</li> </ul> <pre><code>flowchart LR\ni[/Input/] --&gt; AND --&gt; OR --&gt; XOR --&gt; o[/Output/]</code></pre>"},{"location":"2_Core/Digital_Design/16_Memory/#pla-with-3-ip-4-product-terms-2-outputs","title":"PLA with 3 i/p, 4 product terms, 2 outputs","text":"<p>Fig 7.14</p> <p>Outputs of AND gate</p> <ol> <li>AB\u2019</li> <li>AC</li> <li>BC</li> <li>\\(A\u2019BC\u2019\\)</li> </ol> <p>Outputs of OR gate</p> <ol> <li>\\(AB\u2019 + AC + A\u2019BC\u2019\\)</li> <li>\\(AC + BC\\)</li> </ol> <p>Outupts of XOR gates</p> <ol> <li>\\(F_1 = AB\u2019 + AC + A\u2019BC\u2019\\)</li> <li>\\(F_2 = (AC + BC)'\\), as other input is 1 and hence, the output gets complemented</li> </ol>"},{"location":"2_Core/Digital_Design/16_Memory/#pla-programming-table","title":"PLA Programming Table","text":"<p>converts a diagram into a table</p> <p>there is no 0 for outputs</p> Product Term Inputs(connected to AND)a b c Outputs(connected to OR)\\(F_1 F_2\\)(T) \u00a9 1 AB\u2019 1 0 - 1 - 2 AC 1 - 1 1 1 3 BC - 1 1 - 1 4 \\(A\u2019BC\u2019\\) 0 1 0 1 -"},{"location":"2_Core/Digital_Design/16_Memory/#convert-the-following-into-pla-diagram","title":"Convert the following into PLA diagram","text":"A B C \\(F_1\\) \\(F_2\\) 0 0 0 1 1 0 0 1 1 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1 0 1 0 1 0 1 1 1 0 0 1 1 1 1 0 1 <ul> <li>\\(F_1(T)\\) is just the normal one</li> <li>\\(F_1(c)\\) means getting the same output as \\(F_1(T)\\) with complemented inputs<ul> <li>so we have to invert the inputs</li> <li>complement the entire thing</li> </ul> </li> </ul> \\[ \\begin{aligned} F_1(T) &amp;= \\sum (0,1, 2, 4) \\\\ &amp;= A'B' + B'C' + A'C' \\\\ F_1(C) &amp;= \\bigg( \\sum (3, 5, 6, 7) \\bigg)' \\\\ &amp;=  \\\\ F_2(T) &amp;= \\sum (0, 5, 6, 7) \\\\ &amp;= \\\\ F_2(c) &amp;= \\bigg( \\sum (1, 2, 3, 4) \\bigg)' \\\\ &amp;= \\end{aligned} \\] <p>We are gonna select \\(F_1(c)\\) and \\(F_2(T)\\), as they have the maximum no of common terms</p> Product Term Inputs outputs\\(F_1 F_2\\)\u00a9 (T) 1 AB 1 1 - 1   1 2 AC 1 - 1 1   1 3 BC - 1 1 1     - 4 A\u2019B\u2019C\u2019 0 0 0 -   1 <p>Draw diagram</p>"},{"location":"2_Core/Discrete_Structures/","title":"Discrete Structures for Computer Science","text":"<p>This course provides a strong foundation in logical and mathematical concepts necessary for understanding computational systems and analyzing complex problems in computer science. Emphasizing formal abstract concepts, the course covers essential mathematical skills that are foundational for advanced topics like algorithm design. Students will study recursion, learning to write recursive definitions for sequences and collections, which is fundamental in various computational contexts.</p> <p>Additionally, the course delves into graph theory, exploring concepts such as directed and planar graphs and their applications in circuit design and map coloring. It also introduces the basics of group theory, providing insights into its relevance in computer science. These topics collectively equip students with the mathematical reasoning needed for more specialized computer science courses and problem-solving in real-world computational challenges.</p>"},{"location":"2_Core/Discrete_Structures/01_Sets_and_Relations/","title":"01 Sets and Relations","text":""},{"location":"2_Core/Discrete_Structures/01_Sets_and_Relations/#set","title":"Set","text":"<p>A set is a collection of elements</p> <ul> <li>\\(A \\cup B\\)</li> <li>\\(A \\cap B\\)</li> <li>\\(A - B = A \\cap B'\\)</li> </ul>"},{"location":"2_Core/Discrete_Structures/01_Sets_and_Relations/#power-set","title":"Power Set","text":"<p>Set of all subsets</p> <p>No of elements in subsets \\(|A| = n( \\ P(A) \\ ) = 2^n\\) </p> <p>Always includes \\(\\phi\\)</p>"},{"location":"2_Core/Discrete_Structures/01_Sets_and_Relations/#functions","title":"Functions","text":"<p>If A has m elements and B has n elements, then the number of \\(f: A \\to B\\) is\\(n^m\\), because each of the m elements can relate to n elements, so no of functions \\(= \\underbrace{ n \\times n \\times \\dots}_{m \\text{ times} } = n^m\\)</p>"},{"location":"2_Core/Discrete_Structures/01_Sets_and_Relations/#types","title":"Types","text":""},{"location":"2_Core/Discrete_Structures/01_Sets_and_Relations/#one-one-injective","title":"One-one (injective)","text":"<p>Every element has exactly one image</p> <p>if \\(f(x)= f(y) \\implies x=y\\)</p> <p>if \\(x \\ne y \\rightarrow f(x) \\ne f(y)\\)</p>"},{"location":"2_Core/Discrete_Structures/01_Sets_and_Relations/#onto-surjective","title":"Onto (surjective)","text":"<p>Range = codomain, ie every element of B should have a pre-image</p> <p>x should be able to be expressed in terms of y let \\(f(x) = y  \\implies x = g(y)\\)</p>"},{"location":"2_Core/Discrete_Structures/01_Sets_and_Relations/#bijective","title":"Bijective","text":"<p>a function that is both one-one and onto</p>"},{"location":"2_Core/Discrete_Structures/01_Sets_and_Relations/#properties","title":"Properties","text":""},{"location":"2_Core/Discrete_Structures/01_Sets_and_Relations/#domain","title":"Domain","text":"<p>The set of values that the input can take, ensuring that the function is defined</p>"},{"location":"2_Core/Discrete_Structures/01_Sets_and_Relations/#codomain","title":"Codomain","text":"<p>The set that the codomain is related to</p>"},{"location":"2_Core/Discrete_Structures/01_Sets_and_Relations/#range","title":"Range","text":"<p>The set of values that the output can take, ensuring that the function is defined</p>"},{"location":"2_Core/Discrete_Structures/01_Sets_and_Relations/#composition","title":"Composition","text":"<p>\\(g \\circ f \\ (x) = g(\\ f(x) \\ )\\)</p> <p>Domain of \\(g \\circ f \\ (x) = \\set{x \\in \\text{domain } f | f(x) \\in \\text{domain } g}\\)</p> <p>\\(g \\circ f \\ (x)\\) not necessarily equal to \\(f \\circ g \\ (x)\\)</p>"},{"location":"2_Core/Discrete_Structures/01_Sets_and_Relations/#relation","title":"Relation","text":"<p>A relation between two sets is a collection of ordered pairs containing one object from each set.</p> <p>Consider a binary relation \\(R \\subseteq A \\times B\\), where \\(A \\times B = \\{ (a,b)/ a \\in A, b \\in B \\}\\). If A and B contain \\(m\\)and \\(n\\) elements respectively, then A x B contains \\(m \\times n\\) elements</p> <p>Consider \\(R \\subseteq A \\times A\\), where \\(A \\times A = \\{(a,b) / a \\in A, b \\in A \\}\\). If A has n elements, then A x A contains \\(n^2\\) elements</p>"},{"location":"2_Core/Discrete_Structures/01_Sets_and_Relations/#complement-of-relation","title":"Complement of relation","text":"<p>if \\(R \\subseteq A \\times A,\\) then its complement is \\(R' = \\set{A \\times A} - R\\)</p>"},{"location":"2_Core/Discrete_Structures/01_Sets_and_Relations/#properties-of-relations","title":"Properties of Relations","text":""},{"location":"2_Core/Discrete_Structures/01_Sets_and_Relations/#reflexivity","title":"Reflexivity","text":"<p>\\((a, a) \\in R, \\forall a \\in R\\)</p> <p>Self loop at all vertices</p>"},{"location":"2_Core/Discrete_Structures/01_Sets_and_Relations/#irreflexivity","title":"Irreflexivity","text":"<p>\\((a,a) \\in R, \\forall a \\in R\\)</p> <p>Self loop at no vertex</p>"},{"location":"2_Core/Discrete_Structures/01_Sets_and_Relations/#symmetry","title":"Symmetry","text":"<p>\\((a,b) \\in R \\implies (b,a) \\in R\\)</p> <p>Requires self loops everywhere; otherwise it is not symmetric</p>"},{"location":"2_Core/Discrete_Structures/01_Sets_and_Relations/#asymmetry","title":"Asymmetry","text":"<p>\\((a, b) \\in R \\implies (b,a) \\notin R\\)</p>"},{"location":"2_Core/Discrete_Structures/01_Sets_and_Relations/#antisymmetric","title":"Antisymmetric","text":"<ul> <li> <p>\\((a, b) \\in R, (b, a) \\in R \\implies a = b\\)</p> </li> <li> <p>\\((a, b) \\in R, a \\ne b \\implies (b,a) \\notin R\\) </p> </li> </ul> <p>No pair of vertices are connected in both directions, and there are self-loops</p>"},{"location":"2_Core/Discrete_Structures/01_Sets_and_Relations/#transitive","title":"Transitive","text":"<p>\\((a, b) \\in R, (b, c) \\in R \\implies (a, c) \\in R\\)</p> <p>Asymmetry \\(\\implies\\) Anti-symmetry, but not vice-versa</p>"},{"location":"2_Core/Discrete_Structures/01_Sets_and_Relations/#notes","title":"Notes","text":"<p>if R is asymmetric, it is irreflexive</p> <p>if R is transitive and irreflexive, it is asymmetric</p>"},{"location":"2_Core/Discrete_Structures/01_Sets_and_Relations/#equivalence-relation","title":"Equivalence Relation","text":"<p>Relation which is</p> <ol> <li>Reflexive</li> <li>Symmetric</li> <li>Transitive</li> </ol> <p>or</p> <ol> <li>reflexive</li> <li>circular</li> </ol>"},{"location":"2_Core/Discrete_Structures/01_Sets_and_Relations/#no-of-unique-equivalence-relations","title":"No of unique equivalence relations","text":"<ul> <li>\\(n = 4 \\to 15\\)</li> <li>\\(n = 5 \\to 52\\)</li> </ul> <p>15 and 52 are called bell numbers</p>"},{"location":"2_Core/Discrete_Structures/01_Sets_and_Relations/#equivalence-class","title":"Equivalence Class","text":"<p>Equivalence relation R divides/partitions A into disjoint union of non-empty subsets called as equivalence classes</p> <p>it is denoted by [any element of the main set] \\([x], x \\in A\\)</p> <p>naming is not unique</p> <p>eg: [0], [1], [x], [y], [January]</p> <p>\\(A= \\set{1, 2, 3}, \\quad R = \\set{(1,1), (1,2), (2,3)}\\) \\([1] = \\set{1, 2}, [2] = \\set{3}\\)</p>"},{"location":"2_Core/Discrete_Structures/01_Sets_and_Relations/#properties_1","title":"Properties","text":"<ul> <li>\\(x \\ R \\ y \\implies [x] = [y],\\) even if \\(x \\ne y\\)</li> <li>\\(x, y \\in A \\implies [x] = [y] \\text{ or } [x] \\cap [y] = \\phi\\)</li> </ul>"},{"location":"2_Core/Discrete_Structures/01_Sets_and_Relations/#converse","title":"Converse","text":"<p>If P is partition of A into non-empty disjoint subsets, then P is the set of equivalence classes for the equivalence relation E defined on A by </p> <p>\\(a \\ R \\ b \\iff\\) a and b belong to the same subset of P</p>"},{"location":"2_Core/Discrete_Structures/01_Sets_and_Relations/#examples-of-equivalence-relation","title":"Examples of Equivalence Relation","text":""},{"location":"2_Core/Discrete_Structures/01_Sets_and_Relations/#congruence-modulo","title":"Congruence modulo","text":"<p>returns the remainder basically <code>x % m</code> in programming (smallest result)</p> <p>\\(x \\equiv y(mod \\ m), \\text{ if } x = y + am, \\quad  a, m \\in \\mathbb{Z} \\\\ \\implies x \\% m = y\\)</p> <p>eg:\\(12 \\equiv 2 (mod \\ 5), -12 = 3(mod \\ 5)\\)</p>"},{"location":"2_Core/Discrete_Structures/01_Sets_and_Relations/#conclusions","title":"Conclusions","text":"<ol> <li>\\(m\\) divides \\(x-y\\)</li> <li>\\(x \\equiv y(mod \\ m) \\implies y \\equiv x (mod \\ m)\\)</li> <li>\\(\\equiv (mod \\ m)\\) divides \\(\\mathbb{Z}\\) into \\(m\\) equivalence classes    \\(\\Z_m = [0], [1], [2], \\dots, [m-1]\\)</li> <li>\\([0]\\) contains the set of all elements that return 0 as remainder, when divided by m</li> <li>\\([m] = [0], [m+1] = [1], \\dots\\)</li> <li>\\([x] + [y] = [x+y], [x][y] = [xy], -[x] = [-x]\\)</li> </ol>"},{"location":"2_Core/Discrete_Structures/01_Sets_and_Relations/#circular","title":"Circular","text":"<p>A relation r on a set A is said to be circular if \\((a,b) \\in R \\text{ and }(b,c) \\in R \\implies (c,a) \\in R\\)</p> <p>R is reflexive &amp; circular \\(\\iff\\) R is an equivalence relation</p>"},{"location":"2_Core/Discrete_Structures/01_Sets_and_Relations/#operations-on-relations","title":"Operations on Relations","text":"<ul> <li> <p>\\(R_1 - R_2 = \\{ (a,b)| (a,b) \\in R_1 \\text{ and } (a,b) \\notin R_2 \\}\\) \\(R_1 - R_2 \\subseteq R_1\\)</p> </li> <li> <p>\\(R_1 \\cup R_2 = \\{ (a,b)| (a,b) \\in R_1 \\text{ or } (a,b) \\in R_2 \\}\\)</p> </li> <li> <p>\\(R_1 \\cap R_2 = \\{ (a,b)| (a,b) \\in R_1 \\text{ and } (a,b) \\in R_2 \\}\\)</p> </li> <li> <p>\\(R^{-1} = \\{ (b,a) | (a,b) \\in R \\}\\)</p> </li> </ul>"},{"location":"2_Core/Discrete_Structures/01_Sets_and_Relations/#notes_1","title":"Notes","text":"Property of \\(R_1\\) and \\(R_2\\) alone Property of \\(R_1 \\cup R_2\\) Property of \\(R_1 \\cup R_2\\) Reflexive and Symmetric same same Transitive not necessary same Equivalence not necessary same Anti-symmetric not necessary same Partial-ordering not necessary same"},{"location":"2_Core/Discrete_Structures/01_Sets_and_Relations/#composition-of-relations","title":"Composition of Relations","text":"<p>Let \\(R \\subseteq A \\times B\\) and \\(S \\subseteq B \\times C\\)</p> <p>Then, the composition of \\(R\\) and \\(S\\) is \\(R \\circ S = \\{(x,z) | (x,y) \\in R \\text{ and } (y,z) \\in S \\}\\)</p> <p>\\(R \\circ S \\ne S \\circ R\\)</p>"},{"location":"2_Core/Discrete_Structures/01_Sets_and_Relations/#composition-of-relation-on-itself","title":"Composition of relation on itself","text":"<ul> <li>\\(R \\circ R\\) can be denoted by \\(R^2\\)</li> <li>\\(R^2 \\circ R\\) can be denoted by \\(R^3\\)</li> <li>\\(R^k \\circ R^l = R^{k+l}, \\quad k,l \\ge 1\\)</li> </ul>"},{"location":"2_Core/Discrete_Structures/01_Sets_and_Relations/#transitive-closure","title":"Transitive Closure","text":"<p>\\(R^+ = R \\cup R^2 \\cup \\dots \\cup R^n\\) </p> <p>\\(R^+\\) is the smallest relation containing R that is transitive</p>"},{"location":"2_Core/Discrete_Structures/01_Sets_and_Relations/#transitive-reflexive-closure","title":"Transitive Reflexive Closure","text":"<p>\\(R^* = R^+ \\cup \\set{ (a,a) | \\textcolor{orange}{\\forall} a \\in A }\\) (add all reflexive elements whether or not they exist in the relation)</p> <p>\\(*\\) is more than + so it is transitive and reflexive</p>"},{"location":"2_Core/Discrete_Structures/01_Sets_and_Relations/#symmetric-closure","title":"Symmetric Closure","text":"<p>\\(R \\cup R^{-1} = \\set{(x,y), (y,x) \\ | \\ (x, y) \\in \\textcolor{orange}{R}, (y,x) \\in \\textcolor{orange}{R^{-1}} }\\)  (only add those symmetric elements that exist in the relation and its inverse)</p>"},{"location":"2_Core/Discrete_Structures/02_Digraph/","title":"02 Digraph","text":""},{"location":"2_Core/Discrete_Structures/02_Digraph/#digraph","title":"Digraph","text":"<p>Directed Graph</p> <p>A digraph G is defined as (V, E), if \\(E \\subseteq V \\times V\\), where V = vertices set of G, E = edge set of G</p> <p>\"edge incident from 1 to 3\"</p> <p>Every element of set is a vertex of digraph; every relation is an edge of digraph</p>"},{"location":"2_Core/Discrete_Structures/02_Digraph/#indegree","title":"Indegree","text":"<p>no of edges incident TO vertex</p>"},{"location":"2_Core/Discrete_Structures/02_Digraph/#outdegree","title":"Outdegree","text":"<p>no of edges incident FROM vertex</p>"},{"location":"2_Core/Discrete_Structures/02_Digraph/#subgraph","title":"Subgraph","text":"<p>\\(G' = (V', E')\\) is a subgraph of \\(G = (V, E)\\) if \\(V' \\subseteq V\\) and \\(E' \\subseteq E \\cap(V' \\times V')\\) // not sure</p>"},{"location":"2_Core/Discrete_Structures/02_Digraph/#graph-isomorphism","title":"Graph Isomorphism","text":"<p>\\(G_1 = (V_1, E_1)\\) and\\(G_2 = (V_2, E_2)\\) are isomorphic if there is one-one and onto function f between them that preserves adjacency</p> <p>\\(f: V_1 \\to V_2\\), where \\(f\\) preserves adjacency \\(E_2 = \\{ (f(v), f(w)) | (v, w) \\in E_1 \\}\\)</p> <p>// not sure // You have to check if they have the same properties (like reflexivity, symmetry, etc...); otherwise the 2 graphs won't maintain adjacency</p>"},{"location":"2_Core/Discrete_Structures/02_Digraph/#adjacency","title":"Adjacency","text":"<p>x and y are adjacent vertices if they are connected to each other</p>"},{"location":"2_Core/Discrete_Structures/02_Digraph/#preservation-of-adjacency","title":"Preservation of adjacency","text":"<p>if x adjacent to y, then f(x) and f(y) should also be adjacent to each other</p>"},{"location":"2_Core/Discrete_Structures/02_Digraph/#features-of-isomorphism-digraphs","title":"Features of isomorphism digraphs","text":"<p>If G1 and G2 are isomorphic</p> <ol> <li>no of vertices equal in G1 and G2</li> <li>no of edges equal in G1 and G2</li> <li>degree spectrum of G1 and G2 are same</li> </ol> <p>However, converse isn't necessarily true - the above 3 features don't necessarily imply isomorphism</p>"},{"location":"2_Core/Discrete_Structures/02_Digraph/#degree-spectrum","title":"Degree Spectrum","text":"<p>... of a graph is set of indegree and outdegree for all vertices of a digraph</p> <p>For every V:(i,j) where i = indegree and j = outdegree of vertex V.</p> <p>Then degree spectrum of the graph \\(= \\{ (i,j) | i=\\text{indegree}, j = \\text{outdegree} \\}\\)</p>"},{"location":"2_Core/Discrete_Structures/03_Ordering_Relations/","title":"03 Ordering Relations","text":""},{"location":"2_Core/Discrete_Structures/03_Ordering_Relations/#ordering-relations-lattices","title":"Ordering Relations &amp; Lattices","text":""},{"location":"2_Core/Discrete_Structures/03_Ordering_Relations/#partially-ordered-setsposets","title":"Partially-ordered sets(POSETS)","text":"<p>A relation R defined on a set S is said to be partially ordered if it is</p> <ol> <li>reflexive</li> <li>Anti-symmetric (uni-directional)</li> <li>Transitive</li> </ol> <p>Then (S, R) is a POSET.</p> <p>Eg: \\((\\mathbb{Z}, \\le), (\\mathbb{Z}, \\ge), (\\mathbb{Z}^+ , /), (P(S) , \\subseteq)\\)</p>"},{"location":"2_Core/Discrete_Structures/03_Ordering_Relations/#notes","title":"Notes","text":"<ol> <li> <p>Any partial ordering relation is denoted by \\(\\preceq\\)</p> </li> <li> <p>if \\(a \\prec b\\) denotes that \\(a \\preceq b\\) but \\(a \\ne b\\)</p> </li> <li> <p>if R is a partial order relation on S, the R<sup>-1</sup> is also a partial order relation on S, where \\(R^{-1} = \\{ (b, a) | (a,b) \\in R \\}\\)    (S, R<sup>-1</sup>) is called the dual of (S, R)</p> </li> <li> <p>Let\\(a, b \\in S\\) where \\((S, \\preceq)\\) is a POSET.    a and b are said to be comparable, if either \\(a \\preceq b\\) or \\(b \\preceq a\\)    otherwise a and b are not comparable</p> </li> </ol> <p>Eg: \\((\\mathbb{Z}^+, /)\\)    (2, 10) , (16, 8) is comparable; but (2, 7) isn't comparable as neither 2 nor 7 can divide each other</p>"},{"location":"2_Core/Discrete_Structures/03_Ordering_Relations/#totally-ordered-set","title":"Totally ordered Set","text":"<p>\\(\\preceq\\) is a totally-ordered relation if every 2 elements of S is comparable, and S is a totally-ordered set</p> <p>Eg: \\((Z, \\le), (D_8,/)\\) (divisors of 8)</p>"},{"location":"2_Core/Discrete_Structures/03_Ordering_Relations/#not-totally-ordered","title":"Not totally ordered","text":"<p>eg: \\((Z^+, /), (D_{12}, /)\\) (divisors of 12)</p> <p>D<sub>n</sub>: set of all positive divisors of positive integer n it is always a POSET, but not necessarily a TOSET</p>"},{"location":"2_Core/Discrete_Structures/03_Ordering_Relations/#well-ordered-set","title":"Well Ordered Set","text":"<p>A relation that is</p> <ol> <li>totally ordered</li> <li>every subset of S has least element in the Hasse diagram    doesn't have to be the least mathematically, such as the case of \\((Z^-, \\ge)\\)</li> </ol> <p>WOSET \\(\\implies\\) TOSET not all TOSETs are WOSETs, but all finite TOSETs are</p> <p>Eg: \\((N, \\le), (Z^+, \\le), (Z^-, \\ge)\\) \\((Z^-, \\le)\\) is TOSET but not WOSET, as there is no subset (\\(- \\infin\\) is the least element)</p>"},{"location":"2_Core/Discrete_Structures/03_Ordering_Relations/#posethasse-diagram","title":"POSET/HASSE Diagram","text":"<ol> <li>draw from lower to upper direction</li> <li>no loops</li> <li>eliminate edges that are implied by transitiveness</li> <li>no arrows</li> </ol>"},{"location":"2_Core/Discrete_Structures/03_Ordering_Relations/#elements","title":"Elements","text":""},{"location":"2_Core/Discrete_Structures/03_Ordering_Relations/#minimal-elements","title":"Minimal elements","text":"<p>indegree = 0 (excluding self)</p> <p>a is a minimal element in S if there is no \\(b \\in S\\) such that \\(b \\preceq a\\)</p>"},{"location":"2_Core/Discrete_Structures/03_Ordering_Relations/#maximal-elements","title":"Maximal elements","text":"<p>outdegree = 0 (excluding self)</p> <p>a is a maximal element in S if there is no \\(b \\in S\\) such that \\(a \\preceq b\\)</p>"},{"location":"2_Core/Discrete_Structures/03_Ordering_Relations/#least-element","title":"Least element","text":"<p>Element a is called least element of S, if \\(a \\preceq b, \\forall b \\in S\\)</p> <p>The lowermost element of Hasse diagram</p> <p>It has to be unique - ie the only minimal element</p>"},{"location":"2_Core/Discrete_Structures/03_Ordering_Relations/#greatest-element","title":"Greatest Element","text":"<p>Element a is called greatest element of S if \\(b \\preceq a, \\forall b\\in S\\) </p> <p>The uppermost element of Hasse diagram</p> <p>It has to be unique - ie the only maximal element</p>"},{"location":"2_Core/Discrete_Structures/03_Ordering_Relations/#summary","title":"Summary","text":"<p>Minimal and maximal points are end points that are related to all elements of the question set B, while least and greatest point are unique end point related to all elements of the question set B</p>"},{"location":"2_Core/Discrete_Structures/03_Ordering_Relations/#bounds","title":"Bounds","text":"<p>Let A be a subset of S</p> <p>Bound is a set of the above elements</p>"},{"location":"2_Core/Discrete_Structures/03_Ordering_Relations/#upper-bound","title":"Upper bound","text":"<p>If u is an element of S such that \\(a \\preceq u, \\forall a \\in A\\), then u is called upper bound of A</p> <p>should be related to both a and b</p>"},{"location":"2_Core/Discrete_Structures/03_Ordering_Relations/#lubsupremum","title":"LUB/Supremum","text":"<p>Least upper bound</p> <p>L==U==B/S==u==premum</p>"},{"location":"2_Core/Discrete_Structures/03_Ordering_Relations/#lower-bound","title":"Lower bound","text":"<p>If \\(l\\) is an element of S such that \\(l \\preceq a, \\forall a \\in A\\), then \\(l\\) is called lower bound of A</p> <p>should be related to both a and b</p>"},{"location":"2_Core/Discrete_Structures/03_Ordering_Relations/#glbinfimum","title":"GLB/Infimum","text":"<p>Greatest lower bound</p>"},{"location":"2_Core/Discrete_Structures/03_Ordering_Relations/#lattice","title":"Lattice","text":"<p>If the HASSE diagram starts and ends with a single point, it's called as a lattice.</p> <p>A lattice is a POSET \\((S, \\preceq)\\) in which each pair of elements has</p> <ol> <li>LUB    LUB of a and b: \\(a \\lor b\\) (join of {a, b}) -&gt; sup(a, b)</li> <li>GLB    GUB of a and b: \\(a \\land b\\) (meet of {a,b}) -&gt; inf(a,b)</li> </ol> <p>eg: \\((D_6, /)\\)</p>"},{"location":"2_Core/Discrete_Structures/03_Ordering_Relations/#examples","title":"Examples","text":"\\[ \\left(P(S), \\subseteq\\right)\\\\ A, B \\in P(S): A \\lor B = A \\cup B, A \\land B = A \\cap B \\] \\[ \\left(P(S), \\supseteq \\right)\\\\ A, B \\in P(S): A \\lor B = A \\cap B, A \\land B = A \\cup B \\] \\[ \\left(P(S), \\le\\right)\\\\ A, B \\in P(S): A \\lor B = \\text{max}(A,B), A \\land B = \\text{min}(A,B) \\] \\[ \\left(D_n, / \\right)\\\\ A, B \\in D_n: A \\lor B = \\text{lcf}(A,B), A \\land B = \\text{hcf}(A,B) \\]"},{"location":"2_Core/Discrete_Structures/03_Ordering_Relations/#semi-lattice","title":"Semi-Lattice","text":""},{"location":"2_Core/Discrete_Structures/03_Ordering_Relations/#join-semi-lattice","title":"Join Semi-Lattice","text":"<p>Lattice with only LUB</p> <p>multiple starting points</p> <p>Eg: \\(( \\{2, 3, 60,180\\},  /)\\)</p>"},{"location":"2_Core/Discrete_Structures/03_Ordering_Relations/#meet-semi-lattice","title":"Meet Semi-Lattice","text":"<p>Lattice with only GLB</p> <p>multiple ending points</p> <p>Eg: \\((I_{12}, /)\\)</p>"},{"location":"2_Core/Discrete_Structures/03_Ordering_Relations/#properties-of-lattices","title":"Properties of Lattices","text":"<p>Let \\((L, \\lor, \\land)\\) be an algebraic system defined by lattice \\((L, \\preceq)\\)</p> <ol> <li>Idempotency</li> <li>\\(a \\land a = a\\)</li> <li>\\(a \\lor a = a\\)</li> <li>Commutative</li> <li>\\(a \\land b = b \\and a\\)</li> <li>\\(a \\lor b = b \\lor a\\)</li> <li>Associative</li> <li>\\((a \\land b) \\land c = a \\land (b \\land c)\\)</li> <li>\\((a \\lor b) \\lor c = a \\lor (b \\lor c)\\)</li> <li>Absorption    (opposite operation)</li> <li>\\(a \\land (a \\lor b) = a\\)</li> <li>\\(a \\lor (a \\land b) = a\\)</li> <li>Distributive (not all lattices are distributable)</li> <li>\\(a \\land (b \\lor c) = (a \\land b) \\lor (a \\land c)\\)</li> <li>\\(a \\lor (b \\land c) = (a \\lor b) \\land (a \\lor c)\\)</li> <li>Consistency    \\(a \\land b = a \\text{ and } a \\lor b = b\\)</li> </ol>"},{"location":"2_Core/Discrete_Structures/04_Graphs/","title":"04 Graphs","text":""},{"location":"2_Core/Discrete_Structures/04_Graphs/#graphs","title":"Graphs","text":"<p>Here, we are talking about undirected graphs</p> <p>All undirected graphs are symmetric</p> <p>Adjacency matrix of non-directed graph is a symmetric matrix \\((A = A')\\)</p>"},{"location":"2_Core/Discrete_Structures/04_Graphs/#basic-concepts","title":"Basic Concepts","text":"<p>A graph is represented as \\(G = (V, E)\\) where</p> <ul> <li>V = set of vertices</li> <li>E = set of edges; the edges are undirected</li> </ul> <p>Loops are allowed; graph with no loops is called as simple/loop-free graph</p> <p>maximum degree of a vertex in a simple graph \\(= |V| - 1\\)</p> <p>\\(|V(g)| = |V| =\\) order of G = no of vertices in the graph G</p> <p>\\(|E(g)| = |E| =\\) size of G = no of edges in the graph G</p> <p>No of edges \\(= \\frac{\\sum deg(v_i)}{2}\\)</p> <p>n vertices can only have \\(n-1\\) adjacent vertices</p> <p>Graphic sequence = deg sequence from which valid graph is possible Non-graphic sequence = deg sequence from which graph is not possible</p> <p>no of labelled graphs on a given set of n vertices \\(= 2^{nC_2}\\) out of them, \\((nC_2)C_m\\) contain m edges</p> <p>when n = 3, 4 non-isomorphic graphs are possible when n = 4, 11 non-isomorphic graphs are possible</p>"},{"location":"2_Core/Discrete_Structures/04_Graphs/#multigraph","title":"Multigraph","text":"<p>is a graph with more than one edge b/w a pair of vertices</p>"},{"location":"2_Core/Discrete_Structures/04_Graphs/#degree-sequence","title":"Degree Sequence","text":"<p>is the set of degrees of the vertices</p> <p>Loop is taken as an increment of two (as it starts and ends at the same place)</p> <p>it is written in ascending order: from lowest degree to highest degree \\(\\delta(G) \\to \\Delta (G)\\)</p>"},{"location":"2_Core/Discrete_Structures/04_Graphs/#regular-graph","title":"Regular Graph","text":"<p>loop-free graphs where every vertex has the same degree</p> <p>\\(\\delta(G) = \\Delta(G)\\)</p> <p>The name of the graph \\(= (|V| - 1)\\) Regular graph Eg: for 5 vertices graph, if all the 5 vertices are connected to the others, then the name will be \"4 - regular graph\"</p> <p>\\(|E| = \\frac{n \\times d}{2}, d =\\) the degree of every vertex</p>"},{"location":"2_Core/Discrete_Structures/04_Graphs/#theorems","title":"Theorems","text":""},{"location":"2_Core/Discrete_Structures/04_Graphs/#non-directed-graph","title":"Non-directed graph","text":"<p>If \\(V = {v_1, v_2, v_3, \\dots, v_n}\\) is the vertex set of a non-directed graph, then \\(\\sum\\limits_{i=1}^n deg(v_i) = 2 |E|\\)</p> <p>Each element contributes a count of one to the degree of each of the two vertices on which it is incident</p>"},{"location":"2_Core/Discrete_Structures/04_Graphs/#directed-graph","title":"Directed Graph","text":"<p>\\(\\sum\\limits_{i=1}^n deg^+(v_i) = \\sum\\limits_{i=1}^n deg^-(v_i) = |E|\\)</p>"},{"location":"2_Core/Discrete_Structures/04_Graphs/#cor1","title":"Cor(1)","text":"<p>In any non-directional graph, there is an even number of vertices of odd degree </p> <p>If W: set of vertices of G with odd degree, U: set of vertices of G with even degree</p> \\[ \\sum\\limits_i deg(V_i) = 2|E| \\\\ \\sum\\limits_{i \\in W} deg(V_i) + \\underbrace{ \\sum\\limits_{i \\in U} deg(V_i)}_\\text{even} = \\underbrace{2|E|}_\\text{even} \\\\ \\implies \\sum\\limits_{i \\in W} deg(V_i) \\text{ is also even} \\] <p>But W contains all vertices with odd degree. \\(\\therefore,\\) the no of vertices in W should be even. Hence, |W| is also even.</p>"},{"location":"2_Core/Discrete_Structures/04_Graphs/#cor2","title":"Cor(2)","text":"<p>If \\(k = \\delta (G)\\) is the minimum degree of all vertices of G, then \\(k|V| \\le \\sum\\limits_{i=1}^n deg(v_i)\\)</p> <p>In particular, if G is a k-regular graph (where the degree of all the vertices is k), then \\(k|V| = \\sum\\limits_{i=1}^n deg(v_i) = 2|E|\\)</p>"},{"location":"2_Core/Discrete_Structures/04_Graphs/#path","title":"Path","text":"<p>In a graph G, a sequence P of zero/more edges of the form \\(\\set{v_0, v_1}, \\set{v_1, v_2}, \\dots, \\set{v_{n-1}, v_n}\\)</p>"},{"location":"2_Core/Discrete_Structures/04_Graphs/#graphical-representation","title":"Graphical Representation","text":"<pre><code>graph LR\nv0 --- v1 --- v2 --- ... --- Vn-1 --- Vn</code></pre> <p>is called a path from \\(v_0\\) to \\(v_n\\)</p>"},{"location":"2_Core/Discrete_Structures/04_Graphs/#length","title":"Length","text":"<p>the number of edges in path p</p>"},{"location":"2_Core/Discrete_Structures/04_Graphs/#notes","title":"Notes","text":"<p>In a path, vertices and edges</p> <ol> <li>may be repeated</li> <li>If \\(v_0 = v_n\\), then path p is closed    \\(v_0 = v_n\\), then path p is open</li> <li>a path p is itself a graph, ie, subgraph of G</li> <li>\\(V(P) \\subseteq V(G)\\)</li> <li>\\(E(P) \\subseteq E(G)\\)</li> <li>Path may have no edges at all</li> <li>length = 0 \\((V(P) = \\set{v_0})\\)</li> <li>trivial path (simple, closed path)</li> </ol>"},{"location":"2_Core/Discrete_Structures/04_Graphs/#simple-path","title":"Simple path","text":"<p>Path with all distinct edges and vertices end points(vertices) of a closed path are exempted from this condition</p>"},{"location":"2_Core/Discrete_Structures/04_Graphs/#circuit","title":"Circuit","text":"<ol> <li>closed path</li> <li>length \\(\\ge 1\\)</li> <li>no repeated edges</li> <li>end points are equal (is repeated?)</li> <li>it may have repeated vertices</li> </ol>"},{"location":"2_Core/Discrete_Structures/04_Graphs/#cycle","title":"Cycle","text":"<p>simple circuit no repeated vertices (except start and end points)</p>"},{"location":"2_Core/Discrete_Structures/04_Graphs/#wheel","title":"Wheel","text":"<p>Cycle with 1 vertex connected to all other vertices the vertex doesn\u2019t necessarily have to be inside the cycle</p>"},{"location":"2_Core/Discrete_Structures/04_Graphs/#complete-graph-k_n","title":"Complete Graph \\(k_n\\)","text":"<p>every vertex is connected with every other vertex</p> <p>if \\(|V| = n\\), deg of every vertex \\(= n-1\\)</p> <pre><code>graph LR\n\nsubgraph k1\n    a(( ))\nend\n\nsubgraph k2\n    b(( )) --- c(( ))\nend\n\nsubgraph k3\n    d(( )) --- e(( )) --- f(( )) --- d\nend\n\nsubgraph k4\n    g(( )) --- h(( )) --- i(( )) --- j(( )) --- g\n    h --- j\n    i --- g\nend</code></pre>"},{"location":"2_Core/Discrete_Structures/04_Graphs/#linear-graphs-l_n","title":"Linear graphs \\(L_n\\)","text":"<p>Open graph \\(|V| = n\\)</p> <pre><code>graph LR\n\nsubgraph L2\n    a(( )) --- b(( ))\nend\n\nsubgraph L5\n    c(( )) --- d(( )) --- e(( )) --- f(( )) --- g(( ))\nend</code></pre> Closed Open Circuit Cycle Wheel Regular graph Complete Graph \\vert V \\vert n n n n n n deg of vertex d n-1 \\vert E \\vert n n-1 n n \\(\\frac{n \\times d}{2}\\) \\(\\frac{n(n-1)}{2} = nC_2\\)"},{"location":"2_Core/Discrete_Structures/04_Graphs/#theorem","title":"Theorem","text":"<p>In a graph G, every u-v path contains a simple u-v path</p>"},{"location":"2_Core/Discrete_Structures/04_Graphs/#proof","title":"Proof","text":"<p>Mathematical induction</p> <p>Taking a u-v path. It can either be</p> <ul> <li> <p>closed</p> <p>obviously contains a trivial path (of length 0)</p> <p>simple path</p> </li> <li> <p>open</p> <p>Consider an open u-v path</p> <p>To show it contains a simple u-v path</p> <pre><code>graph LR\nu((u / v0)) --- v1((v1)) --- v2((...)) --- v((v / vn))</code></pre> <p>Proof by induction on the length of path p</p> <ul> <li> <p>Length = 1 (basic)</p> </li> <li> <p>then path p is open as it contains only one edge</p> </li> <li> <p>length = k, where \\(1 \\le k \\le n\\) (induction hypothesis)</p> </li> <li> <p>assume that when the length is k, then u-v path contains a simple u-v</p> </li> <li> <p>length = n+1 (induction proof)</p> </li> <li> <p>trying to prove that it contains a simple path (using the induction hypothesis)</p> <p><code>mermaid graph LR u((u / v0)) --- v1((v1)) --- v2((...)) --- vn((vn)) --- v((v / vn+1))</code></p> <p>this path</p> <ul> <li>has no repeated vertices \\(\\implies\\) it is simple</li> </ul> </li> <li> <p>contains repeated vertices</p> <ul> <li>Let \\(v_i = v_j\\) be the vertices for \\(i &lt; j\\) \\(v_0 - v_1 - \\ldots - v_i - v_{i+1} - \\ldots - v_j - v_{j+1} - \\ldots - v_{n+1}\\)</li> <li>remove \\(v_{i+1} - \\ldots - v_j\\) from P</li> </ul> </li> <li> <p>now, \\(v_{0} - v_1 - \\ldots - v_i - v_{j+1} - \\ldots - v_{n+1}\\) is a simple path</p> </li> </ul> </li> </ul> <p>Hence, proved</p>"},{"location":"2_Core/Discrete_Structures/04_Graphs/#isomorphism","title":"Isomorphism","text":"<p>denoted by \\(\\cong\\)</p> <p>2 graphs G1 and G2 are isomorphic if there is a function \\(f: V(G_1) \\to V(G_2)\\) such that</p> <ol> <li>f is one-one</li> <li>f is onto</li> <li>f preserves adjacency of vertices</li> <li>\\(\\forall (u,v) \\in E(G_1) \\implies (f(u), f(v)) \\in E(G_2)\\)</li> </ol> <p>f need not be unique; there can be various mappings that preserve adjacency</p>"},{"location":"2_Core/Discrete_Structures/04_Graphs/#implications-of-isomorphism","title":"Implications of isomorphism","text":"<ol> <li>\\(| V(G_1) | = | V(G_2) |\\)</li> <li>\\(| E(G_1) | = | E(G_2) |\\)</li> <li>deg seq(G1) = deg seq(G2)</li> <li>Loops: \\((v,v) \\in E(G_1) \\implies (\\ f(v), f(v) \\ ) \\in E(G_2)\\)</li> <li>if there is a cycle of length n in G1, ie \\(v_0 - v_1 - \\ldots - v_k (=v_0)\\)    then \\(f(v_0) - f(v_1) - \\ldots - f(v_k) (=f(v_0))\\) is also a cycle of length n in G2</li> <li>Cycle vector \\(\\set{c_1, c_2, \\dots, c_k}\\) of G = cycle vector \\(\\set{d_1, d_2, \\dots, d_k}\\) where</li> <li>cn = cycle of length n</li> <li>dn = cycle of length n</li> <li>the induced subgraphs (by a set W) of isomorphic graphs are also isomorphic</li> <li>even the complements of the graphs are isomorphic</li> </ol>"},{"location":"2_Core/Discrete_Structures/04_Graphs/#incident-matrix","title":"Incident Matrix","text":"<p>Let G = (V, E) be an undirected graph with n vertices and m edges</p> <p>\\(B_{n \\times m} = [b_{ij}]\\) is called the incident matrix of G, where</p> \\[ b_{ij} = \\begin{cases} 1, \\text{ when } e_j \\text{ is incident on } v_i \\\\ 0, \\text{ otherwise} \\end{cases} \\]"},{"location":"2_Core/Discrete_Structures/04_Graphs/#subgraph","title":"Subgraph","text":"<p>H is a subgraph of G \\(\\iff V(H) \\subseteq V(G)\\) and \\(E(H) \\subseteq E(G)\\)</p>"},{"location":"2_Core/Discrete_Structures/04_Graphs/#spanning-subgraph","title":"Spanning subgraph","text":"<p>\\(\\iff V(H) = V(G)\\) and \\(E(H) \\subseteq E(G)\\)</p>"},{"location":"2_Core/Discrete_Structures/04_Graphs/#minimal-spanning-subgraph","title":"Minimal Spanning subgraph","text":"<p>spanning subgraph with minimum no of edges required to make the graph connected</p> <p>removal of any edge makes the subgraph disconnected</p> <p>need not be a unique; there can many variations of subgraphs with the above property</p>"},{"location":"2_Core/Discrete_Structures/04_Graphs/#induced-subgraph","title":"Induced Subgraph","text":"<p>it\u2019s the subgraph using only vertices contained in set W and all the pre-existing edges</p> <p>If \\(W \\subseteq G\\), then the subgraph induced by W in G is the one with the vertices set W and contains all edges connecting a pair of vertices in W</p>"},{"location":"2_Core/Discrete_Structures/04_Graphs/#complement-of-graph","title":"Complement of graph","text":"<p>If H is a simple graph with n vertices, then complement denoted as \\(\\bar H\\) of H is the complement of H in \\(k_n\\), where \\(k_n =\\) complete graph with n vertices</p> <p>\\(V(\\bar H) = V(H)\\)</p> <p>2 vertices in \\(\\bar H\\) are adjacent/connected only if they are not adjacent/connected in H</p>"},{"location":"2_Core/Discrete_Structures/04_Graphs/#complement-of-subgraph","title":"Complement of subgraph","text":"<p>\\(\\bar H = G - H\\)</p> <ul> <li>\\(V(\\bar H) = V(H)\\)</li> <li>\\(E(\\bar H) = E(G) - E(H)\\)</li> </ul>"},{"location":"2_Core/Discrete_Structures/04_Graphs/#operations-on-graphs","title":"Operations on Graphs","text":"<ul> <li> <p>\\(G_1 \\cap G_2\\) is a graph with</p> <ul> <li>vertices set \\(V(G_1) \\cap V(G_2)\\)</li> <li>edge set \\(E(G_1) \\cap E(G_2)\\)</li> </ul> </li> <li> <p>\\(G_1 \\cup G_2\\) is a graph with</p> <ul> <li>vertices set \\(V(G_1) \\cup V(G_2)\\)</li> <li>edge set \\(E(G_1) \\cup E(G_2)\\)</li> </ul> </li> <li>\\(\\bar G \\cup G = k_n\\)</li> <li>\\(\\bar G \\cap G = N_7\\), where \\(N_7\\) is a null graph (with vertices but no edges)</li> </ul>"},{"location":"2_Core/Discrete_Structures/04_Graphs/#connected-graph","title":"Connected graph","text":"<p>if there is a path from any vertex to any other vertex in that graph</p> <p>ie every vertex is of degree\\(\\ge 1\\)</p> <p>otherwise it is disconnected</p>"},{"location":"2_Core/Discrete_Structures/04_Graphs/#bipartite-graph","title":"Bipartite Graph","text":"<p>is a simple graph in which V(G) can be partitioned into 2 sets M and N, such that</p> <ol> <li>if vertex \\(v \\in M,\\) then it can only be adjacent to vertices in N</li> <li>If vertex \\(v \\in N,\\) then it can only be adjacent to vertices in M</li> <li>\\(M \\cap N = \\phi\\)</li> <li>\\(M \\cup N = V(G)\\)</li> </ol> <p>When drawing the graph, by convention, M comes up and N comes down</p>"},{"location":"2_Core/Discrete_Structures/04_Graphs/#properties","title":"Properties","text":"<ol> <li>\\(\\sum\\limits_{v \\in M} deg(v) = \\sum\\limits_{v \\in n} deg(v)\\)</li> <li>A bipartite graph contains no odd cycles</li> <li>Every subgraph of a bipartite graph is also bipartite</li> <li>each edge joins a vertex in M to a vertex in N</li> </ol>"},{"location":"2_Core/Discrete_Structures/04_Graphs/#complete-bipartite-graph","title":"Complete Bipartite graph","text":"<p>Every vertex of M is connected to every vertex of N, and vice-versa</p> <p>G is denoted as \\(k_{m, n}\\)</p> <p>if \\(|M| = m, |N| = n\\), then \\(|V(G)| = m+n, |E(G)| = mn\\)</p> <p>in order to traverse a cycle, you need to traverse even no of edges</p>"},{"location":"2_Core/Discrete_Structures/05_Trees/","title":"05 Trees","text":""},{"location":"2_Core/Discrete_Structures/05_Trees/#trees","title":"Trees","text":"<p>is a simple graph such that there is a unique simple non-directed path (which are not closed) between each pair of vertices</p> <ol> <li>always connected</li> <li>no cycles/circuits</li> </ol> <p>order of tree \\(= |V|\\)</p>"},{"location":"2_Core/Discrete_Structures/05_Trees/#properties","title":"Properties","text":"<ol> <li>Trivial tree is a graph with one vertex</li> <li>In every non-trivial tree, there is at least 2 vertices of degree 1</li> <li>A tree with n vertices has exactly \\((n-1)\\) edges</li> <li>If 2 non-adjacent vertices of a tree T are connected by adding an edge, then the resulting graph will contain a cycle; hence, no more a cycle</li> <li>G is a tree \\(\\iff\\) G has no cycles and \\(|E| = |V| - 1\\)</li> </ol>"},{"location":"2_Core/Discrete_Structures/05_Trees/#rooted-trees","title":"Rooted Trees","text":"<p>is a tree in which there is one designated vertex called as the root</p> <p>level(root) = 0; index(root) = 1</p>"},{"location":"2_Core/Discrete_Structures/05_Trees/#directed-tree","title":"Directed Tree","text":"<p>is a rooted tree containing a root from which there is a directed path to each vertex</p> <p>contains hierarchical levels, measured by no of edges away from the root</p> <p>level of a path = length of the path required to reach the vertex from the root</p>"},{"location":"2_Core/Discrete_Structures/05_Trees/#spanning-tree","title":"Spanning Tree","text":"<p>Tree containing all vertices of source graph, and minimum required edges to span the entire graph.</p> <p>It is a subgraph of G</p> <p>It is obtained by removing cycles</p> <p>Height of spanning tree = max level</p>"},{"location":"2_Core/Discrete_Structures/05_Trees/#minimum-spanning-tree","title":"Minimum Spanning Tree","text":"<p>spanning tree with minimum sum of weights</p>"},{"location":"2_Core/Discrete_Structures/05_Trees/#finding-spanning-tree","title":"Finding Spanning Tree","text":"<p>for small trees, we can perform directly; we need to use algorithms for large trees</p> <p>(check mail of Tut 7)</p>"},{"location":"2_Core/Discrete_Structures/05_Trees/#depth-first-searchback-track-algorithm","title":"Depth-First search/Back-Track algorithm","text":"<p>(write T={} step-by-step and backtracking)</p> <ol> <li>pick an arbitrary vertex as the root</li> <li>add 1 adjacent vertex and edge at a time</li> <li>avoid formation of cycles</li> <li>if you come across something that contradicts, perform backtrack</li> </ol>"},{"location":"2_Core/Discrete_Structures/05_Trees/#breadth-first-search-algorithm","title":"Breadth-First Search algorithm","text":"<ol> <li>pick an arbitrary vertex as the root</li> <li>add multiple adjacent vertices and edges (try to get more vertices with max edges)</li> </ol>"},{"location":"2_Core/Discrete_Structures/05_Trees/#prims-algorithm","title":"Prim\u2019s Algorithm","text":"<p>used for weighted spanning graphs Eg: GMaps</p> <ol> <li>start with minimum edge (e,f)</li> <li>select next minimum edge, which is incident to the either vertex of the starting edge</li> <li>if you have 2 edges with the same priority, take the alphabetically</li> <li>then add the other ones too after the above one</li> </ol>"},{"location":"2_Core/Discrete_Structures/05_Trees/#kruskals-algorithm","title":"Kruskal\u2019s Algorithm","text":"<ol> <li>start with minimum edge</li> <li>do minimum edges that aren\u2019t even incident(don\u2019t connect them you dummy), making sure that you don\u2019t get cycles</li> </ol> <p>Independent of starting address</p>"},{"location":"2_Core/Discrete_Structures/05_Trees/#prim-vs-kruskal","title":"Prim vs Kruskal","text":"Prim Kruskal Starting Edge \u2705 \u274c Chooses ___ at every edge nearest/cheapest neighbor cheapest edge Better for ___ graph Denser Sparse Insertion of vertices \ud83d\udc4d \ud83d\udc4e <p>Pr***i***m - start***i***ng edge</p> <p>Krusk***a***l - ***a***ny</p>"},{"location":"2_Core/Discrete_Structures/05_Trees/#trees-terminology","title":"Trees terminology","text":""},{"location":"2_Core/Discrete_Structures/05_Trees/#cut-edgebridge","title":"Cut edge/Bridge","text":"<p>The edge you remove which makes the graph disconnected</p>"},{"location":"2_Core/Discrete_Structures/05_Trees/#cut-vertex","title":"Cut Vertex","text":"<p>The vertex you remove which makes the graph disconnected (obviously, even the edges associated with the vertex is also removed)</p>"},{"location":"2_Core/Discrete_Structures/05_Trees/#branchinternal-vertex","title":"Branch/Internal Vertex","text":"<p>Vertex with degree &gt; 1</p>"},{"location":"2_Core/Discrete_Structures/05_Trees/#leafterminal-vertex","title":"Leaf/Terminal Vertex","text":"<p>vertex with degree 1</p>"},{"location":"2_Core/Discrete_Structures/05_Trees/#forest","title":"Forest","text":"<p>Any graph without cycles</p> <p>need not be connected graph</p> <p>All trees are forest; not vice-versa Trees are components of forest</p>"},{"location":"2_Core/Discrete_Structures/05_Trees/#parts-of-rooted-directed-tree","title":"Parts of Rooted Directed Tree","text":"<ul> <li>Root</li> <li>Children</li> <li>Parents</li> <li>Descendants</li> <li>Ancestors</li> <li>Leaves</li> <li>Branches</li> </ul>"},{"location":"2_Core/Discrete_Structures/05_Trees/#binary-tree","title":"Binary Tree","text":"<p>tree where every vertex has at most 2 children</p>"},{"location":"2_Core/Discrete_Structures/05_Trees/#regular-binary-tree","title":"Regular Binary Tree","text":"<p>tree where every vertex has 0 or 2 children</p>"},{"location":"2_Core/Discrete_Structures/05_Trees/#ordering","title":"Ordering","text":"<p>Labels are given to edges</p> <ul> <li>Left edge = 0</li> <li>Right edge = 1</li> </ul>"},{"location":"2_Core/Discrete_Structures/05_Trees/#binary-string-equivalent-of-a-node","title":"Binary String equivalent of a node","text":"<ol> <li>Write edge ordering from the root to the node</li> <li>Add 1 as MSD</li> </ol>"},{"location":"2_Core/Discrete_Structures/05_Trees/#level-order-indexing","title":"Level-order indexing","text":"<p>Root \\(\\to 1\\) (different from level; level of root is 0)</p> <p>other vertices are designated as (assuming index of parent = p)</p> <ul> <li>left child \\(\\to 2p\\) </li> <li>right child \\(\\to 2p + 1\\)</li> </ul> <p>however, for irregular binary tree, some indices might be skipped</p>"},{"location":"2_Core/Discrete_Structures/05_Trees/#level-of-a-node","title":"Level of a node","text":"<p>if \\(i\\) is the index of a node</p> <p>Level = floor(\\(\\log i\\)) (ie, lower integer value)</p>"},{"location":"2_Core/Discrete_Structures/05_Trees/#complete-binary-tree","title":"Complete Binary Tree","text":"<p>consider a binary with \\(|V|=n\\)</p> <p>if the index set of a binary tree is \\([1,n]\\), then the binary tree is called as a complete binary</p>"},{"location":"2_Core/Discrete_Structures/05_Trees/#characteristics","title":"Characteristics","text":"<ol> <li>Regular</li> <li>Ordered</li> </ol>"},{"location":"2_Core/Discrete_Structures/05_Trees/#fields","title":"Fields","text":"<ol> <li>Data science</li> <li>Searching</li> <li>Efficient Logic and Computing</li> <li>eliminates the need for parenthesis</li> </ol>"},{"location":"2_Core/Discrete_Structures/05_Trees/#operationexpression-tree","title":"Operation/Expression Tree","text":"<p>Mathematical operations and expression can be represented</p> <p>consists of</p> <ol> <li>operators (branches)</li> <li>operands (leaves)</li> </ol>"},{"location":"2_Core/Discrete_Structures/05_Trees/#traversal-algorithms","title":"Traversal Algorithms","text":""},{"location":"2_Core/Discrete_Structures/05_Trees/#pre-order-traversal","title":"Pre-order traversal","text":"<p>polish expression</p> <p>basically prefix</p> <p>\\(a+b \\to +ab\\)</p> <p>Algorithm</p> <ol> <li>Visit the root</li> <li>recursively traverse the left subtree</li> <li>recursively traverse the right subtree</li> </ol>"},{"location":"2_Core/Discrete_Structures/05_Trees/#post-order-traversal","title":"Post-order traversal","text":"<p>Reverse-polish expression</p> <p>basically post-fix</p> <p>\\(a+b \\to ab+\\)</p> <p>Algorithm</p> <ol> <li>Visit the root</li> <li>recursively traverse the right subtree</li> <li>recursively traverse the left subtree</li> </ol>"},{"location":"2_Core/Discrete_Structures/05_Trees/#in-order-traversal","title":"In-order traversal","text":"<p>basically in-fix</p> <p>\\(a+b\\)</p> <p>Algorithm</p> <ol> <li>recursively traverse the left subtree</li> <li>Visit the root</li> <li>recursively traverse the right subtree</li> </ol>"},{"location":"2_Core/Discrete_Structures/05_Trees/#binary-search-tree","title":"Binary Search Tree","text":"<p>Sort Tree</p> <p>every node has a value called as key</p> <p>has parent, left child, right child</p>"},{"location":"2_Core/Discrete_Structures/05_Trees/#properties_1","title":"Properties","text":"<ol> <li>left key &lt; parent key</li> <li>Right key &gt; parent key</li> </ol>"},{"location":"2_Core/Discrete_Structures/05_Trees/#diagram","title":"Diagram","text":""},{"location":"2_Core/Discrete_Structures/06_Planar_Graphs/","title":"06 Planar Graphs","text":""},{"location":"2_Core/Discrete_Structures/06_Planar_Graphs/#planar-graph","title":"Planar Graph","text":"<ol> <li>either the graph itself or at least one isomorphic form of the graph is a plane graph (can be drawn on a plane surface)</li> <li>no crossover of edges</li> </ol> <p>eg:</p> <ul> <li>Complete Graphs \\(k_n\\) \\(n \\le 4\\)</li> <li>\\(Q_3\\)</li> <li>Bipartite graph \\(k_{m,n}\\)   either \\(m \\le 2\\) or \\(n \\le 2\\)</li> </ul> <p>Non-planar graphs eg</p> <ul> <li>\\(k_5\\) and larger</li> <li>\\(k_{3,3}\\)<ul> <li>find longest cycle</li> <li>draw it as a circle</li> </ul> </li> </ul>"},{"location":"2_Core/Discrete_Structures/06_Planar_Graphs/#uses","title":"Uses","text":"<p>coloring, classification, analysis of graphs</p> <p>Plane form helps identify different phases (connected regions) of a planar graph</p>"},{"location":"2_Core/Discrete_Structures/06_Planar_Graphs/#degree-of-region","title":"Degree of Region","text":"<p>\\(|R|\\) = No of edges in the boundary of that region</p> <p>cut edge is counted twice</p>"},{"location":"2_Core/Discrete_Structures/06_Planar_Graphs/#dual-of-planar-graph","title":"Dual of Planar Graph","text":"<ul> <li>every region will become vertices of the dual, and vice versa   if G is primal graph and G* is the dual,<ul> <li>\\(|R^*| = |V|\\)</li> <li>\\(|V^*| = |R|\\)</li> </ul> </li> <li>if 2 regions have common boundary line, then the corresponding new vertices of the dual graph will get connected to each other</li> </ul>"},{"location":"2_Core/Discrete_Structures/06_Planar_Graphs/#theorems","title":"Theorems","text":"<p>If G is a planar graph, then</p> <ol> <li>\\(\\sum deg(r_i) = 2|E|\\)</li> <li>\\(3|R| \\le 2|E|\\)</li> <li>if G is a connected planar graph, then \\(|V| - |E| + |R| = 2\\)</li> <li>\\(|E| \\le 3|V| - 6\\)</li> <li>There exists a vertex v in G such that \\(deg(v) \\le 5\\)</li> </ol>"},{"location":"2_Core/Discrete_Structures/06_Planar_Graphs/#eulers-theorem-for-planar-graphs","title":"Euler\u2019s theorem for planar graphs","text":"<p>\\(|V| - |E| + |R|= 2\\)</p>"},{"location":"2_Core/Discrete_Structures/06_Planar_Graphs/#polyhedral-graphs","title":"Polyhedral Graphs","text":"<p>connected planar graphs</p> <ul> <li>\\(deg(v_i) \\ge 3\\)</li> <li>\\(deg(r_i) \\ge 3\\)</li> <li>using degree of region theorem,<ul> <li>\\(3|V| \\le 2|E|\\)</li> <li>\\(3|R| \\le 2|E|\\)</li> </ul> </li> </ul> <p>eg: \\(k_4, Q_3\\)</p>"},{"location":"2_Core/Discrete_Structures/06_Planar_Graphs/#eulerian-graph","title":"Eulerian Graph","text":"<p>Graph with at least one Eulerian circuit</p> Eulerian Circuit Eulerian Path Path Type closed open passes every edge of original graph exactly once exactly once passes every vertex of original graph at least once at least once repeated vertices allowed allowed"},{"location":"2_Core/Discrete_Structures/06_Planar_Graphs/#cases","title":"Cases","text":"<ul> <li>All vertices are of even degree - both possible</li> <li>Only 2 vertices are of odd degree and the rest are even degree - eulerian path possible not circuit</li> <li>All vertices are of odd degree - both not possible</li> </ul>"},{"location":"2_Core/Discrete_Structures/06_Planar_Graphs/#hamiltonian-graph","title":"Hamiltonian Graph","text":"<p>Graph with at least one Hamiltonian cycle</p> Hamiltonian Cycle Hamiltonian Path Path Type simple, closed simple, closed passes every edge of original graph exactly once exactly once passes every vertex of original graph exactly once exactly once repeated vertices not allowed not allowed when we have cut edge, possible? not possible possible"},{"location":"2_Core/Discrete_Structures/06_Planar_Graphs/#diracs-theorem","title":"Dirac\u2019s Theorem","text":"<p>A simple graph with n vertices \\((n \\ge 3)\\) and \\(deg(v_i) \\ge \\frac n 2\\) has a Hamiltonian circuit eg: \\(k_n, n \\ge 3\\)</p> <p>this is not a necessacity for existence of hamiltonian circuit; the converse is not necessarily true eg: cycle of \\(n\\) vertices each of deg 2; there obviously is hamiltonian circuit even though dirac\u2019s theorem isn\u2019t satisfied</p>"},{"location":"2_Core/Discrete_Structures/06_Planar_Graphs/#dual-graph","title":"Dual graph","text":"<p>dual graph is a graph where the</p> <ul> <li>vertices are the regions of primal graph</li> <li>regions are the vertices of primal graph</li> </ul>"},{"location":"2_Core/Discrete_Structures/06_Planar_Graphs/#properties","title":"Properties","text":"<p>If \\(G(V,E,R) \\implies G^*(V^*,E^*,R^*)\\), where G is primal and G* is dual graph</p> <ol> <li>\\(|V^*| = |R|\\)</li> <li>\\(|R^*| = |V|\\)</li> <li>\\(|E^*| = |E|\\)</li> <li>\\(deg(r_i) = deg(r^*_i)\\)</li> <li>Dual graph is always planar</li> <li>there is a cut vertex placed in region \\(r \\implies\\) you will get a self loop at \\(v^*\\) of G, where \\(v^*\\) represents the corresponding vertex of \\(r\\)</li> </ol>"},{"location":"2_Core/Discrete_Structures/06_Planar_Graphs/#graph-coloring","title":"Graph Coloring","text":"<p>A coloring of a simple graph is the assignment of a color to each vertex of the graph such that no 2 adjacent vertices are assigned the same color.</p>"},{"location":"2_Core/Discrete_Structures/06_Planar_Graphs/#chromatic-number-of-g","title":"Chromatic number of G","text":"<p>\\(\\chi (G) =\\) the least number of colors need for coloring G</p> <p>eg:</p> <ul> <li>Star graph requires only 2 colors</li> <li>\\(k_n\\) requires \\(n\\) colors</li> <li>\\(k_{m,n}\\) requires only 2 colors</li> <li>\\(C_n\\) (cycle of \\(n\\) vertices) requires<ul> <li>2 colors when n = even</li> <li>3 colors when n = odd</li> </ul> </li> </ul>"},{"location":"2_Core/Discrete_Structures/06_Planar_Graphs/#theorem","title":"Theorem","text":"<p>For a planar graph,</p> <p>The chromatic number is no greater than 4, ie \\(\\chi(G) \\le 4\\)</p> <p>no proof for this</p>"},{"location":"2_Core/Discrete_Structures/06_Planar_Graphs/#coloring-rules","title":"Coloring Rules","text":"<ol> <li>\\(\\chi \\le |V|\\)</li> <li>a triangle/triangular subgraph \\((C_3)\\) requires 3 colors</li> <li>if some subgraph of \\(G\\) requires \\(k\\) colors, then    \\(X(G) \\ge k\\)</li> <li>if deg\\((v) = d\\), then d colors are required to color the vertices adjacent to v</li> <li>\\(\\chi(G) = max \\{ \\chi(C)\\) where C is a connected component of G</li> <li>every \\(k\\) chromatic graph \\((\\chi(G) = k)\\) has atleast \\(k\\) vertices such that the \\(deg(v_i) \\ge k-1\\)</li> <li>For any graph \\(G, \\chi(G) \\le 1 + \\Delta(G)\\) \\(\\Delta(G)\\) is the largest degree of any vertex in G</li> <li>\\(\\chi(G) \\ge \\frac{|V|}{ |V| - \\delta(G) }\\) \\(\\delta(G)\\) is the largest degree of any vertex in G</li> </ol>"},{"location":"2_Core/Discrete_Structures/06_Planar_Graphs/#properties-of-chromatic-number","title":"Properties of chromatic number","text":"<ol> <li> <p>\\(k\\)-critical graph is a graph where</p> <ul> <li>\\(\\chi(G) = k\\)</li> <li>\\(\\chi(G-V) = k-1\\)</li> </ul> </li> </ol> <p>possible only if \\(\\delta(G) \\ge k-1\\)</p> <ol> <li> <p>G is 1-chromatic, then G is totally disconnected</p> </li> <li> <p>\\(\\chi(G) = 2 \\iff\\) G is bipartite graph \\(\\iff\\) every cycle of G has even length</p> </li> <li> <p>otherwise it will be a triangular subgraph and hence \\(\\chi\\) has to be 3</p> </li> <li> <p>\\(\\chi(G) \\le \\Delta(G) + 1\\)</p> </li> <li> <p>For complete graphs, \\(\\chi(G) = \\Delta + 1\\)</p> </li> <li> <p>For other graphs, \\(\\chi(G) &lt; \\Delta + 1\\)</p> </li> <li> <p>If G1, G2, \u2026 Gk are disconnected components of graph G, then \\(\\chi(G) = max\\set{\\chi(G_i)}\\)</p> </li> <li> <p>Every tree with \\(|V| \\le 2\\) is 2-chromatic</p> </li> <li> <ul> <li>\\(\\chi(G) \\ge 3 \\iff\\) G has a cycle of odd length</li> <li>\\(\\chi(G) = 2 \\iff\\) G has no cycle of odd length  (we already learnt this for bipartite graphs)</li> </ul> </li> <li> <p>Every connected k-connected graph contains a critical k-chromatic graph</p> </li> <li> <p>Only type of 3-critical graph is \\(C_{2n+1}\\)</p> </li> </ol>"},{"location":"2_Core/Discrete_Structures/08_Pigeon_Hole/","title":"08 Pigeon Hole","text":""},{"location":"2_Core/Discrete_Structures/08_Pigeon_Hole/#summary","title":"Summary","text":"\\[ \\begin{aligned} r &amp;= \\left\\lceil \\frac n k \\right \\rceil \\\\ n &amp;= k(r-1) + 1 \\\\ k &amp;= \\left\\lceil \\frac n r \\right \\rceil \\\\ \\end{aligned} \\]"},{"location":"2_Core/Discrete_Structures/08_Pigeon_Hole/#pigeon-hole-principle","title":"Pigeon Hole Principle","text":"<p>if \\(m\\) holes are assigned for \\(n\\) pigeons, and \\(m&lt;n\\), then atleast one hole will have atleast 2 pigeons</p> <p>in other words, if \\(k+1\\) or more objects are places into \\(k\\) boxes, then there is atleast one box containing two or more objects.</p> <p>if \\(n\\) objects are placed in \\(k\\) boxes, then there is atleast one box with atleast \\(\\lceil \\frac n k \\rceil\\) objects</p> <ul> <li>ceiling \\(\\lceil x \\rceil\\) means that \\(x\\) is rounded up</li> <li>floor \\(\\lfloor x \\rfloor\\) means that \\(x\\) is rounded down</li> </ul>"},{"location":"2_Core/Discrete_Structures/08_Pigeon_Hole/#reason","title":"Reason","text":"<p>A function from a finite set to a smaller finite set cannot be one-one, and hence there will be 2 elements in the domain that have the same image in the co-domain.</p>"},{"location":"2_Core/Discrete_Structures/08_Pigeon_Hole/#application","title":"Application","text":"<p>Minimum no of objects \\(n\\) to be distributed among \\(k\\) boxes such that \\(r\\) objects must be in one of the boxes is given by \\(n = k(r-1) + 1\\)</p> <p>here, \\(r \\le \\lceil \\frac{n}{k} \\rceil\\)</p> <p>This is reversed statement of Pigeon Hole statements</p>"},{"location":"2_Core/Discrete_Structures/09_Inclusion-Exclusion/","title":"09 Inclusion Exclusion","text":""},{"location":"2_Core/Discrete_Structures/09_Inclusion-Exclusion/#principle-of-inclusion-exclusion","title":"Principle of Inclusion-Exclusion","text":"<p>\\(n(A)\\) can also be represented as \\(|A|\\)</p>"},{"location":"2_Core/Discrete_Structures/09_Inclusion-Exclusion/#basic-counting","title":"Basic Counting","text":""},{"location":"2_Core/Discrete_Structures/09_Inclusion-Exclusion/#sum-rule","title":"Sum Rule","text":"<p>Let \\(A_1 , A_2, \\dots, A_n\\) be disjoint(mutually-exclusive) sets</p> <p>\\(n (A_1 \\cup A_2 \\cup \\dots \\cup A_n) = n(A_1) + n(A_2) + \\dots + n(A_n)\\)</p> <p>OR operation</p>"},{"location":"2_Core/Discrete_Structures/09_Inclusion-Exclusion/#product-rule","title":"Product Rule","text":"<p>\\(n (A_1 \\cap A_2 \\cap \\dots \\cap A_n) = n(A_1) \\times n(A_2) \\times \\dots \\times n(A_n)\\)</p> <p>AND operation</p>"},{"location":"2_Core/Discrete_Structures/09_Inclusion-Exclusion/#inclusion-exclusion","title":"Inclusion-Exclusion","text":"<ul> <li>\\(n(A \\cup B) = 1 \\iff\\) mutually-exhaustive</li> <li>\\(n(A \\cap B) = 0 \\iff\\) mutually-exclusive</li> </ul> <p>Formulae</p> <ol> <li> <p>\\(n(A \\cup B) = n(A) + n(B) - n(A \\cap B)\\)</p> </li> <li> \\[    \\begin{aligned}    n(A \\cup B \\cup C)&amp;= n(A) + n(B) + n(C) \\\\    &amp; \\qquad - n(A \\cap B) - n(B \\cap C) - n(A \\cap C) \\\\   &amp; \\qquad + n(A \\cap B \\cap C)    \\end{aligned}    \\] </li> <li> <p>\\(A' = S - A\\)</p> </li> <li> <p>Demorgan</p> </li> <li> <p>\\((A \\cup B)' = A' \\cap B'\\)</p> </li> <li>\\((A \\cap B)' = A' \\cup B'\\)</li> </ol> \\[ \\begin{aligned} |A'| &amp;= |U| - |A| \\\\ |A-B| &amp;= |A \\cup B'| \\\\ &amp;= |A| - |A\\cup B| \\\\ |A \\cap B \\cap C'| &amp;= |A \\cap B| - |A \\cap B \\cap C| \\\\ |A \\cap B' \\cap C| &amp;= |A \\cap C| - |A \\cap B \\cap C| \\\\ |A \\cap B' \\cap C'| &amp;= |B' \\cap C'| - |A' \\cap B' \\cap C'| \\end{aligned} \\]"},{"location":"2_Core/Discrete_Structures/09_Inclusion-Exclusion/#gen-principle","title":"Gen Principle","text":"\\[ \\begin{aligned} \\vert  A_1 \\cup A_2 \\cup \\ldots \\cup A_n  \\vert  &amp;= S_1 - S_2 + S_3 - \\ldots + (-1)^{n-1} S_n \\\\ &amp;= \\sum\\limits_{i = 1}^n |A_i| - \\sum\\limits_{i,j} |A_i \\cap A_j| + \\sum\\limits_{i, j, k} |A_i \\cap A_j \\cap A_k| \\\\ &amp; \\qquad + \\ldots + (-1)^{n-1} |A_1 \\cap A_2 \\cap \\dots \\cap A_n| \\end{aligned} \\]"},{"location":"2_Core/Discrete_Structures/09_Inclusion-Exclusion/#selection","title":"Selection","text":"Permutation Combination ordered? Y N with rep \\(n^r\\) \\(V(n,r)\\) without rep \\(nP_r = \\frac{n!}{(n-r)!}\\) \\(nC_r = \\frac{n!}{r!\\ (n-r)!} = nC_{n-r}\\) <p>\\(nC_r = nP_r = 0 \\iff n&lt;r\\)</p> <p>The no of \\(r\\) combinations of \\(n\\) distinct objects with unlimited repetitions</p> \\[ \\begin{aligned} &amp;= V(n,r) \\\\ &amp;= (n-1+r)C_r &amp;= (n-1+r)C_{n-1} \\\\ &amp;= \\frac{(n-1+r)!}{r! \\ (n-1)!} \\end{aligned} \\] <p>Uses</p> <ul> <li>This is the no of ways of distributing \\(r\\) similar balls into \\(n\\) number boxes</li> <li>no of non-negative integer solutions of \\(x_1 + x_2 + \\dots + x_n = r\\)</li> <li>no of binary nos with \\((n-1)\\) ones and \\(r\\) zeros</li> </ul>"},{"location":"2_Core/Discrete_Structures/09_Inclusion-Exclusion/#integral-solutions","title":"Integral Solutions","text":"<p>The no of non-negative integer solutions is given by \\(V(n,r)\\)</p> \\[ \\set{ x_1 a_1, x_2 a_2, \\dots, x_n a_n } \\iff x_1 + x_2 + \\dots + x_n = r \\]"},{"location":"2_Core/Discrete_Structures/09_Inclusion-Exclusion/#derangement","title":"Derangement","text":"<p>special type of permutation of any \\(n\\) objects such that no number takes it\u2019s own place</p> <p>\\(i_1, i_2, \\dots, i_n \\iff i_1 \\ne 1, i_2 \\ne 2, i_n \\ne n\\)</p> <p>normally, for any arrangement of \\(n\\) numbers, no of arrangements = \\(n!\\)</p> <p>\\(D_n =\\) no of derangments possible for derangement of \\(n\\) numbers</p> \\[ \\begin{aligned} D_1 &amp;= 0 \\\\ D_2 &amp;= 1 \\\\ D_3 &amp;= 2 \\qquad \\set{(3, 1, 2), (2, 3, 1)} \\\\ \\vdots &amp; \\\\ D_n &amp;= n! \\left[ 1- \\frac{1}{1!}  + \\frac{1}{2!} - \\frac{1}{3!} + \\dots + (-1)^n \\frac{1}{n!} \\right] \\\\ &amp;= n! \\left[ 1 + \\sum_{i=1}^n (-1)^i \\frac{1}{i!} \\right] \\end{aligned} \\]"},{"location":"2_Core/Discrete_Structures/11_Groups/","title":"11 Groups","text":""},{"location":"2_Core/Discrete_Structures/11_Groups/#order-of-an-element","title":"Order of an element","text":""},{"location":"2_Core/Discrete_Structures/11_Groups/#for-oplus","title":"For \\(+, \\oplus\\)","text":"<p>Regular/modulo addition operator</p> <p>\\(x \\oplus y =\\) ??</p> <p>what positive number multiplied gives the product as the identity element</p>"},{"location":"2_Core/Discrete_Structures/11_Groups/#for-times-otimes","title":"For \\(\\times, \\otimes\\)","text":"<p>Regular/modulo multiplication operator</p> <p>what number raised to gives the final answer as the identity element</p>"},{"location":"2_Core/Discrete_Structures/11_Groups/#for-both","title":"For Both","text":"<p>If \\(a \\in G\\), G is a group, then the order of \\(a\\) is the order of the cyclic group</p>"},{"location":"2_Core/Discrete_Structures/11_Groups/#cosets-lagranges-theorem","title":"COSETS &amp; Lagrange\u2019s Theorem","text":"<p>Let</p> <ul> <li>\\(G\\) be a group</li> <li>\\(H\\) be its subgroup</li> <li>\\(a \\in G\\)</li> <li> <p>\\(h \\in H\\)</p> </li> <li> <p>COSETS may be duplicated, but we are only concerned about disjoint COSETS</p> </li> <li>Union of disjoint COSETS will be \\(G\\)</li> <li>no of elements in COSET = no of elements in \\(H\\)</li> </ul> Left COSET Right COSET \\(+\\) \\(a + H = \\set{a + h, h \\in H}\\) \\(H + a = \\set{h + a, h \\in H}\\) \\(\\oplus\\) \\(\\times\\) \\(a H = \\set{a \\times h}\\) \\(Ha = \\set{h \\times a}\\) \\(\\otimes\\)"},{"location":"2_Core/Discrete_Structures/11_Groups/#theorems","title":"Theorems","text":"<p>If \\(b \\in G, b \\ne a\\)</p> <ol> <li>\\(a \\in H \\iff aH = H\\)</li> <li>\\(aH = bH \\iff a^{-1} b \\in H\\)</li> <li>\\(a \\in bH \\iff a^{-1} \\in H b^{-1}\\)</li> <li>\\(a \\in bH \\iff aH = bH\\)</li> </ol> <p>\\(^{-1}\\) means inverse (could be additive or multiplicative inverse)</p>"},{"location":"2_Core/Discrete_Structures/11_Groups/#lagranges-theorem","title":"Lagrange\u2019s Theorem","text":"<p>Let \\(G\\) be a finite group of order \\(n\\) and \\(H\\) be any subgroup of \\(G\\). Then the order of H divides the order of \\(G\\).</p> <p>\\([G:H]\\) = index of H = no of distinct left COSETS of H in G</p> <p>Let \\(r\\) be index of H. Let \\(|G| = n,|H| = m\\). Then \\(n = mr \\implies \\frac n m = r\\). Clearly, \\(m\\) divides \\(n\\)</p>"},{"location":"2_Core/Discrete_Structures/11_Groups/#application","title":"Application","text":"<p>A cyclic group can only have subgroups with no of elements which divides the </p> <p>eg: \\((Z_7, \\oplus_7)\\) can only have subgroups having no of elements dividing \\(7\\). So, it can either be \\(&lt;1&gt;\\) or \\(&lt;7&gt;\\).</p>"},{"location":"2_Core/Logic_in_CS/","title":"Logic in CS","text":"Class Instructor Lecture Dr. Siddhaling Urolagin Tutorial Dr. Siddhaling Urolagin <p>This course introduces the foundational concepts of logic essential to computer science, emphasizing both theoretical and practical aspects. It begins with propositional logic, covering syntax, semantics, and concepts like satisfiability and validity, and progresses to predicate logic, where students learn about completeness, compactness, undecidability, and incompleteness. These topics establish a basis for understanding the logical structure of computation and problem-solving in computer science.</p> <p>The course further explores advanced topics such as model checking for verification, including linear-time temporal logic (LTL) and computational tree logic (CTL), which are crucial for verifying program behavior. Hoare logic is introduced for program verification, allowing students to construct correctness proofs. Modal logic and logic programming are also covered, broadening students\u2019 understanding of logic as applied in various computer science applications.</p>"},{"location":"2_Core/Logic_in_CS/01_Propositional_Logic/","title":"01 Propositional Logic","text":""},{"location":"2_Core/Logic_in_CS/01_Propositional_Logic/#propositional-symbols","title":"Propositional Symbols","text":"Symbol Meaning \\(\\top\\) True \\(\\bot\\) False \\(\\land, \\lor\\) and, or (whichever comes first) \\(\\to\\) implies \\(\\vdash\\) Conclusion"},{"location":"2_Core/Logic_in_CS/01_Propositional_Logic/#natural-deduction","title":"Natural Deduction","text":"<p>\\(\\phi_1, \\phi_2, \\dots\\) are Premises</p> <p>\\(\\psi\\) is Conclusion</p> <p>\\(\\phi_1, \\phi_2, \\dots, \\phi_n \\vdash\\psi\\) is called sequent</p>"},{"location":"2_Core/Logic_in_CS/01_Propositional_Logic/#rules","title":"Rules","text":"<ol> <li>\\(\\frac{\\phi \\quad \\psi}{\\phi \\land \\psi} \\quad (\\land i)\\)</li> <li>\\(\\frac{\\phi \\land \\psi}{\\phi}, \\frac{\\phi \\land \\psi}{\\psi} \\quad (\\land e_1, \\land e_2)\\)</li> <li>\\(\\frac{\\phi}{\\lnot \\lnot \\phi} \\quad(\\lnot \\lnot i)\\)</li> <li>\\(\\frac{\\lnot \\lnot \\phi}{\\phi} \\quad(\\lnot \\lnot e)\\)</li> <li>\\(\\frac{     \\begin{bmatrix} \\phi \\\\ \\vdots \\\\ \\psi    \\end{bmatrix}    }{\\phi \\to \\psi} \\quad (\\to i)\\)</li> <li>\\(\\frac{\\phi \\quad \\phi \\to \\psi}{\\psi} \\quad (\\to e)\\)</li> <li>\\(\\frac{\\lnot \\psi \\quad \\phi \\to \\psi}{\\lnot \\phi}\\) (MT)</li> <li>\\(\\frac{\\phi}{\\phi \\lor \\psi}, \\frac{\\psi}{\\phi \\lor \\psi} \\quad (\\lor i_1, \\lor i_2)\\)</li> <li>\\(\\frac{\\phi \\lor \\psi \\quad     \\begin{bmatrix}    \\phi \\\\ \\vdots \\\\ \\chi    \\end{bmatrix}    \\begin{bmatrix}    \\psi \\\\ \\vdots \\\\ \\chi    \\end{bmatrix}    }{\\chi} \\quad (\\lor e)\\)</li> <li>Copy rule</li> <li>\\(\\frac{\\begin{bmatrix}     \\phi \\\\ \\vdots \\\\ \\bot     \\end{bmatrix}     }{\\lnot \\phi} \\quad (\\lnot i)\\)</li> <li>\\(\\frac{\\begin{bmatrix}     \\lnot \\phi \\\\ \\vdots \\\\ \\bot     \\end{bmatrix}     }{\\phi}\\) PBC</li> <li>\\(\\frac{\\phi \\quad \\lnot \\phi}{\\bot} \\quad (\\lnot e)\\)</li> <li>\\(\\frac{\\bot}{\\phi} \\quad (\\bot e)\\)</li> <li>\\(\\frac{}{\\phi \\lor \\lnot \\phi}\\)(LEM)</li> </ol>"},{"location":"2_Core/Logic_in_CS/01_Propositional_Logic/#equivalence-relation","title":"Equivalence Relation","text":"<p>If a formula can be proved in both directions, then it is called as an equivalence relation.</p> <p>Denoted by \\(\\dashv \\vdash\\)</p>"},{"location":"2_Core/Logic_in_CS/01_Propositional_Logic/#wff","title":"WFF","text":"<p>Well-formed formula</p> <p>There's brackets for every operation</p> <p>\\((p \\land (\\lnot q)) \\to (p \\lor (q \\lor (\\lnot r) ))\\)</p>"},{"location":"2_Core/Logic_in_CS/01_Propositional_Logic/#parse-tree","title":"Parse Tree","text":"<p>Shows the order at which the terms and operations are parsed</p>"},{"location":"2_Core/Logic_in_CS/01_Propositional_Logic/#semantic-entailment","title":"Semantic entailment","text":"<p>\\(\\phi \\models \\psi\\)</p> <p>this means that whenever\\(\\phi\\) is true, \\(\\psi\\) is true</p> <p>ie \\(\\phi \\to \\psi\\) is true for all cases</p>"},{"location":"2_Core/Logic_in_CS/01_Propositional_Logic/#soundness","title":"Soundness","text":"<p>if ND is true, then even semantic entailment is true</p> <p>\\(\\phi \\vdash \\psi \\implies \\phi \\models \\psi\\)</p>"},{"location":"2_Core/Logic_in_CS/01_Propositional_Logic/#completeness","title":"Completeness","text":"<p>if semantic entailment is true, then even ND is true</p> <p>\\(\\phi \\models \\psi \\implies \\phi \\vdash \\psi\\)</p>"},{"location":"2_Core/Logic_in_CS/01_Propositional_Logic/#semantic-equivalence","title":"Semantic equivalence","text":"<p>\\(\\phi \\equiv \\psi\\)</p> <p>\\(\\phi \\models \\psi\\) and \\(\\psi \\models \\phi\\)</p> <p>both have the same truth table</p>"},{"location":"2_Core/Logic_in_CS/01_Propositional_Logic/#interpretation","title":"Interpretation","text":"<p>assigning values (putting inputs)</p>"},{"location":"2_Core/Logic_in_CS/01_Propositional_Logic/#valuation","title":"Valuation","text":"<p>getting outputs</p>"},{"location":"2_Core/Logic_in_CS/01_Propositional_Logic/#tautology","title":"Tautology","text":"<p>function whose output is always true</p>"},{"location":"2_Core/Logic_in_CS/01_Propositional_Logic/#valid","title":"Valid","text":"<p>true for all interpretations</p>"},{"location":"2_Core/Logic_in_CS/01_Propositional_Logic/#satisfiable","title":"Satisfiable","text":"<p>true for at least one interpretation</p>"},{"location":"2_Core/Logic_in_CS/01_Propositional_Logic/#conclusions","title":"Conclusions","text":"<ul> <li>A is valid \\(\\iff \\lnot A\\) is un-satisfiable</li> <li>A is satisfiable \\(\\iff \\lnot A\\) is invalid</li> </ul>"},{"location":"2_Core/Logic_in_CS/01_Propositional_Logic/#cnf","title":"CNF","text":"<p>basically POS</p> <ol> <li>literal - variable</li> <li>clause</li> <li>formula</li> </ol> <p>\\(\\underbrace{ ( \\underbrace{p}_\\text{literal} \\lor q) \\land \\underbrace{(r \\lor s)}_\\text{clause}  }_\\text{formula}\\)</p> <p>if \\(p\\) and \\(p\u2019\\) both exist within all clauses, all clauses are true then formula is valid</p>"},{"location":"2_Core/Logic_in_CS/01_Propositional_Logic/#implies-conversion","title":"Implies Conversion","text":"<p>\\(p \\to q = \\lnot p \\lor q\\)</p>"},{"location":"2_Core/Logic_in_CS/01_Propositional_Logic/#horn-clauses","title":"Horn Clauses","text":"<p>useful for checking satisfiability</p> <p>only contains \\(\\land\\) and \\(\\to\\)</p> <p>every clause contains \\(\\to\\)</p> <p>cannot contain</p> <ul> <li>\\(\\lor\\)</li> <li>\\(\\lnot\\)</li> </ul> \\[ \\underbrace{ ( \\underbrace{p}_\\text{proposition} \\land q \\to s)  \\land  \\underbrace{( \\underbrace{ p \\land q \\land r }_\\text{assumption}\\to s)}_\\text{clause} }_\\text{formula} \\]"},{"location":"2_Core/Logic_in_CS/01_Propositional_Logic/#checking-satisfiability","title":"Checking Satisfiability","text":"<p>we just have to check if there exists atleast one combination of variables such that the entire formula is true.</p> <p>if nothing is possible, then it is false.</p>"},{"location":"2_Core/Logic_in_CS/02_Predicate_Logic/","title":"02 Predicate Logic","text":""},{"location":"2_Core/Logic_in_CS/02_Predicate_Logic/#predicate-logic","title":"Predicate Logic","text":"<ul> <li>\\(\\exists\\) there exists (at least one)   similar to \\(\\lor\\)</li> <li>\\(\\forall\\) for all   similar to \\(\\land\\)</li> </ul>"},{"location":"2_Core/Logic_in_CS/02_Predicate_Logic/#predicates","title":"Predicates","text":"<p>return true or false</p> <p>all caps</p> <p>eg:</p> <ul> <li>MAN(x): x is a man   // returns T/F</li> <li>ADULT(x): x is an adult   // returns T/F</li> </ul>"},{"location":"2_Core/Logic_in_CS/02_Predicate_Logic/#functions","title":"Functions","text":"<p>returns object</p> <p>eg:</p> <ul> <li>mother(x)   // returns the mother of x (object)</li> <li>age(x)   // returns the age of x (integer)</li> </ul> <p>All predicates are functions, but not all functions are predicates</p>"},{"location":"2_Core/Logic_in_CS/02_Predicate_Logic/#variables","title":"Variables","text":"<p>they can occur in 2 places</p> <ol> <li>along with \\(\\forall\\) and \\(\\exists\\)</li> <li>eg: \\(\\forall x, \\exists y\\)</li> <li>as leaf nodes (terminal vertices)</li> <li>\\(x, y\\)</li> </ol>"},{"location":"2_Core/Logic_in_CS/02_Predicate_Logic/#bounded-variables","title":"Bounded variables","text":"<p>under \\(\\forall\\) or \\(\\exists\\) in parse tree</p>"},{"location":"2_Core/Logic_in_CS/02_Predicate_Logic/#free-variables","title":"Free variables","text":"<p>are not under any restrictions they can be substituted/replaced by bounded/free variables</p>"},{"location":"2_Core/Logic_in_CS/02_Predicate_Logic/#substitution","title":"Substitution","text":"<p>\\(\\phi[t/x]\\) means that \\(t\\) replaces free variable \\(x\\)</p> <p>\\(t\\) can be anything - bounded/free var for eg:</p> <ul> <li>\\(t = t\\)</li> <li>\\(t = f(x,y)\\)</li> <li>\\(t = g(x,y,z)\\)</li> </ul> <p>any of the above can replace free variable</p>"},{"location":"2_Core/Logic_in_CS/02_Predicate_Logic/#nd","title":"ND","text":"<ol> <li>\\(= e\\)</li> <li>\\(a=b, b = c \\implies a = c\\)</li> <li> <p>Basically transitiveness</p> </li> <li> <p>\\(\\forall e\\)</p> </li> <li>\\(\\frac{\\forall x \\ \\phi}{ \\phi [x_0/x] }\\)</li> <li>if \\(\\phi\\) is true for all \\(x\\), then we can replace \\(x\\) by \\(x_0\\) in \\(\\phi\\), and conclude that \\(\\phi[x_0/x]\\) is also true</li> <li> <p>eg: if all students are teens, then a student Ahmed is also a teen</p> </li> <li> <p>\\(\\forall i\\)</p> </li> <li> <p>\\(\\frac{       \\begin{bmatrix}       x_0 \\\\ \\vdots \\\\ \\phi       \\end{bmatrix}       }{\\forall x}\\)</p> </li> <li>assumption box</li> <li> <p>if we prove that a student is a teen, then all students are teens (considering that all \\(x\\), ie students are identical)</p> </li> <li> <p>\\(\\exists i\\)</p> </li> <li> \\[       \\frac{       \\phi[t/x]       }{\\exists x \\quad \\phi} \\quad       \\exists x \\quad i       \\] </li> <li> <p>We can deduce \\(\\exists x \\quad \\phi\\) whenever we have \\(\\phi[t/x]\\); \\(t\\) has to be free for \\(x\\) in \\(\\phi\\)</p> </li> <li>\\(\\exists e\\)</li> <li> \\[       \\frac{       \\exists x \\ \\phi \\quad       \\begin{bmatrix}       x_0 \\quad \\phi[x_0/x] \\\\       \\vdots \\\\       \\chi       \\end{bmatrix}       }{\\chi} \\quad \\exists x \\quad e       \\] </li> <li> <p>if \\(\\exists x \\quad \\phi\\) is true, there should be atleast one value of \\(x\\) for which \\(\\phi\\) is true</p> </li> <li> <p>Let \\(x_0\\) represent those values</p> </li> <li> <p>Substituting \\(x_0\\) for \\(x\\), we arrive at formula \\(\\chi\\)</p> </li> <li> <p>we then conclude \\(\\chi\\)</p> </li> </ol>"},{"location":"2_Core/Logic_in_CS/02_Predicate_Logic/#quantifier-equivalences","title":"Quantifier equivalences","text":"<ol> <li>De-Morgan\u2019s rule</li> </ol> <p>Convert bw \\(\\forall\\) and \\(\\exists\\) when there is negation      - \\(\\lnot \\forall x (\\phi) \\dashv \\vdash \\exists x (\\lnot \\phi)\\)      - \\(\\lnot \\exists x (\\phi) \\dashv \\vdash \\forall x (\\lnot \\phi)\\) 2. Distributive    1. \\(\\forall x (\\phi) \\land \\forall x(\\psi) \\dashv \\vdash \\forall x (\\phi \\land \\psi)\\)    2. \\(\\exists x (\\phi) \\lor \\exists x (\\psi) \\dashv \\vdash \\exists(\\phi \\lor \\psi)\\) 3. Commutative    1. \\(\\forall x \\forall y (\\phi) \\dashv \\vdash \\forall y \\forall x (\\phi)\\)    2. \\(\\exists x \\exists y (\\phi) \\dashv \\vdash \\exists y \\exists x (\\phi)\\)</p>"},{"location":"2_Core/Logic_in_CS/02_Predicate_Logic/#idk","title":"IDK","text":"Symbol Term Meaning \\(P\\) predicate \\(l\\) lookup table gives us environment environment conditions \\(\\mathbb M\\) Model shows relations \\(\\models\\) Semantic Entailment \\(\\models_l\\) Models wrt lookup table \\(l\\) \\(\\Gamma\\) Set of formulae Arity no of vars/relations??"},{"location":"2_Core/Logic_in_CS/02_Predicate_Logic/#properties","title":"Properties","text":""},{"location":"2_Core/Logic_in_CS/02_Predicate_Logic/#compactness","title":"Compactness","text":"<p>let \\(\\Gamma\\) is a set of formulae in predicate logic.</p> <p>If all finite subsets of \\(\\Gamma\\) are satisfiable, then \\(\\Gamma\\) is satisfiable</p>"},{"location":"2_Core/Logic_in_CS/02_Predicate_Logic/#godels-completeness-theorem","title":"Godel\u2019s Completeness Theorem","text":""},{"location":"2_Core/Logic_in_CS/02_Predicate_Logic/#underdesirablility","title":"Underdesirablility","text":"<p>If there are a large instances, it will be hard to determine the validity of a formula. This is called as undesirability.</p>"},{"location":"2_Core/Logic_in_CS/02_Predicate_Logic/#verification","title":"Verification","text":"<ol> <li>framework for modelling</li> <li>specification language - eg: Predicate logic</li> <li>verification language - eg: ND</li> </ol> Verification Type Proof-Based \\(\\Gamma \\vdash \\phi\\) Model Based \\(\\mathbb M \\models \\phi\\)"},{"location":"2_Core/Logic_in_CS/03_Temporal_Logic/","title":"03 Temporal Logic","text":""},{"location":"2_Core/Logic_in_CS/03_Temporal_Logic/#temporal-logic","title":"Temporal Logic","text":""},{"location":"2_Core/Logic_in_CS/03_Temporal_Logic/#time","title":"Time","text":"Linear Time Branching Time sequential conditional infinite straight line infinite tree branches N Y"},{"location":"2_Core/Logic_in_CS/03_Temporal_Logic/#temporal-connectives","title":"Temporal Connectives","text":""},{"location":"2_Core/Logic_in_CS/03_Temporal_Logic/#state-quantifiers","title":"State Quantifiers","text":"<p>For both LTL and CTL</p> Symbol Meaning \\(X\\) ne==X==t state \\(F\\) some ==F==uture state, including the current state XF some future state, from the next state onwards \\(G\\) all future states, including the current state (==G==lobally) XG all future states, from the next state onwards \\(U\\) ==U==ntil \\((&lt;)\\)\\(\\phi U \\psi\\) means that \\(\\phi\\) is true initially and then suddenly \\(\\psi\\) becomes true. anything after that doesn\u2019t matter \\(R\\) ==R==elease\\(\\phi R \\psi\\) means that both \\(\\phi\\) and \\(\\psi\\) occur once together. anything after that doesn\u2019t matter \\(W\\) ==W==eak-until \\((\\le)\\)(not really sure)"},{"location":"2_Core/Logic_in_CS/03_Temporal_Logic/#path-quantifiers","title":"Path Quantifiers","text":"<p>(only for CTL) | Symbol | Meaning                 | | -----: | ----------------------- | |      A | for ==A==ll paths       | |      E | there ==E==xists a path |</p>"},{"location":"2_Core/Logic_in_CS/03_Temporal_Logic/#operator-precedence","title":"Operator Precedence","text":"<ol> <li>Unary operators</li> <li>Temporal binary operators</li> <li>Non-temporal binary operators</li> </ol>"},{"location":"2_Core/Logic_in_CS/03_Temporal_Logic/#states","title":"States","text":"\\[ \\underbrace{s_0  \\underbrace{\\to}_\\text{transition} s_1}_\\text{path} \\] <p>State diagram </p> <p>\\(\\mathcal{P}\\) is the power set</p>"},{"location":"2_Core/Logic_in_CS/03_Temporal_Logic/#paths-pi","title":"Paths (\\(\\pi\\))","text":"<p>\\(\\pi^i\\) is the path originating from state \\(s_i\\)</p>"},{"location":"2_Core/Logic_in_CS/03_Temporal_Logic/#deadlock","title":"Deadlock","text":"<p>state having no further transitions</p>"},{"location":"2_Core/Logic_in_CS/03_Temporal_Logic/#removing-deadlock","title":"Removing deadlock","text":"<p>add a another state \\(s_d\\) which has a self-loop</p>"},{"location":"2_Core/Logic_in_CS/03_Temporal_Logic/#state-diagram","title":"State Diagram","text":"<pre><code>flowchart LR\n        s0 --&gt; s1 &amp; s2\n        s1 --&gt; s1</code></pre>"},{"location":"2_Core/Logic_in_CS/03_Temporal_Logic/#unwinding","title":"Unwinding","text":"<p>Representing a state diagram using a binary tree is called as unwinding</p> <pre><code>flowchart TB\ns0 --&gt; s1 &amp; s2\ns1 --&gt; s[s1]</code></pre>"},{"location":"2_Core/Logic_in_CS/03_Temporal_Logic/#ctl-equivalences","title":"CTL Equivalences","text":"Paths States Universal Quantifier A G Existential Quantifier E F \\[ \\begin{aligned} \\lnot AF \\phi &amp;= EG \\lnot \\phi \\\\ \\lnot EF \\phi &amp;= AG \\lnot \\phi \\\\ \\lnot AX \\phi &amp;= EX \\lnot \\phi \\\\ AF \\phi &amp;= A[T \\cup \\phi] \\\\ EF \\phi &amp;= E[T \\cup \\phi] \\end{aligned} \\]"},{"location":"2_Core/Logic_in_CS/03_Temporal_Logic/#adequate-sets","title":"Adequate Sets","text":"\\[ \\begin{aligned} AX \\phi &amp;= \\lnot EX  \\lnot \\phi \\\\ AG &amp;=\\\\ \\end{aligned} \\] <p>there are more</p>"},{"location":"2_Core/Logic_in_CS/04_Program_Verification/","title":"04 Program Verification","text":""},{"location":"2_Core/Logic_in_CS/04_Program_Verification/#hoare-triple-notation","title":"Hoare Triple Notation","text":"<p>\\(\\set{\\phi} P \\set{\\psi}\\)</p> <p>eg: \\(\\set{x \\ge 0} \\text{ fact } \\set{f=x!}\\)</p>"},{"location":"2_Core/Logic_in_CS/04_Program_Verification/#correctness","title":"Correctness","text":"<p>Total Correctness \\(\\implies\\) Partial Correctness</p> Partial Total loop termination not neccessary necessary correctness \\(\\models_\\text{par} \\set{\\phi} P \\set{\\psi}\\) \\(\\models_\\text{tot} \\set{\\phi} P \\set{\\psi}\\) incorrectness \\(\\not \\models_\\text{par} \\set{\\phi} P \\set{\\psi}\\) \\(\\not \\models_\\text{tot} \\set{\\phi} P \\set{\\psi}\\) holds when \\(\\vdash_\\text{par} \\set{\\phi} P \\set{\\psi}\\) is valid \\(\\vdash_\\text{tot} \\set{\\phi} P \\set{\\psi}\\) is valid <p>eg:</p> \\[ \\begin{aligned} \\models_\\text{tot} \\set{x \\ge 0} &amp;\\text{ fact } \\set{f=x!} \\\\ \\not \\models_\\text{tot} \\set{\\top} &amp;\\text{ fact } \\set{f=x!} \\text{(as the loop will not terminate)} \\\\ \\models_\\text{par} \\set{x \\ge 0} &amp;\\text{ fact } \\set{f=x!} \\\\ \\models_\\text{par} \\set{\\top} &amp;\\text{ fact } \\set{f=x!} \\end{aligned} \\]"},{"location":"2_Core/Logic_in_CS/04_Program_Verification/#proof-tableaux","title":"Proof Tableaux","text":"\\[ \\begin{aligned} &amp; \\top \\\\ &amp; \\qquad \\phi_0 &amp; (implied)\\\\ &amp; C_1 \\\\ &amp; \\qquad \\phi_1 &amp; \\text{(assignment)}\\\\ &amp; C_2 \\\\ &amp; \\qquad \\phi_2 &amp; \\text{(assignment)}\\\\ &amp; \\vdots \\\\ &amp; C_{n-1} \\\\ &amp; \\qquad \\phi_{n-1} &amp; \\text{(assignment)}\\\\ &amp; C_n \\\\ &amp; \\qquad \\phi_n &amp; \\text{(assignment)}\\\\ \\end{aligned} \\] <p>justification could be</p> <ol> <li>assigned</li> <li>implied</li> </ol>"},{"location":"2_Core/Logic_in_CS/05_Modal_Logic/","title":"05 Modal Logic","text":""},{"location":"2_Core/Logic_in_CS/05_Modal_Logic/#modal-logic","title":"Modal Logic","text":"<p>it extends propositional and predicate logic</p>"},{"location":"2_Core/Logic_in_CS/05_Modal_Logic/#world","title":"World","text":"<p>is similar to state</p> <p>it\u2019s like a reality in Rick and Morty. We can assume every thing that can/cannot happpens as part of infinite realities.</p>"},{"location":"2_Core/Logic_in_CS/05_Modal_Logic/#symbols","title":"Symbols","text":"Symbol Meaning Interpretation CTL Equivalent \\(\\Box\\) Necessarily All worlds \\(AX\\) \\(\\Diamond\\) Possibly Some world \\(EX\\)"},{"location":"2_Core/Logic_in_CS/05_Modal_Logic/#scenarios","title":"Scenarios","text":"Type Representation Interpretation Possibility \\(\\Diamond \\phi = \\lnot \\Box (\\lnot \\phi)\\) possibly true; not necessarily false Necessity \\(\\Box \\phi = \\lnot \\Diamond (\\lnot \\phi)\\) necessarily true; not possibly false Uncertainity \\(\\lnot(\\Box \\phi) = \\Diamond (\\lnot \\phi)\\) not necessarily true; possibly false Impossibility \\(\\lnot (\\Diamond \\phi) = \\Box (\\lnot \\phi)\\) not possibly true; necessarily false"},{"location":"2_Core/Logic_in_CS/05_Modal_Logic/#notes","title":"Notes","text":"<ol> <li> <p>Necessity requires possibility, impossibility requires uncertainity</p> </li> <li> <p>Necessity \\(\\implies\\) possibility, impossibility \\(\\implies\\) uncertainity</p> </li> <li> <p>Necessity and impossibility are not symbolically contradictory    (look at the position of the \\(\\lnot\\) symbol)</p> </li> </ol> \\[ \\begin{aligned} &amp;\\Box(\\phi) \\\\    \\underbrace{}_\\text{not here} &amp;\\Box ( \\lnot \\phi) \\end{aligned} \\]"},{"location":"2_Core/Math_3/","title":"Math 3","text":"Class Instructor Lecture Dr. Baskaran Tutorial Dr. Baskaran <p>This course delves deeper into the study of differential equations, emphasizing classical methods for solving boundary value problems. It serves as a foundation for applying differential equations, Fourier series, and Laplace transforms across various engineering and scientific disciplines.</p> <p>The curriculum integrates software-based solution procedures to enhance understanding and visualization of concepts. Students will explore the applications of these mathematical techniques, preparing them to address complex problems in real-world scenarios and providing a solid groundwork for further studies in mathematics and engineering.</p>"},{"location":"2_Core/Math_3/#note","title":"Note","text":"<p>In this course, I\u2019ve defined my notes in the following manner. This might not be mathematically perfect way to write, but it makes it easier to read and understand.</p> Meaning Key Constant \\(p, q, a, b, A, B\\) Variable \\(x, y\\) Function \\(P, Q\\) <p>I\u2019ve excluded function symbols to make it easier to read. For eg: \\(P(x) \\overset{\\text{written as}}{\\longrightarrow} P\\)</p>"},{"location":"2_Core/Math_3/01_Intro/","title":"01 Intro","text":""},{"location":"2_Core/Math_3/01_Intro/#de","title":"DE","text":"<p>Differential equation is an equation that relates one or more unknown functions and their derivatives.</p>"},{"location":"2_Core/Math_3/01_Intro/#order","title":"Order","text":"<p>The order of a differential equation is defined to be that of the highest order derivative it contains.</p>"},{"location":"2_Core/Math_3/01_Intro/#degree","title":"Degree","text":"<p>The degree of a differential equation is defined as the power to which the highest order derivative is raised.</p>"},{"location":"2_Core/Math_3/01_Intro/#1st-order-de","title":"1<sup>st</sup> Order DE","text":"\\[ \\frac{dy}{dx} = f(x, y) \\] <p>Aim is to find the value of \\(y\\) in terms in \\(x\\). We do this by integrating (anti-derivative).</p>"},{"location":"2_Core/Math_3/01_Intro/#separable-variables","title":"Separable Variables","text":"<p>We can directly solve this by separating the variables</p> \\[ \\begin{aligned} f(x, y) &amp;= g(x) \\cdot h(y) \\\\ \\frac{dy}{dx} &amp;= g(x) \\cdot h(y) \\\\ \\int \\frac{dy}{h(y)}  &amp;= \\int g(x) dx \\end{aligned} \\]"},{"location":"2_Core/Math_3/01_Intro/#homogeneous-equation","title":"Homogeneous Equation","text":"<p>Special type of Homogeneous Expression</p> \\[ M dx + N dy = 0 \\] <p>If both \\(M(x, y)\\) and \\(N(x,y)\\) are homogeneous of the same degree.</p> \\[ \\begin{aligned} \\frac{dy}{dx} &amp;= \\frac{- M(x, y)}{N(x, y)} \\\\ \\text{Let }  v &amp;= \\frac{y}{x} \\implies y = vx \\\\ \\frac{dy}{dx} &amp;= v + x \\frac{dv}{dx} \\end{aligned} \\]"},{"location":"2_Core/Math_3/01_Intro/#homogeneous-expression","title":"Homogeneous Expression","text":"\\[ \\begin{aligned} f(tx, ty) = t^n \\cdot f(x, y) \\end{aligned} \\] Example Degree \\(\\sin(\\frac{x}{y})\\) 0 \\(\\sqrt{x^2 + y^2}\\) 1 \\(x^2 + y^2\\) 2"},{"location":"2_Core/Math_3/01_Intro/#integration-rules","title":"Integration Rules","text":"<p>Grade 12 Integration Rules</p>"},{"location":"2_Core/Math_3/01_Intro/#transposed-differential","title":"Transposed Differential","text":"<p>Be able to identify transposition to simplify</p> \\[ \\begin{aligned} d(\\log x) &amp;= \\left( \\frac{1}{x} \\right) dx \\\\ \\text{because  } \\frac{d(\\log x)}{dx} &amp;= \\frac{1}{x} \\\\ \\end{aligned} \\]"},{"location":"2_Core/Math_3/02_Exact_DE/","title":"02 Exact DE","text":""},{"location":"2_Core/Math_3/02_Exact_DE/#family-of-curves","title":"Family of Curves","text":"\\[ \\begin{aligned} f(x, y) &amp;= c \\\\ d( \\ f(x, y) \\ ) &amp;= d(c) \\\\ f_x dx + f_y dy &amp;= 0 \\end{aligned} \\] <p>This last step</p> <ul> <li>looks like Homogeneous Equation</li> <li>is the exact differential equation of the given curve</li> </ul> <p>The solution is the given equation, and from that we derived the exact differential equation.</p>"},{"location":"2_Core/Math_3/02_Exact_DE/#solution-of-a-de","title":"Solution of a DE","text":"<p>Consider a first-order differential equation of the form</p> \\[ M dx + N dy = 0 \\] <p>if there happens to be a function \\(f(x, y)\\) such that</p> \\[ \\begin{aligned} f_x = M(x, y), \\quad f_y &amp;= N(x, y) \\\\ \\frac{\\partial f}{\\partial x} dx + \\frac{\\partial f}{\\partial y} dy &amp;= 0 \\\\ d( f(x, y) ) &amp;= 0 \\\\ f(x, y) &amp;= c \\end{aligned} \\] <p>The final step is the general solution of the given differential equation.</p>"},{"location":"2_Core/Math_3/02_Exact_DE/#exact-de","title":"Exact DE","text":"<p>is a differential equation where \\(M_y = N_x\\)</p>"},{"location":"2_Core/Math_3/02_Exact_DE/#shortcut-method-for-exact-de","title":"Shortcut Method for Exact DE","text":"<p>This is only for exact DE</p> <p>Consider this DE</p> \\[ (\\ y + y \\cos(xy)  \\ )dx + (\\ x + x \\cos(xy) \\ ) = 0 \\] <ol> <li> <p>Check if the given DE is exact</p> </li> <li> <p>Put integration sign for both sides</p> </li> </ol> \\[ \\int (\\ y + y \\cos(xy)  \\ )  dx + \\int(\\ x + x \\cos(xy) \\ )  dx = \\int 0 \\] <ol> <li>Simplifications</li> <li>Treat \\(y\\) as a constant in the \\(dx\\) integral</li> <li>Drop all terms containing \\(x\\) in the \\(dy\\) integral       think like this: drop your ex       example<ul> <li>\\(y \\cos(x) \\to 0\\)</li> <li>\\(x \\cos(x) \\to 0\\)</li> <li>\\(y + y \\cos(x) \\to y\\)</li> </ul> </li> </ol> \\[ \\int (\\  y + y \\cos(xy) \\ ) dx + \\int (0 + 0) dy = c \\] <ol> <li>Integrate</li> </ol> \\[ \\begin{aligned} yx + y \\left( \\frac{ \\sin xy }{ y } \\right) &amp;= c \\\\ yx + \\sin(xy) &amp;= c \\end{aligned} \\]"},{"location":"2_Core/Math_3/02_Exact_DE/#exact-de-formulae","title":"Exact DE Formulae","text":"\\[ \\begin{aligned} d(xy) &amp;= xdy + y dx \\\\ d(x^2 + y^2) &amp;= 2x dx + 2y dy \\\\ d \\left(\\frac{x^2 + y^2}{2} \\right) &amp;= x dx + y dy \\end{aligned} \\] \\[ \\begin{aligned} d\\left(\\frac{x}{y}\\right) &amp;= \\frac{ydx - xdy}{y^2} \\quad \\left(\\frac{u}{v} \\right)' \\text{ formula}\\\\ &amp;= \\frac{1}{y}dx - \\frac{x}{y^2} dy \\\\ d\\left(\\frac{y}{x}\\right) &amp;= \\frac{xdy - ydx}{x^2} \\\\ &amp;= \\frac{1}{x}dy - \\frac{y}{x^2} dx \\end{aligned} \\] \\[ \\begin{aligned} d\\left(\\log{ |\\frac{x}{y}| }\\right) &amp;= \\frac{1}{\\frac{x}{y}} \\left( \\frac{y dx - x dy}{y^2} \\right) \\\\ &amp;= \\frac{y dx - x dy}{xy} \\\\ \\end{aligned} \\] \\[ \\begin{aligned} d\\left( \\log \\vert  \\frac{y}{x}  \\vert \\right) &amp;= \\frac{x dy - y dx}{xy} \\end{aligned} \\] \\[ \\begin{aligned} d \\left( \\tan^{-1} \\frac{x}{y} \\right) &amp;= \\frac{1}{1 + \\frac{x^2}{y^2} } \\left( \\frac{y dx - x dy}{y^2} \\right) \\\\ &amp;= \\frac{y dx - x dy}{x^2 + y^2} \\\\ d \\left( \\tan^{-1} \\frac{y}{x} \\right) &amp;= \\frac{x dy - ydx}{x^2 + y^2} \\end{aligned} \\]"},{"location":"2_Core/Math_3/02_Exact_DE/#idk","title":"IDK","text":"<p>You cannot integrate \\(\\int f(x,y) \\ dx\\) wrt to \\(dx\\) alone</p> <p>it is only possible for something sir said and double integration (there \\(dy\\) will also be there in outer integral)</p>"},{"location":"2_Core/Math_3/03_Inexact_DE/","title":"03 Inexact DE","text":"<p>Consider a 1<sup>st</sup> order inexact DE</p> \\[ M(x, y) dx + N(x, y) dy = 0, \\quad (M_y \\ne N_x) \\]"},{"location":"2_Core/Math_3/03_Inexact_DE/#steps","title":"Steps","text":"<ol> <li>Find \\(M_y - N_x\\)</li> <li>You will get one of the following cases    the simplification will give in terms of a single variable</li> </ol> Case 1 Case 2 \\(\\dfrac{M_y - N_x}{\\color{orange}-M} = h(y)\\) \\(\\dfrac{M_y - N_x}{\\color{orange}N} = g(x)\\) IF \\(e^{\\int h(y) \\cdot dy}\\) \\(e^{\\int g(x) \\cdot dx}\\) <ol> <li>Multiply both sides of equation:    Inexact DE \\(\\times\\) IF \\(\\to\\) Exact DE</li> <li>Then, use Exact DE method</li> </ol>"},{"location":"2_Core/Math_3/03_Inexact_DE/#shortcut","title":"Shortcut","text":"<ul> <li>Try to get everything in terms of simple integrals like \\(dx, dy, d(xy),d(x+y)\\).</li> <li>Then use exact DE formulae</li> </ul> <p>This way we can avoid the IF step</p>"},{"location":"2_Core/Math_3/04_Linear_DE/","title":"04 Linear DE","text":""},{"location":"2_Core/Math_3/04_Linear_DE/#linear-de-general-form","title":"Linear DE General Form","text":"\\[ y' + P y = Q \\] <p>where \\(y\\) is the dependent variable</p>"},{"location":"2_Core/Math_3/04_Linear_DE/#solution","title":"Solution","text":"<ol> <li> <p>Find IF \\(= e^{\\int P(x) dx}\\)</p> </li> <li> <p>Find general solution</p> </li> </ol> \\[ y \\times IF = \\int \\Big( Q \\times IF \\Big) dx \\quad + c \\]"},{"location":"2_Core/Math_3/04_Linear_DE/#bernoullis-de","title":"Bernoulli\u2019s DE","text":"\\[ y' + P y = Q y^n \\]"},{"location":"2_Core/Math_3/04_Linear_DE/#solution_1","title":"Solution","text":"<ol> <li>Divide both sides by \\(y^n\\)</li> </ol> \\[ y^{-n} y' + P y^{1-n} = Q \\] <ol> <li>Take \\(z = y^{1-n}\\)</li> </ol> \\[ \\begin{aligned} z' &amp;= (1-n) y^{(1-n)-1} y' \\\\ y^{-n} y' &amp;= \\left( \\frac{1}{1-n} \\right) z' \\end{aligned} \\] <ol> <li>Convert into a Linear DE</li> </ol> \\[ \\begin{aligned} \\left( \\frac{1}{1-n} \\right) z' + P z &amp;= Q \\\\ z' + \\underbrace{(1-n) P}_{P\\text{ of linear DE}} \\ z &amp;= (1-n) Q \\end{aligned} \\] <ol> <li>Solving using Linear DE method in terms of \\(z\\)</li> </ol> \\[ \\begin{aligned} \\text{IF} &amp;= (1-n) \\int P dx \\\\    z \\times \\text{IF} &amp;= (1-n) \\int (Q \\times \\text{IF}) dx \\quad + c \\end{aligned} \\] <ol> <li>Put \\(z = y^{1-n}\\) back into this</li> </ol>"},{"location":"2_Core/Math_3/05_Reduction_of_Order/","title":"05 Reduction of Order","text":""},{"location":"2_Core/Math_3/05_Reduction_of_Order/#general-form-of-2nd-order-de","title":"General Form of 2<sup>nd</sup> order DE","text":"\\[ F(x, y, y', y'') = 0 \\] <p>This is for variable coefficients.</p>"},{"location":"2_Core/Math_3/05_Reduction_of_Order/#solving","title":"Solving","text":"<ul> <li>2<sup>nd</sup> order DE is reduced into two 1<sup>st</sup> order DE</li> <li>they are solved one after each other</li> </ul> <p>Reduction of order method is possible under 2 cases</p> Case 1 Case 2 missing terms Dependent variable \\(y\\) Independent variable \\(x\\) Form \\(F(x, y', y'') = 0\\) \\(F(y, y', y'') = 0\\) Let \\(y' = P \\implies y'' = P'\\) $y' = P \\ \\implies y'' = P' = \\frac{dP}{dy} y' \\ y''= P \\left(\\frac{dP}{dy}\\right)$ Solve \\(F(x, P, P') = 0\\) \\(F(y, P, P \\frac{dP}{dy}) = 0\\) Substitute \\(y' = P \\implies y'' = P'\\) \\(y' = P \\implies y'' = P'\\) Solve \\(F(x, y)\\) \\(F(x, y)\\)"},{"location":"2_Core/Math_3/06_2nd_Order_DE/","title":"06 2nd Order DE","text":""},{"location":"2_Core/Math_3/06_2nd_Order_DE/#types","title":"Types","text":""},{"location":"2_Core/Math_3/06_2nd_Order_DE/#complete-equation","title":"Complete Equation","text":"\\[ y'' + P(x) y' + Q(x) y = R(x) \\] <p>Also called non-homogeneous DE Particular Solution of complete equation: \\(y_p(x)\\) If \\(y(x)\\) is the solution, then it is given by</p> \\[ y(x) = y_g + y_p \\]"},{"location":"2_Core/Math_3/06_2nd_Order_DE/#reduced-equation","title":"Reduced Equation","text":"<p>Complete equation with \\(R(x) = 0\\)</p> \\[ y'' + P(x) y' + Q(x) y = 0 \\] <p>Also called as homogeneous DE</p> \\[ y(x) = y_g \\quad (y_p(x) = 0) \\]"},{"location":"2_Core/Math_3/06_2nd_Order_DE/#theorems","title":"Theorems","text":""},{"location":"2_Core/Math_3/06_2nd_Order_DE/#1","title":"1","text":"<p>If \\(y_1(x)\\) and \\(y_2(x)\\) are 2 solutions of reduced DE, then \\(\\set{c_1 y_1(x) + c_2 y_2(x)}\\) is another solution of the reduced DE for any constants \\(c_1, c_2\\)</p>"},{"location":"2_Core/Math_3/06_2nd_Order_DE/#2","title":"2","text":"<p>If \\(y_1(x)\\) and \\(y_2(x)\\) are 2 solutions of reduced DE, then they are linearly-dependent \\(\\iff\\) their wronskian = 0</p> \\[ W(y_1, y_2) =  \\begin{vmatrix} y_1 &amp; y_2 \\\\ {y_1}' &amp; {y_2}' \\end{vmatrix} = 0 \\] <p>Else, they are linearly-independent</p> <p>eg:</p> <ul> <li>\\(y_1 = x^2, y_2 = \\frac{3}{2} x^2\\) - linear dependent</li> <li>\\(y_1 = x, y_2 = x^2\\) - linearly independent</li> </ul>"},{"location":"2_Core/Math_3/06_2nd_Order_DE/#3","title":"3","text":"<p>If \\(y_1(x)\\) and \\(y_2(x)\\) are 2 linearly-independent solutions of reduced DE, then \\(y(x) = c_1 y_1(x) + c_2 y_2(x)\\) is called general solution</p>"},{"location":"2_Core/Math_3/06_2nd_Order_DE/#solving","title":"Solving","text":"<ol> <li>Sub \\(y = y_1(x)\\) and \\(y = y_2(x)\\) in the given equation</li> <li>Show that LHS = RHS</li> </ol>"},{"location":"2_Core/Math_3/06_2nd_Order_DE/#principle-of-superposition","title":"Principle of Superposition","text":"<p>If the given DE is of the form</p> \\[ y'' +  py' + qy = f(x) + g(x) \\] <p>Solution is given by</p> \\[ \\begin{aligned} y'' +  py' + qy &amp;= 0 &amp;\\to y_g \\\\ y'' +  py' + qy &amp;= f(x) &amp;\\to y_{p_1} \\\\ y'' +  py' + qy &amp;= g(x) &amp;\\to y_{p_2} \\\\ \\implies y &amp;= y_g + y_{p_1} + y_{p_2} \\end{aligned} \\] <p>This superposition of the solutions is called as principle of superposition.</p>"},{"location":"2_Core/Math_3/07_Known_Soln/","title":"07 Known Soln","text":"<p>Consider a homogeneous 2<sup>nd</sup> order DE.</p> \\[ y'' + P y' + Q y = 0 \\] <p>Let \\(y_1(x)\\) be the known solution of it.</p> <p>To find another linear-independent solution \\(y_2(x)\\)</p> <ol> <li>Let</li> </ol> \\[ \\begin{aligned} v &amp;= \\int \\frac{1}{ {(y_1)}^2 \\times e^{ \\int P dx} } \\\\ y_2 &amp;= v \\cdot y_1 \\end{aligned} \\] <ol> <li>Now, the general solution \\(y(x) = c_1 y_1(x) + c_2 y_2(x)\\)</li> </ol>"},{"location":"2_Core/Math_3/07_Known_Soln/#special-cases","title":"Special Cases","text":"<p>(not important)</p>"},{"location":"2_Core/Math_3/07_Known_Soln/#legendre-de","title":"Legendre DE","text":"\\[ (1-x^2)y'' - 2xy' + k(k+1) y = 0 \\] <p>where \\(k\\) = const</p>"},{"location":"2_Core/Math_3/07_Known_Soln/#bessels-equation","title":"Bessel\u2019s Equation","text":"\\[ x^2 y'' + xy' + (x^2 - k^2) y = 0 \\] <p>\\(k\\) = const</p>"},{"location":"2_Core/Math_3/08_Constant_Coefficient/","title":"08 Constant Coefficient","text":""},{"location":"2_Core/Math_3/08_Constant_Coefficient/#2nd-order-homogeneous-de-with-constant-coefficients","title":"2<sup>nd</sup> Order Homogeneous DE with constant coefficients","text":"\\[ y'' + py' + qy = 0 \\] <p>where \\(p, q\\) are constants</p> <p>Consider \\(y = e^{mx}\\) as a possible solution, where \\(m\\) = unknown constant. So our goal is to find \\(m\\).</p> <p>Then</p> \\[ y' = m \\cdot e^{mx} \\\\ y' = m^2 \\cdot e^{mx} \\\\ \\implies (m^2 \\cdot e^{mx}) + p(m \\cdot e^{mx}) + qe^{mx} = 0 \\\\ e^{mx} ( m^2 + pm + q ) = 0 \\\\ \\]"},{"location":"2_Core/Math_3/08_Constant_Coefficient/#auxiliary-equation","title":"Auxiliary equation","text":"\\[ e^{mx} \\ne 0 \\\\ \\implies ( m^2 + pm + q ) = 0 \\] <p>Solve this to get the value(s) of unknown \\(m\\)</p> Roots General Solution \\(y\\) real and distinct \\(m_1, m_2\\) \\(c_1 e^{m_1 x} + c_2 e^{m_2 x}\\) equal roots \\(m_1 = m_2 = m\\) \\(e^{mx} (c_1 + c_2 x )\\) Complex roots \\(m_1, m_2 = a \\pm ib\\) \\(e^{ax} (c_1\\cos bx + c_2 \\sin bx )\\)"},{"location":"2_Core/Math_3/08_Constant_Coefficient/#boundary-value-problems","title":"Boundary Value Problems","text":"<p>Using given \u2018initial conditions\u2019, we need to find the values of \\(c_1\\) and \\(c_2\\)</p>"},{"location":"2_Core/Math_3/09_Euler/","title":"09 Euler","text":""},{"location":"2_Core/Math_3/09_Euler/#eulers-equidimensional-de","title":"Euler\u2019s Equidimensional DE","text":"\\[ x^2 y'' + px y' + qy = 0 \\]"},{"location":"2_Core/Math_3/09_Euler/#transformation","title":"Transformation","text":"<ol> <li> <p>Let \\(x = e^z \\quad (z = \\log x)\\)</p> </li> <li> <p>Now, \\(y\\) is a function of \\(z\\), which in turn is a function of \\(x\\)</p> </li> <li> <p>Put the following substitutions; Refer to Custom Operators</p> </li> </ol> \\[ \\begin{aligned} xD &amp;= \\theta \\\\    x^2 D^2 &amp;= \\theta(\\theta - 1) \\end{aligned} \\] <ol> <li>equation becomes</li> </ol> \\[ \\begin{aligned} \\Big( \\theta(\\theta - 1) + p \\theta + q \\Big)y &amp;= 0 \\\\    \\theta(\\theta - 1) + p \\theta + q &amp;= 0 &amp; (y \\ne 0) \\end{aligned} \\] <ol> <li> <p>Put \\(\\theta^2 \\to m^2, \\theta \\to m\\)</p> </li> <li> <p>Find gen solution in terms of \\(z : y(z)\\), using Constant Coefficient</p> <ul> <li>\\(y = c_1 e^{m_1 z} + c_2 e^{m_2 z}\\)</li> <li>\\(y = e^{mz}(c_1 + c_2 z)\\)</li> <li>\\(y = e^{az}(c_1 \\cos bz+ c_2 \\sin bz)\\)</li> </ul> </li> <li> <p>Find gen solution in terms of \\(x\\), by subbing \\(z = \\log x\\)</p> </li> </ol>"},{"location":"2_Core/Math_3/09_Euler/#custom-operators","title":"Custom Operators","text":"\\[ \\begin{aligned} D &amp;= \\frac{d}{dx}  &amp; D^2 &amp;= \\frac{d^2}{dx^2} \\\\ \\theta &amp;= \\frac{d}{dz}  &amp; \\theta^2 &amp;= \\frac{d^2}{dz^2} \\end{aligned} \\]"},{"location":"2_Core/Math_3/09_Euler/#formula","title":"Formula","text":"\\[ x Dy = \\theta y \\implies xD = \\theta \\]"},{"location":"2_Core/Math_3/10_Higher_Order_Constant_Coefficient/","title":"10 Higher Order Constant Coefficient","text":""},{"location":"2_Core/Math_3/10_Higher_Order_Constant_Coefficient/#higher-order-constant-coefficient","title":"Higher Order Constant Coefficient","text":"\\[ y^{(n)} + \\dots + py' + qy = 0 \\] <p>Solve using the same method as Constant Coefficient</p> \\[ m^n + \\dots + pm + q = 0 \\] <p>For 3 equal roots,</p> \\[ y(x) = e^{mx}(c_1 + c_2 x + c_3 x^2) \\]"},{"location":"2_Core/Math_3/11_Undetermined_Coefficients/","title":"11 Undetermined Coefficients","text":"<p>The undetermined coefficient method is possible for a few standards functions such as \\(R(x) = e^{ax}, \\sin(ax), \\cos(ax),\\) polynomials. This method also requires a trial solution to compute the required particular solution.</p> <p>Note If RHS \\(= \\sin(2x) + \\cos(2x)\\), then we can consider it as a single function \\(R(x)\\)</p> <p>Consider</p> <ul> <li>Constants \\(l, k, a, b \\in R\\)</li> <li>Undetermined Coefficients \\(A, B, A_0, A_1, \\dots, A_n\\) (unknown constant)</li> </ul> <p>The exception cases are for preventing duplication of terms, and hence prevent linear dependency of the solutions.</p> \\(R(x)\\) Trial Particular Solution Exception based on root \\(m\\) of auxilary eqn \\(l e^{ax}\\) \\(A e^{ax}\\) \\(A x e^{ax}\\) \\(m_1 = a\\) or \\(m_2 = a\\) \\(A x^2 e^{ax}\\) \\(m_1 = m_2 = a\\) \\(l \\cos(ax)\\)\\(l \\sin(ax)\\)\\(l \\cos(ax) \\pm k \\sin(ax)\\) \\(A \\cos ax + B \\sin ax\\) \\(x (A \\cos ax + B \\sin ax)\\) \\(m= 0 \\pm ai\\) \\(a_0 + a_1 x + \\dots + a_n x^n\\)(\\(n^{\\text{th}}\\) degree polynomial) \\((A_0 + A_1 x + \\dots + A_n x^n)\\) \\(x(A_0 + A_1 x + \\dots + A_n x^n)\\) \\(m = 0\\) \\(e^{ax} \\cos bx\\)\\(e^{ax} \\sin bx\\)\\(e^{ax} ( \\cos bx + \\sin bx )\\) \\(e^{ax} ( A \\cos bx + B \\sin bx )\\) \\(xe^{ax} ( A \\cos bx + B \\sin bx )\\) \\(m = a \\pm bi\\)"},{"location":"2_Core/Math_3/11_Undetermined_Coefficients/#trick-for-product-of-3-functions","title":"Trick for product of 3 functions","text":"<p>If \\(y_g\\) and the trial particular solution are similar</p> <ul> <li>instead of using \\((uvw)' = uvw' + uv'w + u'vw\\)</li> <li>we can take</li> </ul> \\[ x e^x ( A \\cos x + B \\sin x) \\to x \\phi \\]"},{"location":"2_Core/Math_3/11_Undetermined_Coefficients/#example","title":"Example","text":"\\[ \\begin{aligned} y'' - 2y' + 2y &amp;= e^x \\sin x \\\\ x e^x ( A \\cos x + B \\sin x) &amp; \\to x \\phi \\\\ {y_g}'' - 2{y_g}' + 2{y_g} &amp;= 0 \\\\ \\implies \\phi'' - 2\\phi' + 2\\phi &amp;= 0 \\end{aligned} \\]"},{"location":"2_Core/Math_3/12_Variation_of_Parameter/","title":"12 Variation of Parameter","text":""},{"location":"2_Core/Math_3/12_Variation_of_Parameter/#variation-of-parameter","title":"Variation of Parameter","text":"<p>for finding particular solution \\(y_p\\)</p> <p>more suitable if the RHS function is a \\(\\log, \\tan, \\cot, \\sec, \\csc,\\) hyperbolic</p> <ol> <li>Find general solution</li> </ol> \\[ y_g = c_1 \\textcolor{orange}{y_1} + c_2 \\textcolor{orange}{y_2} \\] <ol> <li>Let</li> </ol> \\[ \\begin{aligned} y_p &amp;= v_1 y_1(x) + v_2 y_2(x), \\text{where} \\\\ v_1 &amp;= \\int \\frac{ \\textcolor{orange}{-y_2} \\cdot R(x) }{ W(y_1, y_2) } dx \\\\ v_2 &amp;= \\int \\frac{ \\textcolor{orange}{y_1} \\cdot R(x) }{ W(y_1, y_2) } dx \\end{aligned} \\] <p>where \\(W(y_1, y_2)\\) be the Wronskian, then</p> <ol> <li>Complete solution \\(y = y_g + y_p\\)</li> </ol>"},{"location":"2_Core/Math_3/13_Operator_Method/","title":"13 Operator Method","text":"<p>Operator Method is a more general method, so it is good.</p> <p>Consider a 2<sup>nd</sup> order DE</p> \\[ \\begin{aligned} y'' + py' + qy &amp;= R(x) \\\\ (D^2 + pD + q)y &amp;= R(x) \\end{aligned} \\]"},{"location":"2_Core/Math_3/13_Operator_Method/#definition","title":"Definition","text":"\\[ y_p = \\frac{1}{\\phi(D)} R(x) \\] \\[ \\begin{aligned} \\phi(D) y &amp;= R(x) \\\\ \\phi(D) &amp;= D^2 + pD + q \\\\ &amp;=(D-m_1)(D-m_2) \\end{aligned} \\]"},{"location":"2_Core/Math_3/13_Operator_Method/#integrals","title":"Integrals","text":"\\[ \\begin{aligned} \\frac{1}{D} R(x) &amp;= \\int R(x) dx \\\\ \\frac{1}{D^2} R(x) &amp;= \\iint R(x) dx \\cdot dx \\end{aligned} \\] \\[ \\begin{aligned} \\frac{1}{D-m} R(x) &amp;= \\textcolor{orange}{e^{mx}} \\int R(x) \\cdot \\textcolor{hotpink}{e^{-mx}} \\cdot dx \\\\ \\frac{1}{D+m} R(x) &amp;= \\textcolor{hotpink}{e^{-mx}} \\int R(x) \\cdot \\textcolor{orange}{e^{mx}} \\cdot dx \\end{aligned} \\]"},{"location":"2_Core/Math_3/13_Operator_Method/#short-rules-for-standard-functions","title":"Short Rules for standard functions","text":"R(x) \\(y_p\\) Exception \\(e^{\\textcolor{orange}{a}x}\\) \\(e^{\\textcolor{orange}{a}x}\\dfrac{1}{\\phi(\\textcolor{orange}{a})}\\) \\(x e^{\\textcolor{orange}{a}x} \\frac{1}{\\phi'(\\textcolor{orange}{a})}\\) \\(\\phi(a) = 0\\) \\(x^2 e^{\\textcolor{orange}{a}x} \\frac{1}{\\phi''(\\textcolor{orange}{a})}\\) \\(\\phi'(a) = 0\\) \\(\\sin(\\textcolor{orange}{a}x+b), \\cos(\\textcolor{orange}{a}x+b)\\) \\(R(x) \\frac{1}{f(-\\textcolor{orange}{a}^2)}\\) \\(x R(x) \\frac{1}{f'(-\\textcolor{orange}{a}^2)}\\) \\(f(-a^2) = 0\\) \\(x^2 R(x) \\frac{1}{f''(-\\textcolor{orange}{a}^2)}\\) \\(f'(-a^2) = 0\\) \\(x^m\\) \\(\\underbrace{\\Big(\\phi(D) \\Big)^{-1} }_\\text{Binomial Series} x^m\\) \\(e^{\\textcolor{orange}{k}x} h(x)\\)(Exponent Shifting Rule) \\(e^{\\textcolor{orange}{k}x} \\left\\{ \\frac{1}{\\phi(D+\\textcolor{orange}{k})} h(x) \\right\\}\\)Solve using any of the above methods"},{"location":"2_Core/Math_3/13_Operator_Method/#derivatives","title":"Derivatives","text":"\\[ \\begin{aligned} \\phi'(a) &amp;= \\left\\{ \\frac{d \\phi(D)}{dD} \\right\\}_{D \\to a} \\\\ \\phi''(a) &amp;= \\left\\{ \\frac{d^2 \\phi(D)}{d D^2} \\right\\}_{D \\to a} \\\\ f(-a^2) &amp;= \\left\\{ \\frac{d f(D^2)}{dD} \\right\\}_{D^2 \\to -a^2} \\\\ f'(-a^2) &amp;= \\left\\{ \\frac{d^2 f(D^2)}{d D^2} \\right\\}_{D^2 \\to -a^2} \\\\ \\end{aligned} \\]"},{"location":"2_Core/Math_3/13_Operator_Method/#binomial-expansions","title":"Binomial Expansions","text":"\\[ \\begin{aligned} (1+x)^{-1} &amp;= 1 - x + x^2 - x^3 + \\dots \\\\ (1-x)^{-1} &amp;= 1 + x + x^2 + x^3 + \\dots \\end{aligned} \\] \\[ \\begin{aligned} (1+x)^{-2} &amp;= 1 - 2x + 3x^2 - 4x^3 + \\dots \\\\ (1-x)^{-2} &amp;= 1 + 2x + 3x^2 + 4x^3 + \\dots \\end{aligned} \\] \\[ \\begin{aligned} (1+x)^{-n} &amp;= 1 - nx + \\frac{n(n+1) x^2}{2!} -  \\frac{n(n+1)(n+2) x^3}{3!} + \\cdots\\\\ (1-x)^{-n} &amp;= 1 + nx + \\frac{n(n+1) x^2}{2!} + \\frac{n(n+1)(n+2) x^3}{3!} + \\cdots \\end{aligned} \\]"},{"location":"2_Core/Math_3/13_Operator_Method/#cube-formula","title":"Cube Formula","text":"\\[ (a+b)^3 = a^3+b^3+3ab(a+b) \\\\ (a-b)^3 = a^3-b^3-3ab(a-b) \\]"},{"location":"2_Core/Math_3/13_Operator_Method/#long-method","title":"Long Method","text":""},{"location":"2_Core/Math_3/13_Operator_Method/#idk","title":"idk","text":"\\[ \\begin{aligned} y_p &amp;= \\frac{1}{\\phi(D)} R(x) \\\\ &amp;= \\underbrace{     \\left( \\frac{1}{D-m_1} \\right)     \\underbrace{         \\frac{1}{D-m_2} R(x)     }_{R_1(x)} }_{R_2(x)}\\\\ &amp;= \\frac{1}{D-m} R(x) \\end{aligned} \\]"},{"location":"2_Core/Math_3/13_Operator_Method/#if","title":"IF","text":"\\[ \\begin{aligned} Dy - my &amp;= R(x) \\\\ \\frac{dy}{dx} - my &amp;= R(x) \\\\ IF &amp;= e^{\\int P(x) dx} \\\\ &amp;= e^{\\int -m dx} \\\\ &amp;= e^{-mx} \\end{aligned} \\]"},{"location":"2_Core/Math_3/13_Operator_Method/#solution","title":"Solution","text":"\\[ \\begin{aligned} y \\times IF &amp;= \\int R(x) \\cdot IF \\cdot dx \\\\ y e^{-mx} &amp;= \\int R(x) \\cdot e^{-mx} \\cdot dx \\\\ y &amp;= e^{mx} \\int R(x) \\cdot e^{-mx} \\cdot dx \\end{aligned} \\]"},{"location":"2_Core/Math_3/14_Laplace/","title":"14 Laplace","text":""},{"location":"2_Core/Math_3/14_Laplace/#laplace-transformation","title":"Laplace Transformation","text":"<p>Converting differential calculus into algebra</p> <pre><code>flowchart LR\nDE --&gt;\n|LT| ae[Algebraic] --&gt;\n|Solving| sa[Algebraic Solution] --&gt;\n|ILT| sd[DE Solution] -.-&gt;\nDE</code></pre>"},{"location":"2_Core/Math_3/14_Laplace/#lt-function","title":"LT Function","text":"<p>Laplace Transform</p> \\[ \\begin{aligned} L\\{ f(t) \\} &amp;= \\int\\limits_0^\\infty e^{-st} f(t) \\cdot dt  \\quad \\text{ (or a function of } x)\\\\ &amp;= F(s) \\end{aligned} \\] \\(f(t)\\) Time Domain Function \\(s\\) Laplace Variable (real/complex) \\(F(s)\\) Laplace Domain Function"},{"location":"2_Core/Math_3/14_Laplace/#ilt-function","title":"ILT Function","text":"<p>Inverse Laplace Transform</p> \\[ L^{-1} \\{ F(s) \\} = f(t) \\]"},{"location":"2_Core/Math_3/14_Laplace/#basic-rules","title":"Basic Rules","text":"Situation LT ILT Constant Coeffient \\(L\\Big(k f(t) \\Big) = k L(t)\\) \\(L^{-1}(k s) = k L^{-1} \\Big( F(s) \\Big)\\) Sum \\(L \\Big( f(t) \\pm g(t) \\Big) = L \\Big( f(t) \\Big) \\pm L \\Big( g(t) \\Big)\\) \\(L^{-1} \\Big( F(s) \\pm G(s) \\Big) = L^{-1} \\Big( F(s) \\Big) \\pm L^{-1} \\Big( G(s) \\Big)\\)"},{"location":"2_Core/Math_3/14_Laplace/#lt-of-standard-functions","title":"LT of Standard Functions","text":"\\(f(t)\\) \\(L\\Big( f(t) \\Big)\\) \\(1\\) \\(\\frac{1}{s}\\) \\(k\\) \\(\\frac{k}{s}\\) \\(e^{at}\\) \\(\\frac{1}{s\\textcolor{orange}{-}a}\\) \\(\\cos(at)\\) \\(\\frac{s}{s^2 + a^2}\\) \\(\\sin(at)\\) \\(\\frac{a}{s^2 + a^2}\\) \\(\\cosh(at)\\) \\(\\frac{s}{s^2 - a^2}\\) \\(\\sinh(at)\\) \\(\\frac{a}{s^2 - a^2}\\) \\(t^n\\) $\\begin{cases} \\dfrac{n!}{s^{n+1}}, &amp; n \\le 0 \\ \\dfrac{\\Gamma(n+1)}{s^{n+1}}, &amp; \\text{otherwise} \\end{cases}$  where \\(\\Gamma\\) is gamma function \\(e^{at} f(t)\\)(exponent shifting rule) \\(F(s \\textcolor{orange}{-} a) = \\Big\\{ F(s) \\Big\\}_{s \\to s-a}\\) \\(u_a(t)\\) \\(\\frac{e^{-as}}{s}\\) \\(\\delta (t)\\) \\(1\\)"},{"location":"2_Core/Math_3/14_Laplace/#unit-step-function","title":"Unit Step Function","text":"\\[ u_a (t) = \\begin{cases} 0, &amp; t &lt; a \\\\ 1, &amp; t \\ge a \\end{cases} \\]"},{"location":"2_Core/Math_3/14_Laplace/#unit-impulse-function","title":"Unit Impulse Function","text":"\\[ \\delta (t) = \\lim_{\\epsilon \\to 0} f_\\epsilon(t) \\] \\[ f_\\epsilon(t) = \\begin{cases} \\dfrac{1}{\\epsilon}, &amp; 0 \\le t \\le \\epsilon \\\\ 0, &amp; t &gt; \\epsilon \\end{cases} \\] \\[ \\begin{aligned} L\\Big( \\delta(t) \\Big) &amp;= \\lim_{\\epsilon \\to 0} L\\Big( f_\\epsilon (t) \\Big) \\\\ &amp;= \\lim_{\\epsilon \\to 0} \\left[     \\int\\limits_0^\\infty e^{-st}  f_\\epsilon(t) \\cdot dt \\\\ \\right] \\\\ &amp; \\dots \\\\ &amp;= 1 \\end{aligned} \\]"},{"location":"2_Core/Math_3/14_Laplace/#sum-of-gp","title":"Sum of GP","text":"\\[ \\sum GP = \\frac{a}{1-r} \\]"},{"location":"2_Core/Math_3/14_Laplace/#gamma-function","title":"Gamma Function","text":"\\[ \\Gamma(x) = \\int_0^\\infty e^{-x} x^{n-1} dx \\]"},{"location":"2_Core/Math_3/14_Laplace/#properties","title":"Properties","text":"\\[ \\begin{aligned} \\Gamma \\left(\\frac{1}{2} \\right) &amp;= \\sqrt{\\pi} \\\\ \\Gamma(n) &amp;= (n-1)! \\\\ &amp;= (n-1) \\cdot \\Gamma(n-1) \\\\ n! &amp;= \\Gamma (n+1) \\\\ \\end{aligned} \\]"},{"location":"2_Core/Math_3/14_Laplace/#idk","title":"IDK","text":"<p>When doing nested transformations, do it as Part 1 and \\(f(part 1)\\) like how you did it for grade 12 integrals \\(I_1 + I_2\\)</p>"},{"location":"2_Core/Math_3/15_Laplace_Derivatives%2C_Integrals/","title":"15 Laplace Derivatives, Integrals","text":""},{"location":"2_Core/Math_3/15_Laplace_Derivatives%2C_Integrals/#derivatives","title":"Derivatives","text":"\\[ \\begin{aligned} L\\{ f'(t) \\} &amp;= s F(s) - f(0) \\\\ L\\{ f''(t) \\} &amp;= s^2 F(s) - sf(0) - f'(0) \\\\ \\Big( L\\{ f(t) \\} &amp;= F(s) \\Big) \\end{aligned} \\] \\[ \\begin{aligned} f(0) &amp;= \\{ f(t) \\}_{t = 0} \\\\ f'(0) &amp;= \\left\\{ \\frac{d f(t)}{dt} \\right\\}_{t = 0} \\\\ f'(t) &amp;= \\frac{df}{dx}; f''(t) = \\frac{d^2f}{dx^2} \\end{aligned} \\]"},{"location":"2_Core/Math_3/15_Laplace_Derivatives%2C_Integrals/#i-missed-after-this","title":"I missed after this","text":"\\[ \\begin{aligned} L^{-1} \\left[ \\frac{F(s)}{s} \\right] &amp;= \\int\\limits_0^t L^{-1} \\Big( F(s) \\Big)  \\ dt\\\\ L^{-1} \\left[ \\frac{F(s)}{s^2} \\right] &amp;= \\int\\limits_0^t \\int\\limits_0^t L^{-1} \\Big( F(s) \\Big)  \\ dt\\\\ L^{-1} \\left[ \\frac{F(s)}{s^n} \\right] &amp;= \\text{n integrals from } 0 \\to t \\quad L^{-1} \\Big( F(s) \\Big)  \\ dt \\end{aligned} \\]"},{"location":"2_Core/Math_3/16_Convolution/","title":"16 Convolution","text":""},{"location":"2_Core/Math_3/16_Convolution/#definition","title":"Definition","text":"\\[ f(t) \\star g(t) = \\int\\limits_0^\\infty f(t-\\tau) g(\\tau) \\cdot d\\tau \\] \\[ f(t) \\star g(t) = g(t) \\star f(t) \\]"},{"location":"2_Core/Math_3/16_Convolution/#convolution-theorem","title":"Convolution Theorem","text":"<p>It is used for Laplace Transform</p> \\[ L \\{ f(t) \\star g(t) \\} = F(s) \\cdot G(s) \\] \\[ \\begin{aligned} L^{-1} \\{ F(s) \\cdot G(s) \\} &amp;= f(t) \\star g(t) \\\\ &amp;= L^{-1}\\{ F(s) \\} \\star L^{-1}\\{ G(s) \\} \\end{aligned} \\]"},{"location":"2_Core/Math_3/16_Convolution/#trignometric","title":"Trignometric","text":"\\[ \\begin{aligned} \\cos(x) &amp;= \\frac{     e^x \\textcolor{orange}{+} e^{-x} }{2} \\\\ \\sinh(x) &amp;= \\frac{     e^x \\textcolor{orange}{-} e^{-x} }{2} \\\\ \\cos(x) &amp;= \\frac{     e^{\\textcolor{hotpink}{i} x} \\textcolor{orange}{+} e^{-\\textcolor{hotpink}{i} x} }{2i} \\\\ \\sin(x) &amp;= \\frac{     e^{\\textcolor{hotpink}{i} x} \\textcolor{orange}{-} e^{-\\textcolor{hotpink}{i} x} }{2i} \\end{aligned} \\]"},{"location":"2_Core/Math_3/17_Fourier_Series/","title":"Fourier Series","text":"<p>Represent periodic signals in terms of cosines and sines.</p>"},{"location":"2_Core/Math_3/17_Fourier_Series/#periods","title":"Periods","text":"<p>A period signal repeats its pattern at some period \\(T\\).</p> <p>Fourier series of period signal can be used to analyze the signal in another domain.</p> <p>If \\(f(t)\\) is a function with period T, then</p> \\[ f(t+nT) = f(t) \\quad \\forall n \\] Function Period \\(\\cos \\theta, \\sin \\theta\\) \\(2 \\pi\\) \\(\\cos (n\\theta), \\sin (n\\theta)\\) \\(\\dfrac{2\\pi}{n}\\)"},{"location":"2_Core/Math_3/17_Fourier_Series/#fourier-series_1","title":"Fourier Series","text":"<p>of a function \\(f(x)\\) of period \\(2\\pi\\) in the interval \\([-\\pi, +\\pi]\\), is defined as</p> \\[ f(x) = \\frac{a_0}{ \\textcolor{orange}{2} } + \\sum\\limits_{n=1}^\\infty a_n \\cos(nx) + \\sum\\limits_{n=1}^{\\infty} b_n \\sin (nx) \\] <p>It is always continuous.</p> <p>Whenever possible, we have to make it into regular summation, ie from \\(1 \\to \\infty\\).</p>"},{"location":"2_Core/Math_3/17_Fourier_Series/#fourier-constants","title":"Fourier Constants","text":"\\[ \\begin{aligned} a_0 &amp;= \\frac{1}{\\pi} \\int\\limits_{-\\pi}^{\\pi} f(x) \\cdot dx \\\\ a_n &amp;= \\frac{1}{\\pi} \\int\\limits_{-\\pi}^{\\pi} f(x) \\textcolor{orange}{\\cos(nx)} \\cdot dx \\\\ b_n &amp;= \\frac{1}{\\pi} \\int\\limits_{-\\pi}^{\\pi} f(x) \\textcolor{orange}{\\sin(nx)} \\cdot dx \\end{aligned} \\]"},{"location":"2_Core/Math_3/17_Fourier_Series/#sum-of-functions","title":"Sum of Functions","text":"\\[ FS(g_1 \\pm g_2) = FS(g_1) \\pm FS(g_2) \\]"},{"location":"2_Core/Math_3/17_Fourier_Series/#dirchelet-condition","title":"Dirchelet Condition","text":"<p>Even though the function that the FS represents may be discontinuous, the FS itself will be continuous</p> \\[ \\text{FS} \\stackrel{\\text{converges}}{\\longrightarrow} \\begin{cases} f(a) &amp;, a = \\text{Continuous Point} \\\\ \\dfrac{f(a^-) + f(a^+)}{2} &amp;, a = \\text{Discontinuous Point} \\end{cases} \\]"},{"location":"2_Core/Math_3/17_Fourier_Series/#evenodd-functions","title":"Even/Odd Functions","text":"Even Odd \\(f(-x)\\) \\(f(x)\\) \\(-f(x)\\) Fourier Series \\(\\dfrac{a_0}{2} + \\sum\\limits_{n=1}^\\infty a_n \\cos(nx)\\) \\(\\sum\\limits_{n=1}^\\infty b_n \\sin(nx)\\) \\(a_0\\) \\(\\dfrac{2}{\\pi} \\int\\limits_0^\\pi f(x) dx\\) 0 \\(a_n\\) \\(\\dfrac{2}{\\pi} \\int\\limits_0^\\pi f(x) \\cos(nx) dx\\) 0 \\(b_n\\) 0 \\(\\dfrac{2}{\\pi} \\int\\limits_0^\\pi f(x) \\sin(nx) dx\\) <p>This is because \\(\\int f(x) dx = 0\\) when \\(f(x)\\) is even</p>"},{"location":"2_Core/Math_3/17_Fourier_Series/#note","title":"Note","text":"<p>Consider</p> \\[ \\begin{aligned} f(x) &amp;= \\begin{cases}     g_1(x), &amp; (-a, 0) \\\\     g_2(x), &amp; (0, a) \\end{cases}\\\\ \\implies  f(x) &amp;= \\begin{cases} \\text{Even}, &amp; g_1(-x) = +g_2(x) \\\\ \\text{Odd},  &amp; g_1(-x) = -g_2(x) \\end{cases} \\end{aligned} \\]"},{"location":"2_Core/Math_3/17_Fourier_Series/#grahphically","title":"Grahphically","text":"<p>We can also plot the points for \\(x = \\{-\\pi, 0, - \\pi\\}\\). Connect the points.</p> Function Type Symmetric about Even Y axis Odd Origin"},{"location":"2_Core/Math_3/17_Fourier_Series/#sinecosine-series","title":"Sine/Cosine Series","text":"<p>Special types of series, where we represent the fourier series in terms of \\(\\sin\\) alone or \\(\\cos\\) alone in half interval \\((0,\\pi)\\)</p> <p>Sine/Cosine series may be asked for an odd/even function.</p> Cosine Series Sine Series Fourier Series \\(\\dfrac{a_0}{2} + \\sum\\limits_{n=1}^\\infty a_n \\cos(nx)\\) \\(\\sum\\limits_{n=1}^\\infty b_n \\sin(nx)\\) \\(a_0\\) \\(\\dfrac{2}{\\pi} \\int\\limits_0^\\pi f(x) dx\\) 0 \\(a_n\\) \\(\\dfrac{2}{\\pi} \\int\\limits_0^\\pi f(x) \\cos(nx) dx\\) 0 \\(b_n\\) 0 \\(\\dfrac{2}{\\pi} \\int\\limits_0^\\pi f(x) \\sin(nx) dx\\)"},{"location":"2_Core/Math_3/17_Fourier_Series/#arbitrary-interval","title":"Arbitrary Interval","text":"<p>Fourier series of \\(f(x)\\) of period \\(2l\\) defined in the interval \\((-l, l), l \\in R\\) is</p> \\[ f(x) = \\frac{     a_0 }{     \\textcolor{orange}{2} } + \\sum_{n=1}^\\infty a_n \\cos \\left(     \\frac{n \\textcolor{hotpink}{\\pi} x}{\\textcolor{hotpink}{l}} \\right) + \\sum_{n=1}^\\infty b_n \\sin \\left(     \\frac{n \\textcolor{hotpink}{\\pi} x}{\\textcolor{hotpink}{l}} \\right) \\] \\[ \\begin{aligned} a_0 &amp;= \\frac{1}{\\textcolor{hotpink}{l}} \\int\\limits_{-l}^l f(x) dx \\\\ a_n &amp;= \\frac{1}{\\textcolor{hotpink}{l}} \\int\\limits_{-l}^l f(x) \\cos \\left(     \\frac{n \\textcolor{hotpink}{\\pi} x}{\\textcolor{hotpink}{l}} \\right) dx \\\\ b_n &amp;= \\frac{1}{\\textcolor{hotpink}{l}} \\int\\limits_{-l}^l f(x) \\sin \\left(\\frac{n \\textcolor{hotpink}{\\pi} x}{\\textcolor{hotpink}{l}} \\right) dx \\end{aligned} \\]"},{"location":"2_Core/Math_3/17_Fourier_Series/#changes-in-interval","title":"Changes in Interval","text":"From To For \\((-\\pi, \\pi)\\) \\((-l, l)\\) FS \\((0, \\pi)\\) \\((0, l)\\) CS and SS \\(\\cos(nx)\\) \\(\\cos \\left( \\frac{n \\pi x}{l} \\right)\\) FS, CS, SS \\(\\sin(nx)\\) \\(\\sin \\left( \\frac{n \\pi x}{l} \\right)\\) FS, CS, SS \\(\\frac{1}{\\pi} \\int_{-\\pi}^\\pi\\) \\(\\frac{1}{l} \\int_{-l}^l\\) FS, CS, SS \\(\\frac{2}{\\pi} \\int_0^\\pi\\) \\(\\frac{2}{l} \\int_0^l\\) FS, CS, SS"},{"location":"2_Core/Math_3/17_Fourier_Series/#bernoullis-integration-chain-rule","title":"Bernoulli\u2019s Integration Chain Rule","text":"\\[ \\int(uv) dx = u v_1 - u' v_2 - u'' v_3 - \\ldots - u^{(n-1)} v_n \\] Term Meaning \\(u, v\\) Given Functions \\(u', u'', \\dots\\) Derivatives \\(v_1, v_2, v_3, \\dots\\) Integrals"},{"location":"2_Core/Math_3/18_Application_of_Fourier_Series/","title":"18 Application of Fourier Series","text":""},{"location":"2_Core/Math_3/18_Application_of_Fourier_Series/#1d-wave-equation","title":"1D Wave Equation","text":"<p>Assuming that the vibration only happens in one direction.</p> \\[ \\begin{aligned} a^2 &amp;= \\frac{T}{m} &gt; 0 \\\\ \\frac{\\partial^2 y}{\\partial t^2} &amp;= a^2 \\left(     \\frac{\\partial^2 y}{\\partial x^2} \\right) \\end{aligned} \\] <p>\\(a \\ne\\) acceleration</p>"},{"location":"2_Core/Math_3/18_Application_of_Fourier_Series/#conditions","title":"Conditions","text":"For Initial Vertical Displacement at Left End \\(y(0, t)\\) \\(0\\) \\(\\forall t\\) Initial Vertical Displacement at Right End \\(y(\\pi, t)\\) \\(0\\) \\(\\forall t\\) Vertical Velocity \\(\\frac{\\partial y}{\\partial t}(x, 0)\\) \\(0\\) \\(0 \\le x \\le \\pi\\) The function \\(y(x, 0)\\) \\(f(x)\\) \\(0 \\le x \\le \\pi\\)"},{"location":"2_Core/Math_3/18_Application_of_Fourier_Series/#solution","title":"Solution","text":"<p>Solution of equation under the initial conditions</p> \\[ \\begin{aligned} y(x, t) &amp;= \\sum_{n = 1}^\\infty b_n \\sin(nx) \\textcolor{hotpink}{\\cos (nat)} \\\\ b_n &amp;= \\frac{2}{\\pi} \\int\\limits_0^\\pi f(x) \\sin(nx) dx \\end{aligned} \\]"},{"location":"2_Core/Math_3/18_Application_of_Fourier_Series/#1d-heat-equation","title":"1D Heat Equation","text":""},{"location":"2_Core/Math_3/18_Application_of_Fourier_Series/#fourier-thermal-law","title":"Fourier Thermal Law","text":"<p>The amount of heat flowing through a heat-producing body \\(H\\)</p> <ul> <li>\\(H \\propto\\) temperature gradient</li> <li>\\(H \\propto\\) area of cross-section</li> <li>\\(H \\frac{1}{\\propto}\\) resistance</li> </ul> <p>Time 0 is the time at which the external temperature is placed</p>"},{"location":"2_Core/Math_3/18_Application_of_Fourier_Series/#formula","title":"Formula","text":"<p>\\(\\alpha^2\\) is thermal diffusability.</p> \\[ \\begin{aligned} \\alpha^2 &amp;= \\frac{k}{\\rho c} &gt; 0\\\\ \\frac{\\partial u}{\\partial t} &amp;= \\alpha^2 \\left( \\frac{\\partial^2 u}{\\partial x^2} \\right) \\end{aligned} \\]"},{"location":"2_Core/Math_3/18_Application_of_Fourier_Series/#conditions_1","title":"Conditions","text":"For Initial Heat at Left End \\(u(0, t)\\) 0 \\(\\forall t\\) Initial Heat at Right End \\(u(\\pi, t)\\) 0 \\(\\forall t\\) \\(u(x, 0)\\) \\(f(x)\\) \\(0 \\le x \\le \\pi\\)"},{"location":"2_Core/Math_3/18_Application_of_Fourier_Series/#solution_1","title":"Solution","text":"<p>Solution of equation under the initial conditions</p> \\[ \\begin{aligned} u(x, t) &amp;= \\sum_{n = 1}^\\infty b_n \\sin (nx) \\textcolor{hotpink}{e^{-n^2 \\alpha^2 t}} \\\\ b_n &amp;= \\frac{2}{\\pi} \\int\\limits_0^\\pi f(x) \\sin(nx) dx \\end{aligned} \\]"},{"location":"2_Core/Math_3/19_Power_Series/","title":"19 Power Series","text":""},{"location":"2_Core/Math_3/19_Power_Series/#power-series","title":"Power Series","text":"<p>An infinite series in \\(x\\) of the form</p> \\[ \\sum_{n=0}^\\infty a_n x^n = a_0 + a_1 x + a_2 x^2 + \\dots + a_r x^r + \\dots \\] <p>where \\(\\{ a_0, a_1, a_2, \\dots \\}\\) are constants</p> <p>equation is convergent only when \\(x \\to 0\\)</p>"},{"location":"2_Core/Math_3/19_Power_Series/#non-algebraic-elementary-functions","title":"Non-Algebraic Elementary Functions","text":"<p>Transcendental means non-algebraic</p> Function Power Series Intuition \\(e^x\\) \\(1 + \\frac{x}{1!} + \\frac{x^2}{2!} + \\dots\\) \\(\\cos x\\) \\(1 - \\frac{x^2}{2!} + \\frac{x^4}{4!} + \\dots\\) Even function so even numbers \\(\\sin x\\) \\(x - \\frac{x^3}{3!} + \\frac{x^5}{5!} + \\dots\\) Odd function so odd numbers \\(\\cosh x\\) \\(1 + \\frac{x^2}{2!} + \\frac{x^4}{4!} + \\dots\\) \\(\\sinh x\\) \\(x + \\frac{x^3}{3!} + \\frac{x^5}{5!} + \\dots\\) \\(\\log(1+x)\\) \\(x - \\frac{x^2}{2} + \\frac{x^3}{3} - \\frac{x^4}{4} + \\dots\\) \\((1+x)^{-1}\\) \\(1 - x + x^2 - x^3 + \\dots\\) \\((1-x)^{-1}\\) \\(1 + x + x^2 + x^3 + \\dots\\)"},{"location":"2_Core/Math_3/19_Power_Series/#solving","title":"Solving","text":"\\[ \\begin{aligned} y &amp;= \\sum_{n=0}^\\infty a_n x^n &amp;&amp;= a_0 + a_1 x + a_2 x^2 + a_3 x^3 + \\dots \\\\ y' &amp;= \\sum_{n=1}^\\infty a_n n x^{n-1} &amp;&amp;= a_1 + 2a_2 x + 3a_3 x^2 + \\dots \\\\ y'' &amp;= \\sum_{n=2}^\\infty a_n n(n-1) x^{n-2} &amp;&amp;= 2a_2 + (3 \\cdot 2) a_3 x + \\dots \\\\ \\end{aligned} \\]"},{"location":"2_Core/Math_3/19_Power_Series/#comparing-coefficients","title":"Comparing Coefficients","text":"<p>By changing index and re-arranging terms, we have to make the following equal</p> <ol> <li>counter start</li> <li>\\(x\\) power</li> </ol>"},{"location":"2_Core/Math_3/19_Power_Series/#2nd-order","title":"2<sup>nd</sup> Order","text":"<p>Power series solution is only possible if \\(x = 0\\) is an ordinary point of the DE.</p>"},{"location":"2_Core/Math_3/19_Power_Series/#types-of-points","title":"Types of Points","text":"<p>Consider a general 2<sup>nd</sup> order differential equation with polynomials \\(P_1, P_2, P_3\\).</p> \\[ \\begin{aligned} P y'' + Q y' + R y &amp;= 0 \\\\ \\implies y'' + \\frac{Q}{P} y' + \\frac{R}{P} y &amp;= 0 \\end{aligned} \\] Types \\(P(a)\\) Ordinary Point \\(\\ne 0\\) Singular Point \\(= 0\\)"},{"location":"2_Core/Math_3/19_Power_Series/#ordinary-point","title":"Ordinary Point","text":"<p>\\(x=a\\) is an ordinary point of DE equation, if \\(P(a) \\ne 0\\).</p>"},{"location":"2_Core/Math_3/19_Power_Series/#power-series-solution","title":"Power Series Solution","text":"<p>The power series solution of equation is given by</p> \\[ y = \\sum_{n=0}^{\\infty} a_n x^n \\]"},{"location":"2_Core/Math_3/19_Power_Series/#general-solution","title":"General Solution","text":"<p>Solving equation as</p> \\[ y = a(\\text{PS}_1) + b(\\text{PS}_2) \\] <p>where</p> <ul> <li>PS<sub>1</sub> and PS<sub>2</sub> are congruent and linearly-independent power series, for \\(x \\to 0\\)</li> <li>\\(a\\) and \\(b\\) are arbitrary constants</li> </ul>"},{"location":"2_Core/Math_3/19_Power_Series/#singular-points","title":"Singular Points","text":"<p>Consider limits</p> \\[ \\begin{aligned} p&amp;= \\lim_{x \\to a} (x-a) &amp;\\frac{Q(x)}{P(x)}\\\\ q&amp;= \\lim_{x \\to a} (x-a)^{\\textcolor{hotpink}{2}} &amp;\\frac{R(x)}{P(x)} \\end{aligned} \\] Both limits exist Point Type \u2705 Regular \u274c Irregular"},{"location":"2_Core/Math_3/19_Power_Series/#frobenius-series-method","title":"Frobenius Series Method","text":"<p>Differential equations with regular singular points at \\(x=0\\) can be solved using a power series of the form</p> \\[ \\begin{aligned} y &amp;= x^m \\sum_{n=0}^\\infty a_n x^n \\\\ &amp;= \\sum_{n=0}^\\infty a_n x^{m+n} \\end{aligned} \\] <p>where \\(m\\) is constant coefficient called as root/indical/initial value. This is singular points (to be calculated).</p>"},{"location":"2_Core/Math_3/19_Power_Series/#trick-to-find-indical-value","title":"Trick to find indical value","text":"\\[ m(m-1) + p m + q = 0 \\] <p>where \\(p\\) and \\(q\\) are limits from equation</p>"},{"location":"2_Core/Math_3/20_Hyper_Geometric/","title":"20 Hyper Geometric","text":"<p>if \\(a\\) and/or \\(b\\) are negative integers, then it will become a polynomial of degree \\(n\\).</p>"},{"location":"2_Core/Math_3/20_Hyper_Geometric/#general-form","title":"General Form","text":"<p>something</p> \\[ (1-x^2) \\]"},{"location":"2_Core/Math_3/20_Hyper_Geometric/#standard-form","title":"Standard Form","text":"\\[ x(1-x) y'' + \\Big[c - (a+b+1)x \\Big]y' - (ab)y = 0 \\] <p>where \\(a, b, c\\) are real constants</p> <p>\\(x=0, x=1\\) are the regular singular points of equation</p> <p>By Frobenius Series method, at regular singular points, we get 2 initial roots</p> <ul> <li>\\(m=0\\)</li> <li>\\(m=1-c\\)</li> </ul> \\(m=0\\) \\(m=1-c\\) Solution \\(y\\) \\(1 + \\frac{a \\cdot b}{1 \\cdot c}x + \\frac{a(a\\textcolor{hotpink}{+1}) \\cdot b(b\\textcolor{hotpink}{+1})}{1(1\\textcolor{hotpink}{+1}) \\cdot c(c\\textcolor{hotpink}{+1})} x^2 + \\\\ \\frac{a(a\\textcolor{hotpink}{+1})(a\\textcolor{orange}{+2}) \\cdot b(b\\textcolor{hotpink}{+1})(b\\textcolor{orange}{+2})}{1(1\\textcolor{hotpink}{+1})(1\\textcolor{orange}{+2}) \\cdot c(c\\textcolor{hotpink}{+1})(c\\textcolor{orange}{+2})} x^3 + \\dots\\) \\(x^{\\textcolor{hotpink}{1-c}} \\times F(a + \\textcolor{hotpink}{1-c}, b+ \\textcolor{hotpink}{1-c}, 2-c, x)\\) \\(F(a, b, c , x) = F(b, a, c, x)\\)Commutative Constant Outcome \\(a \\le 0\\) or \\(b \\le 0\\) Series breaks off into finite polynomial \\(c\\le 0\\) Solution doesn\u2019t exist"},{"location":"2_Core/Math_3/21_Legendre/","title":"21 Legendre","text":""},{"location":"2_Core/Math_3/21_Legendre/#legendre-de","title":"Legendre DE","text":"\\[ (1-x^2) y'' - 2xy' + n(n+1)y = 0 \\]"},{"location":"2_Core/Math_3/21_Legendre/#solution","title":"Solution","text":"<p>Solution of equation, at the 2 singular points \\(x = \\pm 1\\).</p> <p>We will get</p> \\[ t(t-1)y'' + (1-2t)y' + n(n+1)y = 0 \\] <p>equation is a hyper-geometric function.</p>"},{"location":"2_Core/Math_3/21_Legendre/#solution-of-equation-near-t0","title":"Solution of equation near \\(t=0\\)","text":"\\[ y = F(-n, n+1, 1, t) \\]"},{"location":"2_Core/Math_3/21_Legendre/#solution-of-equation-near-x0","title":"Solution of equation near \\(x=0\\)","text":"\\[ P_n(x) = y = F \\left(-n, n+1, 1, \\frac{1-x}{2} \\right) \\] <p>This is a legendre polynomial of degree \\(n\\).</p>"},{"location":"2_Core/Math_3/21_Legendre/#rodrigues-formula","title":"Rodrigue\u2019s Formula","text":"\\[ P_n(x) = \\frac{1}{2^n \\cdot n!} \\left[     \\frac{d^n}{d x^n} (x^2 - 1)^n \\right] \\] \\[ \\begin{aligned} P_0(x) &amp;= 1 \\\\ P_1(x) &amp;= x \\\\ P_2(x) &amp;= \\frac{1}{2}(3x^2 - 1) \\\\ P_3(x) &amp;= \\frac{1}{2}(5x^3 - 3x) \\end{aligned} \\]"},{"location":"2_Core/Math_3/21_Legendre/#generating-function","title":"Generating Function","text":"\\[ (1-2xt + t^2)^{-1/2} = \\sum\\limits_{n=0}^\\infty P_n (x) t^n \\\\ |t| &lt; 1 \\\\ |x| \\le 1 \\]"},{"location":"2_Core/Math_3/21_Legendre/#binomial-expansion","title":"Binomial Expansion","text":"\\[ (1+t^2)^{-1/2} = \\]"},{"location":"2_Core/Math_3/21_Legendre/#legendre-series","title":"Legendre Series","text":"<p>Similar to Fourier Series, Any function \\(f(x)\\) can be represented as</p> \\[ \\begin{aligned} f(x) &amp;= \\sum_{n=0}^\\infty a_n P_n(x) \\\\ a_n &amp;= \\frac{2n+1}{2} \\int_{-1}^1 f(x) P_n(x) dx \\end{aligned} \\]"},{"location":"2_Core/Math_3/22_Bessel/","title":"22 Bessel","text":""},{"location":"2_Core/Math_3/22_Bessel/#bessels-de","title":"Bessel\u2019s DE","text":"<p>Family of differential equation, with some constant value \\(p\\)</p> \\[ x^2y'' + xy' + (x^2-p^2) y = 0 \\]"},{"location":"2_Core/Math_3/22_Bessel/#bessels-function","title":"Bessel\u2019s Function","text":"<p>is the solution of Bessel\u2019s DE. Denoted by \\(J_p(x)\\)</p> <p>\\(x=0\\) is a regular singular point of equation. Solving using Frobenieus Series method gives 2 initial roots as \\(m = \\pm p\\)</p> \\(+p\\) \\(-p\\) \\(J(x)\\) \\(\\sum\\limits_{n=0}^\\infty \\dfrac{(-1)^n \\left(\\frac{x}{2}\\right)^{2n \\textcolor{hotpink}{+p}}}{n!(n \\textcolor{hotpink}{+p})!}\\) \\(\\sum\\limits_{n=0}^\\infty \\dfrac{(-1)^n \\left(\\frac{x}{2}\\right)^{2n \\textcolor{hotpink}{-p} }}{n!(n \\textcolor{hotpink}{-p} )!}\\) <p>The above 2 formula are not directly possible for negative integers, as \\((n-p)!\\) is not valid when it is negativeUse gamma function</p>"},{"location":"2_Core/Math_3/22_Bessel/#general-solution","title":"General Solution","text":"\\[ y = c_1 J_p(x) + c_2 J_{-p} (x) \\]"},{"location":"2_Core/Math_3/22_Bessel/#properties","title":"Properties","text":""},{"location":"2_Core/Math_3/22_Bessel/#to-remember","title":"To Remember","text":"\\[ \\begin{aligned} J_\\frac{1}{2}(x) &amp;= \\sin x \\sqrt{     \\frac{2}{\\pi x} } \\\\ J_\\frac{-1}{2}(x) &amp;= \\cos x \\sqrt{     \\frac{2}{\\pi x} } \\\\ J_{p-1}(x) + J_{p+1}(x) &amp;= \\frac{2p}{x} J_p(x) \\end{aligned} \\]"},{"location":"2_Core/Math_3/22_Bessel/#other-properties","title":"Other Properties","text":"\\[ \\begin{aligned} \\Big( x^{p} J_p(x) \\Big)' &amp;= x^{p} J_{p-1} (x) \\\\ \\Big( x^{-p} J_p(x) \\Big)' &amp;= - x^{-p} J_{p+1} (x) \\\\ {J_p}'(x) + \\frac{p}{x} J_p(x) &amp;= J_{p-1}(x) \\\\ {J_p}'(x) - \\frac{p}{x} J_p(x) &amp;= - J_{p+1}(x) \\\\ J_{p-1}(x) - J_{p+1}(x) &amp;= 2 {J_p}'(x) \\end{aligned} \\]"},{"location":"2_Core/Math_3/23_Strum-Liouvill/","title":"23 Strum Liouvill","text":"<p>Consider the DE with scalar \\(\\lambda\\) defined in \\([a,b]\\)</p> \\[ \\frac{d}{dx} \\Big[     P(x) y' \\Big] + \\Big[\\lambda Q(x) + R(x) \\Big] y = 0 \\] <p>with the boundary conditions</p> \\[ \\begin{aligned} c_1 y(a) + c_2 y'(a) &amp;= 0 &amp; d_1 y(b) + d_2 y'(b) &amp;= 0 \\\\ c_1 \\text{ or } c_2 &amp;= 0 &amp; d_1 \\text{ or } d_2 &amp;= 0 \\end{aligned} \\]"},{"location":"2_Core/Math_3/23_Strum-Liouvill/#simplest-form","title":"Simplest Form","text":"\\[ \\begin{aligned} y'' + \\lambda y &amp;= 0 \\\\ P(x) &amp;= 1 \\\\ Q(x) &amp;= 1 \\\\ R(x) &amp;= 0 \\end{aligned} \\]"},{"location":"2_Core/Math_3/23_Strum-Liouvill/#legendre-equation","title":"Legendre Equation","text":"<p>Legendre Equation can be represented as Strum-Liouvile Problem.</p> \\[ \\frac{d}{dx} \\Big[     \\underbrace{(1-x^2)}_{P(x)}     y' \\Big] + \\underbrace{n(n+1)}_{\\lambda} \\ y = 0 \\\\ P(x) = 1-x^2 \\\\ Q(x) = 1 \\\\  R(x) = 0 \\\\ \\lambda = n(n+1) \\] <p>Here, \\(\\lambda\\) is the eigen value of equation</p> <p>The corresponding solutions are \\(P_n(x), n = 1, 2, \\dots\\) They are called as eigen functions.</p> <p>\\(n &gt; 0\\) because \\(n \\le 0\\) will give trivial solution.</p>"},{"location":"2_Core/Math_3/23_Strum-Liouvill/#eigen-valuefunction","title":"Eigen Value/Function","text":"\\[ \\begin{aligned} y'' + \\lambda y &amp;= 0 \\\\ y(a) &amp;= 0 \\\\ y(b) &amp;= 0 \\\\ a &amp; \\ne b \\end{aligned} \\]"},{"location":"2_Core/Math_3/24_System_of_DE/","title":"24 System of DE","text":"<p>Consider 2 dependent variables \\(x(t), y(t)\\).</p> \\[ a_1 + something \\\\ a_2 + something \\] <p>These equations are to solved simultaneously.</p>"},{"location":"2_Core/Math_3/25_Chebyshev/","title":"25 Chebyshev","text":""},{"location":"2_Core/Math_3/25_Chebyshev/#chebyshev-de","title":"Chebyshev DE","text":"\\[ (1-x^2)y'' - xy' + n^2 y =0 \\\\ n&gt;0 \\] <p>\\(x = \\pm 1\\) are the regular singular points of equation</p>"},{"location":"2_Core/Math_3/25_Chebyshev/#chebyshev-polynomials","title":"Chebyshev Polynomials","text":"<p>are solutions of Chebyshev DE</p> <p>Using Frobenius method near \\(x=1\\), we get the solution</p> \\[ T_n = F \\left( n, -n, \\frac{1}{2}, \\frac{1-x}{2} \\right) \\] <p>equation is a finite polynomial of degree \\(n\\), as \\(b=-n\\) (-ve integer)</p> <p>Using transformation \\(x=\\cos \\theta\\) in equation, we get another solution</p> \\[ T_n = \\cos(n \\theta), \\quad \\theta = \\cos^{-1}x \\]"},{"location":"2_Core/Math_3/25_Chebyshev/#eulers-theorem","title":"Euler\u2019s Theorem","text":"\\[ \\begin{aligned} e^{i\\theta} &amp;= \\cos \\theta + i \\sin \\theta \\\\ e^{ni\\theta} &amp;= \\cos n\\theta + i \\sin n\\theta \\\\ &amp;= (\\cos \\theta + i \\sin \\theta)^n \\\\ \\end{aligned} \\]"},{"location":"2_Core/MicroProcessors_%26_Interfacing/","title":"Microprocessors and Interfacing","text":"Class Instructor Lecture Dr. MB Srinivas Tutorial Dr. Jagadish Nayak Practical Dr. Shazia Hasan <p>This course provides an essential introduction to processor instruction set architecture (ISA), assembly programming, and both computer and embedded architecture, with a focus on the Intel 80x86 platform. It covers the programmer's model for processors from the 8086 to the 80486, exploring topics such as instruction sets, modular assembly programming techniques, and timing diagrams.</p> <p>Students will learn about interrupts\u2014both hardware and software\u2014along with interrupt handling techniques and controllers. The curriculum also addresses memory types and interfacing, programmable peripheral devices, I/O interfacing, and DMA controllers, equipping students with the skills necessary for designing processor-based systems.</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/01_Intro/","title":"01 Intro","text":""},{"location":"2_Core/MicroProcessors_%26_Interfacing/01_Intro/#what-happens-when-we-switch-on-computer","title":"What happens when we switch on computer","text":"<ol> <li>BIOS(Basic Input Output System) loaded from the ROM(Read-Only Memory)</li> <li>BIOS loads OS into the RAM    RAM should atleast be the size of the OS</li> <li>OS loads programs from disk to RAM    Program: set of instructions executed by \\(\\mu p\\)</li> </ol>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/01_Intro/#what-is-an-instruction","title":"What is an instruction?","text":"<p>Tells the MP what actions to perform</p> <ul> <li>operations<ul> <li>logic</li> <li>arithmetic</li> </ul> </li> <li>read data from input device</li> <li>write to memory</li> <li>reset</li> <li>stop</li> </ul> <p>Assembly program gives these instructions. Each microprocessor understands these instructions in different ways.</p> <p>High-level program use statements which get compiled/interpreted into machine language. Allows programmer focus on the logic, rather than worrying about how it will understood by the MP.</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/01_Intro/#microprocessor","title":"Microprocessor","text":"<p>has both sequential and combinational circuits</p> <ul> <li>Control unit has sequential circuits</li> <li>ALU has combinational circuits</li> </ul> <p>Size of the processor = size of ALU</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/01_Intro/#instruction-handling","title":"Instruction-Handling","text":"<p>Instructions is a set of command, used by the mp to perform to certain taks</p> <ol> <li>Fetch Instruction    Instruction taken from the memory and stored in instruction register</li> <li>Decode Instruction</li> <li>Fetch Operand(s)</li> <li>Execute Cycle    actions are performed</li> <li>Store result</li> </ol> <p>Make this into block diagram</p> <ul> <li>BIU - Bus Interface Unit</li> <li>ALU - Arithmetic Logic Unit</li> <li>Execution Unit</li> </ul>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/01_Intro/#overview","title":"Overview","text":"<p>Microcontrollers like Arduino have the memory also embedded to the processor</p> <p></p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/01_Intro/#detailed","title":"Detailed","text":""},{"location":"2_Core/MicroProcessors_%26_Interfacing/01_Intro/#architectures","title":"Architectures","text":""},{"location":"2_Core/MicroProcessors_%26_Interfacing/01_Intro/#instruction-set-architecture","title":"Instruction Set Architecture","text":"<p>This is the design/theory</p> <ol> <li>Execution Model</li> <li>Processor Register</li> <li>Address and Data Formats</li> </ol>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/01_Intro/#microarchitecture","title":"Microarchitecture","text":"<p>This is the hardware components</p> <ol> <li>Interconnections between elements</li> <li>ALU</li> <li>Data Path</li> <li>Control Path</li> </ol>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/01_Intro/#types-of-mp","title":"Types of MP","text":"RISC CISC Full Form Reduced Instruction Set Computing Complex Instruction Set Computing Amount of Instructions Small Large Decoder Reduced Instruction Decoder Complex Instruction Decoder Architecture Register only Register-Memory Speed Fast Usage Real-Time Operations Application IOT (Internet of Things) Examples Microcontrollers like ArduinoARM x86 processors like 8086"},{"location":"2_Core/MicroProcessors_%26_Interfacing/01_Intro/#nehas-notes","title":"Neha\u2019s Notes","text":"<p>Open the PDF</p> <p>Neha</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/01_Intro/#x86-family","title":"x86 Family","text":"<ul> <li>CISC</li> <li>Instructions are broken into \\(\\micro\\) operations</li> </ul> <p>8086 has 1MB capacity</p> Bits Address Lines 8086 16 80286 16 80386 80486"},{"location":"2_Core/MicroProcessors_%26_Interfacing/01_Intro/#8086-tut","title":"8086 (Tut)","text":"Lines 20 \\(A_0 \\to A_{19}\\) Data bus 16 \\(A_0 \\to A_{15}\\) memory locations \\(2^{20}\\) Size of each memory location 1 byte 8 bits total memory 2 MB <p>Byte-organised</p> <p>We represent the address of each location in 5bit hex \\(00000H \\to FFFFFH\\)</p> <p>If we need to store 16bits of data</p> <ul> <li>we need 2 bytes</li> <li>so we will require 2 contigious memory locations</li> </ul>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/01_Intro/#memory-addressing","title":"Memory Addressing","text":"<p>Each range of addresses is allocated for different **segments **of registers</p> <ul> <li>Code segment</li> <li>Data segment</li> <li>Stack segment</li> <li>Extra segment</li> <li>Instruction pointer</li> </ul> <p>If CS = 10000 and offet = 0002, then DS = 10002</p> <p></p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/01_Intro/#internal-cache","title":"Internal Cache","text":"<p>a small and fast SRAM memory attached to the processor, for pre-fetching data</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/01_Intro/#registers","title":"Registers","text":""},{"location":"2_Core/MicroProcessors_%26_Interfacing/01_Intro/#idk","title":"IDK","text":"<p>The reason we're left-shifting by 1 digit is because</p> <ul> <li>Address is to be 20 bits (5hex digits)</li> <li>Pointer we want to hold 16bits (4 hex digits)   because the blocks in x86 architecture</li> </ul>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/02_Addressing/","title":"02 Addressing","text":""},{"location":"2_Core/MicroProcessors_%26_Interfacing/02_Addressing/#types-of-instructuctions","title":"Types of instructuctions","text":"<ul> <li>Data transfer</li> <li>Arithmetic</li> <li>Logical</li> <li>Branch(conditional) and program control</li> </ul>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/02_Addressing/#little-endian-format","title":"Little Endian Format","text":"<p>In little endian format adopted by Intel and most manufacturers, first the low byte gets stored and then the high byte.</p> <p>Consider a number \\(1234_H\\). It will be stored in memory as follows</p> \\[ \\underset{40000} {\\Large \\fbox{34$\\vphantom{0}$} } \\underset{40001} {\\Large \\fbox{12$\\vphantom{0}$} } \\]"},{"location":"2_Core/MicroProcessors_%26_Interfacing/02_Addressing/#addressing-modes","title":"Addressing Modes","text":"Addressing Register MOV AX, BX AX, BX are registers Immediate MOV AX, 1420<sub>H</sub> 1420H = value of data Direct MOV AX, [2340<sub>H</sub>] 2340 = offset address of DS Register Indirect MOV AX, [BX] BX is the pointer Base-Plus-Index MOV AX, [BX+SI] BX, SI are pointers Register relative MOV AX, BX[10] BX is the pointer Base relative-plus-indexed MOV AX, [BX+SI+10] Scaled Indexed MOV AX, [10BX]"},{"location":"2_Core/MicroProcessors_%26_Interfacing/02_Addressing/#instruction-format","title":"Instruction Format","text":"<p>The assembler converts assembly code into bytecode</p> <ul> <li>Mnemonics like <code>MOV</code>, <code>ADD</code> get converted into opcode</li> <li>Variable names get converted into addresses</li> </ul>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/02_Addressing/#register-addressing","title":"Register Addressing","text":"\\[ \\underbrace{ \\fbox{1} \\fbox{0} \\fbox{0} \\fbox{0} \\fbox{1}\\fbox{0} } _{\\text{Opcode}} \\underset{\\text{D}}{ \\fbox{1}} \\underset{\\text{W}}{ \\fbox{1}} \\underbrace{ \\fbox{1} \\fbox{1} } _{\\text{MOD}} \\underbrace{ \\fbox{0} \\fbox{1} \\fbox{1} } _{\\text{Reg}} \\underbrace{ \\fbox{0} \\fbox{1} \\fbox{1} } _{\\text{R/M}} \\]"},{"location":"2_Core/MicroProcessors_%26_Interfacing/02_Addressing/#meanings","title":"Meanings","text":"0 1 Opcode Operation Code D **D**irection From Reg To Reg W **W**ord Byte Word MOD Addressing **Mod**e of R/M Reg **Reg**ister R/M **R**egister/**M**emory Address"},{"location":"2_Core/MicroProcessors_%26_Interfacing/02_Addressing/#word","title":"Word","text":"W=0 W=1 AL AX CL CX DL DX BL BX AH SP CH BP DH SI DH DI"},{"location":"2_Core/MicroProcessors_%26_Interfacing/02_Addressing/#mod","title":"MOD","text":"MOD Addressing Mode 00 01 10 11 Register Addressing"},{"location":"2_Core/MicroProcessors_%26_Interfacing/02_Addressing/#reg","title":"Reg","text":"Register Code EAX, AX, AL 000 EBX, BX, BL 011 EAX, CX, CL <p>No need to learn 32bit encoding</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/03_Assembly_Programs/","title":"03 Assembly Programs","text":""},{"location":"2_Core/MicroProcessors_%26_Interfacing/03_Assembly_Programs/#types-of-instructions","title":"Types of Instructions","text":"<ol> <li>Data Transfer</li> <li>Arithmetic</li> <li>Logical</li> <li>Branch and Program Control</li> </ol>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/03_Assembly_Programs/#steps","title":"Steps","text":"<ol> <li>initialise segment register</li> </ol> <pre><code>mov ax, 2000h\nmov ds, ax\n</code></pre>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/03_Assembly_Programs/#mov","title":"<code>MOV</code>","text":"\\[ \\textcolor{orange}{ \\underbrace{\\text{MOV}}_\\text{Opcode} } \\ \\ \\textcolor{hotpink}{ \\underbrace{\\text{dest, src}}_\\text{Operands} } \\] <p>Destination/Source could be register/memory location. This is the data, and the operands of the operation</p> <p>4 bits are required to refer to a register: \\(0000-FFFF\\)</p> <p>MOV, ADD, etc\u2026 are called mnemonics</p> \\[ \\text{MOV dest, src} \\] <ul> <li>copies contents from src to dest</li> <li>no flags affected</li> <li>size of src and dest must be same; however smaller data can be inserted into bigger register</li> </ul>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/03_Assembly_Programs/#possible-options","title":"Possible Options","text":"<ul> <li>source can be register, memory location, immediate date</li> <li>dest can be register/memory location</li> </ul> From To Register Memory Memory Register Register Register Index Memory Index Register <p>You cannot do <code>MOV [1234] [5678]</code></p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/03_Assembly_Programs/#inc-dec","title":"<code>INC</code>, <code>DEC</code>","text":"\\[ \\text{INC dest} \\\\ \\text{DEC dest} \\] <p>increments/decrements the content of the affected register by 1.</p> <pre><code>inc ax\n\ninc word ptr[bx]\ninc byte ptr[bx] ; only low byte\n</code></pre>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/03_Assembly_Programs/#add","title":"<code>ADD</code>","text":""},{"location":"2_Core/MicroProcessors_%26_Interfacing/03_Assembly_Programs/#adc","title":"<code>ADC</code>","text":"<p>First you must use <code>CLC</code> to clear the carry flag.</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/03_Assembly_Programs/#equ","title":"<code>EQU</code>","text":"<p>used to assign value to a variable. It doesn\u2019t store anything in memory.</p> <pre><code>count equ 08h\nmov cl, count\n</code></pre>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/03_Assembly_Programs/#dup","title":"<code>DUP</code>","text":"<p>Duplicate</p> <pre><code>array db 5 dup(12h)\narray db 5 dup('A')\n</code></pre>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/03_Assembly_Programs/#flags","title":"Flags","text":"Flag Meaning High when AF Auxiliary internal carry (from lower nibble to higher nibble) CF Carry carry from the entire byte OF Overflow overflow PF Parity even parity (only follows low byte) SF Sign signed number ZF Zero data is 0"},{"location":"2_Core/MicroProcessors_%26_Interfacing/03_Assembly_Programs/#branch-instructions","title":"Branch Instructions","text":"<p>Jump means like <code>go to</code> in C++</p> JZ Jump on Zero JNZ Jump on Non-Zero JE Jump on Equal JNE Jump on Not Equal"},{"location":"2_Core/MicroProcessors_%26_Interfacing/04_String_Instructions/","title":"04 String Instructions","text":""},{"location":"2_Core/MicroProcessors_%26_Interfacing/04_String_Instructions/#string","title":"String","text":"<p>sequence of data bytes/words that are in consecutive memory locations</p> <p>Everywhere</p> <ul> <li>does not affect flags</li> <li>d = 0 -&gt; SI/DI inc</li> <li>d = 1 -&gt; SI/DI dec</li> </ul>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/04_String_Instructions/#movs","title":"<code>MOVS</code>","text":"<p>Moving Strings</p> <p>copies a byte/word/double word fro a location in the data segment to a location in the extra segment</p> <ul> <li>Source - DS:SI</li> <li>Destination - ES:DI</li> </ul> SI/DI inc/dec by <code>MOVSB</code> 1 <code>MOVSW</code> 2 <code>MOVSD</code> 4"},{"location":"2_Core/MicroProcessors_%26_Interfacing/04_String_Instructions/#lea","title":"<code>LEA</code>","text":"<p>Load effective address</p> <pre><code>lea si, array1\nlea di, array2\n</code></pre>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/04_String_Instructions/#lods","title":"LODS","text":"<p>Loads AL/AX/EAX witht the data stored at the data segment</p> <p>offset address indexed by si register</p> Equivalent <code>LODSB</code> AL = <code>LODSW</code> AX = <code>LODSD</code> EAX ="},{"location":"2_Core/MicroProcessors_%26_Interfacing/04_String_Instructions/#stos","title":"STOS","text":"<p>Stores AL/AX/EAX into the extra segment memory at offset address indexed by DI register.</p> Equivalent <code>STOSB</code> <code>STOSW</code> <code>STOSD</code>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/05_More_Instructions/","title":"05 More Instructions","text":""},{"location":"2_Core/MicroProcessors_%26_Interfacing/05_More_Instructions/#rotate","title":"Rotate","text":"<p>ROL/ROR</p> <p>Data does not get lost</p> <p>also, the value gets stored in carry flag</p> <p></p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/05_More_Instructions/#shift","title":"Shift","text":"<p>command is</p> <ul> <li>SAL/SHL</li> <li>SAR/SHR</li> </ul> <p>Shift each bit count times</p> <pre><code>sal dest, count\nshl dest, count\n</code></pre>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/05_More_Instructions/#multiplication","title":"Multiplication","text":"<ul> <li>Mul - unsigned</li> <li>Imul - signed</li> </ul> <pre><code>mul src\n\nmul cx ; ax\nmul cl ; al\n</code></pre> <p>Src times</p> <ul> <li>AL</li> <li>AX</li> <li>EAX</li> </ul> <p>Source can be a register or memory location</p> Multiplication Result Storage Byte AX Word DX:AX Dword EDX:EAX <ul> <li>CF and OF are zero if MSB/MSW/MSD zero</li> <li>AF, PF, SF, ZF - undefined</li> <li>CBW/CWD</li> </ul>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/05_More_Instructions/#conversion","title":"Conversion","text":"<ul> <li><code>CBW</code> converts byte to word</li> <li><code>CWD</code> converts word to double word</li> </ul> <p>When MSB is</p> <ul> <li>0, 0s are added</li> <li>1, 1s are added</li> </ul>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/05_More_Instructions/#division","title":"Division","text":""},{"location":"2_Core/MicroProcessors_%26_Interfacing/05_More_Instructions/#div","title":"<code>div</code>","text":"8bit 16bit dividend AX AX divisor BX BX quotient AL AX remainder AH DX"},{"location":"2_Core/MicroProcessors_%26_Interfacing/05_More_Instructions/#idiv","title":"<code>idiv</code>","text":""},{"location":"2_Core/MicroProcessors_%26_Interfacing/06_Jumps%2C_Loops/","title":"06 Jumps, Loops","text":""},{"location":"2_Core/MicroProcessors_%26_Interfacing/06_Jumps%2C_Loops/#jumps","title":"Jumps","text":"Jump Displacement Range Short 8 bits \\(-128 \\iff 127\\) Near 16 bits"},{"location":"2_Core/MicroProcessors_%26_Interfacing/06_Jumps%2C_Loops/#loops","title":"Loops","text":"<pre><code>count db 09h\nmov cx, count           ; initialization\n\nrepeat:\n    ; code\n    loop repeat\n\n; equivalent to\nrepeat:\n    ; code\n    dec cx                      ; updation\n    jnz repeat              ; condition\n</code></pre>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/07_Stacks/","title":"07 Stacks","text":""},{"location":"2_Core/MicroProcessors_%26_Interfacing/07_Stacks/#stack","title":"Stack","text":"<p>it is a temporary scratch memory, for storing variables</p> <p>memory is segmented into different various segments, and one of them is stack segment</p> <p>2/4 bytes involved</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/07_Stacks/#operations","title":"Operations","text":"Push Pop Direction register/memory to stack stack to register/memory lower register \\(\\leftarrow\\) 1<sup>st</sup> bytehigher register \\(\\leftarrow\\) 2<sup>nd</sup> byte Byte SP - 1 SP + 1 Word SP - 2 SP + 2 [SP-1] \\(\\leftarrow\\) MSB[SP-2] \\(\\leftarrow\\) LSB Double Word SP - 4 SP + 4"},{"location":"2_Core/MicroProcessors_%26_Interfacing/08_Subroutines/","title":"08 Subroutines","text":""},{"location":"2_Core/MicroProcessors_%26_Interfacing/08_Subroutines/#structure","title":"Structure","text":"<pre><code>name proc near\n    ;code\n\n    ret\nname endp\n</code></pre> <pre><code>call procName\n</code></pre>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/08_Subroutines/#example","title":"Example","text":"<pre><code>bcd2bin proc near\n    ; code\n    ret\n\nbcd2bin endp\n</code></pre>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/09_Hardware/","title":"09 Hardware","text":""},{"location":"2_Core/MicroProcessors_%26_Interfacing/09_Hardware/#hardware","title":"Hardware","text":"<ul> <li>ALU is a combinational circuit</li> <li>clock is for the frequency</li> <li>Address lines are uni</li> </ul>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/09_Hardware/#pin-diagram","title":"Pin Diagram","text":"<p>Names</p> <ul> <li>DIP (Dual Inline Package)</li> <li>QFP (Quad Flag Pack)</li> </ul>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/09_Hardware/#multiplexer","title":"Multiplexer","text":"<p>Intel used multiplexer</p> <ul> <li>minimizes the area required for the chip</li> <li>reduces performance</li> </ul>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/09_Hardware/#modes","title":"Modes","text":"Minimum Maximum MN/MX\u2019 MN/MX\u2019 Logic 1 0 Size Smaller Larger Processors Single Multiple Cost Cheaper Expensive 8087 (co-Processor)"},{"location":"2_Core/MicroProcessors_%26_Interfacing/09_Hardware/#mac-operations","title":"MAC operations","text":"<p>Multiplied and Accumulated</p> <p>\\(AX+B\\)</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/09_Hardware/#cycle","title":"Cycle","text":"<ol> <li>Clock = T state</li> <li>Machine - memory access</li> <li>Instruction - instruction access + decoding + \u2026</li> </ol>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/09_Hardware/#setup-time","title":"Setup Time","text":"<p>the time before the clock high, during which the data must be setup, to avoid data corruption</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/09_Hardware/#hold-time","title":"Hold Time","text":"<p>the time after the clock high, during which the data must be held, to avoid data corruption</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/10_Pin_Out_Address/","title":"10 Pin Out Address","text":"<p>This topic goes over how 8086 creates addresses and how data transfers occurs</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/10_Pin_Out_Address/#intro","title":"Intro","text":"<p>8086 can be divided mainly into</p> <ol> <li>Bus Interface Unit</li> <li>Execution Unit</li> </ol>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/10_Pin_Out_Address/#externals","title":"Externals","text":"<p>ROM - Read-Only Memory</p> <p>contains the BIOS (Basic Input/Output System)</p> <p>RAM - Random Access Memory is faster and hence, applications get loaded here during runtime</p> <p>Permanent Memory is non-volatile</p> <p>IO- Input/Output Devices</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/10_Pin_Out_Address/#buses","title":"Buses","text":"Address Data Control Direction 1 2 1 <p>All signals depend on the clock. So faster the frequency of the clock, faster the operations.</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/10_Pin_Out_Address/#address-buses","title":"Address Buses","text":"<p>They are multiplexed with data lines and with selection lines. To reduce area required.</p> <p>Multiplexing happens with inputs that won\u2019t be used simultaneously.</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/10_Pin_Out_Address/#pin-diagram","title":"Pin Diagram","text":"<p>40 pins</p> <p>Dual Inline Package IC</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/10_Pin_Out_Address/#vcc","title":"VCC","text":"<p>\\(5v \\pm 10 \\%\\)</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/10_Pin_Out_Address/#gnd","title":"GND","text":"<p>2 grounds</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/10_Pin_Out_Address/#clk","title":"Clk","text":"<p>One cycle of this clock is called as T state.</p> <p>The time between 2 rising/falling edges is called as a time period.</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/10_Pin_Out_Address/#reset","title":"Reset","text":"<p>Used to initialize the processor. The processor will repeat all given instructions. Any data inside registers will be lost, and flags will be reset.</p> <p>CS FFFFh</p> <p>IP 0000h</p> <p>This signal has to high for atleast 4 clk signals.</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/10_Pin_Out_Address/#mn-overlinemx","title":"\\(MN/ \\overline{MX}\\)","text":"<p>Minimum/Maximum</p> <p>These 2 are different modes of operations.</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/10_Pin_Out_Address/#ale","title":"ALE","text":"<p>Address Latch Enable</p> <p>Whenever there is address, this is set as high. Else, it is data.</p> <p>This is sent to Gate signal.</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/10_Pin_Out_Address/#overlinebhes7","title":"\\(\\overline{BHE}/S7\\)","text":"<p>Bus High Enable</p> <p>Enables the most significant data lines (D8 - D15), only when required</p> <p>S7 is always high</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/10_Pin_Out_Address/#modes","title":"Modes","text":"Minimum Maximum Logic 1 0 No of processors 1Only 8086 or 8088 Multiplerequires 8087 as its co processor for floating point operation Size Smaller Larger Cost Cheaper Costlier"},{"location":"2_Core/MicroProcessors_%26_Interfacing/10_Pin_Out_Address/#octal-latch","title":"Octal Latch","text":"<p>8 bit latch</p> <p>used for de -multiplexing address and data</p> <p>it is used to ensure that address does not get affected, while operations don\u2019t get affected.</p> <p>we are using LS273</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/10_Pin_Out_Address/#g","title":"G","text":"<p>Gate Signal controls whether or not input to the latch get reflected to the output.</p> <p>As soon as the address gets passed through, the signal is turned low.</p> <p>get its value from the ALE</p> <p>When \\(G\\) is high, address is sent out. Else, data is sent out.</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/10_Pin_Out_Address/#overlineoe","title":"\\(\\overline{OE}\\)","text":"<p>Output Enabled Active Low</p> <p>Grounded by default</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/10_Pin_Out_Address/#dont-know","title":"Don\u2019t know","text":"Signal Address Signal Status Signal \\(A_{16}/S_3\\) \\(A_{16}\\) Segment Address \\(A_{17}/S_4\\) \\(A_{17}\\) Segment Address \\(A_{18}/S_5\\) \\(A_{18}\\) Int Flag Status \\(A_{19}/S_6\\) \\(A_{19}\\) 0 \\(\\overline{BHE}/S_7\\) \\(\\overline{BHE}\\) 1 S4 S3 Function 0 0 Extra Segment 0 1 Stack Segment 1 0 Code or no Segment 1 1 Data Segment"},{"location":"2_Core/MicroProcessors_%26_Interfacing/11_Pin_Out_Control_Data/","title":"11 Pin Out Control Data","text":""},{"location":"2_Core/MicroProcessors_%26_Interfacing/11_Pin_Out_Control_Data/#data-bus","title":"Data Bus","text":""},{"location":"2_Core/MicroProcessors_%26_Interfacing/11_Pin_Out_Control_Data/#pins","title":"Pins","text":""},{"location":"2_Core/MicroProcessors_%26_Interfacing/11_Pin_Out_Control_Data/#moverlineio","title":"\\(M/\\overline{IO}\\)","text":"<p>Differentiate between memory and IO access.</p> <p>When high, memory reference instructions.</p> <p>When low, IO instructions.</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/11_Pin_Out_Control_Data/#overlinerd","title":"\\(\\overline{RD}\\)","text":"<p>When it is low, read operation takes place.</p> <p>It is an ouput signal.</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/11_Pin_Out_Control_Data/#overlinewr","title":"\\(\\overline{WR}\\)","text":"<p>When it is low, write operation takes place.</p> <p>It is an ouput signal.</p> \\(M/\\overline{IO}\\) \\(\\overline{RD}\\) \\(\\overline{WR}\\) Bus Cycle 1 0 1; Memory Read 1 1 0 Memory Write 0 0 1 Input/Output Read 0 1 0 Input/Output Write <p></p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/11_Pin_Out_Control_Data/#overlineden","title":"\\(\\overline{DEN}\\)","text":"<p>Data Enable</p> <p>Whenever data is available on \\(AD0- AD15\\), this signal becomes low, to signal that data is coming.</p> <p>Connected to \\(\\bar E\\)</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/11_Pin_Out_Control_Data/#dtoverliner","title":"\\(DT/\\overline{R}\\)","text":"<p>Data Transmit/Receive</p> <p>Controls the direction of data transfer from/to data transceivers, such as Bi-Directional Buffer.</p> <p>When high, data transmitted by processor</p> <p>When low, data received by processor</p> <p>Connected to DIR</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/11_Pin_Out_Control_Data/#bi-directional-buffer","title":"Bi-Directional Buffer","text":"<p>We are using LS245 as the octal buffer.</p> <p>Bus A = MP, Bus B = Data Bus</p> <p>Data can move from Bus A \\(\\to\\) B, or vice-versa.</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/11_Pin_Out_Control_Data/#bar-e","title":"\\(\\bar E\\)","text":"<p>Connected to \\(\\overline{DEN}\\)</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/11_Pin_Out_Control_Data/#dir","title":"DIR","text":"<p>Connected to \\(DT/\\overline{R}\\)</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/12_Pin_Out_System/","title":"12 Pin Out System","text":""},{"location":"2_Core/MicroProcessors_%26_Interfacing/12_Pin_Out_System/#interrupts","title":"Interrupts","text":"<p>8086 has 2 interrupts.</p> Maskable Non-Maskable controlled by the interrupt flag checks interrupt flag \u2705 \u274c works when interrupt flag is high input INTR (Interrupt Request) NMI output \\(\\overline{INTA}\\) (Interrupt Acknowledge)"},{"location":"2_Core/MicroProcessors_%26_Interfacing/12_Pin_Out_System/#hold","title":"Hold","text":"<p>Input Signal to the processor from the bus masters as a request to control the bus.</p> <p>Usually by DMA controller.</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/12_Pin_Out_System/#hlda","title":"HLDA","text":"<p>Hold Acknowledge</p> <p>Output Signal from the processor to the bus master requesting control.</p> <p>When high, acknowledged</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/13_Machine_Cycles/","title":"13 Machine Cycles","text":""},{"location":"2_Core/MicroProcessors_%26_Interfacing/13_Machine_Cycles/#cycles","title":"Cycles","text":"<p>Instuction cycle is the time taken by the processor to execute one instruction.</p> <p>As 8086 is a CISC processor, each instruction cycle consists of multiple machine cycles.</p> <p>Each machine cycle consists of T states.</p> <p>All operations occur sequentially, controlled by the clock signal.</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/13_Machine_Cycles/#machine-cycles","title":"Machine Cycles","text":"<pre><code>flowchart LR\n1[Instruction Fetch] --&gt; 2[Instruction Decode] --&gt; 3[Operand Fetch] --&gt; 4[Instruction Execution] --&gt; 5[Store] --&gt; 1</code></pre>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/13_Machine_Cycles/#t-state","title":"T State","text":"<p>Time Period = \\(\\frac{1}{\\nu}\\)</p> <p>Getting from/to Register does not require anything</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/13_Machine_Cycles/#timing-diagram","title":"Timing Diagram","text":"<p>Tutorial</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/13_Machine_Cycles/#machine-cycles_1","title":"Machine Cycles","text":"<p>Getting from/to Register does not require anything The number of bits readable in 1 cycle = 16bits 1. opcode      - 16bits requires 2      -  1. Reading from memory = 1 1. Writing to memory = 1</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/13_Machine_Cycles/#time","title":"Time","text":"<p>Total Time = No of T States \\(\\times\\) Duration of each T state  = No of cycles \\(\\times\\) No of T States in each cycle \\(\\times\\) Duration of each T state </p> <p>No of T states in each cycle = 4 Duration of each T state = 1 Time Period = \\(\\frac{1}{\\nu}\\)</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/13_Machine_Cycles/#2-memory-operations","title":"2 memory operations","text":"<ol> <li>Read (Data/Instruction)</li> <li>Write (Data)</li> </ol>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/13_Machine_Cycles/#timing-diagram_1","title":"Timing Diagram","text":"<p>In write operation, the data is available in the 2<sup>nd</sup> state itself, as there will not be any delay.</p> <p>In read operation, it is available in the 3<sup>rd</sup> state.</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/13_Machine_Cycles/#slow-device","title":"Slow Device","text":"<p>Active High signal from slow device/memory, acknowledging that it is ready for data transfer.</p> <p>Else, the processor inserts a wait state, before \\(T_3\\) state.</p> <p>Number of wait states depends on the difference in the speed between the microprocessor and the slow device.</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/13_Machine_Cycles/#readwrite","title":"Read/Write","text":""},{"location":"2_Core/MicroProcessors_%26_Interfacing/13_Machine_Cycles/#write","title":"Write","text":""},{"location":"2_Core/MicroProcessors_%26_Interfacing/13_Machine_Cycles/#read","title":"Read","text":""},{"location":"2_Core/MicroProcessors_%26_Interfacing/14_Memory_Banking/","title":"14 Memory Banking","text":""},{"location":"2_Core/MicroProcessors_%26_Interfacing/14_Memory_Banking/#components","title":"Components","text":""},{"location":"2_Core/MicroProcessors_%26_Interfacing/14_Memory_Banking/#2704","title":"2704","text":"<p>2704 is ROM chip</p> <p>two has W Inverted W is M It also has o, so ROM</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/14_Memory_Banking/#ls138","title":"LS138","text":"<p>\\(3 \\times 8\\) decoder</p> <p>The other one will be ram chip We don\u2019t need the entire memory, so we instead use in different way.</p> \\(O_0\\) ROM1 \\(O_3\\) RAM1 \\(O_4\\) RAM2"},{"location":"2_Core/MicroProcessors_%26_Interfacing/14_Memory_Banking/#blah","title":"Blah","text":"\\(O_0\\) \\(A_0\\) \\(\\overline{BHE}\\) Even Odd 0 0 0 \u2705 \u2705 0 0 1 \u2705 0 1 0 \u2705 0 1 1 <p>It is active low.</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/15_Interrupts/","title":"15 Interrupts","text":"<p>ISR</p> <p>Interrupt Service Routine</p> <p>We need 2 bytes of memory location for pushing the CS contents</p> <p>Total 6 bytes are required for an interrupt to occur</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/15_Interrupts/#interrupt-vectors","title":"Interrupt Vectors","text":"Interrupt Physical Address INT 00H \\({00000}_H\\) \\({IP}_0\\) \\({00002}_H\\) \\({CS}_0\\) INT 01H \\({00004_H}\\) \\({IP}_1\\) \\({00006_H}\\) \\({CS}_1\\) INT FFH \\({003FC_H}\\) \\({IP}_{255}\\) \\({003FE_H}\\) \\({CS}_{255}\\)"},{"location":"2_Core/MicroProcessors_%26_Interfacing/15_Interrupts/#interrupts","title":"Interrupts","text":"<code>INT</code> Interrupt When Explanation 0 Divide by Zero 1 Single Step 2 NMI low-to-high transition on NMI input Type 2 interrupts cannot be disabled(masked) by any instruction 3 BreakPoint 4 into 5 <code>bound</code> 6 Invalid opcode 7 Co-Processor not available 8 Double Fault 9 A B C D E F"},{"location":"2_Core/MicroProcessors_%26_Interfacing/16_IO_Interfacing/","title":"16 IO Interfacing","text":"Input Output Buffer Latch"},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/","title":"Intro","text":"<p>We are using \u2018Assembly Language\u2019, which is a lower level language compared to C, C++, Java, Python, etc\u2026</p> <p>It uses an assembler to convert the code into machine language the processor can understand. (high level languages use compiler/interpreter).</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/#installation","title":"Installation","text":""},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/#windows","title":"Windows","text":""},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/#macos","title":"MacOS","text":"<ol> <li>Install <code>dosbox</code></li> <li>https://www.dosbox.com/download.php?main=1</li> <li>Copy <code>8086</code> files to <code>ahmedthahir/dosbox</code>; basically the root folder (next to Desktop, Documents, etc)</li> <li>https://www.mediafire.com/file/mm7cjztce9efj4w/8086.zip/file</li> <li>open <code>dosbox</code></li> <li><code>mount c ~/dosbox/8086</code></li> <li><code>c:</code></li> </ol>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/#basics","title":"Basics","text":""},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/#skeleton-program","title":"Skeleton Program","text":"<pre><code>.model small\n.stack 20\n\n.data\norg 1000h\nnum1 db 05h\nnum2 db 03h\n\n.code\nstart:\nmov ax, @data\nmov ds, ax\n\nmov al, num1\nadd num2, al\n\nint 3\nend start\ncode ends\n</code></pre>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/#steps","title":"Steps","text":"<ol> <li> <p>Open up TurboAssembler</p> </li> <li> <p>Editing</p> <ul> <li>no</li> <li>Dos</li> <li>Type <code>edit fileName.asm</code></li> <li>Type your code</li> <li>Save your code     Click Here to learn how</li> </ul> </li> <li> <p>Assembling    Type <code>tasm fileName.asm</code></p> </li> <li> <p>Linking    Type <code>tlink fileName.obj</code></p> </li> <li> <p>Execution</p> </li> <li>Type <code>td fileName.exe</code></li> <li> <p>Click <code>F7</code> to execute the required lines</p> </li> <li> <p>Viewing results</p> </li> <li>Click <code>Tab</code> key until focus reaches the address-value thing at the bottom</li> <li>Click <code>Ctrl-G</code></li> <li>Enter <code>ds:address</code>       for eg <code>ds:1000</code></li> </ol>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/#saving","title":"Saving","text":"<p>on your keyboard, click</p> <ol> <li><code>Alt+f</code> </li> <li>then <code>s</code> </li> </ol>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/01/","title":"01","text":""},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/01/#program-1","title":"Program 1","text":"<p>Add two 2-digit hexadecimal numbers (method 1)</p> <pre><code>DATA SEGMENT\n\nORG 1000H\nNUM1 DB 89H\nNUM2 DB 7CH\nRES DW ?\n\nDATA ENDS\nCODE SEGMENT\nASSUME CS: CODE, DS: DATA\n\nSTART:\nMOV AX, DATA\nMOV DS, AX\nMOV AH, 0\nMOV AL, NUM1\nADD AL, NUM2\nADC AH, 0\nMOV RES, AX\nINT 3\n\nCODE ENDS\nEND START\n</code></pre> <p></p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/01/#program-2","title":"Program 2","text":"<p>Add two 2-digit hexadecimal numbers (method 2)</p> <pre><code>.model small\n.stack 20\n\n.data\n\norg 1000H\nnum1 DB 80H\nnum2 DB 86H\nres DW ?\n.code\nstart:\n\nmov ax, @data\nmov ds,ax\nmov ah,0\nmov al,num1\nadd al,num2\nadc ah,0\nmov res,ax\nint 3\n\nend start\n</code></pre> <p></p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/01/#program-3","title":"Program 3","text":"<p>Write a program to add two 2-digit decimal numbers available in memory and store the result in memory.</p> \\[ 89+78 = 167 \\] <pre><code>DATA SEGMENT\nORG 1000H\nNUM1 DB 89H\nNUM2 DB 78H\nRES DW ?\nDATA ENDS\n\nCODE SEGMENT\nASSUME CS: CODE, DS: DATA\n\nSTART:\n\nMOV AX, DATA\nMOV DS, AX\nMOV AH, 0\nMOV AL, NUM1\nADD AL, NUM2\nDAA\nADC AH, 0\nMOV RES, AX\nINT 3\n\nCODE ENDS\nEND START\n</code></pre> <p></p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/01/#program-4","title":"Program 4","text":"<p>The above program without H in the input Data</p> <p>This gives wrong answer cuz we do DAA, even though the stored values are decimal</p> <pre><code>DATA SEGMENT\nORG 1000H\nNUM1 DB 89\nNUM2 DB 78\nRES DW ? \nDATA ENDS\nCODE SEGMENT\nASSUME CS: CODE, DS: DATA\n\nSTART:\nMOV AX, DATA\nMOV DS, AX\nMOV AH, 0 \nMOV AL, NUM1\nADD AL, NUM2\nDAA\nADC AH, 0\nMOV RES, AX\nINT 3\n\nCODE ENDS\nEND START\n</code></pre> <p></p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/01/#daa","title":"<code>DAA</code>","text":"<p>DAA corrects the result of a previous addition of two valid packed decimal operands (note that this result must be in AL). This instruction changes the content of AL so that it will contain a pair of valid packed decimal digits.</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/02/","title":"02","text":""},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/02/#program-1","title":"Program 1","text":"<p>Write a program to add two multi-byte binary numbers stored in memory and also store the result in memory.</p> <pre><code>.MODEL SMALL ; Type of the model declaration\n.STACK 20 ; size of the stack declaration\n.DATA ; Data segment declaration\norg 1000H ; Memory address initialization. Data start from memory Location\n\n; 1006 (it may vary, depends upon your processor)\n\nNUM1 DB 25H,35H,45H,32H,56H,98H,76H,76H ; These are array of 8 hex numbers\nNUM2 DB 90H,56H,43H,75H,89H,10H,34H,22H ; These are second array of numbers\nANS DB 10 DUP (?) ; Size of the memory to store the results, it reserves 10 duplicate no. with unknown value.\n\nCOUNT DW 8H ; counter to store 8, since 8 times we need to add\n\n.CODE ; code start here\nSTART:\nMOV AX, @DATA\nMOV DS, AX ; Initializing DS: segment register\nMOV CX, COUNT ; Register CX is initializing to COUNT= 8\nMOV SI, 0H ; Initializing Source Index Register SI= 0\nCLC ; Clears the Carry Flag before addition\nREPEAT: ; initializing the loop\n\n; (REPEAT is a loop name, you may change it to any)\n\nMOV AL, NUM1 [SI] ; Loading the 1st array value to AL register\nADC AL, NUM2 [SI] ; Adding the 2nd array value to AL register value\nMOV ANS [SI], AL ; moving the addition value to ANS variable\nINC SI ; incrementing the Source Index for the next position\nLOOP REPEAT ; go back to the location of REPEAT loop\nINT 3 ; Breakpoint interrupt\nEND START ; stopping the program\n</code></pre> <p></p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/02/#program-2","title":"Program 2","text":"<p>Write a program to subtract two multi-byte binary numbers stored in memory and also store the result in memory.</p> <pre><code>.MODEL SMALL ; Type of the model declaration\n.STACK 20 ; size of the stack declaration\n.DATA ; Data segment declaration\nOrg 1000H ; Memory address initialization. Data start from memory location 1006\n\n; (it may vary, depends upon your processor)\n\nNUM1 DB 89H,35H,45H,32H,56H,98H,76H,76H; These are array of 8 hex numbers\nNUM2 DB 32H,56H,43H,75H,89H,10H,34H,22H; These are second array of numbers\nANS DB 9 DUP(0) ; size of the memory to store the results, it reserves 9 duplicate no. with \u20180\u2019 value.\n\nCOUNT DW 8H ; counter to store 8, since 8 times we need to subtract\n.CODE ; code start here\nSTART:\nMOV AX, @DATA\nMOV DS, AX ; Initializing DS: segment register\nMOV CX, COUNT ; Reg CX is initializing to COUNT=8\nMOV SI, 0H ; Initializing Source Index Register SI=0\nCLC ; Clears the Carry Flag before subtraction\nREPEAT: ; Initializing the loop\nMOV AL, NUM1 [SI] ; Loading the 1st array value to AL register\nSBB AL, NUM2 [SI] ; subtracting the 2nd array value to AL register value\nMOV ANS [SI], AL ; moving the subtracted result to ANS variable\nINC SI ; incrementing the Source Index for the next position\nLOOP REPEAT ; loop repeat\nINT 3 ; Breakpoint interrupt\nEND START ; stopping the program\n</code></pre> <p></p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/02/#program-3","title":"Program 3","text":"<p>Write a program to multiply two 8-bit binary numbers stored in memory and also store the result in memory (both unsigned and signed operation).</p> <ul> <li>Unsigned numbers stored only positive numbers but not negative numbers</li> <li>Signed numbers contain sign flag</li> </ul> <pre><code>.MODEL SMALL\n.STACK 20\n.DATA\nORG 1000H\nN1 DB 35H ; Input Number one\nN2 DB 82H ; Input Number two\nUn_Sign_PROD DW ? ; This the variable to store Unsigned multiplication\nSign_PROD DW ? ; This the variable to store Signed multiplication\n.CODE ; code segment start here\nSTART:\nMOV AX, @DATA ; Initialize DS\nMOV DS, AX\nMOV AL, N1 ; storing the first value (N1)to AL\nMUL N2 ; Unsigned multiplication, multiplying the N2 with N1\nMOV Un_Sign_PROD, AX ; moving the results of AX to Un_sign_Prod variable\nMOV AL, N1 ; storing the first value (N1) to AL\nIMUL N2 ; Signed multiplication, IMUL multiplies signed numbers\nMOV Sign_PROD, AX ; moving the results from AX to sign_PROD variable memory\nINT 3 ; Breakpoint interrupt\nEND START ; stopping the program\n</code></pre> <p></p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/02/#assignment","title":"Assignment","text":"<p>Write a program to find factorial of number.</p> <pre><code>.model small\n.stack 20\n.data\norg 1000h\n\nnum dw 6d\nfact dw 1d\n\n.code\nstart:\nmov dx, @data\nmov ds, dx\nmov cx, num\nmov si, 0h\nmov ax, 1d\n\nrepeat:\nmul cx\nloop repeat\n\nmov fact, ax\n\nint 3\nend start\n</code></pre> <p></p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/03/","title":"03","text":""},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/03/#program-1","title":"Program 1","text":"<p>Write a program to add an array of eight 2-digit hexadecimal numbers stored in memory and store the result in memory.</p> <pre><code>.MODEL SMALL\n.STACK 20\n.DATA ; Data segment start here\nORG 1000H ; Memory address initialization\nNUM DB 25H, 35H, 45H, 32H, 56H, 98H, 76H,76H ; eight 2-digit hex numbers input\nSUM DW ? ; Variable to store Sum\nCOUNT DW 0008H ; Count Variable to store count 8\n\n.CODE ; Code start here\nSTART:\nMOV AX, @DATA ; Initializing DS: segment register\nMOV DS, AX\nMOV CX, COUNT ; Reg CX is initializing to COUNT=8\nMOV SI, 0000H ; Initializing Source Index Register SI=0000\nMOV AX, 0000H ; move AX=0000\n\nREPEAT: ; Loop start here\nADD AL, NUM[SI] ; Moving the 1st array value to AL register\nJNC NEXT ; Jump If not Carry, here Next is 16bit address.\nADD AH, 01 ; Add 01 to AH\nNEXT: ; Next address reference\nINC SI ; Increment SI\nLOOP REPEAT ; Repeat loop Reference\nMOV SUM, AX ; Moving the AX value to Sum variable memory\nINT 3 ; Breakpoint interrupt\nEND START ; stopping the program\n</code></pre> <p></p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/03/#program-2","title":"Program 2","text":"<p>Write a program to count number of occurrences of the byte 25H in the given array of 16-bytes stored starting from 1200H. Also store the result in 1220H memory location.</p> <pre><code>.MODEL SMALL\n.STACK 20\n.DATA ; Data segment start here\nORG 1200H ; Memory address initialization as given in the program\nARRAY DB 25H, 35H, 45H, 32H, 56H, 25H, 76H, 76H, 28H, 56H, 05H, 35H, 25H, 00H, 98H, 21H ; Inputs\nORG 1220H ; Memory address initialization to store results\nRES DB ? ; Variable to store Number of occurrences\nCOUNT DW 0010H ; Count Variable to store length of the array i.e 10\n.CODE ; Code start here\nSTART:\nMOV AX, @DATA ; Initializing DS: segment register\nMOV DS, AX\nMOV CX, COUNT ; Reg CX is initializing to COUNT\nMOV SI, 0000H ; Initializing Source Index Register SI=0000\nMOV AL, 25H ; move AL=25H, We need to find out number of\n\n; occurrences of 25H\nREPEAT: ; Loop start here\nCMP AL, ARRAY[SI] ; The CMP instruction compares two operands.\nJNE NEXT ; Jump if Not Equal, if SI and AL data not equal it jumps\n\n; to INC SI\n\nINC RES ; if SI and AL equal the RES is incremented\nNEXT:\nINC SI ; Increment SI\nLOOP REPEAT ; Loop Repeat\nINT 3 ; Breakpoint interrupt\nEND START ; stopping the program\n</code></pre> <p></p> <p></p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/03/#program-3","title":"Program 3","text":"<p>Write a program to exchange two data blocks of length 10-bytes stored in memory starting from 1200H and 1220H respectively.</p> <pre><code>.MODEL SMALL\n.STACK 20\n.DATA ; Data segment start here\nORG 1200H ; Memory address one initialization as given in the program\nARRAY1 DB 05H, 15H, 25H, 35H, 45H, 55H, 65H, 75H, 85H, 95H ; Inputs\nORG 1220H ; Memory address two initialization as given in the program\nARRAY2 DB 0A1H, 0A2H, 0A3H, 0A4H, 0A5H, 0A6H, 0A7H, 0A8H, 0A9H, 0AAH ; Inputs\nCOUNT DW 000AH ; Count Variable to store length of the array i.e 000A\n.CODE ; Code start here\nSTART:\nMOV AX, @DATA ; Initializing DS: segment register\nMOV DS, AX\nMOV CX, COUNT ; Reg CX is initializing to COUNT\nMOV SI, 0000H ; Initializing Source Index Register SI=0000\nREPEAT: ; Loop start here\nMOV AL, ARRAY1 [SI] ; Moving first element of array one to AL Register\nXCHG AL, ARRAY2 [SI] ; Exchange Data. The XCHG exchange the contents of\n\n; two operands.\n\nMOV ARRAY1 [SI], AL ; Move the Content of AL to Array one address\nINC SI ; Increment SI\nLOOP REPEAT ; Loop Repeat\nINT 3 ; Breakpoint interrupt\nEND START ; stopping the program\n</code></pre> <p></p> <p></p> <p></p> <p></p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/03/#assignment","title":"Assignment","text":"<p>Write a program to arrange the given array of 8-bit binary numbers stored in the memory in ascending order.</p> <pre><code>.MODEL SMALL\n.STACK 20\n.DATA\nNUM DB 11H, 21H, 31H, 31H, 55H, 45H, 35H, 25H\nCOUNT DW 0008H\n\n.CODE\nSTART:\nMOV AX, @DATA\nMOV DS, AX\nMOV CX, COUNT\n\nDEC CX\n\nNEXT:\n    MOV DX, CX\n    MOV SI, 0000H\n\n  REPEAT:\n    MOV AL, NUM[SI]\n    CMP AL, NUM[SI + 1]\n    JC NOEX\n    XCHG AL, NUM[SI +1]\n    MOV NUM[SI], AL\n\n    NOEX:\n        INC SI\n        DEC DX\n        JNZ REPEAT\n        LOOP NEXT\n\nINT 3\nEND START\n</code></pre> <p></p> <p></p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/04/","title":"04","text":""},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/04/#program-1","title":"Program 1","text":"<p>Write a program to convert 4-digit BCD number to HEXADECIMAL number and store the result in memory.</p> <pre><code>.MODEL SMALL\n.STACK 20\n\n.DATA ; Data segment start here\nORG 1000H ; Memory address initialization\nBCD DW 1234H ; 4-digit BCD number is 1234H, here BCD is a variable\nHEX DW 0 ; HEX is a variable to store the results\n\n.CODE ; Code start here\nSTART:\nMOV AX, @DATA ; Initializing DS: segment register\nMOV DS, AX\n\nMOV BX, 0001H ; Weight for LSD, storing 0001H into BX register (once position)\nCALL BCD2BIN ; calling BCD2BIN procedure. See the written procedure in the\n\nMOV BX, 000AH ; Weight for 2nd digit, Storing 000AH in to BX register (10th Position)\nCALL BCD2BIN ; calling BCD2BIN procedure. See the written procedure in the last lines\n\nMOV BX, 0064H ; Weight for 3rd digit, Storing 0064H in to BX register (100th Position)\nCALL BCD2BIN ; calling BCD2BIN procedure. See the written procedure in the last lines\n\nMOV BX,03E8H ; Weight for MSD, Storing 03E8H in th BX register (1000th Position)\nCALL BCD2BIN ; calling BCD2BIN procedure. See the written procedure in the last lines\n\nINT 3 ; Breakpoint interrupt\n\nBCD2BIN PROC NEAR ; BCD2BIN is a procedure called 4 times in the main program\nMOV AX, BCD ; Subroutine multiplies digits with respective weights and adds the partial\n; product to get equivalent HEX moving the desired digit LSD position\n\nAND AX, 000FH ; ANDing operation with BCD value i.e 1234\nMUL BX ; multiplies BX with AX\nADD HEX, AX ; add AX value with Hex variable, initially Hex variable have 0\nMOV CL, 04 ; moving 04 to CL\nROR BCD, CL ; Rotate Right\nRET ; RET instruction stands for return, used at the end of the procedures\nBCD2BIN ENDP ; end the procedure\n\nEND START\n</code></pre> <p></p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/04/#program-2","title":"Program 2","text":"<p>Write a program to convert the given HEXADECIMAL digit to ASCII byte and store the result in memory.</p> <pre><code>.MODEL SMALL\n.STACK 20\n\n.DATA ; Data segment start here\nORG 1000H ; Memory address initialization\nHex_Digit DB 38H ; Given Hex value is 38H, Hex_Digit is a variable\nASCII DB ? ; ASCII is the variable to store the ASCII equivalent Hex of 38H\n\n.CODE ; Code start here\nSTART:\nMOV AX, @DATA ; Initializing DS: segment register\nMOV DS, AX\n\nMOV AL, Hex_Digit ; Moving Hex_Digit value 38H to AL register\nCMP AL, 3AH ; compare 38 with 3A, To see whether it is between 30H to 39H\n\n; or 41H to 46H\n\nJC SUB30 ; if carry generated then jump to location SUB30\nSUB AL, 07H ; if no carry then subtract 07H with AL value\nSUB30: ; User defined location name\nSUB AL,30H ; subtract 30H with AL\nMOV ASCII, AL ; Store the AL value into Hex_Digit variable\n\nINT 3 ; Breakpoint interrupt\nEND START\ncode ends\n</code></pre> <p></p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/04/#program-3","title":"Program 3","text":"<p>Write a program to display the hexadecimal byte 45H on the screen using DOS interrupts.</p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/04/#easy-version","title":"Easy Version","text":"<pre><code>.model small\n.stack 20\n\n.code\nstart:\n\nmov ah, 02h\n\nmov dl, 34h\nint 21h\n\nmov dl, 35h\nint 21h\nmov ah, 4ch\nint 21h\n\nint 3\nend start\ncode ends\n</code></pre> <pre><code>.MODEL SMALL\n.STACK 20\n\n.CODE\nSTART: ; here no data segment, using DOS interrupts\n\nMOV AL, 45H ; Number to be Display on screen, store the 45H in AL\nMOV BL, AL ; Moving 45H to BL\n\nAND AL, 0F0H ; Get upper digit (nibble), AND operation to 45\nROR AL, 4 ; Rotate to Right\n\nCALL HEXASC ; Convert from hex to ASCII\nMOV DL, AL ; moving AL to DL\nMOV AH, 02 ; Function code to display single character\nINT 21H ; DOS interrupt 21H\n\nMOV AL, BL ; Moving BL to AL\nAND AL, 0FH ; Get lower digit by ANDing AL\nCALL HEXASC ; call HEXASC Procedure\n\nMOV DL, AL ; move AL to DL\nMOV AH, 02 ; 02 to AH to Display second digit\nINT 21H ; DOS interrupt 21H\n\nMOV AH, 4CH ; causes the process to terminate\nINT 21H ; DOS interrupt 21H\n\nHEXASC: ; HEX TO ASCII procedure start\nCMP AL,0AH ; compare 38 with 3A ,To see whether it is between 30H to 39H\n; or 41H to 46H,\nJB NUM ; Jump if Carry\nADD AL,07 ; For A-F, add 37H\nNUM: ; jump address reference\nADD AL,30H ; For 0-9, add 30H\nRET ; RET stands for return, used at the end of the procedures\n\nEND START\n</code></pre>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/04/#program-4","title":"Program 4","text":"<p>Write a program to input two single-digit hex numbers from keyboard and display their product on the screen.</p> <pre><code>.MODEL SMALL\n.STACK 20\n.CODE\nSTART:\nCALL READKB ; Read Keyboard Procedure Call\nMOV BL,AL ; move AL to BL\nCALL NXTLINE ; Nextline procedure call\nCALL READKB ; Read Keyboard Procedure Call\nMUL BL ; Multiply BL with AL\nMOV BL, AL ; move AL to BL\nCALL NXTLINE ; Nextline procedure call\nCALL DISP ; Call DISP procedure\nMOV AH, 4CH ; causes the process to terminate\nINT 21H ; DOS interrupt 21H\n\nREADKB PROC NEAR ; Read Keyboard Procedure Start here\nMOV AH,01 ; Accepting number from keyboard\nINT 21H ; DOS interrupt 21H\nCALL ASCHEX ; Procedure to Ascii to Hex\nRET ; RET used at the end of the procedures\nREADKB ENDP ; Read Keyboard Procedure ends here\n\nASCHEX PROC NEAR ; ASCII to Hex procedure start here\nCMP AL, 3AH ; compare AL value with 3AH\nJC SUB30 ; jump if carry, Sub30 is address\nSUB AL, 07H ; no carry the subtract 07H with AL\nSUB30: ; In compare carry generates, SUB30: start executes\nSUB AL,30H ; ASCII to hex conversion\nAND AL,0FH ; AND operation with 0Fh with AL value\nRET ; RET used at the end of the procedures\nASCHEX ENDP ; Ascii to Hex procedure ends here\n\nNXTLINE PROC NEAR ; NEXTLINE procedure start here\nMOV AH, 2 ;\nMOV DL, 0AH ; Line feed\nINT 21H ; DOS interrupt 21H\nMOV DL,0DH ; Carriage return\nINT 21H ; DOS interrupt 21H\nRET ; RET used at the end of the procedures\nNXTLINE ENDP ; NEXTLINE Procedure ends here\n\nDISP PROC NEAR ; DISP procedure start here\nMOV AL, BL ; Moving BL to AL\nAND AL, 0F0H ; AND operation of AL and 0F0h\nROR AL, 4 ; rotate right AL, 4 times\nCALL HEXASC\nMOV DL, AL ; Moving AL contents to DL\nMOV AH, 02 ;\nINT 21H ; DOS interrupt 21H\nMOV AL, BL ; Moving BL contents to AL\nAND AL, 0FH ; AND operation of AL and 0Fh\nCALL HEXASC\nMOV DL, AL ; Moving AL contents to DL\nMOV AH, 02 ;\nINT 21H ; DOS interrupt 21H\nRET ; RET used at the end of the procedures\nDISP ENDP ; DISP procedure ends here\n\nHEXASC PROC NEAR ; HEXASC, Hex to Ascii procedure start here\nCMP AL, 0AH ; compare AL with 0AH\nJB NUM ; i\nADD AL, 07 ; add 07 to AL\nNUM:\nADD AL, 30H ; Add 30h to AL\nRET ; RET instruction used at the end of the procedures\nHEXASC ENDP ; HEXASC procedure ends here\n\nEND START\n</code></pre> <p></p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/04/#assignment","title":"Assignment","text":"<p>Write a program to accept a character from keyboard and display its ASCII equivalent value on the screen.</p> <pre><code>.model small\n.stack 20\n.code\nstart:\ncall readkb\nmov bl, al\ncall nxtline\ncall disp\nmov ah, 4ch\nint 21h\n\nreadkb proc near\nmov ah, 01\nint 21h\nret\nreadkb endp\n\nnxtline proc near\nmov ah, 2\nmov dl, 0Ah\nint 21h\nmov dl, 0dh\nint 21h\nret\nnxtline endp\n\ndisp proc near\nmov al, bl\nand al, 0F0h\nror al, 4\ncall hexasc\nmov dl, al\nmov ah, 02\nint 21h\nmov al, bl\nand al, 0Fh\ncall hexasc\nmov dl, al\nmov ah, 02\nint 21h\nret\ndisp endp\n\nhexasc proc near\ncmp al, 0Ah\njb num\nadd al, 07\nnum:\nadd al, 30h\nret\nhexasc endp\n\nend start\n</code></pre> <p></p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/05/","title":"05","text":""},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/05/#dos","title":"DOS","text":""},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/05/#display-number","title":"Display Number","text":"<pre><code>mov dl, 34h\nmov ah, 02h\nint 21h\n</code></pre>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/05/#display-string","title":"Display String","text":"<pre><code>lea dx, msg\nmov ah, 09h\nint 21h\n</code></pre>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/05/#reading","title":"Reading","text":"<pre><code>readkb proc near\n    mov ah, 01h\n    int 21h\n\n    cmp al, 3Ah\n    jc number\n\n    sub al, 07h\n\n    number:\n        sub al, 30\n\n    ret\nreadkb endp\n</code></pre>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/05/#program-1","title":"Program 1","text":"<p>Write a program to transfer the given string from source to destination using string instruction and also display the destination string.</p> <pre><code>.MODEL SMALL\n.STACK 20\n.DATA\nSRCSTR DB 'ELECTRONICS' ; Source String 'ELECTRONICS'\nLEN DW $-SRCSTR ; String Length\nMSG DB 'The Transferred String=' ; MSG is variable to store the transferred string\nDSTSTR DB 40 DUP ('$') ; Destination string\n\n.CODE ; code start\nSTART:\nMOV AX, @DATA ; Data segment start here\nMOV DS, AX ; Move AX to DS\nMOV ES, AX ; Move AX to ES\n\nMOV CX, LEN ; Move length of string to CX register\nLEA SI, SRCSTR ; Load Effective Address of source string to SI\nLEA DI, DSTSTR ; Load Effective Address of Destination to DI\nCLD ; for auto increment of SI and DI\nREP MOVSB ; Repeat prefix to MOVSB, Move data as bytes\nLEA DX, MSG\n\nMOV AH, 09 ; Displays a message terminated by $\nINT 21H ; DOS interrupt 21H\n\nMOV AH, 4CH ; program termination\nINT 21H ; DOS interrupt 21H\n\nEND START\n</code></pre> <p></p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/05/#program-2","title":"Program 2","text":"<p>Write a program to read two digit decimal number using keyboard and search whether the number is present in an array or not. Display suitable message.</p> <pre><code>.MODEL SMALL\n.STACK 20\n.DATA\nARRAY DB 35H, 56H, 82H, 89H, 90H, 23H, 12H, 51H, 88H ; Input array\nLEN DW $-ARRAY ; Length of the array value\nMSG1 DB 0DH, 0AH, ' Enter two digit numbers: $' ; Message one for input\nMSG2 DB 0DH, 0AH, ' The number is present $' ; Message to present output\nMSG3 DB 0DH, 0AH, ' The number is not present $' ; Message to present output\n.CODE\nSTART:\nMOV AX, @DATA ; Data segment start here\nMOV DS, AX\nMOV ES, AX ; MOVE AX, TO Extra Segment\n\nMOV CX, LEN ; load the length of the array to CX\nLEA DX, MSG1 ; Load effective address to DX\nMOV AH, 09 ; To display MSG1\nINT 21H ; DOS interrupt 21H\n\nCALL READKB ; Call to READKB procedure\nROR AL, 4 ; shifting the digit to MSD position, by rotating Right by 4\n; times of AL\nMOV BL, AL ; Move AL value to BL\n\nCALL READKB ; Call the READKB procedure\nADD AL, BL ; To make 2-digit number\nLEA DX, MSG2 ; Load effective address to DX\nLEA DI, ARRAY ; Load the effective address of array to DI\nCLD ; Clear Direction Flag\nREPNE SCASB ; Compares AL with memory pointed by DI\nJE GO ; conditional jump when zero flat equal to 1\nLEA DX, MSG3 ; Load Effective Address of MSG3 to DX\n\nGO:\nMOV AH, 09 ; To display the output\nINT 21H ; DOS interrupt 21H\nMOV AH, 4CH ; To terminate\nINT 21H ; DOS interrupt 21H\n\nREADKB PROC NEAR\nMOV AH,01 ; Accepting number from keyboard\nINT 21H ; DOS interrupt 21H\nCMP AL, 3AH ; compare AL with 3AH\nJC SUB30 ; Jump if carry\nSUB AL, 07H ; subtract 07H with AL\n\nSUB30:\nSUB AL, 30H ; ASCII to hex conversion\nRET ; RET instruction used at the end of the procedures\nREADKB ENDP ; end to READKB procedure\n\nEND START ; end to start\n</code></pre> <p></p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/05/#program-3","title":"Program 3","text":"<p>Write a program to read a string using DOS interrupts, reverse the entered string and display the same on the screen. Use MACRO for display.</p> <pre><code>.MODEL SMALL\n.STACK 20\n\nDISP MACRO MSG ; Macro Declaration, DISP is the name of the Macro.\nMOV AH, 09H ; To display message\nMOV DX, OFFSET MSG ; Load the MSG offset address to DX\nINT 21H ; DOS interrupt 21H\nENDM ; End Macro\n\n.DATA ; Data segment start here\nMSG1 DB 0DH, 0AH, 'Input a string:$' ; 0DH,0AH are carriage Return &amp; Line Feed\n\n; $ to terminate\nSRC DB 80 ; Maximum size of the string\nDB ? ; Actual size of the string\nDB 30 DUP (?) ; To store actual string\nMSG2 DB 0DH, 0AH, 'The reversed string is:' ; MSG2 to store the reversed string is\nREV DB 30 DUP (?) ; To store the reversed string\n\n.CODE ; code start here\nSTART:\nMOV AX, @DATA ; Data segment start here\nMOV DS, AX\nMOV ES, AX ; moving AX to ES\nDISP MSG1 ; MSG1 is a parameter to MSG of macro DISP, it displays\n; input a string:\nMOV DX, OFFSET SRC ; move Source offset address to DX\nMOV AH, 0AH ; Function code to read a string\nINT 21H ; DOS interrupt 21H\nMOV SI, OFFSET SRC+2 ; increment the source offset by 2 and store the address\n; to SI\nMOV DI, OFFSET REV-1 ; decrement the reverse offset address by 1 and store the\n; results in DI\nMOV CL, SRC+1 ; Length of the string, here SRC is size of the string, it is\n; incremented by 1\nMOV CH, 00 ; make CH=0\nADD DI, CX ; Add cout value to DI\nMOV BYTE PTR [DI+1], '$' ; End character for function 09H\nCLD ; Clear Direction Flag\n\nNEXT:\nMOVSB ; move string of words depending on CLD, SI,DI\n; automatically increase or decrease by 2\n\nSUB DI, 0002 ; subtract 0002 with DI\nLOOP NEXT\n\nDISP MSG2 ; Display message 2\nMOV AH, 4CH ; program termination\nINT 21H ; DOS interrupt 21H\n\nEND START\n</code></pre> <p></p>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/05/#assignment","title":"Assignment","text":"<p>Write a program to compare two arrays. If they are same then display 'IDENTICAL', if not, display \u2018NOT IDENTICAL\u2019. Make use of the string instruction CMPSB.</p> <p>Tried, but did not work correctly</p> <pre><code>.model small\n.stack 20\n\n.data\narray1 db 'Electronic$'\narray2 db 'Electronics$'\n\nequalMsg do 'Identical$'\nunequalMsg do 'Not Identical$'\n\n.code\nstart:\nmov ax, @data\nmvo ds, ax\nmov es, ax\n\nlea si, array1\nlea di, array2\n\ncmpsb\njz equal\njnz unequal\n\nequal:\nmov ah, 09h\nmov dx, offset equalMsg\nint 21h\nmov ah, 4Ch\nint 21h\n\nunequal:\nmov ah, 09h\nmov dx, offset unequalMsg\nint 21h\nmov ah, 4Ch\nint 21h\n\nend start\n</code></pre>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/06/","title":"06","text":""},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/06/#program-1","title":"Program 1","text":"<p>Write a program to create a file and write text data into it using DOS interrupts.</p> <pre><code>.MODEL SMALL\n.STACK 20\nPRINT MACRO MSG ;Macro Declaration as print\nMOV AH, 09H ;DOS function 09h: display a string of ;characters whose\n\noffset is specified by DX.\n\nLEA DX, MSG ;load effective address of MSG to DX\nINT 21H\nENDM\nREAD MACRO STR ;Macro declaration as read\nLEA DX, STR ;load the effective address of STR to DX\nMOV AH, 0AH ;0AH = Reading a string from keyboard\nINT 21H\nENDM\n.DATA\nMSG1 DB 0DH, 0AH, 'Enter a filename:$'\nMSG2 DB 0DH, 0AH, 'File is created$'\nMSG3 DB 0DH, 0AH, 'Error in File creation$'\nMSG4 DB 0DH, 0AH, 'Enter a text:$'\nMSG5 DB 0DH, 0AH, 'Error in File opening$'\nMSG6 DB 0DH, 0AH, 'Error in writing$'\nMSG7 DB 0DH, 0AH, 'Creating and writing successful$'\nFNAME DB 80\nDB ?\nDB 80 DUP(0)\nTEXT DB 80\nDB ?\nDB 80 DUP(?)\n\n.CODE\nSTART:\nMOV AX,@DATA\nMOV DS, AX\nMOV ES, AX\nPRINT MSG1\nREAD FNAME ;Reading a file name\nLEA SI,FNAME+2 ;FNAME is add of file name,\n;FNAME+1 is add of file length\nMOV CL,FNAME+1 ;Getting the length of file name\nMOV CH,00 ;clear the ch register\nADD SI,CX ;To move SI at next location\nMOV BYTE PTR[SI],00 ;Terminating file name by Zero\nLEA DX,FNAME+2 ;Starting address to write text in file\nMOV AH,3CH ;Function code to create file\nMOV CX,0000H ;File attributes of the new file\nINT 21H\nJNC SUCC1 ;If file creation is success CY=0\nPRINT MSG3 ;Error msg, if file is not created\nJMP EXIT\nSUCC1:\nPRINT MSG2 ;Success msg of file creation\nPRINT MSG4 ;msg to write text in created file\nREAD TEXT ;Reading text to be written to file\nMOV AH,3DH ;Function code to open file\nMOV AL,02H ;To open file in read/write mode\nLEA DX,FNAME+2\nINT 21H\nJNC SUCC2 ;If file open is success CY=0\nPRINT MSG5 ;error msg in opening the file\nJMP EXIT\nSUCC2:\nMOV BX,AX ;File handling returned during open\nMOV AH,40H ;Function code to write text\nMOV CH, 00H ;Number of characters to written into the file\n\nMOV CL,TEXT+1\nLEA DX,TEXT+2\nINT 21H\nJNC SUCC3 ; If file writing is success CY=0\nPRINT MSG6\nMOV AH,3EH ;Function code to close the file.\nINT 21H\nJMP EXIT\nSUCC3:\nPRINT MSG7 ;Disp msg, Creation and writing successful.\nEXIT:\nMOV AH,4CH ;To terminate the program\nINT 21H\nEND START\n</code></pre>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/06/#program-2","title":"Program 2","text":"<p>Write a program to read system time and display at the center of the screen</p> <pre><code>.MODEL SMALL\nSTACK 20.DATA\nMS DB \"PRESENT TIME IS: $\"\n.CODE\nSTART:\nMOV AX,@DATA\nMOV DS,AX\nMOV AH,00 ; Function code to clear screen\nMOV AL,03H ; text video mode\nINT 10H ; Calling BIOS interrupts\nMOV AH,02 ; Function code to set Cursor position.\nMOV BH,0 ; video page number\nMOV DH,12 ; Row number\nMOV DL,30 ; Column number\nINT 10H ; Calling BIOS interrupts\nLEA DX,MS ; Display the msg\nMOV AH,09\nINT 21H\n\nMOV AH,2CH ; Function code to read the system time, Hour in CH, ; Minute in\n\nCL, Second in DH\n\nINT 21H\nMOV AL,CH ; To display Hour\nCALL DISP ; Calling DISP procedure for displaying Hour\nMOV DL,':' ; to display \u2018:\u2019 after Hour\nMOV AH,02 ; display the content of DL\nINT 21H\nMOV AL,CL ; To display Minute\nCALL DISP ; Calling DISP procedure for displaying Minute\nMOV DL,':' ; to display \u2018:\u2019 after Minute\nMOV AH,02 ; WRITE CHARACTER TO STANDARD OUTPUT\nINT 21H\nMOV AL,DH ; To display Second\nCALL DISP ; Calling DISP procedure for displaying second\nMOV AH,02 ; WRITE CHARACTER TO STANDARD OUTPUT\nMOV BH,0 ; video page number\nMOV DH,24 ; ROW\nMOV DL,00 ; COLUMN\nINT 10H ; Calling BIOS interrupts\nMOV AH,4CH ; Exit the program to OS\nINT 21H\nDISP PROC NEAR\nAAM ; stands for ASCII adjust for Multiplication or BCD\n\n; Adjust after Multiply\n\nADD AX,3030H\nMOV BX,AX ; move AX to BX\nMOV DL,BH ; move BH to DL\nMOV AH,02 ; WRITE CHARACTER TO STANDARD OUTPUT\nINT 21H\nMOV DL,BL ; move BL to DL\nMOV AH,02 ; WRITE CHARACTER TO STANDARD OUTPUT\nINT 21H\nRET ; Return\nENDP ; end procedure\nEND START\n</code></pre>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/06/#program-3","title":"Program 3","text":"<p>Write a program to read system date and display in this format DD-MM-YEAR.</p> <pre><code>.MODEL SMALL\n.STACK 20\nPRINT MACRO MSG\nMOV AH,09H\nMOV DX,OFFSET MSG\nINT 21H\nENDM\n.DATA\nDAY DB ?, ?, '-'\nMONTH DB ?, ?, '-'\nYEAR DB ?, ?, ?, ?, '$'\n.CODE\nSTART:\nMOV AX,@DATA\nMOV DS,AX\nMOV ES,AX\nMOV AH,2AH ; Function code to get Date\nINT 21H ; Call DOS service\nPUSH CX ; Saving year\nPUSH DX ; Saving Day and Month\nMOV AL,DL ; move DL to AL\nLEA SI,DAY ; load effective address of Day to SI\nMOV AH,00H ; Move 00H to AH\nCALL CONV ; To call CONV subroutine\nPOP DX ; Putting ASCII value in memory\nMOV AL,DH ; move DH to AL\nLEA SI,MONTH ; Load the effective address of Month to SI\nMOV AH,00H ; move 00H to AL\nCALL CONV ; To convert month to ASCII\nPOP AX ; Putting ASCII value in memory\nLEA SI,YEAR ; Load effective address of YEAR to SI\nCALL CONV ; To convert year to ASCII\n\nPRINT DAY ; To display Date\nMOV AH,4CH ; Exit program\nINT 21H\nCONV PROC NEAR ; Procedure convert Hexadecimal\nMOV CX,0000H ; Day, Month and Year to BCD\nMOV BX,000AH ; and ASCII for display\nNEXT:\nMOV DX,0000 ; move 0000 to DX\nDIV BX ; Separating the digits\nADD DL,30H ; Converting BCD to ASCII\nPUSH DX\nINC CX ; increment CX\nCMP AX,000AH\nJGE NEXT\nADD AL,30H\nMOV [SI],AL\nUP:\nPOP AX ; Putting ASCII value in memory\nINC SI ; increment SI\nMOV [SI],AL\nLOOP UP\nRET ; return\nCONV ENDP ; end of the procedure conv\nEND START\n</code></pre>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/07/","title":"07","text":""},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/07/#7-segment-led-interfacing","title":"7-Segment LED Interfacing","text":"<pre><code>DATA SEGMENT \nPORTA EQU 00H\nPORTB EQU 02H\nPORTC EQU 04H\nPORT_CON EQU 06H \nDATA ENDS \n\nCODE SEGMENT \nMOV AX, DATA \nMOV DS, AX \nORG 0000H \nSTART: \nMOV DX, PORT_CON  MOV AL, 10000000B  OUT DX, AL \nMOV SI, 0 \nMOV DI, 0 \nL0: MOV CX, 1FFFH  L1: MOV AL, S1[SI]  MOV DX, PORTA  OUT DX, AL \nLOOP L1 \nINC SI \nCMP SI, 16 \nJL L0 \nMOV DX, PORT_CON  MOV AL, 10000000B  OUT DX, AL \nJMP START \nORG 1000H \nS1 DB 11000000B \nDB 11111001B \nDB 10100100B \nDB 10110000B \nDB 10011001B \nDB 10010010B \nDB 10000010B \nDB 11011000B \nDB 10000000B \nDB 10010000B \nDB 10001000B \nDB 10000011B \nDB 11000110B \nDB 10100001B \nDB 10000110B \nDB 10001110B \nCODE ENDS \nEND\n</code></pre>"},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/08/","title":"08","text":""},{"location":"2_Core/MicroProcessors_%26_Interfacing/Practicals/08/#stepper-motor-interfacing","title":"Stepper Motor Interfacing","text":"<pre><code>DATA SEGMENT\nPORTA EQU 00H\nPORTB EQU 02H\nPORTC EQU 04H\nPORT_CON EQU 06H\nDATA ENDS\nCODE SEGMENT\nMOV AX, DATA\nMOV DS, AX\nORG 0000H\nSTART:\nMOV DX, PORT_CON\nMOV AL, 10000000B\nOUT DX, AL\nMOV SI, 0\nMOV DI, 0\nLL0:MOV CX, 2FFFH\nLL1:MOV AL, S2[DI]\nMOV DX, PORTC\nOUT DX, AL\nLOOP LL1\nINC DI\nCMP DI, 4\nJL LL0\nJMP START\nORG 1000H\nS2 DB 1101B\nDB 1011B\nDB 0111B\nDB 1110B\nCODE ENDS\nEND\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/","title":"Object-Oriented Programming","text":"Class Instructor Lecture Dr. Pranav M. Pawar Tutorial Dr. Pranav M. Pawar Practical Dr. Sujala D. Shetty <p>This course offers a comprehensive introduction to object-oriented programming (OOP), focusing on principles of object-oriented design, design patterns, and the Java programming language, including graphical user interface (GUI) development.</p> <p>Topics include inheritance, polymorphism, interfaces, and exception handling, as well as advanced topics like threading, event handling, and GUI development. Students will also learn design patterns such as Observer, Decorator, Factory, and Singleton, along with UML diagrams for modeling software structure and behavior. Additionally, the course covers file handling, JDBC for database interactions, and principles of software design, emphasizing cohesion, clarity, and maintainability in OOP.</p>"},{"location":"2_Core/Object_Oriented_Programming/01_Intro/","title":"01 Intro","text":""},{"location":"2_Core/Object_Oriented_Programming/01_Intro/#java-basics","title":"Java Basics","text":"<p>4 bit precision - returns 4 bits for numbers by default</p> <p>hexadecimal is small characters A B a b</p> <p>JDK 16</p> <p><code>.java</code> is the file extension</p> <p>java is case-sensitive</p> <p>file name should be the name of the class containing the main function (TesterClass)</p> <p>main method should always be <code>public static void main(String[] args)</code></p>"},{"location":"2_Core/Object_Oriented_Programming/01_Intro/#stages-of-java-program","title":"Stages of Java Program","text":"<pre><code>flowchart LR\ns[Source Code] --&gt;\n|Compiled,&lt;br/&gt;Interpreted| b[Byte Code] --&gt;\n|Executed| Output</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/01_Intro/#compilation","title":"Compilation","text":"<p><code>javac file.java</code></p> <p>Program gets compiled and intrepreted to a .class file bytecode</p> <p>Bytecode offers platform independence</p>"},{"location":"2_Core/Object_Oriented_Programming/01_Intro/#execution","title":"Execution","text":"<p><code>java file</code> (no extension)</p>"},{"location":"2_Core/Object_Oriented_Programming/01_Intro/#compilation-execution-in-one-go","title":"Compilation-Execution in one-go","text":"<p>If entire program is contained in a single class, then you just have to use <code>java abc.java</code> (just a single command)</p>"},{"location":"2_Core/Object_Oriented_Programming/01_Intro/#basic-program","title":"Basic Program","text":"<pre><code>class Tester\n{\n  public static void main(String args[]) // String is a class in java\n    // or (String ... args)\n  {\n    System.out.print(\"hello world\"); // same line\n\n    System.out.println(\"hello world\"); // new line\n  }\n}\n</code></pre> <p><code>print()</code> and <code>println()</code> belong to outstream of System class</p>"},{"location":"2_Core/Object_Oriented_Programming/01_Intro/#main","title":"main()","text":"<p>to pass mutiple arguments, we can also use <code>main(String ... args)</code></p> <p>Optional keywords</p> <ul> <li> <p>final = constant in c</p> </li> <li> <p>synchronized = for threading</p> </li> <li> <p>strictfp = strict floating point operations</p> </li> </ul>"},{"location":"2_Core/Object_Oriented_Programming/01_Intro/#errors","title":"Errors","text":"<p>Runtime error: <code>NoSuchMethodError:main</code> when there is a problem inside main() Eg: [ ] missing</p>"},{"location":"2_Core/Object_Oriented_Programming/01_Intro/#data-types","title":"Data Types","text":"<p>Float ends with f: 34.342f</p> <p>Double ends with d: 34.342d</p> <p>double 0x443 (will print 104, cuz 443 is taken as hex) (no need of d at the end)</p> <pre><code>flowchart TB\nTypes --&gt; primitive &amp; non-primitive\n\nprimitive --&gt; void &amp; boolean &amp; char &amp; int &amp; float &amp; double &amp; String\n\nnon-primitive --&gt; Arrays &amp; Structures &amp; Classes &amp; Lists</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/01_Intro/#table-from-slides","title":"Table (from slides)","text":"Data Type Default Value Memory (bytes) boolean false 1 bit char \\u0000 1 int 0 4 float 0.0f 4 double 0.0d 8 String null (without \"\") 0"},{"location":"2_Core/Object_Oriented_Programming/01_Intro/#control-statements","title":"Control Statements","text":""},{"location":"2_Core/Object_Oriented_Programming/01_Intro/#if-else","title":"if-else","text":"<p>same as c</p> <pre><code>if (cond)\n  statement;\nelse if (cond)\n  statement;\nelse\n  statement;\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/01_Intro/#switch","title":"switch","text":"<pre><code>switch(var)\n{\n  case 1: \n    statement; \n    break;\n  case 2: \n    statement;\n    break;\n  default: \n    statement;\n}\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/01_Intro/#loops","title":"Loops","text":"<pre><code>while(cond)\n{\n  statement;\n}\n\ndo {\n  statement;\n} while (cond);\n\nfor(init, cond, upd)\n  statement;\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/01_Intro/#input","title":"Input","text":"<pre><code>import java.util.Scanner;\n\npublic class Tester\n{\n  public static void main(String[] args)\n  {\n    Scanner inp = new Scanner(System.in);\n    // variable to access input stream\n\n    int x = inp.nextInt();\n  }\n}\n\n// scanner methods\n\n//check\ninp.hasNextLine();\ninp.hasNextInt();\ninp.hasNextFloat();\ninp.hasNextDouble();\n\n//input\nString line = inp.nextLine();\nString entireThing = inp.next();\nint i = inp.nextInt();\nfloat f = inp.nextFloat();\ndouble d = inp.nextDouble();\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/01_Intro/#operators","title":"Operators","text":""},{"location":"2_Core/Object_Oriented_Programming/01_Intro/#arithmetic","title":"Arithmetic","text":"<p>\\(+ - * / \\%\\)</p>"},{"location":"2_Core/Object_Oriented_Programming/01_Intro/#unary","title":"Unary","text":"<p>\\(+ \\quad - \\quad ++x \\quad y-- \\quad !\\)</p>"},{"location":"2_Core/Object_Oriented_Programming/01_Intro/#conditional","title":"Conditional","text":"<p>&amp;&amp;</p> <p>||</p> <p>?: ternary</p> <pre><code>var = cond?tVal:fVal;\n\nint x = (x&lt;5)?5:0;\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/01_Intro/#bitwisebitshift","title":"Bitwise/Bitshift","text":"Operator Name Function Example ~ Bitwise Complement y = ~x &lt;&lt; left shift Multiply by 2<sup>n</sup> x&lt;&lt;2 &gt;&gt; right shift Divide by 2<sup>n</sup> x&gt;&gt;2 &gt;&gt;&gt; unsigned right shift Divide by 2<sup>n</sup> x&gt;&gt;&gt;3 &amp; Bitwise AND perform AND bit-by-bit x&amp;y ^ Bitwise exclusive OR perform OR bit-by-bit x^y \\vert Bitwise inclusive OR perform OR bit-by-bit x \\vert y"},{"location":"2_Core/Object_Oriented_Programming/01_Intro/#type-casting","title":"Type Casting","text":"<p>changing the data type of variable during runtime for a momentary purpose</p> <pre><code>Integer.parseInt(); // string to int\nInteger.toString(); // int to string\n\nFloat.parseFloat();\nFloat.toString();\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/01_Intro/#method-overloadingoverriding","title":"Method Overloading/Overriding","text":"<p>Method Overloading: Multiple methods having the same name but different functionality Static polymorphism, as compiler knows overloading is happening beforehand</p> <p>Operator overloading: same operator symbol with different functionality Static polymorphism, as compiler knows overloading is happening beforehand eg: - is used for subtraction and also for negative numbers, % is used for mod and for percentages</p> <p>Method Overriding: parent and child class with same function name, but different functionality Dynamic polymorphism, as compiler does not know beforehand, and overriding occurs during run-time</p> <p>For method overloading, the function signature/argument/parameter list of the methods should be differ in atleast one of the following ways</p> <ol> <li>number of parameters</li> <li>data type of parameters</li> <li>order of data type of parameters</li> </ol> <p>Two functions of the same name but different return type is not valid</p> <pre><code>int add(int, int);\nFloat add(int, int);\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/01_Intro/#command-line-argument","title":"Command Line Argument","text":"<p>An information that directly follows program's name on the command line when it is executed.</p> <p>Arguments are stored as strings in the String args[] argument of the main().</p> <p>Each argument should be separated by (space)</p> <pre><code>java filename 10 134 3 // command line argument\n\nclass filename\n{\n  public static void main(String args[])\n  {\n    for (int i = 0; i&lt;args.length(); i++)\n    {   \n      int x = Integer.parseInt(args[i]);\n    }\n  }\n}\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/01_Intro/#keywords","title":"Keywords","text":""},{"location":"2_Core/Object_Oriented_Programming/01_Intro/#static","title":"<code>static</code>","text":"<p>only one instance</p>"},{"location":"2_Core/Object_Oriented_Programming/01_Intro/#variable","title":"variable","text":"<p>variable that acts as common property of all objects</p> <p>gets memory only once in the class area at time of class loading</p> <p><code>classname.var</code>, <code>object.var</code></p>"},{"location":"2_Core/Object_Oriented_Programming/01_Intro/#method","title":"method","text":"<p>belongs to class itself, not the object of the class</p> <p>This is why main() is declared as static: to avoid the need for calling it through an object</p> <p>only static methods can change the value of a static variable</p> <p>they cannot change values of non-static variables</p> <p><code>classname.method()</code></p>"},{"location":"2_Core/Object_Oriented_Programming/01_Intro/#block","title":"block","text":"<p>gets executed before main</p> <p>useful to initialize static data members</p>"},{"location":"2_Core/Object_Oriented_Programming/01_Intro/#class","title":"class","text":"<p>only a nested class can be static</p> <p>methods inside it will also be static</p> <pre><code>class Student\n{\n  static int x = 5;\n  public static void change()\n  {\n    x = 15;\n  }\n  static class Nested\n  {\n    public void nest() // static because it is inside a static class\n    {\n      x = 2;\n    }\n  }\n\n}\nclass Tester\n{\n  static\n  {\n    System.out.println(\"Static block executed\");\n  }\n  public static void main(String args[])\n  {\n    Student s = new Student();\n        System.out.println(s.x); // 5\n\n    Student.change();\n        System.out.println(s.x); // 10\n\n    Student.Nested n = new Student.Nested();\n  }\n}\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/01_Intro/#final","title":"<code>final</code>","text":""},{"location":"2_Core/Object_Oriented_Programming/01_Intro/#variable_1","title":"variable","text":"<p>value is fixed</p>"},{"location":"2_Core/Object_Oriented_Programming/01_Intro/#method_1","title":"method","text":"<p>cannot override it</p> <p>method overriding is when parent and subclass' methods have the same name</p>"},{"location":"2_Core/Object_Oriented_Programming/01_Intro/#class_1","title":"class","text":"<p>cannot extend/inherit it</p> <pre><code>final class Student\n{\n  final int x = 5;\n  public static void change()\n  {\n    x = 15; // compiler error\n  }\n  final void override()\n  {\n    System.out.println(\"hello\"); \n  }\n}\n\nclass Life extends Student // compiler error\n{\n  final void override()\n  {\n    System.out.println(\"hi\"); // compiler error\n  }\n}\n\nclass Tester\n{\n  public static void main(String args[])\n  {\n    Student s = new Student();\n        System.out.println(s.x); // 5\n\n    Student.change();\n        System.out.println(s.x); // 10\n\n    Student.Nested n = new Student.Nested();\n  }\n}\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/01_Intro/#packages","title":"Packages","text":"<p>is a collection of classes</p> <p>similar to (not same as) header files in C</p> <p><code>java.lang</code> is the default package in java; it is imported by default</p>"},{"location":"2_Core/Object_Oriented_Programming/01_Intro/#importing","title":"Importing","text":"<p><code>import packagename.*</code>, <code>import packagename.ClassName</code></p> <p>can contain</p> <ul> <li>classes</li> <li>subpackages</li> <li>interfaces</li> </ul> <p>(Libraries in C/C++ can only contain functions)</p>"},{"location":"2_Core/Object_Oriented_Programming/01_Intro/#uses","title":"Uses","text":"<ol> <li>prevent naming conflicts    allows to use the same class name multiple times in different packages</li> <li>simplify modular usage of classes</li> <li>control access using access specifier</li> <li>data encapsulation/hiding</li> </ol>"},{"location":"2_Core/Object_Oriented_Programming/01_Intro/#types","title":"Types","text":"<ol> <li>built-in</li> <li>java.lang       default package, primitive data types, math operations</li> <li>java.io       read/write from files/device</li> <li>java.util       Data structures</li> <li>java.applet</li> <li><code>java.awt</code></li> <li><code>java.swing</code></li> <li>java.net</li> <li>user-defined</li> <li>creating<ol> <li>create a directory with name of package</li> <li>create <code>MyClass.java</code> in directory with the the first statement as <code>package packagename</code></li> </ol> </li> <li>compiling<ol> <li><code>javac MyClass.java</code></li> <li><code>javac -d . MyClass.java</code></li> </ol> </li> <li>using       the main file should be inside the package folder<ol> <li><code>import myPackage1.MyClass</code></li> </ol> </li> </ol>"},{"location":"2_Core/Object_Oriented_Programming/01_Intro/#accessor-method","title":"Accessor Method","text":"<p>methods for accessing/getting data</p> <p>should always return data; hence return type shouldn't be void; display() isn't exactly an accessor</p> <p><code>public returnType getData()</code></p>"},{"location":"2_Core/Object_Oriented_Programming/01_Intro/#mutator-method","title":"Mutator Method","text":"<p>Methods for changing/setting data</p> <p>should be void</p> <p><code>public void setData(parameter)</code></p>"},{"location":"2_Core/Object_Oriented_Programming/01_Intro/#math-class","title":"Math Class","text":"<p>it is inbuilt in <code>java.lang</code> itself</p> <pre><code>double y = Math.sqrt(x),\n    t =  Math.pow(x, n);\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/02_Classes/","title":"02 Classes","text":""},{"location":"2_Core/Object_Oriented_Programming/02_Classes/#class","title":"Class","text":"<p>collection of data and related functions into a single entity</p> <p>contains</p> <ul> <li>fields/properties - variables</li> <li>methods - functions<ul> <li>constructor</li> <li>custom</li> </ul> </li> <li>nested classes</li> </ul> <p>Naming convention is TitleCase</p>"},{"location":"2_Core/Object_Oriented_Programming/02_Classes/#object","title":"Object","text":"<p>Instance of a class</p> <p>declared using <code>new</code> keyword</p> <p>. operator is called object reference operator / relationship operator</p> <pre><code>Classname var = new Constructor();\n</code></pre> <pre><code>class Student\n{\n  String name;\n  int age;\n  Student(String aName, int aAge) // constructor\n  {\n    name = aName;\n    age = aAge;\n  }\n  void display()\n  {\n    System.out.println(name + age); // abc10\n  }\n}\npublic class StudentTester\n{\n  public static void main(String args[])\n  {\n        Student s1 = new Student(\"abc\", 10);\n        s1.display(); \n  }\n}\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/02_Classes/#instanceof-operator","title":"<code>instanceof</code> operator","text":"<p>Checks if an object belongs to a particular class</p> <p>returns Boolean true/false</p> <p>Syntax: <code>(object reference var) instanceof (class/interface type)</code></p> <pre><code>boolean result = varName instanceof String;\nboolean result = varName instanceof CustomClass;\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/02_Classes/#object-reference","title":"Object reference","text":"<p>Just assigning one object name to another object name just assigns the pointer location; doesn't copy the data over</p> <pre><code>Student s1 = new Student(\"abc\" , 10);\ns1.display(); // abc10\n\nStudent s2 = s1;\n\ns2.setAge(20); \ns1.display(); // abc20\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/02_Classes/#constructor","title":"Constructor","text":"<p>function that gets invoked during object creation</p> <p>no return type Not even void, as constructor kinda returns object of class</p> <p>If the formal and actual parameter have the same name, then it will output the default values</p> <ul> <li>null for String</li> <li>0 for int</li> <li>0.0f for float</li> </ul>"},{"location":"2_Core/Object_Oriented_Programming/02_Classes/#types-of-constructors","title":"Types of constructors","text":"<ul> <li>default constructor (by compiler)</li> <li>Non-parameterized constructor</li> <li>parameterized contructor</li> <li>copy constructor</li> </ul>"},{"location":"2_Core/Object_Oriented_Programming/02_Classes/#copy-constructor","title":"Copy Constructor","text":"<p>Truly copy data from one object to another</p> <pre><code>class Student\n{\n  private int year;\n  Student(int year)\n  {\n    this.year = year;\n  }\n  Fruit(Student source)\n  {\n    this.year = source.year;\n  }\n}\nclass StudentTester\n{\n  public static void main()\n  {\n    Student s1 = new Student(2020);\n    Student s2 = new Student(s1); // will have the same values of s1\n  }\n}\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/02_Classes/#constructor-overloading","title":"Constructor Overloading","text":"<p>multiple constructors having the same name but different functionality. They differ in their function signature</p>"},{"location":"2_Core/Object_Oriented_Programming/02_Classes/#custom-print-message","title":"Custom Print Message","text":"<p>In order to get a custom output for <code>System.out.println(objectName)</code>, we can create a custom <code>public String toString()</code> for the class.</p> <pre><code>class Student\n{\n  int roll;\n  String name;\n\n  public String toString()\n  {\n    String text = \"Name is \" + this.name + \" Roll no is \" + this.roll;\n    return text;\n  }\n}\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/02_Classes/#this-keyword","title":"this Keyword","text":""},{"location":"2_Core/Object_Oriented_Programming/02_Classes/#refer-to-current-object","title":"refer to current object","text":"<p>useful when the actual and formal parameter have the same name</p> <pre><code>class Student\n{\n  int rno;\n  String name;\n\n    Student(int rno, String name)\n    {\n    this.rno = rno;\n      this.name = name;\n    } \n}\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/02_Classes/#invoke-current-class-method","title":"invoke current class method","text":"<pre><code>this.m();\n//equivalent to\nm(); // compiler adds this.\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/02_Classes/#invoke-current-class-constructor","title":"invoke current class constructor","text":"<p>the invoked contructor should have already been defined useful for chaining of constructors to avoid redundancy</p> <p>**Note: ** <code>this()</code> cannot be at the end</p> <pre><code>class Student\n{\n  int rno;\n  String name;\n  boolean student;\n  float fee;\n\n  Student()\n  {\n        student = true;\n  }\n\n  Student(int rno, String name)\n  {\n        this(); // calls Student()\n    this.rno = rno;\n    this.name = name;\n    // this(); here will give error\n    // Thanks Firas\n  }\n\n    Student(int rno, String name, float fee)\n    {\n    this(rno, name); // calls Student(int rno)\n    this.fee = fee;\n    } \n}\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/02_Classes/#pass-current-obj-as-argument-in-method-call","title":"pass current obj as argument in method call","text":"<pre><code>class Student\n{\n    void display(Student s1)\n  {\n    System.out.println(\"blah\");\n  }\n  void m()\n  {\n        display(this);\n  }\n}\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/02_Classes/#passed-as-argument-in-constructor-call","title":"passed as argument in constructor call","text":"<pre><code>class Student\n{\n  School sch;\n    Student(School sch)\n  {\n    this.obj = obj;\n  }\n  void display()\n  {\n    System.out.println(sch.city);\n  }\n}\nclass School\n{\n  int year = 2000;\n  String city = \"Dubai\";\n  School()\n  {\n    Student s1 = new Student(this);\n  }\n}\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/02_Classes/#return-current-object","title":"return current object","text":"<pre><code>class Student\n{\n  Student getStudent()\n  {\n    return this;\n  }\n  void msg()\n  {\n    System.out.println(\"hello\");\n  }\n}\nclass main\n{\n  public static void main(String args[])\n  {\n    new Student().getStudent().msg();\n    //equivalent to\n    new Student().msg();\n  }\n}\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/02_Classes/#access-modifier","title":"Access Modifier","text":"Accessibility <code>private</code> <code>default</code> <code>protected</code> <code>public</code> Same class Y Y Y Y Same package subclass N Y Y Y Same package non-subclass N Y Y Y Diff package subclass N N Y Y Diff package non-subclass N N N Y <p><code>private</code> does not get inherited hence not accessible even in subclass(child class); it is only accessible in the same class/nested class</p>"},{"location":"2_Core/Object_Oriented_Programming/02_Classes/#other-types","title":"Other Types","text":""},{"location":"2_Core/Object_Oriented_Programming/02_Classes/#nested-classes","title":"Nested Classes","text":"<p>is a class that is inside a function or another class. </p> <p>Nested Inner Class can access any private instance of outer class</p> <pre><code>Outer.Inner obj = new Outer().new Inner();\n\n//or\n\nOuter objo = new Outer();\nOuter.Inner obji = objo.new Inter();\n\n//or\n\n// create a function in the outer class that creates objects of outer class\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/02_Classes/#anonymous-classes","title":"Anonymous Classes","text":"<p>class that does one or more of the following, without even creating the class</p> <ul> <li>implements an interface</li> <li>inherits a class</li> </ul> <pre><code>Student s = new HelloWorld()\n{\n  int x = 5;\n  public void func()\n  {\n    x++;\n  }\n};\n</code></pre> <p>the entire  statement ends with <code>;</code> (just like any other java statement)</p>"},{"location":"2_Core/Object_Oriented_Programming/03_Arrays/","title":"03 Arrays","text":"<p>(write this somewhere else in future)</p> <p>String and Object are different classes directly under <code>java.lang</code> and hence that\u2019s why it is </p> <ul> <li><code>name.length()</code> - String</li> <li><code>a.length</code> without brackets - int</li> </ul>"},{"location":"2_Core/Object_Oriented_Programming/03_Arrays/#array","title":"Array","text":"<p>a non-primitive linear data structure that is a collection of elements of the same type</p> <p>starts with index 0</p> <p>in java, arrays are classes</p> <p>arrays are derived from <code>Object</code> class</p> <p>Steps</p> <ol> <li>declaration</li> <li>memory allocation    in java, memory for arrays is dynamically-allocated</li> <li>initialization</li> </ol> <pre><code>System.out.println(inta.getClass() +\n                   bytea.getClass() +\n                   shorta.getClass() + \n\n                   inta.getClass().getSuperClass() // same for bytea, shorta\n\n                   name.getClass() // name = \"hello\"\n                   )\n</code></pre> <pre><code>(Output)\n\nclass [I\nclass [B\nclass [S\nclass java.lang.Object\nclass [Ljava.lang.String //(String is an class under java.lang itself)\n</code></pre> <pre><code>graph TB\njava.lang --&gt; Object &amp; String\nObject --&gt; arrays</code></pre> <pre><code>// declaration, memory allocation, initialization in a single statement\ndouble[] myList = {1.9, 2.4, 34, 34};  \nint[] a = new int[10];\n\n// individually -both are correct\n// declaration\nint[] a;\nint b[];\n// memory allocation\na = new int[10];\n// initialization\na[3] = 100;\n// accessing\nint length = a.length;\nlength = b.length; // for 2d array, it will return no of rows\nlength = b[2].length; // returns the no of columns in 2nd index row\n\nSystem.out.println(a[3]);\n\n// display\nfor(int i = 0; i&lt;a.length; i++)\n  System.out.println(a[i]);\n\n// sum\nint sum = 0;\nfor(int i = 0; i&lt;a.length; i++)\n  sum += a[i];\n\n// largest element\nint max = a[0];\nfor(int i = 0; i&lt;a.length; i++)\n  if(a[i]&gt;max)\n    max = a[i];\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/03_Arrays/#array-of-objects","title":"Array of objects","text":"<p>we have to dynamically</p> <ol> <li>create the array</li> <li>create each location</li> </ol> <pre><code>// creating the array\nStudent[] s = new Student[10];\n\n// creating individual locations\nfor(int i = 0; i&lt; s.length; i++)\n{\n  s[i] = new Student();\n}\n\n// or\nfor(int i = 0; i&lt; s.length; i++)\n{\n  int x = inp.nextInt();\n  s[i] = new Student(x);\n}\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/03_Arrays/#passing-array-to-method","title":"Passing array to method","text":"<p>Pass the name of the array - you're basically passing the pointer</p> <pre><code>display(arr); // without []\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/03_Arrays/#returning-array-from-method","title":"Returning array from method","text":"<pre><code>public static int[] mod()\n{\n  return new int[] {1,2,3};\n  // or\n  int[] a = new int[3]; a[0] = 1; a[1] = 2; a[2] = 3;\n  return a;\n\n    // or \n  int[] a = {1,2,3};\n  return a;\n}\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/03_Arrays/#copying-an-array","title":"Copying an Array","text":"<pre><code>// same array, but new pointer\nint[] b = a; \n\n//independent copied array\nint[] b = new int[a.length];\n// Thanks Firas\n// Firas said just int[] b is not enough. why tho?\n\nSystem.arraycopy(a, 0, b, 0, a.length); // completely copy the array\nSystem.arraycopy(a, 1, b, 0, 2);\n\n// skeleton of arraycopy\npublic static void arraycopy(\n        Object src, int srcPos, \n    Object dest, int destPos,\n    int length\n    )\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/03_Arrays/#multidimensional-array","title":"Multidimensional Array","text":"<p>Array e (below code example) will look like</p> \\[ \\begin{bmatrix} 2 &amp; 3 &amp; 7 \\\\ 3 &amp; 5 &amp; 6 \\end{bmatrix} \\] <pre><code>// declaration\nint[][] a = new int[10][20];\nint[][][] b = new int[10][20][30];\n\nint[][] e = { \n  {2, 3, 7}, // row wise allocation\n  {3, 5, 6}\n};\n\nint[][] a = new int[10][10];\nfor(int i = 0; i &lt; a.length; i++) //row\n  for(int j = 0; j &lt; a[i].length; j++) // column\n    a[i][j] = inp.nextInt();\n\n// or\nint[][] a = new int[m][n];\nfor(int i = 0; i&lt;m; i++)\n  for(int j = 0; j&lt;n; j++)\n    a[i][j] = inp.nextInt();\n\n// addition of 2 arrays\nfor(int i = 0; i&lt;m; i++)\n  for(int j = 0; j&lt;n; j++)\n    add[i][j] = a[i][j] + b[i][j];\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/04_Strings/","title":"04 Strings","text":""},{"location":"2_Core/Object_Oriented_Programming/04_Strings/#strings","title":"Strings","text":"<p>immutable - you cannot change anything directly</p> <p><code>Object</code> and <code>String</code> are interchangeable</p> <pre><code>// both are valid\nObject test1 = \"hello\";\nString test2 = \"hello\"; \n\n// strings\nString s1 = \"hello world\";\nString s = new String(\"hello world\");\n\n// length\ns.length(); // 4\n\n// accessing char\ns.charAt(3);\ns.substring(i); // i to end of the string\ns.substring(i, j); // i to j-1\n\n//concat\ns1 += s2;\n// DONT FORGET s1 = \ns1 = s1.concat(s2); \n// s2 is added to s1 and returns it\n// basicaly s1 is the object in focus, and it is getting modified\n\n// search\n// returns int position of the 1st char of the string\ns1.indexOf(\"HELLO\"); // the string is case-sensitive\ns1.indexOf(s2); // searches for s2 inside s1\ns1.indexOf(s2, 3); // (string, startIndex)\n\n\"Hello\".equals(\"hello\"); // returns false\n\"Hello\".equalsIgnoreCase(\"hello\"); // returns true\n\nchar[] dh = s1.toCharArray();\n\nInteger.parseInt(numberString); // String to int\nFloat.parseFloat(numberString);\n\nInteger.toString(numbervar); // int to String\nFloat.toString(numbervar);\nDouble.toString(numbervar);\nBoolean.toString(numbervar);\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/04_Strings/#comparing-strings","title":"Comparing Strings","text":"<pre><code>s1 = \"hello\";\ns2 = \"hello\";\n\n// equality\nif(s1 == s2) // compares address\nif( s1.equals(\"hello\") ) // compares characters\n\nif( s1.compareTo(\"hello\") ) // compares objects including strings\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/04_Strings/#char-array","title":"Char Array","text":"<pre><code>char ch = 'w';\nchar[] dh = { ch, 'o', 'r', 'd'};\n\nchar dh[] = new char[ buffer.length() ];\n\n//char array\ndh = s1.toCharArray();\nArrays.equals(dh1, dh2); // 2 char arrays\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/04_Strings/#char-array-vs-string","title":"Char array vs String","text":"<p>Char array is the primitive strings we have in C/C++, but it doesn't have the advanced features of the String objects in java</p> <p>Another difference is that String objects have automatically-included <code>\\0</code></p>"},{"location":"2_Core/Object_Oriented_Programming/04_Strings/#stringbuffer","title":"<code>StringBuffer</code>","text":"<p>gives you the best of strings and char array; they are basically mutable Strings</p> <pre><code>StringBuffer sb = new StringBuffer(\"hello there\");\n\nString s = sb.toString();\n</code></pre> <p>you can append, set without creating a new String each time</p> <p>default capacity = 16 (not size) has a minimum allocation of memory for 16 characters; \\(\\ge 16\\) is valid</p> <ul> <li>length = current number of characters</li> <li>capacity = max length</li> </ul>"},{"location":"2_Core/Object_Oriented_Programming/04_Strings/#functions","title":"Functions","text":"<ul> <li><code>boolean length()</code> returns length of StringBuffer</li> <li><code>boolean capacity()</code> returns capacity of </li> <li><code>setLength(length)</code></li> <li><code>ensureCapacity()</code></li> <li><code>charAt(index)</code> return the character at index</li> <li><code>setCharAt(index, newchar)</code> change the character at index</li> <li><code>getChars(startIndex, nOfCharacters, arrayName, 0)</code> takes substring and returns as character array</li> <li><code>reverse()</code> reverses string</li> <li><code>append(string, startIndex, endIndex)</code> append data</li> <li><code>String insert(insertPos, arrayName, startIndex, endIndex)</code></li> <li><code>deleteCharAt(index)</code> remove a character</li> <li><code>delete(startIndex, endIndex)</code> remove a substring</li> </ul>"},{"location":"2_Core/Object_Oriented_Programming/04_Strings/#stringtokenizer","title":"<code>StringTokenizer</code>","text":"<p>partitions String into individual substring(s)</p> <p>constructor takes 2 parameters</p> <ol> <li>input string</li> <li>delimiter <code>(eg: comma, space, colon)</code></li> </ol> <p><code>import java.util.StringTokenizer</code></p> <pre><code>String s = \"https://ahmedthahir.tk\";\nStringTokenizer st = new StringTokenizer(s, \"://.\"); // order doesn't matter, but the pattern matters\n\nSystem.out.println(st.countTokens());\nwhile(st.hasMoreTokens())\n  System.out.println(st.nextToken());\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/05_Lists/","title":"05 Lists","text":""},{"location":"2_Core/Object_Oriented_Programming/05_Lists/#lists","title":"Lists","text":"<pre><code>flowchart BT\nVector-Class &amp; ArrayList-Class &amp; LinkedList-Class --&gt;\n|implements|List --&gt;\n|extends|Collection</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/05_Lists/#vector","title":"<code>Vector</code>","text":"<p>obsolete/not recommended to use</p> <pre><code>import java.util.Vector;\n\nVector&lt;TypeClass&gt; arrL = new Vector&lt;TypeClass&gt;();\nVector&lt;Integer&gt; arrL = new Vector&lt;Integer&gt;();\n\n//constructors\nVector();\nVector(int initialCapacity);\nVector(int initialCapacity, int capacityIncrement);\n\n// E is element type\nvoid add(int index, E element);\nboolean add(E e);\nvoid addElement(E obj);\nboolean addAll(Collection C);\nboolean addAll(int index, Collection C);\n\nvoid setElementAt(Object element, int index);\nboolean removeElement(Object element);\nboolean removeAll(Collection c);\n\nint capacity();\nvoid clear();\n\nv.get(int index);\n\nboolean contains(Object element);\nboolean containsAll(Collection c);\n\nObject elementAt(int index);\nObject firstElement();\nObject lastElement();\n\nboolean isEmpty();\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/05_Lists/#arraylist","title":"<code>ArrayList</code>","text":"<p>As opposed to arrays, array lists are</p> <ul> <li>mutable</li> <li>size is dynamic</li> <li>only for non-primitive data types</li> <li>you have to access element using <code>arrL.get(index)</code> rather than <code>[]</code></li> </ul> <pre><code>import java.util.ArrayList;\n\nArrayList&lt;TypeClass&gt; arrL = new ArrayList&lt;TypeClass&gt;(); // size is not compulsory\nArrayList&lt;Integer&gt; arrL = new ArrayList&lt;Integer&gt;();\n\nList&lt;Integer&gt; l = new ArrayList&lt;&gt;();\n// this is also allowed\n// List is the parent class of LinkedList\n// no need to specify type for the second &lt;&gt;\n\n// constructors\nArrayList(); // build an empty array list\nArrayList(int capacity); // build an array list with initial capacity\nArrayList(Collection c); // build an array list intialized with the elements from collection c\n// you can pass linkedlist as a parameter (collection class)\n\narrL.add(Object e);\narrL.add(20); // 20\narrL.add(50); // 20 50\n\narrL.add(int index, Object e);\narrL.add(index, 130); // 20 50 130\narrL.add(2, \"hello\");\n\nal.addAll(Collection c);\n\nal.set(int index, Object newData);\n\narrL.remove(int index);\narrL.remove(2); // 20 50\narrL.remove(Object data); // String, Integer, Float\n\narrL.get(index);\narrL.get(1); // 50\n\narrL.size(); // 2\n\nCollections.sort(list); // ascending\n\n// display\nSystem.out.println(arrL); // [20, 50]\n\nfor(int i = 0; i&lt;arrL.size(); i++)\n  System.out.print(arrL.get(i) + \" \"); // 20 50\n\n// enhanced for loop\nfor(String str:arrL)\n  System.out.println(str);\nfor(Integer i:arrL)\n  System.out.println(i);\n\nal.indexOf(Object o);\nal.contains(Object o); // searches for element and returns true/false\nal.clear(); // delete all elements of the arraylist\n\nArrayList&lt;String&gt; newal = (ArrayList&lt;String&gt;) al.clone();\n\nal.ensureCapacity(int minCapacity); // increases the size of the arraylist to the minCapacity\n// it is overriden method as it is available for StringBuffer also\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/05_Lists/#linkedlist","title":"<code>LinkedList</code>","text":"<pre><code>import java.util.LinkedList;\n\nLinkedList&lt;Type&gt; l = new LinkedList&lt;Type&gt;();\nLinkedList&lt;Integer&gt; l = new LinkedList&lt;Integer&gt;();\n\n// constructors\nLinkedList();\nLinkedList(Collection c);\n// you can pass arraylist as a parameter (collection class)\n\nl.size();\n\nl.add(Object e) // returns boolean - true/false - after doing the thingy to indicate if the element was added or not\nl.add(3);\n\nl.add(int index, Object e); // returns void\nl.add(2, 3);\n\nl.addAll(Collection c); // take all elements of arraylist and it to the linked list \n\nl.addFirst(Object e);\nl.addLast(Object e); // most performance-efficient\n\nl.remove(); // removes the first element, ie the default index passed is 0\nl.remove(int index);\nl.revove(Object o);\n\nl.removeFirst();\nl.removeLast();\nl.removeFirstOccurence(Object e);\nl.removeLastOccurence(Object e);\n\nl.get(index);\nl.get(2);\n\nl.set(int index, Object newdata);\nl.set(2, \"hello\");\n\nl.clear(); // delete all elements\nObject obj = l.clone();\nboolean contains(Object item);\n\nl.indexOf(Object item); //returns null if it doesn't find the element\nl.lastIndexOf(Object item);\nObject poll(); // removes and returns the last element\n// more efficent than remove() as there is only instance of it\nObject pollFirst();\nObject pollLast();\n\nCollections.sort(list); // ascending\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/05_Lists/#differences","title":"Differences","text":"<code>Vector</code> <code>ArrayList</code> <code>LinkedList</code> Each location contains data data 1. data2. pointer to previous node3. pointer to next node Type Dynamic Array double linked list setting data in between slowest intermediate fastest reason for performance insertion/deletion involves affecting all succeeding elements only pointers and data of few nodes are affected; other nodes are left unaffected getting data fastest slowest mincapacity 10 10 - Increase in size 2x (if not mentioned)else user increment 1.5x -"},{"location":"2_Core/Object_Oriented_Programming/05_Lists/#iterator","title":"<code>Iterator</code>","text":"<p>can only iterate in the forward direction</p> <pre><code>import java.util.Iterator;\n\nboolean hasNext();\n(Type) itr.next(); // returns the current element and moves the pointer to the next position\n// same like p++ in C++\n\n// the default return type of next() is String, so we have to type cast\n\nvoid remove(); // removes the last element returned by the iterator\n\n// Integer\nIterator&lt;Integer&gt; itr = al.iterator(); // al is the arraylist\n// itr is an indirect pointer/cursor to the location\n// JUST BEFORE THE 0TH INDEX\n\nwhile(itr.hasNext())\n{\n  int i = (Integer) itr.next();\n\n    if(i%2 == 0)\n      itr.remove(); // removes odd\n}\n\n// String\nIterator&lt;String&gt; itr = l.iterator();\nwhile(itr.hasNext())\n{\n  itr.remove();\n  Iterator&lt;String&gt; itr2 = itr.iterator(); // not sure\n  while(itr2.hasNext()) // to go through each element\n  {\n    System.out.println(itr2.next() + \"\");\n  }\n}\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/05_Lists/#listiterator","title":"<code>ListIterator</code>","text":"<p>iterate forward and backward</p> <pre><code>import java.util.ListIterator;\n\nListIterator&lt;String&gt; litr = null;\nlitr = al.listIterator();\n\nwhile(litr.hasNext())\n{\n  System.outprintln(litr.next());\n}\n\nwhile(litr.hasPrevious())\n{\n  System.outprintln(litr.previous());\n}\n\nboolean hasNext();\nE next();\nint nextIndex();\n\nboolean hasPrevious();\nE previous(); // returns the current element and moves the pointer to the previous location \n// same like p-- in C++\nint previousIndex();\n\nvoid remove(); // remove the last element which is returned by the next() or previous()\nvoid set(E e); // replace the last element which is returned by the next() or previous()\nvoid add(E e); // insert before next element returned by next() method\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/06_Inheritance/","title":"06 Inheritance","text":""},{"location":"2_Core/Object_Oriented_Programming/06_Inheritance/#encapsulation","title":"Encapsulation","text":"<p>put data and related functions in a single capsule, using classes and access specifiers</p> <p>helps in data-hiding</p> <p>default is the default access specifier</p> <ul> <li>trying to call a private member incorrectly will give a run-time error</li> <li>if a class has private constructor, then you cannot create the object of the class from outside that class; this will be a compile-time error</li> </ul> <p>if a class/interface is of private, then you cannot access that class at all; that\u2019s why don\u2019t pick this option</p>"},{"location":"2_Core/Object_Oriented_Programming/06_Inheritance/#inheritance","title":"Inheritance","text":"<p>Inheritance creates an is-a relation</p> <p>Constructors are not inherited</p> <p><code>extends</code> keyword is used for inheritance in Java</p> <p>Base class constructor==s== are called before the current class constructor</p> <pre><code>public class Derived extends Base //public inheritance\n{\n\n}\n\nBase obj = new Derived(); // will have characteristic of the Derived class\n// this is like what we did for\n// - String and Object\n// - List and ArrayList/LinkedList\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/06_Inheritance/#types-of-inheritance","title":"Types of Inheritance","text":"<p>java does not support multiple inheritance using classes</p> <p>we need to use interfaces</p> <pre><code>graph TD\n\nsubgraph Single\na --- b\nend\n\nsubgraph Multi-level\nc --- d --- e\nend\n\nsubgraph Hierarchical\nf --- g &amp; h &amp; i\ng --- j &amp; k\nend\n\nsubgraph Multiple\nx &amp; y --- z\nend</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/06_Inheritance/#super-keyword","title":"<code>super</code> keyword","text":"<p>refer to the base class</p> <ol> <li><code>super()</code> base class constructor</li> <li>I guess this is why classes don\u2019t support multiple inheritance</li> <li>cuz Java won\u2019t know which base class to refer to when using <code>super()</code></li> <li><code>super.var</code> base class property</li> <li><code>super.func()</code> base class function</li> </ol> <p>not valid - <code>super.super(), super.super.method(), super.super.var</code></p>"},{"location":"2_Core/Object_Oriented_Programming/06_Inheritance/#function-overriding","title":"Function Overriding","text":"<p>Base and derived classes have a function with the same name, but with different functionality</p> <p>private, static and final methods cannot be over-ridden</p> <p>doubt: private methods won\u2019t even be inherited, so it\u2019s not considered as over-riding, right?</p> <pre><code>class Derived extends Base\n{\n  void function1() // run-time binding\n  {\n\n  }\n  @Override\n  void function2() // compile-time binding\n  {\n\n  }\n}\n</code></pre> <p>Compile-time overriding is better for performance and bug prevention</p> <ul> <li>if you keep everything as runtime, execution of program will be slow</li> <li>Therefore, it is better to make everything as compile-time</li> </ul>"},{"location":"2_Core/Object_Oriented_Programming/06_Inheritance/#abstract-method","title":"Abstract Method","text":"<p>Method that only has function prototype (declared, but not defined)</p> <pre><code>public abstract void func();\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/06_Inheritance/#abstract-class","title":"Abstract Class","text":"<p>conceptual class which acts a bridge bw class and interface</p> <p>Abstract class is a class containing abstract method</p> <ul> <li>can be inherited</li> <li>can contain constructor<ul> <li>called when creating objects of child classes</li> <li>we cannot create objects of the abstract class itself</li> </ul> </li> <li>can also have final methods</li> </ul> <pre><code>abstract class Vehicle\n{\n  Vehicle()\n  {\n    System.out.println(\"This comes under Vehicle class\"); // gets printed when creating object of any Vehicle subclasses\n  }\n  abstract public void sound();\n\n  String name; // gets inherited for all Vehicle subclasses\n  public String getName() \n  {\n    return name;\n  }\n}\n\nclass Car extends Vehicle\n{ \n  public void sound() // NOT OVERRIDING, as sound was just an abstract method in base class\n  {\n    System.out.println(\"Woof\");\n  }\n}\n\nCar c = new Car();\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/06_Inheritance/#interface","title":"Interface","text":"<p>all types of inheritances are possible using interfaces. Methods from interface cannot use <code>protected</code></p> <p>In new versions, we can have</p> <ul> <li><code>static/default</code> methods in interfaces with their definition also</li> <li><code>private</code> methods</li> </ul>"},{"location":"2_Core/Object_Oriented_Programming/06_Inheritance/#automatic","title":"Automatic","text":"<ol> <li>variables are automatically</li> <li>public</li> <li>static</li> <li>final</li> <li>all functions are automatically</li> <li>public</li> <li>abstract</li> <li>functions can only have prototype - declared, but not defined</li> </ol>"},{"location":"2_Core/Object_Oriented_Programming/06_Inheritance/#conditions","title":"Conditions","text":"<ul> <li>A class that <code>implements</code> an interface must have function definition for all the functions of the interface (and extended interfaces)</li> <li>a single interface can extend multiple interfaces (multiple inheritance)</li> <li>can have <code>default</code> methods</li> <li>can have <code>static</code> methods<ul> <li>can be called just by using the interface name</li> </ul> </li> </ul> <p>We use both <code>extends</code> and <code>implements</code> here</p> <pre><code>interface i1\n{\n  void f1();\n}\n\ninterface i2\n{\n  void f2();\n}\n\ninterface Vehicle extends i1, i2\n{\n  void f();\n}\n\nclass Bicycle implements Vehicle\n{\n  void f1(){;}\n  void f2(){;}\n  void f(){;}\n}\n\n// array of instruments of various data types\nInstrument[] orchestra = {\n  new Wind(),\n  new Percussion(),\n  new Brass()\n};\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/06_Inheritance/#abstract-vs-interfaces","title":"Abstract vs Interfaces","text":"Abstract Interface Supported methods abstract, concrete abstract Supported variables all types of variables only static and final multiple inheritance N Y <code>extends</code> only classes only interfaces <code>implements</code> Y N creation of objects N N member access any only public"},{"location":"2_Core/Object_Oriented_Programming/07_Compare/","title":"07 Compare","text":""},{"location":"2_Core/Object_Oriented_Programming/07_Compare/#comparable-and-comparator","title":"Comparable and Comparator","text":"<p>they both are interfaces</p> <p>useful for elements of <code>Collection</code></p> <ul> <li>sorting</li> <li>comparing</li> </ul> <pre><code>Collections.sort(list); // sort() is an abstract function\n\nCollections.sort( list, new NameComparator() );\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/07_Compare/#comparable","title":"Comparable","text":"<pre><code>// Comparable\nclass Student implements Comparable&lt;Student&gt;\n{\n  String name;\n  int age;\n\n  Student(String n, int a)\n  {\n    name = n;\n    a = age\n  }\n\n  public int compareTo(Student s)\n  {\n    if(age &lt; s.age)\n      return -1;\n    else if(age &gt; s.age)\n      return 1;\n    else\n      return 0;\n  }\n}\n\n// ArrayList\n\nCollections.sort(al); // sorting based on age\n//display\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/07_Compare/#comparator","title":"Comparator","text":"<pre><code>// Comparable\nclass Student implements Comparable&lt;Student&gt;\n{\n  String name;\n  int age;\n\n  Student(String n, int a)\n  {\n    name = n;\n    a = age\n  }\n}\n\nclass AgeComparator implements Comparator&lt;Student&gt;\n{\n  public int compare(Student s1, Student s2)\n    {\n    if(s1.age &lt; s2.age)\n      return -1;\n    else if(s1.age &gt; s2.age)\n      return 1;\n    else\n      return 0;\n    }\n}\n\nclass NameComparator implements Comparator&lt;Student&gt;\n{\n  public void compare(Student s1, Student s2)\n  {\n    return s1.name.compareTo(s2.name);\n  }\n}\n\n// ArrayList\n\nCollections.sort(al, new AgeComparator());\n// display\n\nCollections.sort(al, new NameComparator());\n// display\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/07_Compare/#difference","title":"Difference","text":"Comparable Comparator package <code>java.lang</code> <code>java.util</code> method to implement <code>compareTo()</code> <code>compare()</code> Sorting Sequence Single Multiple Affect original class Y N Sorting <code>Collections.sort(list);</code> <code>Collections.sort( list, new NameComparator() );</code>"},{"location":"2_Core/Object_Oriented_Programming/08_Exceptions/","title":"08 Exceptions","text":""},{"location":"2_Core/Object_Oriented_Programming/08_Exceptions/#exceptions","title":"Exceptions","text":"<p>warnings, not exactly errors</p> Checked Unchecked handled at compile-time runtime inherited from (class) Exception RuntimeException"},{"location":"2_Core/Object_Oriented_Programming/08_Exceptions/#exception","title":"Exception","text":"<p>For accessing out of bounds array data, JVM throws <code>ArrayIndexOutOfBoundsException</code></p> <ol> <li>JVM will show a warning</li> <li>it will just skip the exception</li> <li>then proceed with the rest of the program</li> </ol>"},{"location":"2_Core/Object_Oriented_Programming/08_Exceptions/#exception-handling","title":"Exception-Handling","text":"<p>use <code>try-catch</code></p> <ul> <li>logic in <code>try</code></li> <li>exception will be caught by <code>catch</code></li> <li><code>finally</code> block contains all statements that must be executed when exception does or does not occurs</li> </ul> <p>IDK</p> <ol> <li>neither can exist independently, but not finally is not compulsory</li> <li>nested try is possible, but nested catch is not</li> <li>nothing can come up after <code>finally</code> - unreachable catch block error</li> </ol> <pre><code>try {\n  // code to test\n\n  try {\n    // something\n  }\n  catch(Exception E)\n  {\n    //something\n  }\n}\ncatch(ArrayIndexOutOfBoundsException e) {\n  //\n}\ncatch(Exception2 e2) {\n  //\n}\ncatch(Exception e) { // all exceptions (checked/unchecked)\n  //\n}\nfinally {\n    //\n}\n\nSystem.out.println(\"Program done\"); // doesn't get executed\n</code></pre> <p>here, statement1 runs, but statement2 doesn\u2019t. this is because, the flow of control goes to the catch after the throw</p> <p>Similarly, the last statement doesn\u2019t get executed because of <code>finally</code> block</p>"},{"location":"2_Core/Object_Oriented_Programming/08_Exceptions/#throw-and-throws","title":"<code>throw</code> and <code>throws</code>","text":"<p>you can explicitly throw any kind of exception</p> <p>can come with/without <code>try-catch</code></p> <code>throw</code> <code>throws</code> no of exceptions at a time only one multiple i\u2019m not sure this, butdoesn\u2019t actually throw - just shows that the function might throw location function definition function prototype can come inside <code>throws</code> cannot come inside <code>throw</code> type of exception unchecked checked/unchecked followed by exception instance exception class example - <code>throw new ArithmeticException(\u201cblah\u201d);</code>- <code>throw e</code> <code>void test() throws IOException{}</code>"},{"location":"2_Core/Object_Oriented_Programming/08_Exceptions/#idk","title":"IDK","text":"<p>error</p> <ul> <li>Compile time - Syntax errors</li> <li>Runtime error - wrong constructor for initialization</li> </ul> <p>exceptions</p> <ul> <li>Runtime exception <ul> <li>unexpected values - divide by 0</li> <li>array index out of bound</li> </ul> </li> </ul>"},{"location":"2_Core/Object_Oriented_Programming/08_Exceptions/#custom-exceptions","title":"Custom Exceptions","text":"<pre><code>class CustomException extends IllegalArgumentException\n{\n  String message = \"Blah\";\n  CustomException(String s)\n  {\n    super(s); // or super(message);\n  }\n  @Override\n  public String toString() \n  {\n    return message;\n  }\n}\n\n// somewhere else\ntry {\n  throw new CustomExcepiton(\"specific message\"); // prints specific message\n} catch(CustomException e) {\n  System.out.println(e); // prints Blah\n}\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/08_Exceptions/#common-exceptions","title":"Common Exceptions","text":"<ol> <li><code>ArithmeticException</code></li> <li><code>ArrayIndexOutOfBoundException</code></li> <li><code>IOException</code></li> <li><code>NullPointerException</code></li> <li><code>StringIndexOutOfBoundsException</code></li> <li><code>FileNotFoundException</code></li> <li><code>NumberFormatException</code></li> </ol>"},{"location":"2_Core/Object_Oriented_Programming/08_Exceptions/#exception-methods","title":"Exception Methods","text":"<pre><code>e.func();\n\npublic String getMessage();\n// inside System.out.println()\n// details of why the exception happened\n// eg: / by zero\n\npublic String toString(); // name + getMessage()\n// eg: java.lang.ArithmeticException / by zero\n\npublic void printStackTrace();\n// outside System.out.println()\n// toString() + location of exception\n// eg: java.lang.ArithmeticException / by zero at Test.main(Testjava:9)\n\npublic Throwable getCause(); // toString()\n// eg: java.lang.ArithmeticException / by zero\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/09_Thread/","title":"09 Thread","text":""},{"location":"2_Core/Object_Oriented_Programming/09_Thread/#threads","title":"Threads","text":"<p>independent subprocess</p> <p>Multi-threading allows for multiple subprocesses to occur for perform a task</p>"},{"location":"2_Core/Object_Oriented_Programming/09_Thread/#processor","title":"Processor","text":"<p>has 3 stages to perform tasks</p> <p>\\(\\fbox F \\fbox D \\fbox E\\)</p> <ul> <li>Fetch instruction</li> <li>Decode instruction</li> <li>Execute instruction</li> </ul>"},{"location":"2_Core/Object_Oriented_Programming/09_Thread/#life-cycle-of-thread","title":"Life Cycle of Thread","text":"<pre><code>flowchart LR\nnew &amp; blocked &amp; waiting --&gt; runnable\nrunnable --&gt; blocked &amp; waiting &amp; terminate</code></pre> <ul> <li>new (born) state<ul> <li><code>start</code> is called implicitly(on it\u2019s own)</li> </ul> </li> <li>Blocked state<ul> <li>paused / waiting for I/O or notification</li> <li>short duration</li> </ul> </li> <li>Waiting state<ul> <li>processor is busy</li> <li>when finally going to runnable, <code>notify() / notifyall()</code> method is called</li> </ul> </li> <li>runnable state<ul> <li>highest-priority thread enters</li> </ul> </li> <li>terminate (Dead) state<ul> <li>thread has been processed</li> </ul> </li> <li>sleeping state<ul> <li><code>sleep(t)</code> is called</li> <li>\\(t\\) is in ms</li> <li>long duration</li> <li>exits this state when sleep timer has expired</li> </ul> </li> </ul>"},{"location":"2_Core/Object_Oriented_Programming/09_Thread/#priorities","title":"Priorities","text":"<ul> <li><code>Thread.MIN_PRIORITY</code> - 1</li> <li><code>Thread.NORM_PRIORITY</code> - 5 (default)</li> <li><code>Thread.MAX_PRIORITY</code> - 10</li> </ul> <p>New threads inherit the priority of the thread that created it</p>"},{"location":"2_Core/Object_Oriented_Programming/09_Thread/#timeslicing","title":"Timeslicing","text":"<p>Round robin fashion</p> <p>The initial run takes place based on priority ensuring that each task gets run for 4s. Then, purely based on priority, tasks are run</p> <p>In the following example, let\u2019s say that priority is \\(T_2 &gt; T_4 &gt; T_3 &gt; T_1\\),</p> \\[ \\color{hotpink} \\underbrace{ \\underset{4s}{\\fbox{$T_2$}} \\underset{4s}{\\fbox{$T_4$}} \\underset{4s}{\\fbox{$T_3$}} \\underset{4s}{\\fbox{$T_1$}} }_\\text{initial run} \\color{orange} \\underbrace{ \\underset{4s}{\\fbox{$T_2$}} \\underset{4s}{\\fbox{$T_2$}} \\underset{4s}{\\fbox{$T_2$}} \\underset{4s}{\\fbox{$T_4$}} \\underset{4s}{\\fbox{$T_3$}} \\underset{4s}{\\fbox{$T_3$}} \\underset{4s}{\\fbox{$T_1$}} }_\\text{purely based on priority} \\]"},{"location":"2_Core/Object_Oriented_Programming/09_Thread/#implementation","title":"Implementation","text":"<p>Both methods are pretty much identical</p> <ul> <li>implementing <code>Runnable</code> (better)</li> <li>or, extending <code>Thread</code><ul> <li>not recommended</li> <li>cuz then we can\u2019t inherit any other class (java doesn\u2019t support class multiple inheritance)</li> </ul> </li> </ul>"},{"location":"2_Core/Object_Oriented_Programming/09_Thread/#implement-runnable","title":"Implement <code>Runnable</code>","text":"<pre><code>class MyThread implements Runnable \n{\n  public void run() {\n      // logic    \n    }\n}\n\npublic class Tester\n{\n    Thread t1 = new Thread(new MyThread() );\n    Thread t2 = new Thread(new MyThread() );\n\n  // or\n    Runnable r = new MyThread();\n  Thread t = new Thread(r);\n}\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/09_Thread/#extending-thread","title":"Extending <code>Thread</code>","text":"<pre><code>class MyThread extends Thread\n{\n  public void run()\n  {\n    // logic\n  }\n}\n\nclass Tester\n{\n  public static void main()\n  {\n        MyThread t = new MyThread();\n  }\n}\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/09_Thread/#inter-thread-communication","title":"Inter Thread Communication","text":"<p>proper coordination/communication between thread helps take care of deadlock situation</p> <pre><code>MyThread t1 = new MyThread();\n\nt1.start();\nt1.sleep(4000); // ms\nt1.join();\nt1.suspend(); // stop it indefinitely, unless resumed\nt1.resume();\n\nt1.wait();\nt1.notify();\nt1.notifyAll();\n</code></pre> <p>synchronized ensures that only thread runs at a time</p> <pre><code>public static class PC\n{\n  public void produce() throws InterruptedException\n  {\n    synchronized(this)\n    {\n\n    }\n  }\n\n  // or\n\n  public synchronized void produce() throws InterruptedException\n  {\n    System.out.println(\"Producer Thread Running\");\n    wait(); //wait tills another does notify()\n    System.out.println(\"\");\n  }\n\n  public synchronized void consume() throws InterruptedException\n  {\n    Thread.sleep(1000);\n    Scanner inp = new Scanner(System.in);\n\n        synchronized(this)\n    {\n      System.out.println(\"Consumer Thread Running\");\n      inp.nextLine();\n      System.out.println(\"Return Key Pressed\");\n\n      notify();\n\n      Thread.sleep(2000);\n    }\n\n  }\n}\n\nclass Tester\n{\n  public static void main()\n  {\n    final PC p = new PC();\n\n    // create a thread object that calls pc.produce()\n    Thread t1 = new Thread(new Runnable())\n    {\n      @Override\n            public void run()\n      {\n        try {\n          pc.produce();\n        } catch(InterruptedException e) {\n          e.printStackTrace();\n        }\n      }\n    }; // anonymous class ends with ;\n\n    Thread t2 = new Thread(new Runnable())\n    {\n      @Override\n            public void run()\n      {\n        try {\n          pc.consume();\n        } catch(InterruptedException e) {\n          e.printStackTrace();\n        }\n      }\n    }; // anonymous class ends with ;\n\n    t1.start();\n    t2.start();\n\n    // when t1 finishes, t2 starts\n    // then t1 finishes, t1 starts\n    t1.join();\n    t2.join();\n  }\n}\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/09_Thread/#thread-methods","title":"<code>Thread</code> methods","text":"<pre><code>public void start(); \npublic void run(); // contains logic of the thread\n// logic should always be inside try block\n// and there should also be a catch block with InterruptedException\n\njoin();\nwait(); // sends thread to wait state\nresume(); // takes thread out of block state\nsuspend(); // sends thread to block state\nnotify();\nnotifyAll();\n\npublic final boolean isAlive(); // check if alive \npublic static void sleep(long millisec); // send thread to block state for a while\npublic final void setDaemon(boolean on); // set this thread as a daemon thread\npublic void interrupt(); // not sure\npublic static void yield(); // give the runtime to other threads with the same priority\n\npublic final void setName(String name);\npublic final void setPriority(int priority); // 1 &lt;= p &lt;= 10\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/09_Thread/#daemon-thread","title":"Daemon Thread","text":"<p>is a low priority thread that runs in the background, to perform tasks such as garbage collection</p>"},{"location":"2_Core/Object_Oriented_Programming/10_Cloning/","title":"10 Cloning","text":""},{"location":"2_Core/Object_Oriented_Programming/10_Cloning/#cloning","title":"Cloning","text":"<p>creating an exact copy of an existing object in the memory</p> <p><code>clone()</code> from class <code>java.lang.Object</code></p> <p>Only objects of classes which implement <code>Cloneable</code> interface are eligible for cloning</p> <p>By default, shallow copy occurs</p> Shallow Deep definition custom <code>clone()</code> original and clone dependent on each other independent changes affect each other no effect preferred if object has only primitive fields references to other objects as fields performance faster and cheaper slower and costlier"},{"location":"2_Core/Object_Oriented_Programming/10_Cloning/#deep-copy","title":"Deep Copy","text":"<pre><code>class Course implements Cloneable\n{\n  String sub1, sub2, sub3;\n\n    // constructor\n\n  protected Object clone() throws CloneNotSupportedException\n  {\n    return super.clone();\n  }\n}\n\nclass Student implements Cloneable\n{\n  int id;\n  String name;\n   Course c;\n\n    // constructor\n\n  // this is what defines a deep copy\n  // without this, it will just be a shallow copy\n  protected Object clone() throws CloneNotSupportedException\n  {\n    Student s = (Student) super.clone();\n    student.c = (Course) c.clone();\n    return s;\n  }\n}\n\nclass Tester\n{\n  public static void main(String args[])\n  {\n    Course c = new Course(\"Phy\", \"Chem\", \"Bio\");\n\n    Student s1 = new Student(111, \"John\", c);\n    Student s2 = null;\n\n    try {\n      s2 = (Student) s1.clone();\n    } catch (CloneNotSupportedException e) {\n      e.printStackTrace();\n    }\n\n    System.out.println(s1.c.sub3); // Bio\n\n    s2.course.sub3 = \"Math\"; // will not affect s1\n\n  }\n}\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/11_GUI/","title":"11 GUI","text":""},{"location":"2_Core/Object_Oriented_Programming/11_GUI/#event","title":"Event","text":"<p>Change in the state of an object</p> <p>generated by user\u2019s interaction with the GUI (graphical user interface), such as</p> <ul> <li>mouse movements</li> <li>mouse clicks</li> <li>pressing keys</li> </ul>"},{"location":"2_Core/Object_Oriented_Programming/11_GUI/#event-handling","title":"Event Handling","text":"<p>control what happens if an event occurs</p>"},{"location":"2_Core/Object_Oriented_Programming/11_GUI/#delegation-event-model","title":"Delegation Event Model","text":"<p>UI and event logic are separate</p>"},{"location":"2_Core/Object_Oriented_Programming/11_GUI/#source","title":"Source","text":"<p>object on which event occurs</p> <p>eg: buttons</p>"},{"location":"2_Core/Object_Oriented_Programming/11_GUI/#listenerhandler","title":"Listener/Handler","text":"<p>object that</p> <ol> <li>waits for event to occur</li> <li>generates response for the event</li> </ol> <p>eg: mouse click</p> <p>listener should be registered with the source</p>"},{"location":"2_Core/Object_Oriented_Programming/11_GUI/#important-events-and-listeners","title":"Important events and listeners","text":"<ul> <li><code>java.util</code></li> <li><code>java.awt</code></li> <li><code>java.awt.event</code></li> </ul> Event Classes Interface Generated when <code>ActionEvent</code> <code>ActionListener</code> - button press- menu-item selected- list-item is double clicked <code>MouseEvent</code> <code>MouseListener</code> - mouse dragged, moved, pressed, released- enter/exit a component <code>KeyEvent</code> <code>KeyListener</code> keyboard input <code>ItemEvent</code> <code>ItemListener</code> checkbox/list item is clicked <code>TextEvent</code> <code>TextListener</code> textarea/textfield edited <code>MouseWheelEvent</code> <code>MouseWheelListener</code> mousewheel moved <code>WindowEvent</code> <code>WindowListener</code> window activated, deactivated, opened, closed <code>ComponentEvent</code> <code>ComponentEventListener</code> component hidden <code>ContainerEvent</code> <code>ContainerListener</code> <code>AdjustmentEvent</code> <code>AdjustmentListener</code> <code>FocusEvent</code> <code>FocusListener</code>"},{"location":"2_Core/Object_Oriented_Programming/11_GUI/#swing","title":"Swing","text":"<p>part of JFC(Java Foundation Classes)</p> <p><code>import javax.swing</code></p> <p>swing classes all start like <code>J...</code></p> <p>We are using Swing Library, cuz</p> <ul> <li>Swing is for cross-platform applications; AWT is only for windows applications</li> <li>Swing is ligher than AWT</li> </ul> <pre><code>graph TB\nObject --&gt; Component --&gt; Container &amp; JComponent\n\nContainer --&gt; Window &amp; Panel\nWindow --&gt; JFrame &amp; Dialog\nPanel --&gt; Applet\n\nJComponent --&gt; JLabel &amp; JButton &amp; x[many more...]</code></pre> \\[ \\color{hotpink} \\fbox{$ \\underset{ \\color{orange} \\fbox{$ \\underset{ \\color{orange} \\fbox{Components} }{Panel} $}} {\\text{Frame}}$} \\]"},{"location":"2_Core/Object_Oriented_Programming/11_GUI/#containers","title":"Containers","text":"Top Level Lightweight Weight heavy light Dependence independent requires top level eg <code>JFrame, JDialog, JApplet</code> <code>JPanel</code>"},{"location":"2_Core/Object_Oriented_Programming/11_GUI/#idk","title":"IDK","text":"<p>There are 2 ways</p> <ol> <li>instantiate <code>JFrame</code> class</li> <li>extend <code>JFrame</code> class</li> </ol> <pre><code>import java.awt.*;\nimport java.swing.*;\n\nclass GUI implements ActionListener\n{\n    GUI()\n  {\n    JFrame f = new JFrame();\n\n    JLabel l = new JLabel(\"Blah Blah\");\n\n    JPanel p1 = new JPanel();\n    JPanel p2 = new JPanel();\n\n    JTextField tf = new JTextField();\n    tf.setText(\"Blah\");\n    tf.setEditable(false);\n\n    JButton btn = new JButton(\"Click me\");\n    btn.addActionListener(this);\n\n    JCheckBox jc = new JCheckBox();\n    jc.addActionListener(this);\n\n    ButtonGroup bg = new ButtonGroup();\n    JRadioButton r1 = new JRadioButton(),\n        r2 = new JRadioButton(),\n        r3 = new JRadioButton();\n    bg.add(r1); bg.add(r2); bg.add(r3);\n\n    p1.add(tf); p1.add(btn);\n    p2.add(jc); p2.add(bg);\n\n        f.add(p1); f.add(p1);\n    f.add(l);\n\n    f.setLayout(new FlowLayout() ); //null\n    f.setSize(200, 400);\n    f.setVisible(true);\n  }\n\n  public void actionPerformed(ActionEvent e)\n  {\n    JOptionPane.showMessageDialog(this, \"message\");\n    if( e.getSource() == btn )\n    {\n      if( jc.isSelected() )\n                ;\n      if( r1.isSelected() )\n        ;\n      if( r2.isSelected() )\n                ;\n    }\n  }\n}\n</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/12_UML/","title":"12 UML","text":""},{"location":"2_Core/Object_Oriented_Programming/12_UML/#phases-of-software-engineering","title":"Phases of Software Engineering","text":"<pre><code>flowchart LR\na[Requirement Gathering] --&gt;\nb[Requirement Analysis] --&gt;\nd[\"Design&lt;br /&gt;(UML Diagrams)\"] --&gt;\nCoding --&gt;\nTesting --&gt;\nDeployment --&gt;\nSupport</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/12_UML/#uml","title":"UML","text":"<p>Unified Modeling Language</p>"},{"location":"2_Core/Object_Oriented_Programming/12_UML/#use-case-diagram","title":"Use Case Diagram","text":""},{"location":"2_Core/Object_Oriented_Programming/12_UML/#actors","title":"Actors","text":"<p>classes</p> <p>could be</p> <ol> <li>users of the system</li> <li>external system</li> <li>physical environment</li> </ol> <p>Properties</p> <ol> <li>unique name</li> <li>description (optional)</li> </ol>"},{"location":"2_Core/Object_Oriented_Programming/12_UML/#use-cases","title":"Use Cases","text":"<p>basically like functions</p> <p>properties</p> <ol> <li>unique name</li> <li>participating actors</li> <li>entry conditions</li> <li>exit conditions</li> <li>event flow</li> <li>exceptional cases</li> </ol>"},{"location":"2_Core/Object_Oriented_Programming/12_UML/#extends","title":"<code>&lt;&lt;extends&gt;&gt;</code>","text":"<p>for exceptions/showing use cases that are rarely used</p> <p>The direction of a <code>&lt;&lt;extends&gt;&gt;</code> relationship is to the extended use case</p>"},{"location":"2_Core/Object_Oriented_Programming/12_UML/#includes","title":"<code>&lt;&lt;includes&gt;&gt;</code>","text":"<p>for use cases that require/depend on another use case</p> <p>The direction of a <code>&lt;&lt;includes&gt;&gt;</code> relationship is to the using use case (unlike <code>&lt;&lt;extends&gt;&gt;</code> relationships).</p> <p></p>"},{"location":"2_Core/Object_Oriented_Programming/12_UML/#class-diagram","title":"Class Diagram","text":""},{"location":"2_Core/Object_Oriented_Programming/12_UML/#access-specifiers","title":"Access Specifiers","text":"<ul> <li>(nothing) default</li> <li><code>-</code> private</li> <li><code>+</code> public</li> <li><code>#</code> protected</li> </ul>"},{"location":"2_Core/Object_Oriented_Programming/12_UML/#connections","title":"Connections","text":"<ul> <li> <p>association</p> <ul> <li>can be 1-way or 2-way</li> <li>can be one-one or many-many \\(1-1, \\quad 5\\ldots* - *, \\quad *- 3\\ldots *, \\quad * - *\\)</li> <li>arrow from a towards b, means that a depends on b</li> </ul> </li> <li> <p>aggregation</p> </li> <li>composition (strong aggregation)</li> <li>inheritance<ul> <li>class inheritance</li> <li>interface inheritance</li> </ul> </li> </ul>"},{"location":"2_Core/Object_Oriented_Programming/12_UML/#example","title":"Example","text":"<pre><code>classDiagram\nInterface &lt;|.. Base: implements\nBase &lt;|-- Teacher\nBase &lt;|-- Student\nBase &lt;.. Tester\nClassRoom *-- Teacher\nClassRoom o-- Student\nclass Interface {\n    &lt;&lt;interface&gt;&gt;\n    +func() void\n}\nclass Base {\n    &lt;&lt;abstract&gt;&gt;\n    #var : int\n    +func() void\n}\nclass Tester {\n    +main() void\n}</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/12_UML/#sequence-diagram","title":"Sequence Diagram","text":"<p>Shows the interactions bw the classes/objects</p> <p>calls are solid, returns are dashed</p> <pre><code>sequenceDiagram\nautonumber\n\nactivate d\n\nd -&gt;&gt;+ p: Discharge Advice\n\nactivate r\n\np -&gt;&gt;- r: Discharge Request\n\nr -&gt;&gt; + pd: Check Details\npd --&gt;&gt; - r: Detailed Summary\nr -&gt;&gt; r:  Prepare Bill\nr --&gt;&gt;+ p: Send Bill\np -&gt;&gt;- r: Request Discount\nr -&gt;&gt; d: Check Possibility of discount\nd --&gt;&gt; r: Approve Discount\n\ndeactivate d\n\nr -&gt;&gt; r: Update bill\nr -&gt;&gt;+ p: Send bill\np --&gt;&gt;- r: Pay bill\nr -&gt;&gt; +pd: Update Bill\ndeactivate pd\n\nactivate p\nr -&gt;&gt; p: Discharge note\n\ndeactivate r\n\np -&gt;&gt; + Ward: Show note\nWard --&gt;&gt; - p: Discharge\ndeactivate p\n\n%% participant\nparticipant d as Doctor\nparticipant p as Patient\nparticipant r as Reception\nparticipant pd as Patient Database</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/12_UML/#state-diagrams","title":"State Diagrams","text":"<pre><code>stateDiagram-v2\n[*] --&gt; dr\ndr --&gt; cp: Patient Register\ncp --&gt; er: Resign\ner --&gt; [*]\n\nstate \"Doctor Registration\" as dr\nstate \"Check Patient\" as cp\nstate \"End Service\" as er</code></pre>"},{"location":"2_Core/Object_Oriented_Programming/13_Design_Patterns/","title":"13 Design Patterns","text":"<p>Object Oriented design must be</p> <ul> <li>specific to the problem, and</li> <li>general to adress future problems and requirements</li> </ul>"},{"location":"2_Core/Object_Oriented_Programming/13_Design_Patterns/#design-patterns","title":"Design Patterns","text":"<p>Descriptions of communicating objects and classes, that are customized to solve a general design problem, in a particular context.</p> <p>allows</p> <ul> <li>re-usability of design</li> <li>faster production of projects</li> <li>more accessible</li> <li>easier documentation and maintenance</li> </ul>"},{"location":"2_Core/Object_Oriented_Programming/13_Design_Patterns/#parts","title":"Parts","text":"<ol> <li>Pattern Name</li> <li>Problem</li> <li>Solution</li> <li>Context</li> <li>Class Diagram</li> </ol>"},{"location":"2_Core/Object_Oriented_Programming/13_Design_Patterns/#types-of-patterns","title":"Types of Patterns","text":"<ol> <li>Creational Patterns    deal with object creation</li> <li>Singleton       Single object of a class is created, and all other objects can access it globally</li> <li>Structural Patterns    deal with relationship between entities</li> <li>Composite       when you put components inside containers       eg: JPanel</li> <li>Decorator       something that surrounds component       eg: scroll bars</li> <li>Adapter Pattern       a middle interface \u2018adapts\u2019 main interface based on the requirement</li> <li>Proxy Pattern       a class acts a proxy to access another class, to keep that hidden</li> <li>Behavioural Patterns    communication between objects</li> <li>Iterator       access the elements of an aggregate object sequentially without exposing its underlying implementation</li> <li>Observer       eg: ActionListener</li> </ol>"},{"location":"3_Core/Compiler_Contruction/","title":"Compiler Construction","text":"<p>Course taught by Dr. Santhosh Kumar, Dr. Elakkiya and Dr. Vijaykumar</p> <p>This course is a continuation of Theory of Computation and Principles of Programming Languages, and covers the essential phases of compiler design, including lexical analysis, syntax analysis, intermediate code generation, code optimization, and code generation. Students will gain practical experience with tools like lexical analyzers and parsers and learn to apply compiler design principles in system software development.</p> <p>Key Learning Objectives: - Understand compiler structure and components. - Learn lexical and syntax analysis techniques. - Gain practical skills with compiler tools.</p> <p>Topics Covered: 1. Compiler Structure - Overview of phases. 2. Lexical Analysis - Tokens, patterns, and lexical analyzers. 3. Syntax Analysis - Context-free grammars and parsing techniques. 4. Symbol Table - Organization and management. 5. Syntax Directed Translation - Attributes and abstract syntax trees (ASTs). 6. Intermediate Code Generation - Three-address code. 7. Code Generation - Design issues and basic blocks. 8. Liveness Analysis - Data flow equations. 9. Register Allocation - Techniques and assignment. 10. Code Optimization - Peephole and global optimizations. 11. Garbage Collection - Techniques overview. 12. Runtime Memory Models - Stack allocation.</p>"},{"location":"3_Core/Compiler_Contruction/01_Introduction/","title":"01 Introduction","text":""},{"location":"3_Core/Compiler_Contruction/01_Introduction/#language-translators","title":"Language Translators","text":"<p>You should know by this stage :/</p> <p>If you don\u2019t, refer this.</p>"},{"location":"3_Core/Compiler_Contruction/01_Introduction/#programming-language-processing","title":"Programming Language Processing","text":"<pre><code>flowchart LR\na(( )) --&gt;\n|Source&lt;br/&gt;Program| Preprocessor --&gt;\n|Modified&lt;br/&gt;Source&lt;br/&gt;Program| Compiler --&gt;\n|Target&lt;br/&gt;Assembly&lt;br/&gt;Code| Assembler --&gt;\n|Relocatable&lt;br/&gt;Machine&lt;br/&gt;Code| Linker/Loader --&gt;\n|Target&lt;br/&gt;Machine&lt;br/&gt;Code| b(( ))</code></pre> <p>Compiler outputs assembly code, as it is easier to</p> <ul> <li>produce as output</li> <li>debug</li> </ul> <p>Linker resolves ext mem addresses, where code in one file may refer to location in another file.</p>"},{"location":"3_Core/Compiler_Contruction/01_Introduction/#stages-of-compiler","title":"Stages of Compiler","text":"<pre><code>flowchart LR\n\na[/Input/] --&gt;\n|Char&lt;br/&gt;Stream| la\n\nsubgraph Analysis/Front End\nla[\"Lexical&lt;br/&gt;Analysis&lt;br/&gt;(Scanning)\"] --&gt;\n|Token&lt;br/&gt;Stream| sya[\"Syntax&lt;br/&gt;Analysis&lt;br&gt;(Parsing)\"] --&gt;\n|Syntax&lt;br/&gt;Tree| sea[Semantic&lt;br/&gt;Analysis]\nend\n\nsea --&gt; |Syntax&lt;br/&gt;Tree| icg\n\nsubgraph Synthesis/Back End\nicg[Intermediate&lt;br/&gt;Code&lt;br/&gt;Generation] --&gt;\n|Intermediate&lt;br/&gt;Representation| mico[Machine-Independent&lt;br/&gt;Code Optimization] --&gt;\n|Intermediate&lt;br/&gt;Representation| c[Code&lt;br/&gt;Generator] --&gt;\n|Target&lt;br/&gt;Machine&lt;br/&gt;Code| mdco[Machine-Dependent&lt;br/&gt;Code Optimization]\nend\n\nmdco --&gt; o[/Output/]</code></pre> Stage Input Task LexicalAnalysis/Scanning Source prog - Group characters into lexemes (meaningful sequences)- Generate a token for every lexeme- Access/Update symbol tableSecondary- Stripping comments, whitespaces (blanks, newlines, tokens)- Keep track of line number for errors- Macro expansion SyntaxAnalysis/Parsing Tokens - Check if structure follows [context-free] grammar of lang- Creates tree representationof grammatical structure of token stream SemanticAnalysis Syntax treeSymbol table - Check semantic consistency w/ lang definition- Gathers type information &amp; saves it in syntax tree/symbol table- Type checking: each operator has matching operands- Label Checking- Keywords misuse- Flow Control checking (no <code>break</code> outside loop)- Type conversions called coercions IntermediateCodeGeneration Parse tree from semantic analyzer Generate program in low-level/machine-like intermediate representation CodeOptimization Intermediate code Improve code so that target code uses lesser resources CodeGeneration Intermediate representation - Produces target language (machine/assembly code)- Choose registers &amp; mem locations for vars in prog"},{"location":"3_Core/Compiler_Contruction/01_Introduction/#error-detection-reporting","title":"Error Detection &amp; Reporting","text":"<p>At every phase, if any error is identified, it is reported and handled</p>"},{"location":"3_Core/Compiler_Contruction/01_Introduction/#tasks","title":"Tasks","text":"<ul> <li>Report the presence of errors clearly &amp; accurately. One error can mask another &amp; cause correct code to look faulty. </li> <li>Recover from each error quickly enough to detect subsequent errors</li> <li>Add minimal overhead to processing of correct programs</li> </ul>"},{"location":"3_Core/Compiler_Contruction/01_Introduction/#types-of-errors","title":"Types of Errors","text":"Types Meaning Example Lexical Misspelled identifier/keyword <code>fi (a == b)</code>(<code>fi</code> could be identifier/misspelled keyword (<code>if</code>)/function nameBut lexical analysis considers it as identifier) Syntax Statement not following lang rules Missing <code>;</code>Arithmetic expression with unbalanced parenthesis Semantic Divide by 0Operation incompatible operand typesWrong number of array index Logical No rules broken, but incorrect logic Using &lt; instead of &lt;=Infinite recursive call"},{"location":"3_Core/Compiler_Contruction/01_Introduction/#symbol-table","title":"Symbol-Table","text":"<p>Data structure (usually hash table - for efficiency) containing a record for each identifier (variables, constants, functions) with fields for the attributes of the identifier</p> <p>It is accessed at every phase of compiler.</p> <ul> <li>Scanner, parser, and semantic analyzer put names of identifiers in symbol table.</li> <li>The semantic analyzer stores more information (e.g. types) in the table.</li> <li>The intermediate code generator, code optimizer and code generator use information in symbol table to generate appropriate code.</li> </ul>"},{"location":"3_Core/Compiler_Contruction/01_Introduction/#contains","title":"Contains","text":"<ul> <li>Attributes of variables are name, type, scope, etc.</li> <li>Attributes of procedure names which provide info about</li> <li>no and types of its arguments</li> <li>method of passing each argument (call by value/reference)</li> <li>type returned</li> </ul>"},{"location":"3_Core/Compiler_Contruction/01_Introduction/#passes","title":"Passes","text":"<p>Several phases are sometimes combined into a single \u2018pass\u2019</p> <p>A pass reads an input file process it and writes an output file</p>"},{"location":"3_Core/Compiler_Contruction/01_Introduction/#normal-passes-in-compilers","title":"Normal Passes in Compilers","text":"<ul> <li>Front-end phases are combined into a pass</li> <li>Code optimization is an optional pass </li> <li>Back-end phase can be made into a pass</li> </ul>"},{"location":"3_Core/Compiler_Contruction/01_Introduction/#misc","title":"Misc","text":""},{"location":"3_Core/Compiler_Contruction/01_Introduction/#compilation-examples","title":"Compilation Examples","text":""},{"location":"3_Core/Compiler_Contruction/01_Introduction/#c","title":"C","text":"<pre><code>cc gx.c\nobjdump -d a.out\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/01_Introduction/#java","title":"Java","text":"<p>This command shows how your class file is treated</p> <pre><code>javac File.java\njavap -c File.class\n</code></pre> <p>It is cross platform, as it executes as a station machine</p>"},{"location":"3_Core/Compiler_Contruction/01_Introduction/#python","title":"Python","text":"<pre><code>python file.py\npython decompile file.py\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/01_Introduction/#android-sdk","title":"Android SDK","text":"<p>How does it show how your java program will work on mobile, when mobile is ARM architecture, but your laptop is usually x86 architecture.</p> <p>This is because java program is cross-platform, and the simulator simulates execution of the program as if it is executed on an ARM processor.</p>"},{"location":"3_Core/Compiler_Contruction/02_Lexical_Analysis/","title":"02 Lexical Analysis","text":"<p>Also called as scanning</p> <p>We specify tokens using Regular Expressions - sequence of characters specifying search pattern in input</p> <p>We use NFA/DFA</p>"},{"location":"3_Core/Compiler_Contruction/02_Lexical_Analysis/#parts","title":"Parts","text":"Meaning Example Tokens Pair containing <code>token_name</code> and <code>attribute</code><code>token_name</code> is a symbol representing a \u2018lexical unit\u2019, which is processed by parser <code>&lt;id, pointer_to_symbol_table_entry&gt;</code> Lexical Units Identifiers, Keywords, Operators, Constants (numeric/string) Pattern Rule describing the format of the lexemes of a token For keywords it\u2019s the sequence of characters itself Lexeme Sequence of characters that matches that pattern for a token Sentinals Special characters that cannot be part of the source program eof character can be used to denote the end of a buffer"},{"location":"3_Core/Compiler_Contruction/02_Lexical_Analysis/#example","title":"Example","text":"<pre><code>e = m * c ** 2\n</code></pre> <pre><code>&lt;id, pointer_to_symbol_table_entry_for_e&gt;\n&lt;assign_op&gt;\n&lt;id, pointer_to_symbol_table_entry_for_m&gt;\n&lt;mult_op&gt;\n&lt;id, pointer_to_symbol_table_entry_for_c&gt;\n&lt;exp_op&gt;\n&lt;number, 2&gt;\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/02_Lexical_Analysis/#input-buffering","title":"Input Buffering","text":"<p>In Fortran, spaces are ignored. So, <code>he l lo</code> is the <code>hello</code>. This is because, there may exist blank instances in the magnetic tape.</p> <p>We can\u2019t tell if the statement <code>do 5 i = 1.25</code> is to be treated as</p> <pre><code>do {i=1}\n// or  \ndo51 = 1.52\n</code></pre> <p>until we reach the <code>.</code></p>"},{"location":"3_Core/Compiler_Contruction/02_Lexical_Analysis/#fortran-loops","title":"Fortran Loops","text":"<pre><code>do index_variable = start, end, step\n    statements\nend do\n\n// or\n\ndo n index_variable = start, end, step\n    statements\nn continue\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/02_Lexical_Analysis/#lookahead","title":"Lookahead","text":"<p>Lookahead of atleast one/more characters beyond the next lexeme before we can be sure that we have the right lexeme.</p> <p>Helps speed up reading source program</p> <p>We usually use 2 buffer scheme lookahead, which are alternatively-reloaded; each buffer is of the same size \\(n\\), where \\(n\\) is size of a disk block</p>"},{"location":"3_Core/Compiler_Contruction/02_Lexical_Analysis/#advantages","title":"Advantages","text":"<ul> <li>Using one system read command we can read \\(n\\) characters into a buffer, rather than using one system call per character</li> <li>A special character, represented by eof (character different from any possible character of source code), marks the end of the source file</li> </ul>"},{"location":"3_Core/Compiler_Contruction/02_Lexical_Analysis/#2-buffer-scheme","title":"2 Buffer Scheme","text":"Pointer Purpose <code>lexeme_begin</code> marks the beginning of the current lexeme, whose extent we are attempting to determine <code>forward</code> scans ahead until a pattern match is found <p>When the next lexeme is determined, the following steps are taken:</p> <ul> <li><code>forward</code> is set to the character at its right end</li> <li>Record the lexeme as the attribute of the token</li> <li><code>lexemeBegin</code> is set to the character immediately after the lexeme just found</li> </ul> <p></p> <p>Advancing forward requires checking if end of a buffer is reached. </p> <code>forward</code> is at end of buffer - Reload other buffer from input- Move <code>forward</code> to beginning of newly loaded buffer <code>eof</code> character at the middle of buffer - marks the end of the input- terminate lexical analysis"},{"location":"3_Core/Compiler_Contruction/02_Lexical_Analysis/#recovery-strategies","title":"Recovery Strategies","text":"<p>Recovery strategies are used when no pattern for tokens matches any prefix of remaining input, preventing lexical analyzer from proceeding</p> <p>Goal: Transform prefix of remaining input into valid lexeme</p> <p>Possible error-recovery actions are:</p> <ul> <li>Panic Mode Recovery: Delete successive characters from the remaining input, until the lexical analyzer can find a well-formed token at the beginning of what input is left</li> <li>Insert a missing character into the remaining input</li> <li>Replace a character by another character</li> <li>Transpose two adjacent characters</li> </ul>"},{"location":"3_Core/Compiler_Contruction/02_Lexical_Analysis/#parts-of-string","title":"Parts of String","text":"Term Meaning Example\\(s =\\) banana Prefix Starting character(s) \\(\\epsilon\\), b, ba, ban, bana, banan, banana Suffix Trailing character(s) \\(\\epsilon\\), a, an, ana, nana, anana, banana Substring Middle character(s) prefix_set \\(\\cup\\) suffix_set Proper Prefix Non-empty prefix \\(\\ne\\) original string b, ba, ban, bana, banan Proper Suffix Non-empty suffix \\(\\ne\\) original string a, an, ana, nana, anana Proper Substring Non-empty substring \\(\\ne\\) original string proper_prefix_set \\(\\cup\\) proper_suffix_set Subsequence Collection of characters of string(not necessarily contiguous, but left \\(\\to\\) right) baaa(too many combinations to list out)"},{"location":"3_Core/Compiler_Contruction/02_Lexical_Analysis/#operations","title":"Operations","text":"<p>In order of precedence</p> Operation Operator Strings Exponentiation \\(s^i\\) Concatenation \\(s_1 \\cdot s_2\\) Languages Kleene closure \\(L^*\\) Posive closure \\(L^+\\) Concatenation \\(L_1 \\cdot L_2\\) Union \\(L_1\\) \\vert \\(L_2\\)\\(L_1 \\cup L_2\\)"},{"location":"3_Core/Compiler_Contruction/02_Lexical_Analysis/#regular-definitions","title":"Regular Definitions","text":"<p>Helps to give names to regular expressions and use those names in subsequent expressions</p> <pre><code>d1 -&gt; r1\nd2 -&gt; r2\n...\ndn -&gt; rn\n</code></pre> <p>where </p> <ul> <li>\\(d_i\\) is a new symbol, such that</li> <li>\\(d_i \\not \\in \\epsilon\\)</li> <li>\\(d_i \\ne d_j\\)</li> <li>\\(r_i\\) is a RE over alphabet \\(\\epsilon \\cup \\{d_1, \\dots, d_{i-1} \\}\\)</li> </ul>"},{"location":"3_Core/Compiler_Contruction/02_Lexical_Analysis/#lex","title":"Lex","text":"<p>Language that allows us to create our own lexical analyzer, without having handcode</p> <p>It represents everything in terms of a Finite State Machine, and then generates the code</p>"},{"location":"3_Core/Compiler_Contruction/02_Lexical_Analysis/#lex-symbols","title":"Lex Symbols","text":"Symbol Meaning Example c non-operator character c \\c character \\(c\\) literally \u201cs\u201d string \\(s\\) literally . any character except <code>\\n</code> hello\\(\\implies 5 \\ .\\) ^ beginning of line ^abc $ end of line abc$ [s] any character in \\(s\\) \\([abcde]\\)$a [^s] any character not in \\(s\\) \\([\\wedge abcde]\\)$(a r* 0/more strings matching \\(r\\) (something)* r+ 1/more strings matching \\(r\\) (something)+ r? 0/1 strings matching \\(r\\) (something)? r{m,n} \\(\\text{count} \\in [m, n]\\) strings matching \\(r\\) (something){1, 5} \\(r_1r_2\\) \\(r_1\\) followed by \\(r_2\\)(select \\(r_1\\) and \\(r_2\\)) \\(r_1/r_2\\) \\(r_1\\) followed by \\(r_2\\)(select only \\(r_1\\)) $r_1 r_2$ \\(r_1\\) or \\(r_2\\) \\((r)\\) Same as \\(r\\) $(a"},{"location":"3_Core/Compiler_Contruction/02_Lexical_Analysis/#transitionstate-diagrams","title":"Transition/State Diagrams","text":"<p>Reg Exprs are translated into transition diagrams (representing Finite State Machines), which are then translated into program code for lexical analyzer</p>"},{"location":"3_Core/Compiler_Contruction/02_Lexical_Analysis/#relational-operators","title":"Relational Operators","text":""},{"location":"3_Core/Compiler_Contruction/02_Lexical_Analysis/#reserved-wordsidentifiers","title":"Reserved Words/Identifiers","text":""},{"location":"3_Core/Compiler_Contruction/02_Lexical_Analysis/#unsigned-numbers","title":"Unsigned Numbers","text":""},{"location":"3_Core/Compiler_Contruction/02_Lexical_Analysis/#whitespace","title":"Whitespace","text":""},{"location":"3_Core/Compiler_Contruction/02_Lexical_Analysis/#conflict-resolution","title":"Conflict Resolution","text":"<ul> <li>Longer prefix preferred</li> <li>If there are multiple matches for longest prefix, first pattern in lex program is preffered</li> </ul> <pre><code>a       {printf (\u201c1A\u201d);}\naa  {printf (\u201c2A\u201d);}\n\nInput : aaa\nOutput: 2A1A\n</code></pre> <pre><code>%%\nletter(letter|digit)*   { printf (\u201cID\u201d); }\nif                                      { printf (\u201cIF\u201d); }\n\nInput : if\nOutput: ID\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/03_Syntax_Analysis/","title":"03 Syntax Analysis","text":"<p>Also called as Parsing</p> <p>Regular expressions cannot be used, due to nested structure.</p> <p>Hence, we need a context-free grammar and PDA</p>"},{"location":"3_Core/Compiler_Contruction/03_Syntax_Analysis/#syntaxparse-tree","title":"Syntax/Parse Tree","text":"<p>Each interior node represents an operation and the children of the node represent the arguments of the operation. It shows the order of operations.</p>"},{"location":"3_Core/Compiler_Contruction/03_Syntax_Analysis/#yacc","title":"Yacc","text":"<p>Yet another compiler compiler</p> <p>This is covered in practicals</p>"},{"location":"3_Core/Compiler_Contruction/03_Syntax_Analysis/#cfg","title":"CFG","text":"<p>Context-Free Grammar</p> <ul> <li>Set of terminals \\(T\\)</li> <li>Set of non-terminals \\(N\\)</li> <li>Start symbol \\(S\\) (non-terminal)</li> <li>Set of productions</li> </ul> \\[ X \\to Y_1 Y_2 \\dots Y_n \\] <p>where</p> <ul> <li>\\(X \\in N\\)</li> <li>\\(Y_i \\in T \\cup N \\cup \\{ \\epsilon \\}\\)</li> </ul> <p>LHS of any production/rule can only be a single non-terminal</p> <p>If \\(S\\) is the start symbol and \\(L(G)\\) is the language of \\(G\\), then \\(L(G)\\) is the set of strings that can be derived from \\(S\\)</p> \\[ (a_1 \\dots a_n) | S \\overset{*}{\\implies} a_1 \\dots a_n , \\forall a_i \\in T \\]"},{"location":"3_Core/Compiler_Contruction/03_Syntax_Analysis/#derivation","title":"Derivation","text":"<p>A derivation defines a parse tree. One parse tree may have multiple derivations.</p> <p>We have 2 different derivation types, which allows for different parser implementations.</p>"},{"location":"3_Core/Compiler_Contruction/03_Syntax_Analysis/#types","title":"Types","text":"Type Parser Implementation Single-step Derivation Leftmost Top-Down Leftmost non-terminal replaced with corr RHS of non-terminal Rightmost Bottom-Up Rightmost non-terminal replaced with corr RHS of non-terminal"},{"location":"3_Core/Compiler_Contruction/03_Syntax_Analysis/#example","title":"Example","text":"<pre><code>G: E \u2192 E+E | E\u2217E | (E) | id | num\nInput: (a + 23) * 12\nTokens: LP ID PLUS NUM RP MUL NUM\n</code></pre> <pre><code>Leftmost\n\nE\n\u21d2  E * E\n\u21d2  (E) * E\n\u21d2  (E+E) * E\n\u21d2  (id+E) * E\n\u21d2  (id+num) * E\n\u21d2  (id+num) * num\n\nRightmost\n\nE\n\u21d2  E * E\n\u21d2  E * num\n\u21d2  (E) * num\n\u21d2  (E+E) * num\n\u21d2  (E+num) * num\n\u21d2  (id+num) * num\n</code></pre> <p>Parse tree is same for both</p> <p></p>"},{"location":"3_Core/Compiler_Contruction/03_Syntax_Analysis/#grammar","title":"Grammar","text":""},{"location":"3_Core/Compiler_Contruction/03_Syntax_Analysis/#types_1","title":"Types","text":"Type Production Form Parser Left-Recursive \\(X \\to Xa\\) Bottom-Up Right-Recursive \\(X \\to aX\\) Bottom-Up/Top-Down <p>where</p> <ul> <li>\\(X\\) is a non-terminal</li> <li>\\(a\\) is a string of terminals, it is called left recursive production</li> </ul> <p>Top-down parser cannot work with Left recursive grammar, but both parsing works with right recursive grammar</p> <pre><code>G1: X \u2192  Xa | a // left recursive\nG2: X \u2192  XA | a // right recursive\n</code></pre> <pre><code>// left recursive\nX()\n{  \n  X();\n  match('a');\n}\n\n// right recursive\nX()\n{  \n  match('a');\n  X();\n}\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/03_Syntax_Analysis/#ambiguous-grammar","title":"Ambiguous Grammar","text":"<p>Grammar where the same string has multiple</p> <ul> <li>parse trees</li> <li>leftmost derivations</li> <li>rightmost derivations</li> </ul> <p>It gives incorrect results.</p>"},{"location":"3_Core/Compiler_Contruction/03_Syntax_Analysis/#example_1","title":"Example","text":"<pre><code>Grammar G: E \u2192 E+E | E*E | (E) | id | num\nInput String s: id+id+id\n</code></pre> <p>In this case, leftmost derivation is incorrect, as <code>+</code> is left-associative, and should be treated as such.</p> <pre><code>// leftmost approach 1\nE\n\u2192 E+E\n\u2192\ufffc id + E\n\u2192 id + E + E\n\u2192 id + id + E\n\u2192 id + id + id\n\n// leftmost approach 2\nE\n\u2192 E + E\n\u2192 E + E + E\n\u2192 id + E + E\n\u2192 id + id + E\n\u2192 id + id + id\n</code></pre> <p></p>"},{"location":"3_Core/Compiler_Contruction/03_Syntax_Analysis/#if-statement","title":"<code>if</code> statement","text":"<pre><code>stmt \u2192 if expr then stmt\n     | if expr then stmt else stmt\n     | other\n</code></pre> <p>This has two leftmost derivations for</p> <pre><code>if E1 then if E2 then S1 else S2\n</code></pre> <p></p>"},{"location":"3_Core/Compiler_Contruction/03_Syntax_Analysis/#disambiguation","title":"Disambiguation","text":"<p>It is not possible to automatically convert ambiguous grammar into an unambiguous one. Hence, we need to use one of the following to eliminate ambiguity</p>"},{"location":"3_Core/Compiler_Contruction/03_Syntax_Analysis/#rewrite-grammar","title":"Rewrite Grammar","text":"<pre><code>E \u2192 E + E | E * E | (E) | id\n\ncan be converted to\n\nE \u2192 E + T | T\nT \u2192 T * F | F\nF \u2192 (E) | id\n</code></pre> <pre><code>if grammar previously\n\ncan be converted to\n\nstmt        \u2192 m_stmt\n            | o_stmt\n            | other\nm_stmt  \u2192 if expr then m_stmt else m_stmt\n        | other\no_stmt  \u2192 if expr then stmt\n        | if expr then m_stmt else o_stmt\n</code></pre> <p>where</p> <ul> <li>matched statement: with <code>else</code></li> <li>open statement: without <code>else</code></li> </ul>"},{"location":"3_Core/Compiler_Contruction/03_Syntax_Analysis/#tool-provided-disambiguation","title":"Tool-Provided Disambiguation","text":"<p>Yacc provides disambiguation declarations</p> <pre><code>%left + - * /\n%right = ^\n</code></pre> <p>This is covered in detail in practicals</p>"},{"location":"3_Core/Compiler_Contruction/03_Syntax_Analysis/#example-grammar-for-prog-lang","title":"Example Grammar for prog lang","text":"<p>Consider a language consisting of semicolon (;) separated list of statements (except the last statement), where</p> <ul> <li>A statement can be</li> <li>id := expr </li> <li>print(expression list)</li> <li>expr can be expr + expr/num/id/ ( statement list, expr)</li> <li>expression list is comma-separated list of expressions</li> </ul> <pre><code>S \u2192 S ; S | id := E | print (L)\nE \u2192 E + E | num | id | ( S , E )\nL \u2192 L,E  | E\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/03_Syntax_Analysis/#trees","title":"Trees","text":"Parse Tree Syntax Tree Alternate Name Concrete Syntax Tree Abstract Syntax Tree Grammar symbols? \u2705 \u274c(only terminals) Example"},{"location":"3_Core/Compiler_Contruction/03_Syntax_Analysis/#parsing","title":"Parsing","text":"<p>Parser decides</p> <ul> <li>which production rule is to be used, when required</li> <li>what is the next token</li> <li>Reserved word <code>if</code>, open paranthesis</li> <li>what is the structure to be built</li> <li><code>if</code> statement, expression</li> <li></li> </ul>"},{"location":"3_Core/Compiler_Contruction/03_Syntax_Analysis/#types-of-parsers","title":"Types of Parsers","text":"Bottom-Up Top-Down Alternate Names LRShift-Reduction LLDerivation Finds rightmost derivation in reverse order Parse treeconstruction leaves \\(\\to\\) root root \\(\\to\\) leaves Start Input string Start symbol End Start symbol Input string Steps Shift &amp; Reduction Replace leftmost nonterminal w/ production rule Automatic Tools Handwritten parsersPredictive parsers Size ofGrammar class Larger Smaller"},{"location":"3_Core/Compiler_Contruction/03_Syntax_Analysis/#handle","title":"Handle","text":"<p>Substring matching right side of a production rule, which gets reduced in a manner that is reverse of rightmost derivation</p> <p>Not every substring that matches the right side of a production rule is a handle; only the one that gets chosen for reduction</p>"},{"location":"3_Core/Compiler_Contruction/03_Syntax_Analysis/#sentential-form","title":"Sentential Form","text":"<p>Any string derivable from the start symbol</p> <p>A derivation is a series of rewrite steps</p> \\[ S \\implies \\gamma_0 \\implies \\gamma_1 \\implies \\dots \\implies \\gamma_n \u21d2 \\text{Sentence} \\] Non-terminals in \\(\\gamma_i\\) \\(\\gamma_i\\) is \\(0\\) Sentence in \\(L(G)\\) \\(\\ge 1\\) Sentential Form <p>Right sentential form occurs in rightmost derivation. If the grammar is unambiguous, then every right-sentential form of the grammar has exactly one handle</p>"},{"location":"3_Core/Compiler_Contruction/03_Syntax_Analysis/#handle-pruning","title":"Handle Pruning","text":"<p>A right-most derivation in reverse can be obtained by handle-pruning.</p> <ol> <li>Start from \\(\\gamma_n\\), find a handle \\(A_n \\to \\beta_n\\) in \\(\\gamma_n\\)</li> <li>Replace \\(\\beta_n\\) by \\(A_n\\) to get \\(\\gamma_{n-1}\\)</li> <li>Repeat the same until we reach \\(S\\)</li> </ol>"},{"location":"3_Core/Compiler_Contruction/03_Syntax_Analysis/#shift-reduce-parsing","title":"Shift-Reduce Parsing","text":"<ul> <li>Initial stack only  contains end-marker <code>$</code></li> <li>End of input string is marked by end-marker <code>$</code></li> </ul> <p>Shift input symbols into stack until reduction can be applied</p> <p>Parser has to find the right handles</p> <p>If a shift-reduce parser cannot be used for a grammar, that grammar is called as non-LR(k) grammar; ambiguous grammar can never be LR grammar.</p>"},{"location":"3_Core/Compiler_Contruction/03_Syntax_Analysis/#steps","title":"Steps","text":"Step Action Shift New input symbol pushed to stack Reduction Replace handle at top of stack by non-terminal Accept Successful completion of parsing Error Syntax error discoveredParser calls error recovery routine"},{"location":"3_Core/Compiler_Contruction/03_Syntax_Analysis/#types_2","title":"Types","text":"<ol> <li> <p>Operator-Precedence Parser</p> </li> <li> <p>LR Parser</p> </li> </ol> <p>There are 3 sub-types; only their parsing tables are different</p> <ul> <li>Simple LR</li> <li>LookAhead LR (intermediate)</li> <li>Canonical LR (most general)</li> </ul> <p>Yacc creates LALR</p> <p></p>"},{"location":"3_Core/Compiler_Contruction/03_Syntax_Analysis/#conflicts","title":"Conflicts","text":"Type Shift-Reduce - Associativity &amp; precedence not ensured- Default action: prefer shift Reduce-Reduce"},{"location":"3_Core/Compiler_Contruction/03_Syntax_Analysis/#resolving-conflicts","title":"Resolving conflicts","text":"Easy? Rewrite grammar \u274c Yacc_Directives.md \u2705"},{"location":"3_Core/Compiler_Contruction/03_Syntax_Analysis/#solving-stack-questions","title":"Solving stack questions","text":"Stack Input Action <code>$</code> somethign"},{"location":"3_Core/Compiler_Contruction/03_Syntax_Analysis/#lrk-parsing","title":"LR(\\(k\\)) Parsing","text":"<p>Meaning</p> <ul> <li>Left-right scanning</li> <li>Rightmost derivation</li> <li>Lookahead of \\(k\\) i/p symbols at each step; if \\(k\\) not mentioned, \\(k=1\\)</li> </ul> <p>Class of grammars parsable by LR is proper superset of class of grammars parsable with predictive parsers</p> \\[ \\text{LL(1) Grammars} \\subset \\text{LR(1) Grammars} \\] <p>Can detect syntactic error as soon it performs left-to-right scan of input</p>"},{"location":"3_Core/Compiler_Contruction/03_Syntax_Analysis/#why","title":"Why?","text":"<p>It is the most __ shift-reducing parser</p> <ul> <li>Efficient</li> <li>General</li> <li>Non-backtacking</li> </ul>"},{"location":"3_Core/Compiler_Contruction/03_Syntax_Analysis/#lr-parsing-algorithm","title":"LR Parsing Algorithm","text":"Stack Remaining Input \\(S_0 X_1 S_1 \\dots X_m S_m\\) \\(a_i a_{i+1} \\dots a_n \\$\\) <p>where</p> <ul> <li>\\(X_i\\) is a terminal/non-terminal</li> <li>\\(S_i\\) is a state</li> </ul> <p>The parser action is determined by \\(S_m\\), \\(a_i\\), and parsing action table</p>"},{"location":"3_Core/Compiler_Contruction/03_Syntax_Analysis/#actions","title":"Actions","text":"action[\\(S_m, a_i\\)] Meaning Shift s Shift new input symbol and next state \\(s_i\\) into stack Reduce\\(A \\to \\beta\\) (or) \\(r_j\\) 1. Reduce by production no \\(j\\)2. Pop 2 \\(\\vert \\beta \\vert\\) items from stack (grammar symbol &amp; state symbol)3. Use goto table4. Push \\(A\\) and \\(s\\) where \\(s=\\text{goto} [s_{m-r}, A]\\)(current input symbol not affected) acc Accept blank Error"},{"location":"3_Core/Compiler_Contruction/03_Syntax_Analysis/#example_2","title":"Example","text":"<p>Parse <code>id*id+id$</code> using</p> <p></p> <p></p>"},{"location":"3_Core/Compiler_Contruction/04_Top_Down_Parsing/","title":"04 Top Down Parsing","text":""},{"location":"3_Core/Compiler_Contruction/04_Top_Down_Parsing/#top-down-parsing","title":"Top-Down Parsing","text":"Recursive Descent Predictive Parsing Method Backtracking Table-Driven Widely-used? \u274c \u2705 Efficient? \u274c \u2705 Grammar General LL(1) Grammar Tries to find Leftmost derivation <ul> <li>Recursive predictive parsing is a type of Recursive Descent, without backtracking</li> <li>Non-recusive predictive parser is also known as LL(1) parser</li> </ul>"},{"location":"3_Core/Compiler_Contruction/04_Top_Down_Parsing/#recursive-descent-parsing","title":"Recursive Descent Parsing","text":""},{"location":"3_Core/Compiler_Contruction/04_Top_Down_Parsing/#computations","title":"Computations","text":"<p>I would recommend just drawing the tree, and finding the corresponding set of terminals.</p>"},{"location":"3_Core/Compiler_Contruction/04_Top_Down_Parsing/#firstx","title":"First(X)","text":"<p>The first symbol accessible;</p> X = \\(\\exist\\) X \\(\\to\\) First(X) \\(\\supset\\) \\(\\epsilon\\) \\(\\{\\epsilon\\}\\) Terminal \\(\\{X\\}\\) Non-Terminal \\(\\epsilon\\) \\(\\{\\epsilon\\}\\) Non-Terminal \\(Y_1 Y_2 Y_n\\) \\(Y_i \\to a; Y_{1, \\dots, i-1} = \\epsilon\\) \\(\\{a\\}\\) Non-Terminal \\(Y_1 Y_2 Y_n\\) \\(Y_i \\to \\epsilon,  \\forall i\\) \\(\\{\\epsilon\\}\\) <p></p>"},{"location":"3_Core/Compiler_Contruction/04_Top_Down_Parsing/#follownon-terminal","title":"Follow(Non-Terminal)","text":"Non-Terminal Start Symbol \\(S\\) Follow(S) \\(\\supset \\{\\$\\}\\) \\(A \\to \\alpha B \\beta\\) Follow(B) \\(=\\) First(\\(\\beta\\)) - \\(\\{\\epsilon\\}\\) \\(A \\to \\alpha B\\)\\(A \\to \\alpha B \\beta\\), and \\(\\epsilon \\in\\) First(\\(\\beta\\)) Follow(B) \\(\\supset\\) Follow(A)"},{"location":"3_Core/Compiler_Contruction/04_Top_Down_Parsing/#ll1-grammar","title":"LL(1) Grammar","text":"<p>No multiple-defined entries in parsing table</p> <ul> <li>Left-right scanning</li> <li>Leftmost derivation</li> <li>Lookahead of one i/p symbol at each step</li> </ul>"},{"location":"3_Core/Compiler_Contruction/04_Top_Down_Parsing/#ll1-parser","title":"LL(1) Parser","text":""},{"location":"3_Core/Compiler_Contruction/04_Top_Down_Parsing/#components","title":"Components","text":"Input Buffer String to be parsed, ending with <code>$</code> Output Producting rule representing step of leftmost derivation sequence of string in input buffer Stack Contains grammar symbols<code>$</code> signifies bottom of stackInitially, stack contains <code>$S</code>Only <code>$</code> in stack signfies parsing complete Parsing Table 2 dimensional array \\(M[A, a]\\)Each row is a non-terminal symbolEach column is <code>$</code>/terminal symbolEach entry holds production rule"},{"location":"3_Core/Compiler_Contruction/04_Top_Down_Parsing/#construction-of-parser","title":"Construction of Parser","text":"Replace with Elimination of left recursion $A \\to A \\alpha \\beta$ Elimination of left factoring $A \\to \\alpha \\beta_1 \\alpha \\beta_2 Calculate FIRST &amp; FOLLOW Construction of parsing tableFor each \\(A \\to \\alpha\\) For each terminal a in FIRST(\\(\\alpha\\)) Check if input string is accepted/not <p>This part is Incomplete</p>"},{"location":"3_Core/Compiler_Contruction/05_Symbol_Table/","title":"05 Symbol Table","text":""},{"location":"3_Core/Compiler_Contruction/05_Symbol_Table/#block-structure-languages","title":"Block Structure Languages","text":"<p>Basic units are blocks, which can be nested upto any depth</p>"},{"location":"3_Core/Compiler_Contruction/05_Symbol_Table/#scope-of-identifier","title":"Scope of Identifier","text":"<p>Refers to portion of program where the identifier is accessible</p> <p>Related concepts are function overriding, overloading, multiple variable declaration</p> <p>In C and similar languages, <code>{ ... }</code> marks the start and end of scope.</p>"},{"location":"3_Core/Compiler_Contruction/05_Symbol_Table/#example-problem","title":"Example Problem","text":"<pre><code>{ int x; char y; { bool y; x; y; } x; y; }\n</code></pre> <pre><code>{ { x:int; y:bool; } x:int; y:char; }\n</code></pre> <p>(Draw stack for this; couldn\u2019t draw cuz no time cuz of exam prep \u2026 sorry junior!)</p>"},{"location":"3_Core/Compiler_Contruction/05_Symbol_Table/#scoping-algorithm","title":"Scoping Algorithm","text":"When encountering <code>{</code> Push new stack node\u00a0(with empty symbol table within) Variable declaration/definition Update top\u2019s symbol table Variable invokation Check if element in top\u2019s symbol tableElse, check <code>top-1</code>'s symbol table and so on <code>}</code> Pop top stack node"},{"location":"3_Core/Compiler_Contruction/05_Symbol_Table/#push","title":"<code>push</code>","text":"<pre><code>temp = new SymbolTable Node\ntemp.prev = top\ntop = N\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/05_Symbol_Table/#pop","title":"<code>pop</code>","text":"<pre><code>temp = top\ntop = temp.prev\ntemp.prev = null\ndelete temp\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/05_Symbol_Table/#symbol-table-tree-representation","title":"Symbol Table Tree Representation","text":""},{"location":"3_Core/Compiler_Contruction/06_Semantic_Analysis/","title":"06 Semantic Analysis","text":""},{"location":"3_Core/Compiler_Contruction/06_Semantic_Analysis/#attribute","title":"Attribute","text":"<p>May hold almost any thing: a string, a number, a memory location, a complex record.</p>"},{"location":"3_Core/Compiler_Contruction/06_Semantic_Analysis/#semantic-rules","title":"Semantic Rules","text":"<p>Semantic rules set up dependencies between attributes which can be represented by a dependency graph. This dependency graph determines the evaluation order of these semantic rules. </p> <p>Evaluation of a semantic rule defines the value of an attribute. But a semantic rule may also have some side effects such as printing a value.</p>"},{"location":"3_Core/Compiler_Contruction/06_Semantic_Analysis/#sdd","title":"SDD","text":"<p>SDD (Syntax Directed Definition) is a CFG with semantic rules, where each</p> <ul> <li>Grammar symbol is associated with a set of attributes</li> <li>Production rule is associated with a set of semantic rules (will be provided)</li> </ul> <p>If \\(X\\) is a symbol and \\(a\\) is one of its attribute, then \\(X.a\\) denotes value at \\(X\\)</p> <p>This set of attributes for a grammar symbol is partitioned into two subsets called synthesized and inherited attributes of that grammar symbol.</p>"},{"location":"3_Core/Compiler_Contruction/06_Semantic_Analysis/#properties","title":"Properties","text":"<ul> <li>give high-level specifications for translations</li> <li>hide many implementation details such as order of evaluation of semantic actions.</li> <li>We associate a production rule with a set of semantic actions, and we do not say when they will be evaluated</li> </ul>"},{"location":"3_Core/Compiler_Contruction/06_Semantic_Analysis/#idk","title":"IDK","text":"<p>Each production \\(A \\to \\alpha\\) is associated with a set of semantic rules of the form</p> <p>\\(b=f(c_1,c_2, \\dots ,c_n)\\) where \\(f\\) is a function, and \\(b\\) can be one of the following</p> <ul> <li>\\(b\\) is a synthesized attribute of A and \\(c_1, c_2, ... ,c_n\\) are attributes of the grammar symbols in the production \\(A \\to \\alpha\\)</li> <li>\\(b\\) is an inherited attribute one of the grammar symbols in \\(\\alpha\\), and \\(c_1,c_2, \\dots, c_n\\) are attributes of the grammar symbols in the production \\(A \\to \\alpha\\)</li> </ul>"},{"location":"3_Core/Compiler_Contruction/06_Semantic_Analysis/#types-of-attributes","title":"Types of Attributes","text":"<p>Consider</p> <pre><code>A \u2192 BCD (A is parent; B,C,D are children)\n</code></pre> Synthesized Inherited Meaning Node takes values from children Node takes values from parent/sibling Example <code>A.s = B.s</code><code>A.s = C.s</code><code>A.s = D.s</code> <code>C.i = A.i</code><code>C.i = B.i</code><code>C.i = D.i</code>"},{"location":"3_Core/Compiler_Contruction/06_Semantic_Analysis/#types-of-sdd","title":"Types of SDD","text":"Type S-Attributed Definitions/S-Attributed SDD/S-Attributed Grammar Only synthesis L-Attributed Definitions/L-Attributed SDD/L-Attributed Grammar - Synthesis- Inheritance from parent/left-siblings(from previous example: C.S = A.S, C.S=B.S, not C.S = D.S) <p>Unlike regular SDD, these attribute grammars cannot have side effects (such as printing values); they can only evaluate values of attributes.</p>"},{"location":"3_Core/Compiler_Contruction/06_Semantic_Analysis/#translation-schemes","title":"Translation Schemes","text":"<p>Indicate the order of evaluation of semantic actions associated with a production rule</p> <p>Compared to SDD, they give some information about implementation details</p>"},{"location":"3_Core/Compiler_Contruction/06_Semantic_Analysis/#annotated-parse-tree","title":"Annotated Parse Tree","text":"<p>Parse tree showing the values of attributes at each node</p> <p>Process of computing attributes values at nodes is called annotating/decorating</p> <p>Order of these computations depends on the dependency graph induced by the semantic rules</p>"},{"location":"3_Core/Compiler_Contruction/06_Semantic_Analysis/#example-534","title":"Example: \\(5+3*4\\)","text":""},{"location":"3_Core/Compiler_Contruction/06_Semantic_Analysis/#dependency-graph","title":"Dependency Graph","text":"<p>All updward arrows, since this S-Attributed: all parents inherit from child(ren).</p> <p></p>"},{"location":"3_Core/Compiler_Contruction/06_Semantic_Analysis/#example-real-p-q","title":"Example: <code>real p, q</code>","text":""},{"location":"3_Core/Compiler_Contruction/06_Semantic_Analysis/#example-35","title":"Example: \\(3*5\\)","text":""},{"location":"3_Core/Compiler_Contruction/06_Semantic_Analysis/#syntax-dericted-translation","title":"Syntax-Dericted Translation","text":"<p>Combination of CFG, Semantic Rules, and Semantic Actions denoted in <code>{}</code> can be placed anywhere in RHS</p>"},{"location":"3_Core/Compiler_Contruction/06_Semantic_Analysis/#eg-conversion-of-infix-to-postfix-234","title":"Eg: Conversion of infix to postfix:  \\(2+3*4\\)","text":"<pre><code>E \u2192 E+T {print(\u2018+\u2019);}\nE \u2192 T\nT \u2192 T*F {print(\u2018*\u2019);}\nT \u2192 F\nF \u2192 num {print num.val;}\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/06_Semantic_Analysis/#eg-conversion-of-infix-to-postfix-12","title":"Eg: Conversion of infix to postfix:  \\(1+2\\)","text":""},{"location":"3_Core/Compiler_Contruction/06_Semantic_Analysis/#coercions","title":"Coercions","text":"<p>Binary arithmetic operator may be applied to either a pair of integers or to a pair of floating-point numbers.</p> <p>If an operation involves a float and an integer, the compiler may convert/coerce the integer into a float</p>"},{"location":"3_Core/Compiler_Contruction/06_Semantic_Analysis/#type-checking","title":"Type Checking","text":""},{"location":"3_Core/Compiler_Contruction/07_Intermediate_Code_Generation/","title":"07 Intermediate Code Generation","text":"<p>Ideally the details of source language are confined to the front end and the details of target machines to the back end.</p> <p></p>"},{"location":"3_Core/Compiler_Contruction/07_Intermediate_Code_Generation/#why-ic","title":"Why IC?","text":"<p>Rather than creating a compiler for every language to translate code for every machine arch, we create a compiler for every language to translate code to IC; this IC is then translated for the machine arch.</p> <p>This reduces complexity from \\((m \\times n) \\to (m+n)\\), where \\(m\\) and \\(n\\) are number of prog languages and machine arch respectively.</p>"},{"location":"3_Core/Compiler_Contruction/07_Intermediate_Code_Generation/#properties-of-ic","title":"Properties of IC","text":"<ul> <li> <p>easy to produce </p> </li> <li> <p>easy to translate into target machine code</p> </li> <li> <p>Three-address code (TAC)</p> </li> </ul> <p>consisting of sequence of assembly-like three-operand instructions of the form \\(x = y \\text{ op } z\\)</p> <ul> <li>Postfix notation</li> </ul>"},{"location":"3_Core/Compiler_Contruction/07_Intermediate_Code_Generation/#abstract-syntax-tree","title":"Abstract Syntax Tree","text":"<p>Binary tree where</p> <ul> <li>Leaves represent operands</li> <li>Non-terminals represent operators</li> </ul>"},{"location":"3_Core/Compiler_Contruction/07_Intermediate_Code_Generation/#eg-aab-c-b-cd","title":"Eg: \\(a+a*(b-c) +(b-c)*d\\)","text":""},{"location":"3_Core/Compiler_Contruction/07_Intermediate_Code_Generation/#directed-acyclic-graph-dag","title":"Directed Acyclic Graph (DAG)","text":"<p>AST where all subexpressions of an expression (even repeated subexpressions) occur only once.</p> <p>This helps the compiler generate more efficient code.</p>"},{"location":"3_Core/Compiler_Contruction/07_Intermediate_Code_Generation/#eg-aab-cb-cd","title":"Eg: \\(a+a*(b-c)+(b-c)*d\\)","text":""},{"location":"3_Core/Compiler_Contruction/07_Intermediate_Code_Generation/#eg-ii10","title":"Eg: \\(i=i+10\\)","text":""},{"location":"3_Core/Compiler_Contruction/07_Intermediate_Code_Generation/#steps-for-tree-representation","title":"Steps for tree representation","text":"<ol> <li>Check whether an identical node already exists</li> <li>If yes, the existing node is returned</li> <li>If no, create a new node and return it</li> </ol> <pre><code>// for a+a*(b-c)+(b-c)*d\n\np1  = Leaf (id, entry-a)\np2  = Leaf (id, entry-a) = p1\np3  = Leaf (id, entry- b)\np4  = Leaf (id, entry-c)\nP5  = Node ('-', p3, p4)\np6  = Node ('*', p1, p5)\np7  = Node ( '+' p1, p6 )\np8  = Leaf (id, entry-b) = p3\np9  = Leaf (id, entry-c) = p4\np10 = Node ('-', p3, p4) = p5\np11 = Leaf (id, entry-d)\np12 = Node ('*', p5 ,p11)\np13 = Node ('+',p7,p12)\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/07_Intermediate_Code_Generation/#steps-for-array-representation","title":"Steps for array representation","text":"<ul> <li>Search the array for a node M with label op, left child l , and right child r.</li> <li>If there is such a node, return the value number of M.</li> <li>If not, create in the array a new node N with label op, left child l, and right child r, and return its value number.</li> </ul> <p>We refer to nodes by giving the integer index/value number of the record for that node within the array.</p>"},{"location":"3_Core/Compiler_Contruction/07_Intermediate_Code_Generation/#three-address-code","title":"Three Address Code","text":"<p>Linearized representation of syntax tree/DAG in which explicit names correspond to internal nodes of the graph</p>"},{"location":"3_Core/Compiler_Contruction/07_Intermediate_Code_Generation/#features","title":"Features","text":""},{"location":"3_Core/Compiler_Contruction/07_Intermediate_Code_Generation/#arithmetic-expressions","title":"Arithmetic Expressions","text":"<ul> <li>1 operand on LHS</li> <li><code>=</code> if required</li> <li>\\(\\le 1\\) operation on RHS</li> <li>\\(\\le 2\\) operands on RHS</li> </ul>"},{"location":"3_Core/Compiler_Contruction/07_Intermediate_Code_Generation/#jumps","title":"Jumps","text":"<ul> <li><code>goto L</code></li> <li><code>if x goto L</code></li> <li><code>if x &lt;relop&gt; y goto L</code></li> </ul>"},{"location":"3_Core/Compiler_Contruction/07_Intermediate_Code_Generation/#functions","title":"Functions","text":"<ul> <li><code>param x</code></li> <li><code>call p</code></li> <li><code>y = call p</code></li> <li><code>return y</code></li> </ul>"},{"location":"3_Core/Compiler_Contruction/07_Intermediate_Code_Generation/#pointers","title":"Pointers","text":"<ul> <li><code>x = y[i]</code></li> <li><code>x[i] = y</code></li> <li><code>x = &amp;y</code></li> <li><code>x = *y</code></li> <li><code>*x = y</code></li> </ul>"},{"location":"3_Core/Compiler_Contruction/07_Intermediate_Code_Generation/#eg-xyz","title":"eg: \\(x+y*z\\)","text":"<pre><code>t1 = y * z\nt2 = x + t1\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/07_Intermediate_Code_Generation/#eg-idk","title":"eg: idk","text":"<pre><code>double a[10];\ndo i = i+1;\nwhile (a[i] &lt; v);\n</code></pre> <pre><code>// using symbolic labels\nL:  t1 = i + 1\n        i = t1\n        t2 = i * 8\n        t3 = a[t2]\n        if t3 &lt; v goto L\n\n// or\n\n// using position numbers\n\n100: t1 = i+1\n101: i  = t1\n102: t2 = i*8\n103: t3 = a[t2]\n104: if t3&lt;v goto 100\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/07_Intermediate_Code_Generation/#types-of-three-address-codes","title":"Types of Three Address Codes","text":"Fields Notes Quadruples op, arg1, arg2, result unary operators don\u2019t have arg2for assignment, op is <code>=</code>param does not have args2 and resultJumps put target label in result Triples op, arg1, arg2 We assume that result is stored in the corresponding index Indirect Triples Triples + List of pointers to triples <p>A benefit of quadruples and indirect triples over triples can be seen in an optimizing compiler, where instructions are often moved around.</p>"},{"location":"3_Core/Compiler_Contruction/07_Intermediate_Code_Generation/#ab-cb-c","title":"\\(a=b*-c+b*-c\\)","text":""},{"location":"3_Core/Compiler_Contruction/07_Intermediate_Code_Generation/#translations-arrays","title":"Translations: Arrays","text":"<p>Let</p> BA Base Address W Size of Element \\(R\\) Total number of rows \\(C\\) Total number of cols \\(L_r\\) Row starting index \\(L_c\\) Col starting index \\(i\\) Row index \\(j\\) Col index 1D \\(\\text{BA} + W*(i-L_r)\\) 2D \\(\\text{BA} + W*[(i-L_r)*C + (j-L_c)]\\) \u2026"},{"location":"3_Core/Compiler_Contruction/07_Intermediate_Code_Generation/#c-aij-for-int-a43","title":"<code>c + a[i][j]</code> for <code>int a[4][3];</code>","text":""},{"location":"3_Core/Compiler_Contruction/07_Intermediate_Code_Generation/#a-bi-ci","title":"<code>a = b[i] + c[i]</code>","text":"<p>Correction: Row 6 for triple should be <code>+ (2) (5)</code></p>"},{"location":"3_Core/Compiler_Contruction/07_Intermediate_Code_Generation/#ai-b-c-b-d","title":"<code>a[i] = b * c + b * d</code>","text":""},{"location":"3_Core/Compiler_Contruction/07_Intermediate_Code_Generation/#flow-control","title":"Flow Control","text":"<p>The translation of statements such as if-else- statements and while-statements is tied to the translation of boolean expressions</p> <p>Boolean expressions</p> <ul> <li>alter flow of control</li> <li>compute logical values</li> </ul>"},{"location":"3_Core/Compiler_Contruction/07_Intermediate_Code_Generation/#example","title":"Example","text":"<pre><code>while(a&lt;b) {\n  c=a+b;\n  a=a+1;\n}\n</code></pre> <pre><code>L1:\n    ifFalse a &lt; b L2 t1 = a + b\n    c = t1\n    t2 = a + 1\n    a = t2\n    goto L1\nL2:\n</code></pre> op arg1 arg2 result 0 LABEL L1 1 &lt; a b t1 2 iff(if false) t1 - L2 3 + a b t2 4 = t2 - c 5 + a 1 t3 6 = t3 a 7 goto L1 8 LABEL L2"},{"location":"3_Core/Compiler_Contruction/07_Intermediate_Code_Generation/#example_1","title":"Example","text":"<pre><code>sum = 0\nfor(i=0; i&lt; n; i++)\n{\n    sum = sum + i;\n}\n</code></pre> <pre><code>sum = 0\ni = 0\nL1:\n  ifFalse i &lt; n L2\n  t1 = sum + i\n  sum = t1\n  i=i+1\ngoto L1 L2:\n</code></pre> op arg1 arg2 Result 0 = 0 sum 1 = 0 i 2 LABEL L1 3 &lt; i n t1 4 iff t1 - L2 5 + sum i t2 6 = t2 sum 7 goto L1 8 LABEL L2"},{"location":"3_Core/Compiler_Contruction/07_Intermediate_Code_Generation/#basic-blows-flow-graphs","title":"Basic Blows &amp; Flow Graphs","text":""},{"location":"3_Core/Compiler_Contruction/08_Code_Generation/","title":"08 Code Generation","text":"<p>The problem is undecidable and most of  those subproblems, intractable (\\(\\not \\exists\\) efficient algorithms to solve them)</p>"},{"location":"3_Core/Compiler_Contruction/08_Code_Generation/#requirements","title":"Requirements","text":"<ul> <li>Maintain semantic meaning</li> <li>Must generate efficient code and makes maximum use of  available resources</li> <li>Code generator itself must be efficient</li> </ul>"},{"location":"3_Core/Compiler_Contruction/08_Code_Generation/#characteristics","title":"Characteristics","text":"<ul> <li>Target Machine has \\(n\\) registers \\(R_0, \\dots, R_{n-1}\\)</li> <li>All operands are integers</li> </ul>"},{"location":"3_Core/Compiler_Contruction/08_Code_Generation/#instruction-format","title":"Instruction Format","text":"load data <code>ld dest, src</code> store data <code>st dest, src</code> Arithmetic <code>add dest, src1, src2</code><code>sub dest, src1, src2</code><code>mul dest, src1, src2</code> Unconditional jump <code>br L</code> Conditional jump <code>bltz r, l</code><code>bgtz r, l</code><code>blez r, l</code><code>bgez r, l</code><code>bz r, l</code><code>bnz r, l</code>"},{"location":"3_Core/Compiler_Contruction/08_Code_Generation/#adressing-modes","title":"Adressing Modes","text":"Mode Example Meaning Indexed1 ld r1, a(r2) <code>r1 = contents(a+contents(r2))</code> Indexed2 ld r1, 100(r2) <code>r1 = contents(100+contents(r2))</code> Indirect ld r1, *100(r2) <code>r1 = contents(contents(100+contents(r2)))</code> Immediate add r1, r2, 100 <code>r1 = contents(r1) + 100</code>"},{"location":"3_Core/Compiler_Contruction/08_Code_Generation/#examples","title":"Examples","text":"<pre><code>x = y - z\n</code></pre> <pre><code>b = a[i]\n</code></pre> <pre><code>a[j] = c\n</code></pre> <pre><code>x = *p\n</code></pre> <pre><code>*p = y\n</code></pre> <pre><code>if x &lt; y goto M\n</code></pre> <pre><code>y = *q\nq = q  + 4\n*p = y\np = p + 4\n</code></pre> <pre><code>LD R1, q     // R1 = q\nLD R2, 0(R1) // R2 = contents(0+con(R1))\nST y, R2     // y = R2\nADD R1, R1, #4 //R1 = R1 + 4\nST q, R1       // q = R1\nLD R3, p       // R3 = p\nST 0(R3), R2   // contents(0+con(R3)) = R2\nADD R3, R3, #4  // R3 = R3 + 4\nST p, R3        // p = R3\n</code></pre> <pre><code>    s = 0\n    i = 0\nL1: if i &gt; n goto L2\n    s = s + i\n    i = i + 1\n    goto L1\nL2:\n</code></pre> <pre><code>    SUB R1, R1, R1\n    ST s, R1\n    ST i, R1\nL1: LD R1, i\n    LD R2, n\n    SUB R1, R1, R2  // R1 =i - n\n    BGTZ R1, L2\n    LD  R1, s\n    LD  R2, i\n    ADD R1, R1, R2\n    ST  s, R1\n    ADD R2, R2, #1\n    ST i, R2\n    BR L1\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/08_Code_Generation/#memory-representation","title":"Memory Representation","text":""},{"location":"3_Core/Compiler_Contruction/08_Code_Generation/#liveness","title":"Liveness","text":"<p>IC has representation with infinite no of temporaries. This program should be able to run on a machine with limited registers.</p> <p>2 temporaries \\(t_1\\) and \\(t_2\\) can be mapped to the same register, if \\(t_1\\) and \\(t_2\\) are never used at the same time</p> <p>A variable is live if its current value is used in the future, without any intermediary step changing the value</p> <p>Two Data structures</p> <ul> <li>Symbol table</li> <li>metadata associated with current instruction</li> </ul>"},{"location":"3_Core/Compiler_Contruction/08_Code_Generation/#liveness-analysis","title":"Liveness Analysis","text":"<p>Assume the symbol table shows all non-temporary variables as live on exit and \u201cno next use\u201d</p> <p></p> <p>Input: Basic block B of three-address statements</p> <p>Output: At each statement i: <code>x = y op z</code>, we attach to i the liveliness and next-uses of x, y and z, founding using the symbol table</p> <p>We start at the last statement of B and scan backwards</p> <ol> <li>In the symbol table, set x to \u201cnot live\u201d and \u201cno next use\u201d</li> <li>In the symbol table, set y and z to \u201clive\u201d, and next-uses of y and z to i</li> </ol>"},{"location":"3_Core/Compiler_Contruction/08_Code_Generation/#run-time-memory-models","title":"Run Time Memory Models","text":"<p>POPL</p>"},{"location":"3_Core/Compiler_Contruction/08_Code_Generation/#registers-allocation-assignment","title":"Registers Allocation &amp; Assignment","text":""},{"location":"3_Core/Compiler_Contruction/08_Code_Generation/#usage-counts","title":"Usage counts","text":"<ul> <li>use(x, B) is 1 if x is used in block B prior to any definition of x</li> <li>use (x, B) is 0 otherwise</li> <li>live(x, B) is 1 if x is live on exit from B and is assigned a value in B</li> <li>live(x, B) is 0 otherwise</li> </ul>"},{"location":"3_Core/Compiler_Contruction/09_Code_Optimization/","title":"09 Code Optimization","text":"<p>Code optimization can be done at the following </p> After IC generation After code generation Code OptimizationType M/C independent M/C dependent Performed on IC Target Code"},{"location":"3_Core/Compiler_Contruction/09_Code_Optimization/#classification","title":"Classification","text":"Classification Type Category Meaning By Scope Local Within a single basic block Peephole On window of instructions (usually local) Loop-Level On 1/more loops or loop nests Global Entire procedure Inter-Procedural Across multiple procedures/entire program By Machine Information used Machine Independent Machine Dependent By effecton program structure Algebraic Transformations Reordering transformation"},{"location":"3_Core/Compiler_Contruction/09_Code_Optimization/#forms-of-optimizations","title":"Forms of Optimizations","text":"Optimizations Before Optim After Optim Algebraic Simplification/Reduction in strength Simplify operations for machine <code>x*2 + x*1024</code> <code>x+x+(x&lt;&lt;10)</code> Common subexpression elimination Subexpression/Duplicate code are repeated expressionsTry to avoid subexpressions, by creating temporaries <code>A[I+10] = B[I+10]</code> <code>t=I+10</code><code>A[t]=B[t]</code> Copy propagation After <code>x=y</code>, try to use <code>y</code> as much as possible <code>t=I*4</code><code>s=t</code><code>a[s] = a[s]+4</code> <code>t=I*4</code><code>s=t</code><code>a[t] = a[t]+4</code> Constant propagation Replace constant variables with constant value <code>a=10</code><code>b=20</code><code>c=a+b</code> <code>a=10</code><code>b=20</code><code>c=10+20</code> Constant folding Evaluate expression with constants, and replace with the result <code>c=64+2</code> <code>c=66</code> Dead-code elimination Remove code that will never be used <code>if (3&gt;7) then { \u2026 }</code> (nothing) Loop Transformations/Loop Unrolling Change <code>for(i=0;i&lt;n;i++)</code> to <code>for(i=0;i&lt;n-s+1;i+=s)</code>replicate the loop body s times (changing also i as needed to i+1, i+2, etc\u2026).Will need an \u2018epilogue\u2019 if s does not divide n.Creates larger basic blocks and facilitates instruction scheduling. <code>for (i=0; i&lt;n; i++){ a[i]=b[i]*c[i]; }</code> <code>for (i=0; i&lt;n-s+1; i+=s) { a[I]=b[i]*c[i]; a[I+1]=b[i+1]*c[i+1]; \u2026 (the loop body repeated s times, from i to i+s-1) }</code><code>for (j=i; j&lt;n; j++) {a[j]=b[j]*c[j];}</code> Code Motion Move loop-invariant calculation outside loop(loop-invariant = expression having same value regardless of how many times loop is run) <code>while (i&lt;=limit-2)</code> <code>t = limit-2</code><code>while (i&lt;=t)</code>"},{"location":"3_Core/Compiler_Contruction/09_Code_Optimization/#automatic-garbage-collection","title":"Automatic Garbage Collection","text":"<ol> <li>Marking: Identify which pieces of memory are in use (very expensive process)</li> <li>Normal deletion: remove unreferenced objects</li> <li>Compacting: Reduce fragmentation by putting allocated memory next to each other</li> </ol>"},{"location":"3_Core/Compiler_Contruction/09_Code_Optimization/#generational-garbage-collection","title":"Generational Garbage Collection","text":"<p>Generations</p> <ul> <li>Young generation</li> <li>When the young generation fills up, this causes a minor garbage collection. Minor collections can be optimized assuming a high object mortality rate. A young generation full of dead objects is collected very quickly. Some surviving objects are aged and eventually move to the old generation</li> <li>All minor garbage collections are \"Stop the World\" events. This means that all application threads are stopped until the operation completes</li> <li>Old Generation</li> <li>Used to store long surviving objects. Typically, a threshold is set for young generation object and when that age is met, the object gets moved to the old generation. Eventually the old generation needs to be collected. This event is called a major garbage collection.</li> <li>Major garbage collection are also Stop the World events. Often a major collection is much slower because it involves all live objects. So for Responsive applications, major garbage collections should be minimized. Also note, that the length of the Stop the World event for a major garbage collection is affected by the kind of garbage collector that is used for the old generation space.</li> <li>Permanent Generation</li> <li>contains metadata required by the JVM to describe the classes and methods used in the application. The permanent generation is populated by the JVM at runtime based on classes in use by the application. In addition, Java SE library classes and methods may be stored here.</li> <li>Classes may get collected (unloaded) if the JVM finds they are no longer needed and space may be needed for other classes.</li> <li>The permanent generation is included in a full garbage collection.</li> </ul>"},{"location":"3_Core/Compiler_Contruction/09_Code_Optimization/#steps","title":"Steps","text":"<ol> <li>New objects are allocated to the eden space. Both survivor spaces start out empty</li> <li>When the eden space fills up, a minor garbage collection is triggered</li> <li>Referenced objects are moved to the first survivor space. Unreferenced objects are deleted when the eden space is cleared</li> <li>At the next minor GC, the same thing happens for the eden space. Unreferenced objects are deleted and referenced objects are moved to a survivor space. However, in this case, they are moved to the second survivor space (S1). In addition, objects from the last minor GC on the first survivor space (S0) have their age incremented and get moved to S1. Once all surviving objects have been moved to S1, both S0 and eden are cleared. Notice we now have differently aged object in the survivor space.</li> <li>At the next minor GC, the same process repeats. However this time the survivor spaces switch. Referenced objects are moved to S0. Surviving objects are aged. Eden and S1 are cleared.</li> <li>After a minor GC, when aged objects reach a certain age threshold, they are promoted from young generation to old generation</li> <li>As minor GCs continue to occure objects will continue to be promoted to the old generation space</li> <li>Eventually, a major GC will be performed on the old generation which cleans up and compacts that space</li> </ol>"},{"location":"3_Core/Compiler_Contruction/Practicals/","title":"CC Lab","text":"<p>All program codes from Anurag\u2019s GitHub</p>"},{"location":"3_Core/Compiler_Contruction/Practicals/01_Lex_Introduction/","title":"01 Lex Introduction","text":""},{"location":"3_Core/Compiler_Contruction/Practicals/01_Lex_Introduction/#lex","title":"Lex","text":"<p>Language translator, which converts</p> <ul> <li>from lex source program having regular expression, to match tokens in input string</li> <li>to a C program which has the function <code>yylex()</code> which is used to scan the input for tokens</li> </ul> <p>This will be the source file for lexical analysis of a C program. It will take a C program as input.</p> <p>We use regular expressions to match lexemes and generate tokens</p>"},{"location":"3_Core/Compiler_Contruction/Practicals/01_Lex_Introduction/#lex-source-code","title":"Lex Source Code","text":"<pre><code>%{\n#include\nC Declarations\n%}\nLex Symbols\n%%\nRule\n%%\nAuxilliary functions (optional)\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/Practicals/01_Lex_Introduction/#simplest-program","title":"Simplest Program","text":"<p>Default program which copies input to output</p> <pre><code>%%\n%%\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/Practicals/01_Lex_Introduction/#compilation","title":"Compilation","text":"<pre><code>lex analyzer.l\ncc lex.yy.c -ll\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/Practicals/01_Lex_Introduction/#execution","title":"Execution","text":"<pre><code>a.out\n\n// Takes user input\n</code></pre> <pre><code>a.out &lt; my_program.c &gt; sample.txt\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/Practicals/01_Lex_Introduction/#variables","title":"Variables","text":"Data Type Meaning Default Value <code>yytext</code> <code>*char</code> pointer to matched string <code>yyleng</code> <code>int</code> length of matched string <code>yyin</code> <code>*file</code> Input Source STDIN (console) <code>yyout</code> <code>*file</code> Output Destination STDOUT (console)"},{"location":"3_Core/Compiler_Contruction/Practicals/02_Lex_Rules/","title":"02 Lex Rules","text":""},{"location":"3_Core/Compiler_Contruction/Practicals/02_Lex_Rules/#match-real-numbers","title":"Match Real Numbers","text":"<pre><code>digit [0-9]\nsign [+|-]\n%%\n{sign}?{digit}+(\\.{digit}+)? printf(\"Matched real no: %s of length: %d\", yytext, yyleng);\n%%\n</code></pre> <pre><code>300.21\nMatched real no: 300.21 of length: 6\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/Practicals/02_Lex_Rules/#conflict-resolution","title":"Conflict Resolution","text":""},{"location":"3_Core/Compiler_Contruction/Practicals/02_Lex_Rules/#different-rules","title":"Different Rules","text":"<pre><code>%%\na printf(\"Matched %d a\\n\", yyleng);\naa printf(\"Matched %d a\\n\", yyleng);\n%%\n</code></pre> <pre><code>aaaaaaa\nMatched 2 a\nMatched 2 a\nMatched 2 a\nMatched 1 a\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/Practicals/02_Lex_Rules/#similar-rules","title":"Similar Rules","text":"<p><code>Warning: Rule not matched</code></p> <pre><code>letter [a-z A-Z]\ndigit [0-9]\n%%\n{letter}({letter}|{digit})* printf(\"Matched id\");\n{letter}+ printf(\"Matched word\");\n%%\n</code></pre> <pre><code>sum\nMatched id\nhello\nMatched id\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/Practicals/02_Lex_Rules/#match-word-immediately-followed-by-number","title":"Match word immediately followed by number","text":"<pre><code>letter [a-z A-Z]\nword {letter}+\ndigit [0-9]\n%%\n{word}/{digit} printf(\"Found word %s followed by number \", yytext);\n%%\n</code></pre> <pre><code>hello123\nFound word hello followed by number 123\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/Practicals/03_Lex_Auxilliary_Functions/","title":"03 Lex Auxilliary Functions","text":""},{"location":"3_Core/Compiler_Contruction/Practicals/03_Lex_Auxilliary_Functions/#file-input-console-output","title":"File Input &amp; Console Output","text":"<pre><code>%%\n[0-9]+ printf(\"Found a number\");\n%%\nvoid main()\n{\n    yyin = fopen(\"in.txt\", \"r\"); // open file in read mode\n    yyout = fopen(\"out.txt\", \"w\"); // open file in read mode\n    yylex(); // invoke scanner\n}\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/Practicals/03_Lex_Auxilliary_Functions/#file-input-file-output","title":"File Input &amp; File Output","text":"<pre><code>%%\n[0-9]+ fprintf(yyout, \"Found a number\"); // print to file\n%%\nvoid main()\n{\n    yyin = fopen(\"in.txt\", \"r\"); // open file in read mode\n    yyout = fopen(\"out.txt\", \"w\"); // open file in write mode\n    yylex(); // invoke scanner\n}\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/Practicals/03_Lex_Auxilliary_Functions/#user-defined-vars-functions","title":"User-Defined Vars &amp; Functions","text":"<pre><code>%{\nvoid display();\n%}\ndigit [0-9]\nnumber {digit}+\n%%\nnumber display();\n%%\nvoid display()\n{\nprintf(\"Found a number\");\n}\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/Practicals/03_Lex_Auxilliary_Functions/#user-defined-vars","title":"User-Defined Vars","text":"<pre><code>%{\nvoid display();\nint a;\n%}\ndigit [0-9]\nnumber {digit}+\n%%\n{number} {\n  a = atoi(yytext);\n  display(a);\n}\n%%\nvoid display()\n{\nprintf(\"Found number %d\", a);\n}\n</code></pre> <p>or</p> <pre><code>%{\nvoid display();\n%}\ndigit [0-9]\nnumber {digit}+\n%%\n{number} display(yytext)\n%%\nvoid display(yytext)\n{\n  int a = atoi(yytext);\n    printf(\"Found number %d\", a);\n}\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/Practicals/03_Lex_Auxilliary_Functions/#question-1","title":"Question 1","text":"<p>Write a LEX program to recognize the following </p> <ul> <li>Operators: +, -, *, /, |, </li> <li>Numbers</li> <li>newline</li> <li>Any other character apart from the above should be recognized as mystery character</li> </ul> <p>For each of the above mentioned matches (classes of lexeme) in your input, the program should print the following: PLUS, MINUS, MUL, DIV, ABS, NUMBER, NEW LINE, MYSTERY CHAR respectively. Your program should also strip of whitespaces.</p> <pre><code>%%\n\"+\" printf(\"PLUS\");\n\"-\" printf(\"MINUS\");\n\"*\" printf(\"MUL\");\n\"/\" printf(\"DIV\");\n\"|\"\" printf(\"ABS\");\n[0-9]+ printf(\"Number\");\n\\n printf(\"Newline\\n\");\n. printf(\"Wildcard \");\n%%\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/Practicals/03_Lex_Auxilliary_Functions/#question-2","title":"Question 2","text":"<p>Write a LEX program to print the number of words, characters and lines in a given input.</p> <pre><code>%{\nint cc = 0, wc = 0, lc = 0;\n%}\n%%\n[a-zA-Z]+ {wc++; cc+=strlen(yytext);}\n\\n {lc++; cc++;}\n. {cc++;}\n%%\n\nint yywrap()\n{\n    return 1;\n}\n\nvoid main()\n{\n    yylex();\n    printf(\"%d\\n\", cc);\n    printf(\"%d\\n\", wc);\n    printf(\"%d\\n\", lc);\n}\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/Practicals/03_Lex_Auxilliary_Functions/#question-3","title":"Question 3","text":"<p>Write a LEX program to print the number of words, characters and lines in a given input, but a word and its characters are counted only if its length is greater than or equal to 6.</p> <pre><code>%{\n  #include &lt;stdio.h&gt;\n  int num_words = 0;\n  int num_chars = 0;\n  int num_lines = 0;\n%}\n%%\n[\\t]+ {\n  // Ignore whitespace\n}\n\n\\n {\n  num_lines++;\n}\n\n[a - zA - Z]{6,} {\n  num_words++;\n  num_chars += yyleng;\n}\n\n. {\n  if (yyleng &gt;= 6)\n    {\n      num_chars += yyleng;\n    }\n}\n\n%%\nint main ()\n{\n  yylex ();\n  printf (\"Number of words: %d\\n\", num_words);\n  printf (\"Number of characters: %d\\n\", num_chars);\n  printf (\"Number of lines: %d\\n\", num_lines);\n  return 0;\n}\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/Practicals/03_Lex_Auxilliary_Functions/#question-4","title":"Question 4","text":"<p>Write a LEX program to print if the input is an odd number or an even number along with its length. Also, the program should check the correctness of the input (i.e. if the input is one even number and one odd number).</p> <pre><code>%{\n#include&lt;stdlib.h&gt;\n#include&lt;stdio.h&gt;\n  int number_1;\n  int number_2;\n%}\nnumber_sequence [0 - 9]*\n%%\n{number_sequence}[0 | 2 | 4 | 6 | 8] {\n  printf (\"Even number [%d]\", yyleng);\n  return atoi (yytext);\n}\n\n{number_sequence}[1 | 3 | 5 | 7 | 9] {\n  printf (\"Odd number [%d]\", yyleng);\n  return atoi (yytext);\n}\n%%\nint main()\n{\n  printf (\"\\nInput an even number and an odd number\\n\");\n  number_1 = yylex ();\n  number_2 = yylex ();\n  int diff = number_1 - number_2;\n  if (diff % 2 != 0)\n    printf\n      (\"\\nYour inputs were checked for correctness, \\nResult : Correct\\n\");\n  else\n    printf\n      (\"\\nYour inputs were checked for correctness,\\nResult : Incorrect\\n\");\n  return 1;\n}\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/Practicals/04_Lex_Something/","title":"04 Lex Something","text":""},{"location":"3_Core/Compiler_Contruction/Practicals/04_Lex_Something/#question-1","title":"Question 1","text":"<p>Write a LEX program to get a binary input and print whether the given input is a power of two or not.</p> <pre><code>%{\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n%}\n%%\n[01]+ {\n    int num = 0, base = 2, exp;\n    int result;\n    int len = strlen(yytext);\n    for (int i = 0; i &lt; len; i++) {\n        if (yytext[i] == '1') {\n        result = 1;\n        exp = len-i-1;\n            while (exp != 0) {\n            result *= base;\n            --exp;\n        }\n        num += result;\n        }\n    }\n    if (num == 0) {\n        printf(\"The input is not a power of two\\n\");\n    } else if ((num &amp; (num-1)) == 0) {\n        printf(\"The input is a power of two\\n\");\n    } else {\n        printf(\"The input is not a power of two\\n\");\n    }\n}\n. {\n    printf(\"Invalid input\\n\");\n}\n%%\nint main() {\n    yylex();\n    return 0;\n}\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/Practicals/04_Lex_Something/#question-2","title":"Question 2","text":"<p>Write a LEX program to insert line numbers to a file. For this copy your favourite C program \u201cinput.c\u201d to your folder which would be the input to your LEX program.</p> <pre><code>%{\n#include &lt;stdio.h&gt;\nint line_number = 1;\n%}\n\n%%\n\n\\n {\n    line_number++;\n    printf(\"\\n%d: \", line_number);\n}\n\n. {\n    printf(\"%c\", yytext[0]);\n}\n\n%%\n\nint main() {\n    FILE *input_file = fopen(\"input.c\", \"r\");\n    if (input_file == NULL) {\n        printf(\"Error: cannot open file.\\n\");\n        return 1;\n    }\n    yyin = input_file;\n    printf(\"1: \");\n    yylex();\n    fclose(input_file);\n    return 0;\n}\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/Practicals/04_Lex_Something/#question-3","title":"Question 3","text":"<p>Write a LEX program to save the contents of an input file excluding comment lines to another file.</p> <pre><code>%{\n#include &lt;stdio.h&gt;\n#include &lt;stdbool.h&gt;\nbool in_block_comment = false;  /* initialize in_block_comment flag to false */\nbool in_line_comment = false;   /* initialize in_line_comment flag to false */\n%}\n\n%%\n\n\"/*\"    { in_block_comment = true; }  /* set in_block_comment flag to true on start of block comment */\n\"*/\"    { in_block_comment = false; }  /* set in_block_comment flag to false on end of block comment */\n\"//\"    { in_line_comment = true; }   /* set in_line_comment flag to true on start of line comment */\n\\n      {\n            if (in_line_comment) { in_line_comment = false; }  /* set in_line_comment flag to false on end of line */\n            if (!in_block_comment) { fputc('\\n', yyout); }     /* write newline character to output file if not in comment */\n         }\n.       { if (!in_block_comment &amp;&amp; !in_line_comment) { fputc(yytext[0], yyout); } }  /* write character to output file if not in comment */\n%%\n\nint main(int argc, char** argv) {\n    if (argc != 3) {\n        printf(\"Usage: %s input_file output_file\\n\", argv[0]);\n        return 1;\n    }\n\n    FILE* input_file = fopen(argv[1], \"r\");\n    if (input_file == NULL) {\n        printf(\"Error: could not open input file %s\\n\", argv[1]);\n        return 1;\n    }\n\n    FILE* output_file = fopen(argv[2], \"w\");\n    if (output_file == NULL) {\n        printf(\"Error: could not open output file %s\\n\", argv[2]);\n        return 1;\n    }\n\n    yyin = input_file;\n    yyout = output_file;\n    yylex();\n\n    fclose(input_file);\n    fclose(output_file);\n    return 0;\n}\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/Practicals/04_Lex_Something/#question-4","title":"Question 4","text":"<p>Write a LEX program that would take a BITS student's roll number as input and prints the details of the student based on that. You are expected to write regular expressions that would synthesize information like, year of joining, specialization, PS/Thesis, Registration index, Campus (U) etc. from the given roll number. If the given input does not abide by the Roll number format, print some error message.</p> <pre><code>%{\n#include &lt;stdio.h&gt;\n%}\n\n%%\n\n^[0-9]{4}[A-Za-z0-9]{2}(PS|TS)[0-9]{4}[HPUG]$  {\n    printf(\"Year of Joining: %c%c%c%c\\n\",yytext[0], yytext[1],yytext[2], yytext[3]);\n    printf(\"Specialization: %c%c\\n\", yytext[4], yytext[5]);\n    printf(\"Thesis/Practice School: %c%c\\n\", yytext[6], yytext[7]);\n    printf(\"Registration Index: %c%c%c%c\\n\", yytext[8], yytext[9], yytext[10], yytext[11]);\n    printf(\"Campus: %c\\n\", yytext[12]);\n}\n.  {\n    printf(\"Invalid roll number format.\\n\");\n}\n\n%%\n\nint main() {\n    yylex();\n    return 0;\n}\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/Practicals/05_Yacc_Introduction/","title":"05 Yacc Introduction","text":""},{"location":"3_Core/Compiler_Contruction/Practicals/05_Yacc_Introduction/#yacc","title":"Yacc","text":"<p>yet another compiler compiler</p> <p>Parser Generator: Bottom-up parser</p> <p>Lexical analyzer is a dependency for syntax analyzer. In this case, lex is a dependency for yacc.</p>"},{"location":"3_Core/Compiler_Contruction/Practicals/05_Yacc_Introduction/#structure-of-program","title":"Structure of program","text":""},{"location":"3_Core/Compiler_Contruction/Practicals/05_Yacc_Introduction/#filenamel","title":"<code>filename.l</code>","text":"<pre><code>%{\n#include \"y.tab.h\"\nextern int yylval;\n%}\n%%\n\n%%\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/Practicals/05_Yacc_Introduction/#filenamey","title":"<code>filename.y</code>","text":"<pre><code>%{\n  int yylex(void);\n  void yyerror(char *);\n\n  #include &lt;stdio.h&gt;\n  #include &lt;stdlib.h&gt;\n\n  C includes\n  C declaration\n%}\n\n%token token_declaration_1 token_declaration_2\n// lex must be able to identify these tokens\n\n%%\n\nLHS : RHS1  {Action1}\n        | RHS2  {Action2}\n        | RHS3  {Action3}\n        ;\n\n%%\n\nvoid yyerror(char *s)\n{\n  printf(\"%s\", s);\n}\n\nvoid main()\n{\n  yyparse();\n}\n\nC functions\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/Practicals/05_Yacc_Introduction/#compilation-execution","title":"Compilation &amp; Execution","text":"<pre><code>yacc -d filename.y\nlex filename.l\ncc lex.yy.c y.tab.c -ll -lm\na.out\n</code></pre> Required? <code>-d</code> Flag that instructs to generate the definitions of the tokens \u2705 <code>-ll</code> Link lex loader \u2705 <code>-lm</code> Link math"},{"location":"3_Core/Compiler_Contruction/Practicals/05_Yacc_Introduction/#output-files","title":"Output Files","text":"File Name Purpose <code>y.tab.h</code> Header file containing definitions of tokens(must be included in lex file) <code>y.tab.c</code> Parser C Code"},{"location":"3_Core/Compiler_Contruction/Practicals/05_Yacc_Introduction/#value-of-symbols","title":"Value of Symbols","text":"<p>Every yacc grammar symbol has a value associated with it.</p> <ul> <li>LHS = <code>$$</code></li> <li>RHS = <code>$1, $2, ...</code></li> </ul> \\[ \\$\\$ = \\$1 \\text{ Operation } \\$3 \\\\ \\] <p>Example</p> \\[ \\begin{aligned} \\$\\$ &amp;= \\$1 \\text{ + } \\$3 \\\\ \\$\\$ &amp;= \\$1 \\text{ - } \\$3 \\\\ \\$\\$ &amp;= \\$1 \\end{aligned} \\]"},{"location":"3_Core/Compiler_Contruction/Practicals/05_Yacc_Introduction/#question-1","title":"Question 1","text":"<p>Write a simple calculator which can gives result for an addition/subtraction expression (ambiguity allowed).</p>"},{"location":"3_Core/Compiler_Contruction/Practicals/05_Yacc_Introduction/#05_1l","title":"<code>05_1.l</code>","text":"<pre><code>%{\n#include \"y.tab.h\"\nextern int yylval;\n%}\n%%\n[0-9]+ {\n    yylval = atoi(yytext);\n    return INTEGER;\n}\n\"+\" return PLUS;\n\"-\" return MINUS;\n\"\\n\" return NL;\n[ \\t] ; \n. printf(\"Invalid\");\n%%\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/Practicals/05_Yacc_Introduction/#05_1y","title":"<code>05_1.y</code>","text":"<pre><code>%{\n  int yylex(void);\n  void yyerror(char *);\n\n  #include &lt;stdio.h&gt;\n  #include &lt;stdlib.h&gt;\n%}\n\n%token INTEGER PLUS MINUS NL\n\n%%\nprogram:\nexpr NL {printf(\"%d\\n\", $1); exit(0);}\n;\nexpr:\nINTEGER {$$=$1;}\n| expr PLUS expr    {$$=$1+$3;}\n| expr MINUS expr   {$$=$1-$3;}\n;\n%%\nvoid yyerror(char *s)\n{\n    printf(\"%s\\n\", s);\n}\nint main()\n{\n    yyparse();\n}\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/Practicals/05_Yacc_Introduction/#compilation","title":"Compilation","text":"<pre><code>yacc -d calculator.y\n</code></pre> <pre><code>yacc: 4 shift/reduce conflicts\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/Practicals/05_Yacc_Introduction/#output","title":"Output","text":"<pre><code>1 - 5 + 2\n-6 (should actually be -2)\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/Practicals/05_Yacc_Introduction/#conversion-from-bnf-to-yacc-rule","title":"Conversion from BNF to YACC Rule","text":"<pre><code>A -&gt; a | \u03b5\n\nA : a\n    | \n    ;\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/Practicals/06_Yacc_Directives/","title":"06 Yacc Directives","text":""},{"location":"3_Core/Compiler_Contruction/Practicals/06_Yacc_Directives/#directives","title":"Directives","text":"<p>Along with the tokens</p> <pre><code>%left PLUS MINUS\n%left MUL DIV\n%right POW\n%%\n%%\n</code></pre> <p>The directives written later have higher precedence.</p>"},{"location":"3_Core/Compiler_Contruction/Practicals/06_Yacc_Directives/#question-1","title":"Question 1","text":"<p>Write a simple calculator which can gives result for an addition/subtraction expression (without ambiguity).</p>"},{"location":"3_Core/Compiler_Contruction/Practicals/06_Yacc_Directives/#06_1l","title":"<code>06_1.l</code>","text":"<p>same as Basic Calculator </p>"},{"location":"3_Core/Compiler_Contruction/Practicals/06_Yacc_Directives/#06_1y","title":"<code>06_1.y</code>","text":"<pre><code>%{\n  int yylex(void);\n  void yyerror(char *);\n\n  #include &lt;stdio.h&gt;\n  #include &lt;stdlib.h&gt;\n%}\n\n%token INTEGER PLUS MINUS NL\n%left PLUS MINUS                                            /* 1 */\n%%\nprogram:\nexpr NL {printf(\"%d\\n\", $1); exit(0);}\n;\nexpr:\nINTEGER {$$=$1;}\n| expr PLUS expr    {$$=$1+$3;}\n| expr MINUS expr   {$$=$1-$3;}\n;\n%%\nvoid yyerror(char *s)\n{\n    printf(\"%s\\n\", s);\n}\nint main()\n{\n    yyparse();\n}\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/Practicals/06_Yacc_Directives/#output","title":"Output","text":"<pre><code>1-5+2\n-2\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/Practicals/06_Yacc_Directives/#question-2","title":"Question 2","text":"<p>The program should keep going on until the user exits using <code>Ctrl-D</code></p>"},{"location":"3_Core/Compiler_Contruction/Practicals/06_Yacc_Directives/#06_2l","title":"<code>06_2.l</code>","text":"<p>same as Basic Calculator </p>"},{"location":"3_Core/Compiler_Contruction/Practicals/06_Yacc_Directives/#06_2y","title":"<code>06_2.y</code>","text":"<pre><code>%{\n  int yylex(void);\n  void yyerror(char *);\n\n  #include &lt;stdio.h&gt;\n  #include &lt;stdlib.h&gt;\n  %}\n\n%token INTEGER PLUS MINUS NL\n%left PLUS MINUS                                            /* 1 */\n%%\nprogram:\nprogram expr NL {printf(\"%d\\n\", $2);}   /* 2 */\n| \n;\nexpr:\nINTEGER {$$=$1;}\n| expr PLUS expr    {$$=$1+$3;}\n| expr MINUS expr   {$$=$1-$3;}\n;\n%%\nvoid yyerror(char *s)\n{\n  printf(\"%s\\n\", s);\n}\nint main()\n{\n  yyparse();\n}\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/Practicals/06_Yacc_Directives/#output_1","title":"Output","text":"<pre><code>1 - 5 + 2\n-2\n1 - 5 + 2\n-2\n1 - 5 + 2\n-2\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/Practicals/06_Yacc_Directives/#question-3","title":"Question 3","text":"<p>Extend the calculator to incorporate some new functionality. New features include arithmetic operators * and / that can multiply and divide integers respectively. Parentheses may be used to over-ride operator precedence. Note * and / operators have higher precedence over + and \u2013 operators. Also note that * and / are left associative. Ensure this using directive in YACC. </p>"},{"location":"3_Core/Compiler_Contruction/Practicals/06_Yacc_Directives/#06_3l","title":"<code>06_3.l</code>","text":"<pre><code>%{\n#include \"y.tab.h\"\nextern int yylval;\n%}\n%%\n[0-9]+ {\n    yylval = atoi(yytext);\n    return INTEGER;\n}\n\"+\" return PLUS;\n\"-\" return MINUS;\n\"*\" return MUL;     /* 1 */\n\"/\" return DIV;     /* 2 */\n\"^\" return POW;     /* 3 */\n\"\\n\" return NL;     /* 4 */\n[ \\t] ; \n. printf(\"Invalid\");\n%%\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/Practicals/06_Yacc_Directives/#06_3y","title":"<code>06_3.y</code>","text":"<pre><code>%{\n  int yylex(void);\n  void yyerror(char *);\n\n  #include &lt;stdio.h&gt;\n  #include &lt;stdlib.h&gt;\n  #include &lt;math.h&gt;\n%}\n\n%token INTEGER PLUS MINUS NL\n%left PLUS MINUS\n%left MUL DIV\n%right POW                                              /* 1 */\n%%\nprogram:\nprogram expr NL {printf(\"%d\\n\", $2);}\n| \n;\nexpr:\nINTEGER {$$=$1;}\n| expr PLUS expr    {$$=$1+$3;}\n| expr MINUS expr   {$$=$1-$3;}\n| expr MUL expr {$$=$1*$3;}             /* 2 */\n| expr DIV expr {$$=$1/$3;}             /* 3 */\n| expr POW expr {$$=pow($1, $3);}   /* 4 */\n;\n%%\nvoid yyerror(char *s)\n{\n  printf(\"%s\\n\", s);\n}\nint main()\n{\n  yyparse();\n}\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/Practicals/06_Yacc_Directives/#output_2","title":"Output","text":"<pre><code>2 - 2 ^ 2\n-3\n2 - 3 * 3 ^ 6\n-2185\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/Practicals/06_Yacc_Directives/#question-4","title":"Question 4","text":"<p>Modify the calculator application so that it works for floating point values also.</p>"},{"location":"3_Core/Compiler_Contruction/Practicals/06_Yacc_Directives/#question-5","title":"Question 5","text":"<p>Modify the grammar to allow single-character variables to be specified in assignment statements. The following illustrates sample input and calculator output:</p> <pre><code>user: 3 * (4 + 5)\ncalc: 27\nuser: x = 3 * (4 + 5)\nuser: y = 5\nuser: x\ncalc: 27\nuser: y\ncalc: 5\nuser: x + 2*y\ncalc: 37\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/Practicals/06_Yacc_Directives/#06_5l","title":"<code>06_5.l</code>","text":"<pre><code>%{\n  #include \"y.tab.h\"\n  extern int yylval;\n%}\n%%\n\n[a-z] {\n    yylval = *yytext - 'a';\n    return VARIABLE;\n}\n\n[0-9]+ {\n    yylval = atoi(yytext);\n    return INTEGER;\n}\n\n[-+()=*/\\n] { return *yytext; }\n[ \\t] ;\n\n. yyerror(\"invalid character\");\n%%\n\nint yywrap(void) {\n return 1;\n} \n</code></pre>"},{"location":"3_Core/Compiler_Contruction/Practicals/06_Yacc_Directives/#06_5y","title":"<code>06_5.y</code>","text":"<pre><code>%{\n    #include&lt;stdio.h&gt;\n    int flag=0;\n    int yylex(void);\n    int sym[26];\n%}\n%token INTEGER VARIABLE\n%left '+' '-'\n%left '*' '/'\n%%\n\nprogram:\n    program statement '\\n'\n    |\n    ;\nstatement:\n    expr { printf(\"%d\\n\", $1); }\n    | VARIABLE '=' expr { sym[$1] = $3; }\n    ;\nexpr:\n    INTEGER\n    | VARIABLE { $$ = sym[$1]; }\n    | expr '+' expr { $$ = $1 + $3; }\n    | expr '-' expr { $$ = $1 - $3; }\n    | expr '*' expr { $$ = $1 * $3; }\n    | expr '/' expr { $$ = $1 / $3; }\n    | '(' expr ')' { $$ = $2; }\n    ;\n%%\nvoid main()\n{\nprintf(\"\\nEnter Any Arithmetic Expression which can have operations Addition, Subtraction, Multiplication, Division, Modulus and Round brackets:\\n\");\n\nyyparse();\nif(flag==0)\nprintf(\"\\nEntered arithmetic expression is Valid\\n\\n\");\n}\n\nvoid yyerror()\n{\nprintf(\"\\nEntered arithmetic expression is Invalid\\n\\n\");\nflag=1;\n}\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/Practicals/08_09_10_Yacc_Mini_Compiler/","title":"08 09 10 Yacc Mini Compiler","text":"<p>Write the LEX and YACC source to recognize the following:</p>"},{"location":"3_Core/Compiler_Contruction/Practicals/08_09_10_Yacc_Mini_Compiler/#the-template-for-the-c-program-is","title":"The template for the C program is","text":"<pre><code>#include&lt;stdio.h&gt;\nint main( )\n{\n} \nPGM -&gt; HEADER INT MAIN LB RB LCB RCB\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/Practicals/08_09_10_Yacc_Mini_Compiler/#declaration-statements","title":"Declaration statements:","text":"<p>Allow declaration statements inside the program body. Integer variables separated by comma can be declared inside the program body. A program can have multiple declaration statements. Variables are sequence of lower-case alphabets.Each declaration statement is ended by a semicolon. int a, b;</p> <pre><code>PGM -&gt; HEADER INTMAIN LB RB LCB BODY RCB\nBODY -&gt; DECL_STMTS\nDECL_STMTS -&gt; DECL_STMT DECL_STMTS | DECL_STMT\nDECL_STMT -&gt;INT VAR_LIST SC\nVAR_LIST-&gt;VAR COMA VAR_LIST | VAR\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/Practicals/08_09_10_Yacc_Mini_Compiler/#operators-program-statements","title":"Operators &amp; Program Statements","text":"<p>Allow declaration statements to be followed by program statements inside the program body. Program statements are ended by a semicolon. Program statements can be arithmetic expressions involving +-*/ operators.</p> <pre><code>PGM -&gt; HEADER INT MAIN LB RB LCB BODY RCB\nBODY -&gt; DECL_STMTS PROG_STMTS\nDECL_STMTS -&gt; DECL_STMT DECL_STMTS | DECL_STMT\nPROG_STMTS -&gt; PROG_STMT PROG_STMTS | PROG_STMT  \nDECL_STMT -&gt; INT VAR_LIST SC\nVAR_LIST -&gt; VAR COMA VAR_LIST | VAR\nPROG_STMT -&gt; VAR EQ A_EXPN SC\nA_EXPN -&gt; A_EXPN OP A_EXPN | LB A_EXPN RB | VAR\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/Practicals/08_09_10_Yacc_Mini_Compiler/#modify-your-lex-program-to-incorporate-the-following-changes","title":"Modify your LEX program to incorporate the following changes","text":"<p>As per the current set up, the programmer is supposed to use only lower-case alphabets in variable names in their C program. Modify your lex program so as to let the programmer have uppercase letters A to Z together with digits 0 to 9 and underscore character in variable names. Ensure that a variable name always begin with a character.</p> <p>Terminate your program with an error message if in case the programmer uses keywords if, while, do, and for as variable names. Note that it is permitted to have variable names beginning with keywords (ifvar, thenextcount, donut etc.) (hint: rely on conflict resolution rules in LEX).</p> <p>Add provision to declare variables of type float, double and char.</p>"},{"location":"3_Core/Compiler_Contruction/Practicals/08_09_10_Yacc_Mini_Compiler/#adding-operators-to-the-language","title":"Adding operators to the language","text":"<ul> <li>Incorporate arithmetic expressions involving binary operators +, -, *, /, ^ (exponent) into your compiler. Note that the exponent operator is a right associative operator and has higher precedence than other arithmetic operators (+, -, *, /).</li> <li>Incorporate unary pre/post increment ++ and pre/post decrement -- operators too (are of highest precedence and left associative).</li> <li>Incorporate the modulo operator (%). It has the same precedence as * and / operators and is left associative. </li> <li>Include numeric integer constants as expressions.</li> <li>Also include parenthesized expressions</li> <li>Variables can be of int/float/char/double type</li> <li>In the given implementation, the input C file is expected to have all declaration statements in the beginning, followed by program statements. Rewrite your grammar to let the user to have declarative and program statements in any arbitrary interleaved order in their input C program.</li> </ul>"},{"location":"3_Core/Compiler_Contruction/Practicals/08_09_10_Yacc_Mini_Compiler/#combined-program","title":"Combined Program","text":""},{"location":"3_Core/Compiler_Contruction/Practicals/08_09_10_Yacc_Mini_Compiler/#filel","title":"<code>file.l</code>","text":"<pre><code>%{\n#include \"y.tab.h\"\nextern int yylval;\n%}\n%%\n\"#include&lt;stdio.h&gt;\"    { return T_HEADER; }\n\"int\"    { return T_INT; }\n\"float\"  { return T_FLOAT; }\n\"double\" { return T_DOUBLE; }\n\"char\"   { return T_CHAR; }\n\"main\"   { return T_MAIN; }\n\"do\"                        { printf(\"ERROR! Reserved keyword do\\n\"); return -1;}\n\"if\"                        { printf(\"ERROR! Reserved keyword if\\n\"); return -1;}\n\"while\"                 { printf(\"ERROR! Reserved keyword while\\n\"); return -1;}\n\"for\"                       { printf(\"ERROR! Reserved keyword for\\n\"); return -1;}\n\\{       { return T_LCB; }\n\\}       { return T_RCB; }\n\\(       { return T_LB; }\n\\)       { return T_RB; }\n\\n       { yylineno++; }\n[ \\t]    ;\n[0-9]+   { return T_NUMBER; }\n[-+*/]   { return T_OP; }\n=        { return T_EQ; }\n[a-zA-Z][a-zA-Z0-9_]*       { return T_VARIABLE; }\n\",\"        { return T_COMMA; }\n\";\"        { return T_SC; }\n.      { return yytext[0]; }\n%%\nint yywrap()\n{\nreturn 1;\n}\n</code></pre>"},{"location":"3_Core/Compiler_Contruction/Practicals/08_09_10_Yacc_Mini_Compiler/#filey","title":"<code>file.y</code>","text":"<pre><code>%{\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\nint yylex(void);\nvoid yyerror(char *);\n%}\n%token T_HEADER T_INT T_CHAR T_FLOAT T_DOUBLE T_MAIN T_LB T_RB T_LCB T_RCB\n%token T_NUMBER T_VARIABLE T_COMMA T_SC T_OP T_EQ\n%left T_OP\n%%\nprogram: HEADER INT MAIN LB RB LCB BODY RCB\n{\n    printf(\"Valid\\n\");\n}\n;\nBODY: DECL_STMTS PROG_STMTS\n{\n    printf(\"Token: Body\\n\");\n}\n;\nHEADER: T_HEADER\n{\n    printf(\"Token: Header\\n\");\n}\n;\nINT: T_INT\n{\n    printf(\"Token: Int\\n\");\n};\nMAIN: T_MAIN\n{\n    printf(\"Token: Main\\n\");\n}\n;\nLB: T_LB\n{\n    printf(\"Token: Left_Bracket\\n\");\n}\n;\nRB: T_RB\n{\n    printf(\"Token: Right_Bracket\\n\");\n}\n;\nLCB: T_LCB\n{\n    printf(\"Token: Left_Curly_Bracket\\n\");\n}\n;\nRCB: T_RCB\n{\nprintf(\"Token: Right_Curly_Bracket\\n\");\n}\n;\nDECL_STMTS: DECL_STMT DECL_STMTS\n                    |   DECL_STMT\n{\n    printf(\"Token: Statement\\n\");\n}\n;\n\nPROG_STMTS: PROG_STMT PROG_STMTS\n                    | PROG_STMT\n;\nDECL_STMT:  T_INT VAR_LIST T_SC\n                    | T_FLOAT VAR_LIST T_SC\n                    | T_DOUBLE VAR_LIST T_SC\n                    | T_CHAR VAR_LIST T_SC\n                    ;\n\nVAR_LIST: T_VARIABLE T_COMMA VAR_LIST\n                    |   T_VARIABLE\n                    ;\n\nPROG_STMT: T_VARIABLE T_EQ A_EXPN T_SC\n                     ;\n\nA_EXPN: A_EXPN T_OP A_EXPN\n            | T_LB A_EXPN T_RB\n            | T_VARIABLE\n            | T_NUMBER\n            ;\n%%\nvoid main() \n{\n    printf(\"Enter a C program:\\n\");\n    yyparse();\n}\n\nvoid yyerror(char *s) \n{\n    printf(\"The provided code is invalid\\n\", s);\n    exit(1);\n}\n</code></pre>"},{"location":"3_Core/Computer_Architecture/","title":"Computer Architecture","text":"<p>Taught by Dr. Tamizharasan Periyasami</p> <p>This course introduces computer architecture, focusing on design aspects and current trends, alongside essential system resources like memory technology and I/O subsystems.</p> <p>Key learning objectives include:</p> <ul> <li>Understanding computer system architecture and its components, including performance assessment.</li> <li>Learning the MIPS architecture and instruction set.</li> <li>Exploring number representations and computer arithmetic.</li> <li>Designing data paths and control units in MIPS architecture.</li> <li>Understanding pipelining, data hazards, and branch prediction techniques.</li> <li>Investigating memory hierarchy, cache memory, and virtual memory.</li> <li>Analyzing I/O interfacing and modern processors for high-performance computing.</li> </ul> <p>Overall, the course provides a comprehensive foundation in computer architecture principles and applications.</p>"},{"location":"3_Core/Computer_Architecture/01_Intro/","title":"01 Intro","text":""},{"location":"3_Core/Computer_Architecture/01_Intro/#overall-view-of-computer-engineering","title":"Overall View of Computer Engineering","text":"<pre><code>flowchart TB\napp --&gt;\nalgo[Algorithm] --&gt;\npl[Programming Language] --&gt;\nosvm[Operating System/&lt;br /&gt;Virtual Machine] --&gt;\nisa[\"ISA&lt;br /&gt;(Instruction Set Architeture)\"] --&gt;\nma[Microarchitecture] --&gt;\nrtl[Register-Transfer Level] --&gt;\ng[Gates] --&gt;\nc[Circuits] --&gt;\nd[Devices] --&gt;\np[Physics]\nsubgraph app[Application]\n    direction TB\n    sd[Software Development]\n    AI\n    ml[Machine Learning]\nend\n\nsubgraph Computer Science\n    algo\n    pl\nend\n\nsubgraph Computer Architecture\n    isa\n    ma\n    rtl\nend\n\nsubgraph Digital Design\n    g\n    c\n    d\n    p\nend</code></pre>"},{"location":"3_Core/Computer_Architecture/01_Intro/#mips","title":"MIPS","text":"<p>Microprocessors without Interlocked Pipelined Stages</p> Architecture Organization Describes ___ computer does what how Role Interface b/w hardware &amp; software Way comuper components are connected in a system Programmer\u2019s View InstructionsAddressing ModesRegisters Realization of architecture(Circuit Design, signals, peripherals)"},{"location":"3_Core/Computer_Architecture/01_Intro/#cisc-vs-risc","title":"CISC vs RISC","text":"CISC RISC Full Form Complex Instruction Set Computing Reduced Instruction Set Computing Gives importance to Hardware Software Can access memory directly? \u2705 \u274c(requires registers) Coding Instructions Simple Complex Machine Instructions Complex Simple Clock cyles for executing an instruction Multiple \\(1\\) Complexity lies in Microprogram Compiler"},{"location":"3_Core/Computer_Architecture/01_Intro/#microprogram","title":"Microprogram","text":"<p>It is a microinstruction program that controls the functions of a central processing unit or peripheral controller of a computer</p> <p>Microcode is low-level code that defines how a microprocessor should function when it executes machine-language instructions. </p> <p>Typically, one machine-language instruction translates into several microcode instructions</p>"},{"location":"3_Core/Computer_Architecture/01_Intro/#class-of-computers","title":"Class of Computers","text":"Computer Class Purpose Characteristic Size Personal General Cost/Performance Tradeoff Small Server Network Based High CapacityHigh PerformanceHigh Reliability Small-Building Size Super Scientific calculations(weather forecasting, oil exploration) Highest capacity Embedded Embedded within systems(Digital TVs, cameras)Specific application Stringent power/performance/cost constraints Small Datacenters Storage and retrieval of data High performance"},{"location":"3_Core/Computer_Architecture/01_Intro/#levels-of-computing-system","title":"Levels of Computing System","text":"Level Application Software Written in High Level Language System Software CompilerOS Hardware ProcessorMemoryI/O Controllers"},{"location":"3_Core/Computer_Architecture/01_Intro/#levels-of-program-code","title":"Levels of Program Code","text":"<ul> <li>Machine Language</li> <li>Assembly</li> <li>High-Level</li> </ul>"},{"location":"3_Core/Computer_Architecture/01_Intro/#components-of-computer","title":"Components of Computer","text":"<p> | Component |                                                              | | --------- | ------------------------------------------------------------ | | Input     | Write data to memory(from user)                        | | Output    | Read data from memory(to user)                         | | Registers |                                                              | | Cache     | Small fast SRAM                                              | | Datapath  | Performs data operations                                     | | Control   | sends signals that determine operations of datapath, memory, I/O |</p>"},{"location":"3_Core/Computer_Architecture/01_Intro/#isa","title":"ISA","text":"<p>Instruction Set Architecture</p> <p>Interface between hardware and lowest level of software</p> <p>Includes information necessary (instructions, registers, memory access, I/O, and so on) to write a machine language program that will run correctly</p>"},{"location":"3_Core/Computer_Architecture/01_Intro/#abi","title":"ABI","text":"<p>Application Binary Interface</p> <p>Combination of the basic instruction set and the operating system interface</p>"},{"location":"3_Core/Computer_Architecture/02_Performance/","title":"02 Performance","text":""},{"location":"3_Core/Computer_Architecture/02_Performance/#performance-measures","title":"Performance Measures","text":"Measure Definition Goal Concerns Execution Time Time between start and completion of a task \\(\\downarrow\\) individual users Throughput/Bandwidth Total work done per unit time \\(\\uparrow\\) Datacenter managers"},{"location":"3_Core/Computer_Architecture/02_Performance/#performance-and-execution-time","title":"Performance and Execution Time","text":"\\[ \\text{Performance} = \\frac{1}{\\text{Execution Time}} \\] \\[ \\begin{aligned} P_x &amp;\\textcolor{orange}{&gt;} P_y \\\\ \\implies T_x &amp; \\textcolor{orange} &lt; T_y \\quad \\left( \\frac{1}{T_x} &gt; \\frac{1}{T_y} \\right) \\end{aligned} \\] \\[ \\begin{aligned} &amp; X \\text{ is } n \\text{ times as fast as } Y \\\\ \\implies &amp; n  = \\frac{P_{\\textcolor{orange}{x}}}{P_{\\textcolor{hotpink}{y}}} = \\frac{T_{\\textcolor{hotpink}{y}}}{T_{\\textcolor{orange}{x}}} \\end{aligned} \\]"},{"location":"3_Core/Computer_Architecture/02_Performance/#execution-time","title":"Execution Time","text":""},{"location":"3_Core/Computer_Architecture/02_Performance/#components","title":"Components","text":""},{"location":"3_Core/Computer_Architecture/02_Performance/#responseelapsed-time","title":"Response/Elapsed Time","text":"<p>Time to complete a task</p> <ul> <li>Processing</li> <li>I/O activity</li> <li>Memory access</li> <li>OS overhead</li> </ul>"},{"location":"3_Core/Computer_Architecture/02_Performance/#cpu-time","title":"CPU Time","text":"<ul> <li>Time spent by processor to execute a job   Discounts I/O time, other jobs\u2019 shares</li> <li>User CPU time/CPU performance   CPU time spent in user program</li> <li>System CPU Time   CPU time spent in operating system performing tasks on behalf of the program</li> </ul>"},{"location":"3_Core/Computer_Architecture/02_Performance/#formula","title":"Formula","text":"\\[ \\text{Clock Cycle Time} = \\frac{1}{\\text{Clock Frequency Rate}} \\] \\[ \\begin{aligned} &amp; \\text{No of clock cycles} \\\\ &amp;= \\text{No of instructions} \\times \\text{CPI} \\\\ &amp; \\qquad \\qquad \\text{CPI}\\to \\text{(Cycles per Instruction)} \\\\ &amp;= \\sum_{i=1} \\text{(No of instructions)}_i \\times \\text{(CPI)}_i \\\\ &amp; \\qquad\\qquad (\\exists \\text{ different classes of instructions)} \\end{aligned} \\] \\[ \\begin{aligned} &amp;\\text{CPU Execution time for a program} \\\\ =&amp; \\text{No of clock cycles} \\times \\text{Clock cycle time} \\end{aligned} \\] <p>Be careful when calculating avg CPI for a code sequence (simple, but avoid careless mistake)</p> <p>We can improve performance by</p> <ul> <li>Reducing number of clock cycles</li> <li>Increasing clock rate   However, this is not ideal, as this will increase<ul> <li>Power consumption</li> <li>Heat produced</li> </ul> </li> </ul>"},{"location":"3_Core/Computer_Architecture/02_Performance/#cpu-clocking","title":"CPU Clocking","text":"<p>Constant-rate clock that governs operation of digital hardware</p>"},{"location":"3_Core/Computer_Architecture/02_Performance/#performance-factors","title":"Performance Factors","text":"Factor Determines AffectsInstruction Count Affects CPI<sub>avg</sub> Affects T<sub>C</sub> Algorithm No of source-level statementsI/O operations executed \u2705 \u2705 \u274c Programming Language No of machine instructions executed per source-level statement \u2705 \u2705 \u274c Compiler No of machine instructions executed per source-level statement \u2705 \u2705 \u274c ISA No of machine instructions executed per source-level statement \u2705 \u2705 \u2705 ProcessorMemory Speed of instruction execution I/O SystemOS Speed of I/O operations execution"},{"location":"3_Core/Computer_Architecture/02_Performance/#speedup","title":"Speedup","text":"\\[ \\begin{aligned} S &amp;= \\frac{P_\\text{new}}{P_\\text{old}} \\\\ &amp;= \\frac{T_\\text{old}}{T_\\text{new}} \\end{aligned} \\]"},{"location":"3_Core/Computer_Architecture/02_Performance/#amdahls-law","title":"Amdahl's Law","text":"<p>\u2026 gives the theoretical speedup in latency of the execution of a task at fixed workload that can be expected of a system whose resources are improved.</p> <p>If</p> <ul> <li>\\(s\\) is the fraction that is sequential</li> <li>\\(p\\) is the fraction that can be parallelized</li> <li>\\(n\\) is the new parallel processing ability</li> </ul> \\[ \\begin{aligned} n &amp;= \\text{factor of increase in number of processors} \\\\ &amp; \\qquad \\times \\text{factor of increase in processor performance} \\end{aligned} \\] <p>\\((s = 1-p; \\quad p = 1-s)\\)</p> <p>Then, the maximum speed up \\(S\\) is</p> \\[ \\begin{aligned} S &amp;=  \\frac{\\text{Speed with parallelization}}{\\text{Speed without parallelization}} \\\\ &amp;= \\frac{1}{ s + \\frac{p}{n} } \\end{aligned} \\]"},{"location":"3_Core/Computer_Architecture/02_Performance/#design-principles","title":"Design Principles","text":"<ul> <li>Simplicity is favored over regularity</li> <li>Smaller is faster</li> <li>Something</li> <li>Something</li> </ul>"},{"location":"3_Core/Computer_Architecture/02_Performance/#isa","title":"ISA","text":"<p>Instruction Set Architecture</p> <p>The sheet will be given</p> <p>Note</p> <ul> <li>All core instructions have 3 operands</li> <li>All pseudo-instructions have 2 operands</li> </ul>"},{"location":"3_Core/Computer_Architecture/02_Performance/#branching","title":"Branching","text":"<p>Doing the branching opposite of the pseudocode helps eliminate a jump instruction.</p> <p>It may seem small, but if you use a loop, the savings will add up to a lot of cycles.</p>"},{"location":"3_Core/Computer_Architecture/02_Performance/#idk","title":"IDK","text":"P Multiplicand Mr Step 0000000 00001100 1100 00000110 110 Right-Shift multiplierLeft-Shift multiplier Right-Shift multiplierLeft-Shift multiplier"},{"location":"3_Core/Computer_Architecture/03_ISA/","title":"03 ISA","text":"<ul> <li>MIPS_Instruction_Set.pdf</li> <li>MIPS_Instruction_Format.pdf </li> </ul>"},{"location":"3_Core/Computer_Architecture/03_ISA/#starter-program","title":"Starter Program","text":"<pre><code>.data\na: .byte 10\nb: .word 10\n\narray .word 10, 20\n\nchar .ascii 'H'\nstring .asciiz \"Hello world\"\n\n.text\nmain:\n    ## your code\nend main\n</code></pre>"},{"location":"3_Core/Computer_Architecture/03_ISA/#design-principles","title":"Design Principles","text":""},{"location":"3_Core/Computer_Architecture/03_ISA/#simplicity-favours-regularity","title":"Simplicity favours regularity","text":"<ul> <li>keeping all instructions a single size</li> <li>always requiring three register operands in arithmetic instructions</li> <li>keeping the register fields in the same place in each instruction format</li> </ul>"},{"location":"3_Core/Computer_Architecture/03_ISA/#advantages","title":"Advantages","text":"<ul> <li>Regularity makes implementation simpler</li> <li>Simplicity enables higher performance at lower cost</li> </ul>"},{"location":"3_Core/Computer_Architecture/03_ISA/#smaller-is-faster","title":"Smaller is Faster","text":""},{"location":"3_Core/Computer_Architecture/03_ISA/#make-the-common-case-fast","title":"Make the common case fast","text":"<p>Small constants are common</p> <p>Immediate operand avoids a load instruction</p>"},{"location":"3_Core/Computer_Architecture/03_ISA/#operands","title":"Operands","text":""},{"location":"3_Core/Computer_Architecture/03_ISA/#register","title":"Register","text":"<ul> <li>MIPS has a \\(32 \\times 32\\)-bit register file</li> <li>Used for frequently accessed data</li> <li>Numbered 0 to 31</li> </ul> <p>32-bit data is called a \u2018word\u2019</p>"},{"location":"3_Core/Computer_Architecture/03_ISA/#assembler-names","title":"Assembler Names","text":"<ul> <li><code>$t0, $t1, \u2026, $t9</code> for temporary values</li> <li><code>$s0, $s1, \u2026, $s7</code> for saved variables</li> </ul>"},{"location":"3_Core/Computer_Architecture/03_ISA/#preserved-on-call","title":"Preserved on Call","text":"<ul> <li><code>$s0-$s7</code></li> <li><code>$sp</code></li> <li><code>$fp</code></li> <li><code>$ra</code></li> </ul>"},{"location":"3_Core/Computer_Architecture/03_ISA/#memory","title":"Memory","text":"<p>Main memory used for composite data (arrays, structures, dynamic data)</p> <p>To apply arithmetic operations</p> <ul> <li>Load values from memory into registers</li> <li>Store result from register to memory</li> </ul> <p>Memory is byte-addressed</p> <ul> <li>Each address identifies 8-bits</li> <li>Words are aligned in memory<ul> <li>Address must be a multiple of 4</li> </ul> </li> <li>MIPS is Big/Little Endian<ul> <li>In our course, we are taking little endian</li> </ul> </li> </ul>"},{"location":"3_Core/Computer_Architecture/03_ISA/#loadstore","title":"Load/Store","text":"<code>lw rt, offset(rs)</code><code>lh rt, offset(rs)</code><code>lb rt, offset(rs)</code> Load wordLoad half-wordLoad byte <code>sw rt, offset(rs)</code><code>sh rt, offset(rs)</code><code>sb rt, offset(rs)</code> Store wordStore half-wordStore byte <code>li $t0, immediate_value</code> load immediate <code>la $t0, variable_name</code> load address <pre><code>a[12] = h + a[8];\n</code></pre> <p>\\(8 \\text{ word locations} \\implies 8 \\times 4\\)</p> <ul> <li>each word occupies 4 memory locations</li> </ul> <pre><code>la $s0, a\nlw $t0, 32($s0)\nadd $s2, $s1, $t0\nsw $s2, 48($s0)\n</code></pre>"},{"location":"3_Core/Computer_Architecture/03_ISA/#immediate","title":"Immediate","text":"<pre><code>addi $t0, $t0, 1\naddi $t0, $t0, -1\n</code></pre>"},{"location":"3_Core/Computer_Architecture/03_ISA/#zero","title":"Zero","text":"<pre><code>## Initialize variable as 0\nadd $t0, $zero, $zero\n\n## Copy value\nadd $t1, $t0, $zero\n</code></pre>"},{"location":"3_Core/Computer_Architecture/03_ISA/#registers-vs-memory","title":"Registers vs Memory","text":"<p>Registers are faster to access than memory</p> <p>Operating on memory data requires loads and stores \\(\\to\\) More instructions to be executed</p> <p>Compiler must use registers for variables as much as possible. Only spill to memory for less frequently-used variables</p>"},{"location":"3_Core/Computer_Architecture/03_ISA/#instructions","title":"Instructions","text":"<p>Encoded as 32-bit instruction words</p> <p>Register numbers</p> <ul> <li><code>$t0 \u2013 $t7</code> are reg\u2019s 8 \u2013 15</li> <li><code>$t8 \u2013 $t9</code> are reg\u2019s 24 \u2013 25</li> <li><code>$s0 \u2013 $s7</code> are reg\u2019s 16 \u2013 23</li> </ul>"},{"location":"3_Core/Computer_Architecture/03_ISA/#r-format","title":"R-Format","text":"op operation code (opcode) rs first source register number rt second source register number rd destination register number shamt shift amount (00000 for now) funct function code (extends opcode)"},{"location":"3_Core/Computer_Architecture/03_ISA/#number-formats","title":"Number Formats","text":"<ul> <li>Unsigned</li> <li>2\u2019s complement</li> </ul>"},{"location":"3_Core/Computer_Architecture/03_ISA/#sign-extension","title":"Sign Extension","text":"<p>Representing a number using more bits, while preserving numeric value</p> <ul> <li><code>addi</code>: extend immediate value</li> <li><code>lb</code>, <code>lh</code>: extend loaded byte/halfword</li> <li><code>beq</code>, <code>bne</code>: extend the displacement</li> </ul>"},{"location":"3_Core/Computer_Architecture/03_ISA/#bitwise-operations","title":"Bitwise Operations","text":"<p>Useful for inserting/extracting groups of bits from a word</p> Operation Shift Left Logical <code>sll</code> <code>sll</code> by \\(i\\) bits multiplies by \\(2^i\\) <code>sll $t0, $t0, 2</code> Shift Right Logical <code>srl</code> <code>srl</code> by \\(i\\) bits divides by \\(2^i\\)(unsigned only) <code>srl $t0, $t0, 2</code> Bitwise and <code>and</code><code>andi</code> Useful to mask bits in a wordSelect some bits, clear others to 0 <code>and $t0, $t1, $t2</code><code>andi $t0, $t1, 0</code> Bitwise or <code>or</code><code>ori</code> Useful to include bits in a wordSet some bits to 1, leave others unchanged <code>or $t0, $t1, $t2</code><code>ori $t0, $t1, 1</code> Bitwise not <code>nor</code> Useful to invert bits in a wordChange 0 to 1, and 1 to 0 <code>nor $t0, $t1, $zero</code>"},{"location":"3_Core/Computer_Architecture/03_ISA/#jump-statements","title":"Jump Statements","text":"<pre><code>## unconditional\nj exit\n\n## Conditional\nbeq $t0, $t1, exit\nbne $t0, $t1, exit\n\nbgt $t0, $t1, exit\nbge $t0, $t1, exit\n\nblt $t0, $t1, exit\nble $t0, $t1, exit\n</code></pre>"},{"location":"3_Core/Computer_Architecture/03_ISA/#alternate-approach","title":"Alternate Approach","text":"<p>Using</p> <ul> <li>Set less than</li> <li>Set greater than</li> </ul> <p>Faster than <code>bgt</code>, <code>bge</code>, <code>blt</code>, <code>ble</code></p> <ul> <li><code>bgt</code>, <code>bge</code>, <code>blt</code>, <code>ble</code> require combining logical operation with branch involves more work per instruction, requiring a slower clock</li> </ul> <pre><code>## signed\nslt $t0, $s1, $s2 ## if ($s1 &lt; $s2)\nsgt $t0, $s1, $s2 ## if ($s1 &gt; $s2)\n\n## signed immediate\nslti $t0, $s1, 9 ## if ($s1 &lt; 9)\nsgti $t0, $s1, 9 ## if ($s1 &lt; 9)\n\n## unsigned\nsltu $t0, $s1, $s2 ## if ($s1 &lt; $s2)\nsgtu $t0, $s1, $s2 ## if ($s1 &lt; $s2)\n\n## unsigned immediate\nsltui $t0, $s1, 9 ## if ($s1 &lt; 9)\nsgtui $t0, $s1, 9 ## if ($s1 &lt; 9)\n## use with\nbeq $t0, $zero, exit\nbne $t0, $zero, exit\n</code></pre>"},{"location":"3_Core/Computer_Architecture/03_ISA/#loop","title":"Loop","text":"<p>Tip: Doing the opposite conditional statement saves a clock cycle every time the loop runs</p>"},{"location":"3_Core/Computer_Architecture/03_ISA/#for-while","title":"<code>for</code>, <code>while</code>","text":"<pre><code>## initialization\n\nloop:\n    bne $s0, $s1, exit ## condition\n\n    ## statements\n\n    addi $t0, $t1 ## updation\n    j loop\n\nexit:\n</code></pre>"},{"location":"3_Core/Computer_Architecture/03_ISA/#dowhile","title":"<code>do...while</code>","text":"<pre><code>## initialization\n\nloop:\n    ## statements\n\n    addi $t0, $t1 ## updation\n\n    bne $s0, $s1, exit ## condition\n\n    j loop\n\nexit:\n</code></pre>"},{"location":"3_Core/Computer_Architecture/03_ISA/#registers","title":"Registers","text":"\\(a_0 \\iff a_3\\) Argument \\(s_0 \\iff s_7\\) Saved Need to be pushed/popped to/from stack \\(t_0 \\iff t_9\\) Temporary \\(v_0, v_1\\) Return variables"},{"location":"3_Core/Computer_Architecture/03_ISA/#procedure-calling","title":"Procedure Calling","text":""},{"location":"3_Core/Computer_Architecture/03_ISA/#steps","title":"Steps","text":"<ol> <li>Place parameters in registers</li> <li>Transfer control to procedure</li> <li>Acquire storage for procedure</li> <li>Perform procedure\u2019s operations</li> <li>Place result in register for caller</li> <li>Return to place of call</li> </ol>"},{"location":"3_Core/Computer_Architecture/03_ISA/#registers_1","title":"Registers","text":"<ul> <li><code>$a0 \u2013 $a3</code>: arguments (reg\u2019s 4 \u2013 7)</li> <li><code>$v0, $v1</code>: result values (reg\u2019s 2 and 3)</li> <li><code>$t0 \u2013 $t9</code>: temporaries<ul> <li>Can be overwritten by callee</li> </ul> </li> <li><code>$s0 \u2013 $s7</code>: saved<ul> <li>Must be saved/restored by callee</li> </ul> </li> <li><code>$gp</code>: global pointer for static data (reg 28)</li> <li><code>$sp</code>: stack pointer (reg 29)</li> <li><code>$fp</code>: frame pointer (reg 30)</li> <li><code>$ra</code>: return address (reg 31)</li> </ul>"},{"location":"3_Core/Computer_Architecture/03_ISA/#instructions_1","title":"Instructions","text":"<pre><code>jal function_name\n</code></pre> <p>Procedure call: jump and link</p> <ul> <li>Address of following instruction put in <code>$ra</code></li> <li>Jumps to target address</li> </ul> <pre><code>jr $ra\n</code></pre> <p>Procedure return: jump register</p> <ul> <li>Copies $ra to program counter</li> <li>Can also be used for computed jumps<ul> <li>e.g., for case/switch statements</li> </ul> </li> </ul>"},{"location":"3_Core/Computer_Architecture/03_ISA/#example","title":"Example","text":"<ul> <li>Arguments \\(g, \\dots, j\\) in <code>$a0</code> \\(,\\dots,\\) <code>$a3</code></li> <li>\\(f\\) in <code>$s0</code> (hence, need to save <code>$s0</code> on stack)</li> <li>Result in <code>$v0</code></li> </ul> <pre><code>int leaf(int g, int h, int i, int j)\n{\n  int f = (g + h) - (i + j);\n    return f;\n}\n\nvoid main()\n{\n  leaf();\n}\n</code></pre> <p>For solving this question, we don\u2019t need to use stack. However, we are using because it is asked to do so in the question</p> <pre><code>.data\n\n.text\nmain:\n    addi $a0, $zero, 10\n    addi $a1, $zero, 20\n    addi $a2, $zero, 30\n    addi $a3, $zero, 40\n\n    jal leaf\nend main\n\nleaf:\n  ## save $s0 on stack\n\n  addi $sp, $sp, -4\n  sw $s0, 0($sp)\n\n  ## procedure body\n  add $t0, $a0, $a1\n  add $t1, $a2, $a3\n  sub $s0, $t0, $t1\n\n  ## result\n  add $v0, $s0, $zero\n\n  ## restore $s0\n  lw $s0, 0($sp)\n  addi $sp, $sp, 4\n\n  ## return\n  jr $ra\n</code></pre>"},{"location":"3_Core/Computer_Architecture/03_ISA/#non-leaf-procedures","title":"Non-Leaf Procedures","text":"<p>Procedures that call other procedures</p> <p>For nested call, caller needs to save on the stack</p> <ul> <li>its return address</li> <li>any arguments and temporaries needed after the call</li> </ul> <p>Restore from the stack after the call</p> <pre><code>int fact (int n)\n{\n    if (n &lt; 1)\n    return (1);\n    else\n    return n * fact(n - 1);\n}\n</code></pre> <pre><code>fact:\n  addi $sp, $sp, -8 ## adjust stack for 2 items\n  sw $ra, 4($sp) ## save return address\n  sw $a0, 0($sp) ## save argument\n\n  slti $t0, $a0, 1 ## test for n &lt; 1\n  beq $t0, $zero, L1\n\n  addi $v0, $zero, 1 ## if so, result is 1\n  addi $sp, $sp, 8 ## pop 2 items from stack\n  jr $ra ## and return\n\nL1:\n    addi $a0, $a0, -1 ## else decrement n\n  jal fact ## recursive call\n\n  lw $a0, 0($sp) ## restore original n\n  lw $ra, 4($sp) ## and return address\n  addi $sp, $sp, 8 ## pop 2 items from stack\n\n  mul $v0, $a0, $v0 ## multiply to get result\n\n  jr $ra ## and return\n</code></pre>"},{"location":"3_Core/Computer_Architecture/03_ISA/#character-data","title":"Character Data","text":"Encoding Bits Characters ASCII 8 128 95 graphic, 33 control Latin-1 8 256 ASCII + 96 more graphic characters Unicode 32 Most of the world\u2019s alphabets, symbolsUsed in Java, C++ wide characters UTF-8 8 variable-length encodings UTF-16 16 variable-length encodings"},{"location":"3_Core/Computer_Architecture/03_ISA/#bytehalfword-operations","title":"Byte/Halfword Operations","text":"<pre><code>## Sign extend to 32 bits in rt\nlb rt, offset(rs)\nlh rt, offset(rs)\n\n## Zero extend to 32 bits in rt\nlbu rt, offset(rs)\nlhu rt, offset(rs)\n\n## Store just rightmost byte/halfword\nsb rt, offset(rs)\nsh rt, offset(rs)\n</code></pre>"},{"location":"3_Core/Computer_Architecture/03_ISA/#string-copy","title":"String Copy","text":"<pre><code>void strcpy (char x[], char y[])\n{\n  int i = 0;\n\n    while ( (x[i]=y[i])!='\\0' )\n        i += 1;\n}\n</code></pre> <pre><code>strcpy:\n  addi $sp, $sp, -4 ## adjust stack for 1 item\n  sw $s0, 0($sp) ## save $s0\n\n  add $s0, $zero, $zero ## i = 0\n\nloop:\n  add $t1, $s0, $a1 ## addr of y[i] in $t1\n  lbu $t2, 0($t1) ## $t2 = y[i]\n\n  add $t3, $s0, $a0 ## addr of x[i] in $t3\n  sb $t2, 0($t3) ## x[i] = y[i]\n\n    beq $t2, $zero, exit ## exit loop if y[i] == 0\n\n  addi $s0, $s0, 1 ## i = i + 1\n  j loop ## next iteration of loop\n\nexit:\n  lw $s0, 0($sp) ## restore saved $s0\n  addi $sp, $sp, 4 ## pop 1 item from stack\n\n  jr $ra ## and return\n</code></pre>"},{"location":"3_Core/Computer_Architecture/03_ISA/#32-bit-constants","title":"32-bit Constants","text":"<p>16-bit immediate is the default in instructions</p> <p>However, if we need to store 32-bit constant, use load upper immediate</p> <pre><code>lui $s0, upper_16_bits\nori $s0, $s0, lower_16_bits\n</code></pre>"},{"location":"3_Core/Computer_Architecture/03_ISA/#lui","title":"<code>lui</code>","text":"<ul> <li>Copies 16-bit constant to left 16 bits of <code>rt</code></li> <li>Clears right 16 bits of <code>rt</code> to 0</li> </ul> <p>Then we use <code>ori</code> to store the lower 16bits</p>"},{"location":"3_Core/Computer_Architecture/03_ISA/#branch-addressing","title":"Branch Addressing","text":"<p>Branch instructions specify</p> <ul> <li>Opcode</li> <li>two registers</li> <li>target address</li> </ul> <p>Most branch targets are near-branch (forward/backward)</p> <p></p> <p>PC-relative addressing</p> \\[ \\text{Target address} = \\text{PC}_\\text{new} + (\\text{Offset} \u00d7 4) \\] <p>PC already incremented by 4 by this time</p>"},{"location":"3_Core/Computer_Architecture/03_ISA/#jump-addressing","title":"Jump Addressing","text":"<p>Jump (<code>j</code> and <code>jal</code>) targets could be anywhere in text segment</p> <p>Encode full address in instruction</p> <p></p> <p>(Pseudo) Direct jump addressing</p> \\[ \\text{Target address} = (\\text{address} \u00d7 4) \\] <p>Why do we have 00 at the end of immediate when we calculate new instruction for branch/jump instructions? This is because</p> <ul> <li>Instructions are word-aligned, so their addresses always end with 00</li> <li>We aplways jump by a full instruction, which is 4 bytes, so we multiply the jump offset by 4, by shifting it left by 2 places</li> <li>We can jump further, if the offset is in multiple of 4 bytes instead of 1</li> </ul>"},{"location":"3_Core/Computer_Architecture/03_ISA/#branching-far-away","title":"Branching Far Away","text":"<p>If branch target is too far to encode with 16-bit offset, assembler rewrites the code, using unconditional jump (as it has larger range)</p> <p>Example</p> <pre><code>beq $s0, $s1, L1\n\n\u2193\n\nbne $s0, $s1, L2\nj L1\nL2:\n    \u2026\n</code></pre>"},{"location":"3_Core/Computer_Architecture/03_ISA/#addressing-summary","title":"Addressing Summary","text":""},{"location":"3_Core/Computer_Architecture/03_ISA/#assembler-pseudoinstructions","title":"Assembler Pseudoinstructions","text":"<p>Most assembler instructions represent machine instructions one-to-one</p> <p>Pseudoinstructions: figments of the assembler\u2019s imagination</p> <pre><code>move $t0, $t1\n\n\u2193\n\nadd $t0, $zero, $t1\n</code></pre> <pre><code>blt $t0, $t1, L\n\n\u2193\n\nslt $at, $t0, $t1\nbne $at, $zero, L\n</code></pre> <p><code>$at</code> (register 1): assembler temporary</p>"},{"location":"3_Core/Computer_Architecture/04_Arithmetic/","title":"04 Arithmetic","text":""},{"location":"3_Core/Computer_Architecture/04_Arithmetic/#signed-integer-addition","title":"Signed Integer Addition","text":"<p>Overflow if result out of range</p> Operands Overflow? +ve \\(+\\) +ve \u2705 if result sign is 1 +ve \\(+\\) \u2013ve \u274c -ve \\(+\\) -ve \u2705 if result sign is 0"},{"location":"3_Core/Computer_Architecture/04_Arithmetic/#signed-integer-subtraction","title":"Signed Integer Subtraction","text":"<p>Take the twos complement (negation) of the subtrahend and add it to the minuend.</p> <p>Overflow if result out of range</p> Operands Overflow? +ve \\(-\\) +ve \u274c +ve \\(-\\) -ve \u2705 if result sign is 1 -ve \\(-\\) +ve \u2705 if result sign is 0 -ve \\(-\\) -ve \u274c"},{"location":"3_Core/Computer_Architecture/04_Arithmetic/#dealing-with-overflow","title":"Dealing with Overflow","text":"<p>Some languages (e.g., C) ignore overflow</p> <ul> <li>Use MIPS <code>addu</code>, <code>addui</code>, <code>subu</code> instructions</li> </ul> <p>Other languages (e.g., Ada, Fortran) require raising an exception</p> <ul> <li>Use MIPS <code>add</code>, <code>addi</code>, <code>sub</code> instructions</li> <li>On overflow, invoke exception handler<ul> <li>Save PC in exception program counter (EPC) register</li> </ul> </li> <li>Jump to predefined handler address</li> <li><code>mfc0</code> (move from coprocessor 0 reg) instruction can retrieve EPC value, to return after corrective action</li> </ul>"},{"location":"3_Core/Computer_Architecture/04_Arithmetic/#multiplication-hardware","title":"Multiplication Hardware","text":""},{"location":"3_Core/Computer_Architecture/04_Arithmetic/#regular-unsigned-multiplication","title":"Regular Unsigned Multiplication","text":"<p>Length of product is sum of operand lengths</p>"},{"location":"3_Core/Computer_Architecture/04_Arithmetic/#observations","title":"Observations","text":"<ul> <li> <p>Multiplication involves the generation of partial products, one for each digit in the multiplier   | Multiplier bit | Partial Product |   | -------------- | --------------- |   | 0              | 0               |   | 1              | Multiplicand    |</p> </li> <li> <p>Each successive partial product is shifted one position to the left relative to the preceding partial product   (Effectively shifting multiplicand left one position for every   bit of multiplier)</p> </li> <li> <p>The final product is produced by summing the partial products.</p> </li> </ul>"},{"location":"3_Core/Computer_Architecture/04_Arithmetic/#possible-improvements","title":"Possible Improvements","text":"<p>Can be done more efficiently</p> <ul> <li>A running sum of partial products is maintained rather   than waiting until the end to sum all partial products<ul> <li>Saves storage</li> </ul> </li> <li>Shift multiplier right each step<ul> <li>Allows to look at a constant position (i.e) examine the least significant bit only</li> </ul> </li> <li>For each 1 on the LSB of the multiplier<ul> <li>Add (add running sum of partial product with the multiplicand)</li> <li>Shift (shift-left the multiplicand)</li> </ul> </li> <li>For each 0 on the LSB of multiplier<ul> <li>Shift left the multiplicand</li> </ul> </li> </ul> <p></p>"},{"location":"3_Core/Computer_Architecture/04_Arithmetic/#example","title":"Example","text":""},{"location":"3_Core/Computer_Architecture/04_Arithmetic/#booths-multiplication","title":"Booth\u2019s Multiplication","text":"<ul> <li>Multiplier and multiplicand are placed in the \\(Q\\) and \\(M\\) registers</li> <li>1-bit register \\(Q_{-1}\\) placed logically to the right of the least-significant bit of the \\(Q\\) register</li> <li>The results of the multiplication will appear in the \\(A\\) and \\(Q\\) registers</li> <li>\\(A\\) and \\(Q_{-1}\\) are initialized to \\(0\\)</li> </ul> <p>Control logic scans the bits of the multiplier Each bit is examined along with the bit to its right</p> \\(Q_0\\) \\(Q_{-1}\\) Action 0 0 Shift-Right \\(A,Q, Q_{-1}\\) 1 1 Shift-Right \\(A,Q, Q_{-1}\\) 0 1 \\(A = A + M\\)Shift-Right \\(A,Q, Q_{-1}\\) 1 0 \\(A = A \u2013 M\\)Shift-Right \\(A,Q, Q_{-1}\\) <p>In order to preserve sign bit arithmetic shift right is performed. The leftmost bit of \\(A\\), \\(A_{n-1}\\) is not only shifted into \\(A_{n-2}\\) but also remains in \\(A_{n-1}\\)</p> <p></p>"},{"location":"3_Core/Computer_Architecture/04_Arithmetic/#example_1","title":"Example","text":""},{"location":"3_Core/Computer_Architecture/04_Arithmetic/#advantage","title":"Advantage","text":"<ul> <li>Speeds up the multiplication process</li> <li>Blocks of 1s or 0s are skipped over, with an average of only one addition/subtraction per block</li> </ul>"},{"location":"3_Core/Computer_Architecture/04_Arithmetic/#mips-multiplication","title":"MIPS Multiplication","text":"<p>Two 32-bit registers for product</p> <ul> <li><code>HI</code>: most-significant 32 bits</li> <li><code>LO</code>: least-significant 32-bits</li> </ul>"},{"location":"3_Core/Computer_Architecture/04_Arithmetic/#instructions","title":"Instructions","text":"<pre><code>## 64-bit product in HI/LO\nmult rs, rt\nmultu rs, rt\n\n## Move from HI/LO to rd\nmfhi rd\nmflo rd\n\n## Least-significant 32 bits of product \u2013&gt; rd\nmul rd, rs, rt ## (pseudoinstruction!)\n</code></pre>"},{"location":"3_Core/Computer_Architecture/05_Floating_Point_Arithmetic/","title":"05 Floating Point Arithmetic","text":""},{"location":"3_Core/Computer_Architecture/05_Floating_Point_Arithmetic/#floating-point","title":"Floating Point","text":"\\[ x = (-1)^s \\times (1.\\text{Fraction}) \\times 2^{\\text{Exponent - Bias}} \\]"},{"location":"3_Core/Computer_Architecture/05_Floating_Point_Arithmetic/#s","title":"S","text":"<ul> <li>Sign bit</li> <li>0: non-negative</li> <li>1: negative</li> </ul>"},{"location":"3_Core/Computer_Architecture/05_Floating_Point_Arithmetic/#exponent","title":"Exponent","text":"<ul> <li>Excess representation: actual exponent + Bias</li> <li>Ensures exponent is unsigned</li> <li>Single: Bias = 127; Double: Bias = 1023</li> </ul>"},{"location":"3_Core/Computer_Architecture/05_Floating_Point_Arithmetic/#normalized-significand","title":"Normalized Significand","text":"<ul> <li>Significand is Fraction with the <code>1.</code> restored</li> <li>\\(1.0 \\le |\\text{significand}| &lt; 2.0\\)</li> <li>Always has a leading pre-binary-point 1 bit, so no need to represent it explicitly (hidden bit)</li> </ul>"},{"location":"3_Core/Computer_Architecture/05_Floating_Point_Arithmetic/#single-precision-32-bit","title":"Single Precision (32-bit)","text":"<p>Exponents \\(00000000\\) and \\(11111111\\) reserved</p> Smallest value Largest value Exponent 00000001 11111110 actual exponent \u2013126 +127 Fraction 000\u202600 111\u202611 significand 1.0 2.0 Value \\(\u00b11.0 \u00d7 2^{\u2013126} \u2248 \u00b11.2 \u00d7 10^{\u201338}\\) \\(\u00b12.0 \u00d7 2^{+127} \u2248 \u00b13.4 \u00d7 10^{+38}\\)"},{"location":"3_Core/Computer_Architecture/05_Floating_Point_Arithmetic/#double-precision-64-bit","title":"Double Precision (64-bit)","text":"<p>Exponents \\(0000\u202600\\) and \\(1111\u202611\\) reserved</p> Smallest value Largest value Exponent 00000000001 11111111110 actual exponent \u20131022 +1023 Fraction 000\u202600 111\u202611 significand 1.0 2.0 Value \\(\u00b11.0 \u00d7 2^{\u20131022} \u2248 \u00b12.2 \u00d7 10^{\u2013308}\\) \\(\u00b12.0 \u00d7 2^{+1023} \u2248 \u00b11.8 \u00d7 10^{+308}\\)"},{"location":"3_Core/Computer_Architecture/05_Floating_Point_Arithmetic/#floating-point-examples","title":"Floating Point Examples","text":""},{"location":"3_Core/Computer_Architecture/05_Floating_Point_Arithmetic/#denormal-numbers","title":"Denormal Numbers","text":"<p>Exponent = 000...0 \u21d2 hidden bit is 0</p> \\[ x = (-1)^s \\times (0+\\text{Fraction}) \\times 2^{-\\text{Bias}} \\] <ul> <li>Smaller than normal numbers</li> <li>Allow for gradual underflow, with diminishing precision</li> </ul> <p>Denormal with fraction = 000...0 is \\(\\pm 0.0\\) (2 representations of \\(0.0\\))</p>"},{"location":"3_Core/Computer_Architecture/05_Floating_Point_Arithmetic/#infinities-and-nans","title":"Infinities and <code>NaN</code>s","text":"\\(\\pm \\infty\\) <code>NaN</code> Infinity Not-A-Number Exponent \\(111 \\dots 1\\) \\(111 \\dots 1\\) Fraction \\(000 \\dots 0\\) \\(\\ne 000 \\dots 0\\) Can be used in subsequent calculations \u2705 \u2705 Avoids need for overflow check Indicates illegal or undefined resultFor eg: \\(0/0\\)"},{"location":"3_Core/Computer_Architecture/05_Floating_Point_Arithmetic/#floating-point-addition","title":"Floating-Point Addition","text":"<ol> <li>Align binary points    Shift number with smaller exponent</li> <li>Add significands (in binary)</li> <li>Normalize result &amp; check for over/underflow</li> <li>Round and renormalize, if necessary</li> </ol>"},{"location":"3_Core/Computer_Architecture/05_Floating_Point_Arithmetic/#fp-adder-hardware","title":"FP Adder Hardware","text":"<p>Much more complex than integer adder</p> <p>Doing it in one clock cycle would take too long</p> <ul> <li>Much longer than integer operations</li> <li>Slower clock would penalize all instructions</li> </ul> <p>FP adder usually takes several cycles</p> <ul> <li>Can be pipelined</li> </ul>"},{"location":"3_Core/Computer_Architecture/05_Floating_Point_Arithmetic/#mips-floating-point-instructions","title":"MIPS Floating Point Instructions","text":"<p>FPU co-processor is an adjunct processor that extends the ISA</p> <p>Separate FP registers</p> <ul> <li>32 single-precision: <code>$f0, $f1, \u2026 $f31</code></li> <li>Paired for double-precision: <code>$f0/$f1, $f2/$f3,</code></li> </ul> <p>FP instructions operate only on FP registers</p> <ul> <li>Programs generally don\u2019t do integer ops on FP data, or vice versa</li> <li>More registers with minimal code-size impact</li> </ul> <p>Separate FP instructions for single/double precision</p> Precision Bits Instruction Extension Example Single 32 <code>.s</code> <code>add.s $f0, $f1, $f2</code> Double 64 <code>.d</code> <code>add.d $f0, $f2, $f4</code>(equivalent to<code>add.d $f0/$f1, $f2/f3, $f4/$f5</code>)"},{"location":"3_Core/Computer_Architecture/05_Floating_Point_Arithmetic/#fp-arithmetic-operations","title":"FP Arithmetic Operations","text":""},{"location":"3_Core/Computer_Architecture/05_Floating_Point_Arithmetic/#fp-loadstore","title":"FP Load/Store","text":"<code>lwc1</code> load word coprocessor 1 <code>ldc1</code> load double coprocessor 1 <code>swc1</code> store word coprocessor 1 <code>sdc1</code> store double coprocessor 1"},{"location":"3_Core/Computer_Architecture/05_Floating_Point_Arithmetic/#fp-loadstore-pseudo-instructions","title":"FP Load/Store pseudo Instructions","text":"<code>l.s</code> lwc1 load FP single <code>s.s</code> swc1 store FP single <code>l.d</code> ldc1 load FP double <code>s.d</code> sdc1 store FP double"},{"location":"3_Core/Computer_Architecture/05_Floating_Point_Arithmetic/#fp-loading-immediate-value","title":"FP Loading immediate value","text":"<p>(pseudoinstruction)</p> <pre><code>li.s $f1, 1.0\n\nli.s $f10, 1.0e-5 ## $f10 = 0.00001\n</code></pre>"},{"location":"3_Core/Computer_Architecture/05_Floating_Point_Arithmetic/#fp-data-movement-instructions","title":"FP Data Movement Instructions","text":"<p>Moving data between general purpose and FP registers</p> Moving data between general purpose andFP registers <code>mfc1</code> move from coprocessor 1 (to general purpose register) <code>mtc1</code> move to coprocessor 1 (from general purpose register) Moving data between FP registers <code>mov.s</code> move single precision float <code>mov.d</code> move double precision float = even/odd pair of registers <p></p>"},{"location":"3_Core/Computer_Architecture/05_Floating_Point_Arithmetic/#fp-convert-instructions","title":"FP Convert Instructions","text":"<p><code>m.x.y</code></p> <p>Convert</p> <ul> <li>to destination format \\(x\\)</li> <li>from source format \\(y\\)</li> </ul>"},{"location":"3_Core/Computer_Architecture/05_Floating_Point_Arithmetic/#supported-formats","title":"Supported Formats","text":"<ul> <li>Single precision float = <code>.s</code> (single precision float in FP register)</li> <li>Double precision float = <code>.d</code> (double float in even-odd FP register)</li> <li>Signed integer word = <code>.w</code> (signed integer in FP register)</li> </ul>"},{"location":"3_Core/Computer_Architecture/05_Floating_Point_Arithmetic/#fp-compare-and-branch-instructions","title":"FP Compare and Branch Instructions","text":"<p>FP unit (co-processor 1) has a condition flag</p> <ul> <li>Set to 0 (false) or 1 (true) by any comparison instruction</li> </ul> <p>Three comparisons</p> <ul> <li>equal, less than, less than or equal</li> </ul> <p>Two branch instructions based on the condition flag</p> <p></p>"},{"location":"3_Core/Computer_Architecture/05_Floating_Point_Arithmetic/#reading-and-printing-single-and-double-values","title":"Reading and printing single and double values","text":""},{"location":"3_Core/Computer_Architecture/05_Floating_Point_Arithmetic/#floating-point-data-declarations","title":"Floating-Point Data Declarations","text":"<pre><code>.data\npi: .float 3.14\ntao: .double 6.28\n\n.text\nmain:\n\nend main\n</code></pre>"},{"location":"3_Core/Computer_Architecture/05_Floating_Point_Arithmetic/#mips-floating-point-examples","title":"MIPS Floating Point Examples","text":""},{"location":"3_Core/Computer_Architecture/05_Floating_Point_Arithmetic/#area-of-circle","title":"Area of Circle","text":"<pre><code>.data\npi: .double 3.14\n\n.text\nmain:\n    #f2,f3 = pi\n    ldc1 $f2, pi\n\n    ## read double (radius)\n    addi $v0, $zero, 7\n    syscall ## f0, f1 = r\n\n    mul.d $f12, $f0, $f0 ## r^2\n    mul.d $f12, $f2, $f12 ## area = pi * r^2\n\n  ## print value\n  addi $v0, $zero, 3\n  syscall\n</code></pre>"},{"location":"3_Core/Computer_Architecture/05_Floating_Point_Arithmetic/#f-to-c","title":"\u00b0F \\(\\to\\) \u00b0C","text":"<pre><code>.data\nconst5: .float 5.0\nconst9: .float 9.0\nconst32: .float 32.0\nconstf: .float 50.0\n\n.text\nmain:\nl.s $f16, const5\nl.s $f18, const9\ndiv.s $f16, $f16, $f18\nl.s $f12, constf\nl.s $f18, const32\nsub.s $f18, $f12, $f18\nmul.s $f12, $f16, $f18\nli $v0, 2\nsyscall\nli $v0, 10\nsyscall\n</code></pre>"},{"location":"3_Core/Computer_Architecture/05_Floating_Point_Arithmetic/#ax2-bx-c-for-user-inputted-x","title":"\\(ax^2 + bx + c\\) for user-inputted \\(x\\)","text":"<pre><code>.data\na: .float 1.0\nbb: .float 2.0\nc: .float 3.0\nmsg: .asciiz \"Enter x: \"\nblank: .asciiz \" \"\nnewl: .asciiz \"\\n\"\n\n.text\nmain: ## read input\nla $a0,msg ## prompt user for x\nli $v0,4 ## print string\nsyscall\nli $v0,6 ## read single\nsyscall ## $f0 &lt;-- x\n\n## evaluate the quadratic\nl.s $f2,a ## sum = a\nmul.s $f2,$f2,$f0 ## sum = ax\nl.s $f4,bb ## get b\nadd.s $f2,$f2,$f4 ## sum = ax + b\nmul.s $f2,$f2,$f0 ## sum = (ax+b)x = ax^2 +bx\nl.s $f4,c ## get c\nadd.s $f2,$f2,$f4 ## sum = ax^2 + bx + c\n\n## print the result\nmov.s $f12, $f2 ## $f12 = argument\nli $v0,2 ## print single\nsyscall\nla $a0,newl ## new line\nli $v0,4 ## print string\nsyscall\nli $v0,10 ## code 10 == exit\nsyscall ## Return to OS.\n</code></pre>"},{"location":"3_Core/Computer_Architecture/06_Datapath/","title":"06 Datapath","text":""},{"location":"3_Core/Computer_Architecture/06_Datapath/#mips-datapath","title":"MIPS Datapath","text":""},{"location":"3_Core/Computer_Architecture/06_Datapath/#goal-of-datapath","title":"Goal of Datapath","text":"<p>Build an architecture to support the following instructions</p> <ul> <li>Arithmetic: <code>add</code>, <code>sub</code>, <code>addi</code>, <code>slt</code></li> <li>Memory references: <code>lw</code>, <code>sw</code></li> <li>Branches: <code>j</code>, <code>beq</code></li> </ul>"},{"location":"3_Core/Computer_Architecture/06_Datapath/#process","title":"Process","text":"<ol> <li>Design basic framework that is needed by all instructions</li> <li>Build a computer for each operation individually</li> <li>Add MUXs to choose between different operations</li> <li>Add control signals to control the MUXs</li> </ol>"},{"location":"3_Core/Computer_Architecture/06_Datapath/#mips-steps","title":"MIPS Steps","text":"<pre><code>flowchart LR\nif[\"Instruction Fetch&lt;br/&gt;(using Program Counter)\"] --&gt;\n|PC += 4| r[\"Instruction Decode&lt;br /&gt;&amp;&lt;br /&gt;Register Read\"] --&gt;\nalu[\"Execute&lt;br /&gt;(ALU)\"] --&gt;\nm[\"Memory&lt;br /&gt;(Read/Write)\"] --&gt;\nR[\"WriteBack&lt;br /&gt;(Register Write)\"]</code></pre> <p>Register Read may be</p> <ul> <li>One register: <code>addi</code>, <code>lw</code></li> <li>Two registers: <code>add</code>, <code>sub</code>, <code>slt</code>, <code>sw</code>, <code>beq</code></li> </ul> <p>Quick operations may loop twice through machine, getting incorrect result, as clock is dependent on longest path (<code>lw</code>). </p>"},{"location":"3_Core/Computer_Architecture/06_Datapath/#sign-extension","title":"Sign Extension","text":"<p>Important for immediate data operations</p> <p>Take the top bit and copy it to all the other bits</p> <p>example</p> \\[ \\begin{aligned} 7 &amp;\\to 0111 \\to 0000 \\ 0000 \\ 0000 \\ 0111 \\\\ -2 &amp;\\to 1110 \\to 1111 \\ 1111 \\ 1111 \\ 1110 \\end{aligned} \\]"},{"location":"3_Core/Computer_Architecture/06_Datapath/#mips-instruction-available-datapath","title":"MIPS Instruction Available Datapath","text":""},{"location":"3_Core/Computer_Architecture/06_Datapath/#add-instruction","title":"<code>add</code> instruction","text":""},{"location":"3_Core/Computer_Architecture/06_Datapath/#addi-instruction","title":"<code>addi</code> Instruction","text":""},{"location":"3_Core/Computer_Architecture/06_Datapath/#load-instruction","title":"<code>load</code> Instruction","text":""},{"location":"3_Core/Computer_Architecture/06_Datapath/#store-instruction","title":"Store Instruction","text":""},{"location":"3_Core/Computer_Architecture/06_Datapath/#beq-instruction","title":"<code>beq</code> Instruction","text":"<p>Choose between</p> <ul> <li>\\(\\text{PC = (PC + 4)}\\)</li> <li>\\(\\text{PC = (PC + 4) + \\ \\ Imm&lt;&lt;2}\\)</li> </ul> <p></p>"},{"location":"3_Core/Computer_Architecture/06_Datapath/#j-instruction","title":"<code>j</code> Instruction","text":"<p><code>imm</code> is 26 bits, but PC is 32 bits</p> <ul> <li><code>imm &lt;&lt;</code> (shift left)</li> <li>Concatenate PC\u2019s upper bits</li> </ul> <p></p>"},{"location":"3_Core/Computer_Architecture/06_Datapath/#control-unit","title":"Control Unit","text":"<p>Set of control line values that cause appropriate actions to be taken at each step</p> <p>Finite state machine determines what needs to be done at each step</p> <ul> <li>Fetch</li> <li>Decode</li> <li>Action depends on opcode<ul> <li>Execute</li> <li>Memory</li> <li>Writeback</li> </ul> </li> </ul>"},{"location":"3_Core/Computer_Architecture/06_Datapath/#single-cycle-implementation","title":"Single Cycle implementation","text":"<p>An implementation in which an instruction is executed in one clock cycle</p> <p>Also called single clock cycle implementation</p>"},{"location":"3_Core/Computer_Architecture/06_Datapath/#advantage","title":"Advantage","text":"<p>Easy to understand</p>"},{"location":"3_Core/Computer_Architecture/06_Datapath/#disadvantage","title":"Disadvantage","text":"<p>Too slow</p> <p>The clock cycle must have the same length for every instruction. Hence, the longest possible path in the processor determines the clock cycle</p> <ul> <li>usually it is the <code>load</code> instruction, which uses five functional units in series<ul> <li>instruction memory</li> <li>register file</li> <li>ALU</li> <li>data memory</li> <li>register file</li> </ul> </li> </ul> <p>Single long clock cycle makes <code>add</code> take as long as <code>load</code></p>"},{"location":"3_Core/Computer_Architecture/06_Datapath/#solution","title":"Solution","text":"<ul> <li>Break single instruction execution into small execution steps</li> <li>Improve performance by pipelining</li> </ul>"},{"location":"3_Core/Computer_Architecture/07_Pipelining/","title":"07 Pipelining","text":""},{"location":"3_Core/Computer_Architecture/07_Pipelining/#pipelining","title":"Pipelining","text":"<p>Pipelining improves efficiency by executing multiple instructions simultaneously (through overlapped execution)</p> <p>Pipelining produces speedup, while maintaining similar datapath</p> <p>Assume time for stages is</p> <ul> <li>100ps for register read or write</li> <li>200ps for other stages</li> </ul>"},{"location":"3_Core/Computer_Architecture/07_Pipelining/#mips-pipeline","title":"MIPS Pipeline","text":"<p>Each of the 5 stages have their own step</p> <p></p>"},{"location":"3_Core/Computer_Architecture/07_Pipelining/#diagram","title":"Diagram","text":""},{"location":"3_Core/Computer_Architecture/07_Pipelining/#resource-usage-form","title":"Resource Usage Form","text":""},{"location":"3_Core/Computer_Architecture/07_Pipelining/#traditional-form","title":"Traditional Form","text":""},{"location":"3_Core/Computer_Architecture/07_Pipelining/#time","title":"Time","text":"\\[ \\begin{aligned} \\text{Time with pipeline} &amp;= \\frac{\\text{Time without pipeline}}{\\text{No of stages}} \\\\ &amp;= \\frac{800}{5} \\\\ &amp;= \\cancel{160} \\\\ &amp;= 200 \\end{aligned} \\] <p>Why 200ps? The longest stage determines the clock cycle time of the pipeline</p> <p></p> <p>Let \\(n\\) be the no of instructions</p> Without With Total Time \\(800 n\\) \\(800 + 200(n-1)\\)"},{"location":"3_Core/Computer_Architecture/07_Pipelining/#speedup","title":"Speedup","text":"<p>Speedup is due to increased throughput (number of instructions per unit time), not reduced latency (time for each instruction remains same)</p> <p>Assuming all stages are balanced/even (all stages take same time)</p> \\[ \\begin{aligned} S_\\text{ideal} &amp;= \\text{No of Stages} \\\\ &amp;= 5 \\end{aligned} \\] <p>If stages are unbalanced, speedup is less. Hence \\(\\downarrow\\)</p> \\[ \\begin{aligned} S_\\text{actual} &amp;= \\frac{\\text{Time without Pipeline}}{\\text{Time with Pipeline}} \\\\ &amp;= \\frac{800}{200} \\\\ &amp;= 4 \\end{aligned} \\] <p>Maximum speed up is achieved only when the no of instructions is very large.</p>"},{"location":"3_Core/Computer_Architecture/07_Pipelining/#convention-of-reg-readwrite","title":"Convention of Reg Read/Write","text":"<p>It only takes half of a cycle to read or write to register file</p> <p>Hence, whenever we are working with registers</p> <ul> <li>Write first half-cycle</li> <li>Read second half-cycle</li> </ul> <p>This is to ensure that we always read the latest value</p>"},{"location":"3_Core/Computer_Architecture/07_Pipelining/#hazards","title":"Hazards","text":"<p>Situations that prevent executing the next instruction in the following (next) clock cycle</p>"},{"location":"3_Core/Computer_Architecture/07_Pipelining/#types","title":"Types","text":"HazardType Meaning Example Structure Instruction cannot execute because a required resource (hardware) is busy In a single-memory system, memory-read of instruction and fetch of instruction 2 happening simultaneously Data Instruction cannot execute because data required to execute instruction is not availableNeed to wait for data <code>add $s0, $t0, $t1</code><code>sub $t2, $s0, $t3</code> Load-Use Specific form of Data HazardData being loaded by a load instruction is not available, which is needed by another instruction <code>lw $s0, 20($t1)</code><code>sub $t2, $s0, $t3</code> Control/Branching Instruction cannot execute because the instruction that was fetched is not the one that is needed."},{"location":"3_Core/Computer_Architecture/07_Pipelining/#solutions","title":"Solutions","text":"Solution Working Advantage Disadvantage 2 Memories Separate instruction/data memories Avoid structure hazard BubblingStalling Delaying using dummy instructionsWaste Avoids hazard Wastes time ForwardingBypassing Using internal buffer/latchForwarding path valid only if destination stage is after source stage We can save clock cycles (depends on the instruction)Reduces time taken Not that useful when using <code>lw</code>; we have to introduce one bubble anyway Code-Rescheduling Reorder serializable code to avoid use of load result in the next instruction, while ensuring there is no dependencySimilar to transactions like in DBMSTry to transfer all <code>lw()</code> to the top Overhead for the compiler/assembler Branch Prediction Make prediction and flush if incorrectStatic means assume branch not takenDynamic can 1 bit/2bit Delayed Branching Code Rescheduling for branching- From before- From below- From fall-through"},{"location":"3_Core/Computer_Architecture/07_Pipelining/#i-missed-some-parts","title":"I missed some parts","text":"<p>Check slides</p> <p>Branch Prediction and all</p>"},{"location":"3_Core/Computer_Architecture/07_Pipelining/#gpu","title":"GPU","text":"<p>(not for exam)</p> <p>Works on SIMD(Single Instruction Multiple Data) architecture</p> <p>Works on matrix operations</p> <p>The same operation is performed over multiple rows/columns</p> <p>You have explicitly select GPU, hence this is explicit parallelism</p>"},{"location":"3_Core/Computer_Architecture/08_Memory/","title":"08 Memory","text":""},{"location":"3_Core/Computer_Architecture/08_Memory/#principle-of-locality","title":"Principle of Locality","text":"<p>Programs access a small proportion of its address space at any time. This exhibits locality.</p> Spatial locality Temporal locality Meaning Load clusters of related data into cache Recently-used data Example - Multiple elements in a loop- Sequential instruction access - Global variables from global region of memory- Instructions of a loop <p>Data can only move between 2 adjacent levels</p> <p>Register is highest level; Tertiary memory is lowest memory</p> <pre><code>flowchart LR\n\nCPU &lt;--&gt;\nRegister &lt;--&gt;\nL1 &lt;--&gt;\nL2 &lt;--&gt;\nL3 &lt;--&gt;\nPM[Primary&lt;br /&gt;Memory] &lt;--&gt;\nSM[Secondary&lt;br /&gt;Memory] &lt;--&gt;\nTM[Tertiary&lt;br /&gt;Memory]\n\nsubgraph Cache\n\n    L1\n    L2\n    L3\nend</code></pre>"},{"location":"3_Core/Computer_Architecture/08_Memory/#memory-performance","title":"Memory Performance","text":"Block(Cache Line) Unit of copyingConsists multiple words Hit Event of data requested by processor being present in upper level Miss Not a hit Hit Rate/Ratio Fraction of memory found in upper level\\(\\frac{\\text{Hits}}{\\text{Access}}\\) Miss Rate/Ratio \\(1 - \\text{Hit Ratio}\\) Hit Time Time to access the upper level + time to determine if access is hit or miss Miss Penalty Time to replace a block in upper level with the corresponding block from lover level AMAT Average Memory Access Time\\(T_\\text{avg} = T_\\text{cache} + (m*\\text{Miss Penalty}) + (m*T_\\text{memory})\\)"},{"location":"3_Core/Computer_Architecture/08_Memory/#cache-memory","title":"Cache Memory","text":"<p>L1 cache can be located on CPU Chip</p>"},{"location":"3_Core/Computer_Architecture/08_Memory/#cache-operation","title":"Cache Operation","text":"<pre><code>flowchart LR\n\nstart((Start)) --&gt;\n1[Retries RA from CPU] --&gt;\n2[Is block containing RA in cache] --&gt;\nsomething</code></pre>"},{"location":"3_Core/Computer_Architecture/08_Memory/#types-of-access","title":"Types of Access","text":"<ul> <li>Regular access</li> <li>Irregular access</li> </ul>"},{"location":"3_Core/Computer_Architecture/08_Memory/#direct-mapped-cache","title":"Direct Mapped Cache","text":"\\[ \\begin{aligned} \\text{Total no of bits} &amp;= 2^n \\times \\Big( \\text{Block size + Tag Size + Valid Field Size} \\Big) \\\\ \\text{where } n &amp;= \\text{No of bits to represent cache lines} \\end{aligned} \\]"},{"location":"3_Core/Computer_Architecture/08_Memory/#cache-line-table","title":"Cache Line Table","text":"<p>Let \\(s\\) be address bus size (number of bits to represent main memory address)</p> Cache Line Main memory blocks held \\(0\\) \\(0, m, 2m, \\dots, 2s-m\\) \\(1\\) \\(1, m+1, 2m+1, \\dots, 2s-m+1\\) \\(\\dots\\) \\(m-1\\) \\(m-1, \\dots,\\)"},{"location":"3_Core/Computer_Architecture/08_Memory/#memory-addressing","title":"Memory Addressing","text":"Tag Line number/Index Line/Byte offset Purpose Distinguish block from other blocks that can fit into a cache line Specify one of the \\(m=2r\\) blocks of main memory Identify unique word/byte within line of cache memory Bits Most significant \\(s-r-w\\) bits Next most significant \\(r\\) bits Least significant \\(w\\) bits <p>No of cache blocks \\(= 2^something\\)</p> \\[ \\begin{aligned} &amp;\\text{No of index bits} \\\\ &amp;= \\text{No of bits for Total Cache Size} -  \\text{No of bits for Offset} \\end{aligned} \\] \\[ \\begin{aligned} &amp;\\text{No of tag bits} \\\\ &amp;= \\text{No of bits for Total Memory Size} \\\\ &amp; \\ -\\Big( \\text{No of bits for Index} + \\text{No of bits for Offset} \\Big) \\end{aligned} \\]"},{"location":"3_Core/Computer_Architecture/08_Memory/#tags-valid-bits","title":"Tags &amp; Valid Bits","text":"<p>Initially valid bit = 0, when you boot up the computer</p>"},{"location":"3_Core/Computer_Architecture/08_Memory/#address-subdivision-diagram","title":"Address Subdivision Diagram","text":"<p>Take from slides</p>"},{"location":"3_Core/Computer_Architecture/08_Memory/#reading-from-cache","title":"Reading from Cache","text":"<ol> <li>Search the index</li> <li>Check validity<ul> <li>If valid, check the tag</li> <li>If equal tag, mark as hit</li> <li>If invalid, mark as miss</li> </ul> </li> </ol>"},{"location":"3_Core/Computer_Architecture/08_Memory/#fully-associative-cache","title":"Fully-Associative Cache","text":""},{"location":"3_Core/Computer_Architecture/08_Memory/#writing-to-cache","title":"Writing to Cache","text":"<p>LRU Replacement</p> <p>\u2705 No calculation happens, so it is efficient and faster</p>"},{"location":"3_Core/Computer_Architecture/08_Memory/#reading-from-cache_1","title":"Reading from Cache","text":"<p>\u274c Comparison of all bits happens, so it is inefficient and slower</p> <ol> <li>Check validity for all valid lines</li> <li>If valid, check the tag</li> <li>If invalid, add new element in first-come-first-serve order</li> </ol>"},{"location":"3_Core/Computer_Architecture/08_Memory/#set-associative-cache","title":"Set Associative Cache","text":"<p>Combination of Direct Mapped Cache and Fully-Associative Cache</p> <p>Particular block address is mapped to a particular set</p> <p>'\\(n\\)-way associated cache' means each set has \\(n\\) blocks, where \\(n \\in \\{2, 4, 8, \\dots \\}\\)</p> \\[ \\begin{aligned} \\text{No of sets } N &amp;= \\frac{\\text{Total no of blocks}}{n} \\\\ &amp;= \\frac{1}{n} \\times \\frac{\\text{Cache Size in bytes}}{\\text{Size of each block in bytes}} \\end{aligned} \\] <p>No of bits for set index = \\(2^N\\)</p> \\[ \\text{Mapped Set} = \\text{Address } \\% \\ N \\] <p>LRU Replacement</p> <p>Note</p> <ul> <li>Direct Mapped Cache is basically one-way associative caching</li> <li>Fully-Associative Cache is basically associative caching with no of sets = total no of blocks</li> </ul>"},{"location":"3_Core/Computer_Architecture/08_Memory/#write","title":"Write","text":""},{"location":"3_Core/Computer_Architecture/08_Memory/#write-through","title":"Write-Through","text":""},{"location":"3_Core/Computer_Architecture/08_Memory/#write-back","title":"Write-Back","text":"<p>Dirty block something</p>"},{"location":"3_Core/Computer_Architecture/08_Memory/#write-buffer","title":"Write-Buffer","text":"<p>If the same data is accessed again, there is overhead of checking queue also, just in case</p>"},{"location":"3_Core/Computer_Architecture/08_Memory/#lru-replacement","title":"LRU Replacement","text":"<p>Least Recently-Used</p> <p>We have to implement a data structure to keep track</p> <ul> <li>doubly linked list</li> <li>hash map</li> </ul>"},{"location":"3_Core/Computer_Architecture/09_Cache_Optimization/","title":"09 Cache Optimization","text":""},{"location":"3_Core/Computer_Architecture/09_Cache_Optimization/#goal-of-cache-optimization","title":"Goal of Cache Optimization","text":"<p>Reduce average memory access time by improving the following aspects</p> Aspect Solution Advantage Disadvantage Reduce Miss Rate(Increase Hit Rate) Larger block size (but not too high) Fewer capacity miss Longer hit time (due to longer search)Costlier Larger cache size Higher Associativity Fewer conflict miss Complicated circuitLonger clock cycle time (increased hit time) Reduce Miss Penalty Multilevel Caches Reduced turnaround time Increased overhead of write-back/write-through/write-buffer Write-through with buffer to serve reads before writes (to give priority to read misses over writes) Write-Back Reduce Hit Time Use virtually-indexed, physically-tagged, to avoid address translation during indexing of cache Small and Simple caches Pipelined cache access Trace caches <p>RAW = Read After Write</p>"},{"location":"3_Core/Computer_Architecture/09_Cache_Optimization/#cache-research-results","title":"Cache Research Results","text":""},{"location":"3_Core/Computer_Architecture/09_Cache_Optimization/#21-cache-rule","title":"2:1 cache rule","text":"<p>Miss rate of Direct </p>"},{"location":"3_Core/Computer_Architecture/09_Cache_Optimization/#8-way-set-associate","title":"8 way Set associate","text":"<p>is as effective as fully associative </p>"},{"location":"3_Core/Computer_Architecture/09_Cache_Optimization/#cache-segment","title":"Cache Segment","text":"<p>Each cache divided into 2 segments</p> <ul> <li>Instruction Segment</li> <li>Data Segment</li> </ul>"},{"location":"3_Core/Computer_Architecture/09_Cache_Optimization/#types-of-miss-rate","title":"Types of Miss Rate","text":""},{"location":"3_Core/Computer_Architecture/09_Cache_Optimization/#local-miss-rate","title":"Local miss rate","text":"\\[ \\frac{\\text{Misses in this cache}}{\\text{Number of accesses of this cache}} \\]"},{"location":"3_Core/Computer_Architecture/09_Cache_Optimization/#global-miss-rate","title":"Global miss rate","text":"\\[ \\frac{\\text{Misses in this cache}}{\\text{Total number of accesses}} \\]"},{"location":"3_Core/Computer_Architecture/09_Cache_Optimization/#multiple-cache-amat","title":"Multiple Cache AMAT","text":"\\[ \\begin{aligned} \\text{AMAT}_\\text{Overall} &amp;= \\Big( \\text{Hit Rate}_{L_1} \\times \\text{Hit Penalty}_{L_1} \\Big) + \\Big( \\text{Miss Rate}_{L_1} \\times \\textcolor{hotpink}{\\text{Miss Penalty}_{L_1}} \\Big) \\\\ \\textcolor{hotpink}{\\text{Miss Penalty}_{L_1}} &amp;=  \\Big( \\text{Hit Rate}_{L_2} \\times \\text{Hit Penalty}_{L_2} \\Big)  + \\Big( \\text{Miss Rate}_{L_2} \\times \\textcolor{orange}{\\text{Miss Penalty}_{L_3}} \\Big) \\\\ &amp;\\dots \\end{aligned} \\]"},{"location":"3_Core/Computer_Architecture/09_Cache_Optimization/#cache-miss-types","title":"Cache Miss Types","text":"Miss Type When Compulsory MissCold Miss Initially caches are empty(Valid bits are all 0) Capacity Miss Not enough space in cache to store Conflict Miss Already there is some data in the same cache location"},{"location":"3_Core/Computer_Architecture/Lab/00_Setup/","title":"00 Setup","text":"<ol> <li>Write your program in a text program</li> <li>Open <code>QtSpim</code></li> <li><code>File</code> &gt; <code>Reinitialize and Load File</code></li> <li>Load your file</li> <li>Run the program</li> <li>View the output on the <code>Console</code></li> </ol>"},{"location":"3_Core/Computer_Architecture/Lab/01/","title":"01","text":""},{"location":"3_Core/Computer_Architecture/Lab/01/#print-hello-world","title":"Print <code>Hello World</code>","text":"<pre><code>.data\nmsg: .asciiz \"Hello World\"\n\n.text\nmain:\n\nli $v0, 4\nla $a0, msg\nsyscall\n\nli $v0, 10\nsyscall\n</code></pre>"},{"location":"3_Core/Computer_Architecture/Lab/01/#add-2-numbers","title":"Add 2 Numbers","text":"<p>I used <code>add</code> to assign values, as <code>li</code> is a pseudo-instruction, which will take 2 cycles</p> <pre><code>.text\n\nmain:\nadd $v0, $zero, 1\n\nadd $11, 10, $zero\nadd $12, 10, $zero\nadd $a0, $11, $12\nsyscall\n\nadd $v0, $zero, 10\nsyscall\n</code></pre>"},{"location":"3_Core/Computer_Architecture/Lab/02/","title":"02","text":"<p>We cannot store an immediate value into memory.</p>"},{"location":"3_Core/Computer_Architecture/Lab/02/#sum-of-first-n-numbers","title":"Sum of first \\(n\\) numbers","text":"<pre><code>.text\nmain:\n\naddi $t0, $zero, 5 ## n\nadd $t1, $zero, 0 ## sum\nadd $t2, $zero, 1 ## i\n\nrepeat:\nbgt $t2, $t0, display\nadd $t1, $t1, $t2\naddi $t2, $t2, 1\nj repeat\n\ndisplay:\nadd $v0, $zero, 1\nadd $a0, $zero, $t1\nsyscall\n\n#exit\nadd $v0, $zero, 10\nsyscall\n</code></pre>"},{"location":"3_Core/Computer_Architecture/Lab/02/#array-memory","title":"Array Memory","text":"<pre><code>.data\narray .word 10, 20, 30\n\n.text\nla $t0, array ## array \n\naddi $t1, $zero, 3 ## n = 3\naddi $t2, $zero, 0 ## i = 0\n\nloop: ## for (i = 0; i &lt; n; i++)\n## i &gt;= n (exit if i = n)\n    beq $t2, $t1, exit\n\n    ## code for displaying 1 element\n    addi $a0, $zero, 3\n    addi $v0, $zero, 4\n    syscall\n\nexit:\n</code></pre> <ol> <li>Initialize array</li> <li>Get address of array</li> <li></li> </ol>"},{"location":"3_Core/Computer_Architecture/Lab/07_Procedures/","title":"07 Procedures","text":""},{"location":"3_Core/Computer_Architecture/Lab/07_Procedures/#question-1","title":"Question 1","text":"<p>Procedures to find __ of 2 inputted numbers</p> <ul> <li>Sum</li> <li>Difference</li> <li>Product</li> <li>Quotient</li> </ul> <p>Arguments</p> <ul> <li>\\(x\\)</li> <li>\\(y\\)</li> </ul> <pre><code>.data\nnewline: .asciiz \"\\n\"\n\n.text\nmain:\n    addi $a0, $zero, 1\n    addi $a1, $zero, 5\n\n    jal sum\n    add $a0, $zero, $v0\n    addi $v0, $zero, 1\n  syscall   \n  la $a0, newline\n    addi $v0, $zero, 4\n    syscall\n\n    addi $a0, $zero, 1\n    addi $a1, $zero, 5\n\n  jal dif\n    add $a0, $zero, $v0\n    addi $v0, $zero, 1\n  syscall\n    la $a0, newline\n    addi $v0, $zero, 4\n    syscall\n\n    addi $a0, $zero, 1\n    addi $a1, $zero, 5\n\n  jal pro\n    add $a0, $zero, $v0\n    addi $v0, $zero, 1\n  syscall\n    la $a0, newline\n    addi $v0, $zero, 4\n    syscall\n    addi $a0, $zero, 1\n    addi $a1, $zero, 5\n\n  jal quo\n    add $a0, $zero, $v0\n    addi $v0, $zero, 1\n  syscall\n    la $a0, newline\n    addi $v0, $zero, 4\n    syscall\n\n    #exit\n    addi $v0, $zero, 10\n    syscall\n.end main\n\nsum:\n    add $v0, $a0, $a1\n\n    jr $ra\n\ndif:\n    sub $v0, $a0, $a1\n\n    jr $ra\n\npro:\n    mult $a0, $a1\n    mflo $v0\n\n    jr $ra\n\nquo:\n    div $a0, $a1\n    mflo $v0\n\n    jr $ra\n</code></pre>"},{"location":"3_Core/Computer_Architecture/Lab/07_Procedures/#question-2","title":"Question 2","text":"<p>Procedure for linear search</p> <p>Arguments</p> <ul> <li>Array address</li> <li>Array length</li> <li>Search Element</li> </ul> <p>Return</p> <ul> <li>Found flag</li> <li>Index, if found</li> </ul> <pre><code>.data\narray: .word 10, 20, 30, 40, 50\nfound_msg: .asciiz \"Found at index \"\nnot_found_msg: .asciiz \"Not Found\"\n\n.text\nmain:\n    la $a0, array\n    addi $a1, $zero, 5\n    addi $a2, $zero, 30\n\n    jal linear_search\n    beq $v0, $zero, print_not_found\n\n    print_found:\n        la $a0, found_msg\n        addi $v0, $zero, 4\n        syscall\n\n        add $a0, $zero, $v1\n        addi $v0, $zero, 1\n        syscall\n\n        j exit\n\n    print_not_found:\n        la $a0, not_found_msg\n        addi $v0, $zero, 4\n        syscall\n\n    exit:\n        addi $v0, $zero, 10\n        syscall\n\nlinear_search:\n    add $t0, $zero, $zero ## i = 0  \n    add $t4, $zero, 4\n    add $v0, $zero, $zero ## status\n\n    loop:\n        beq $t0, $a1, return\n\n        mult $t0, $t4 ## i*4\n        mflo $t1 ## offset = i*4\n        add $t1, $t1, $a0 ## adress = array_base_address + offset\n\n        lw $t2, 0($t1)\n        beq $t2, $a2, found\n\n        addi $t0, $t0, 1\n        j loop\n\n    j return\n\n    found:\n        addi $v0, $zero, 1\n        add $v1, $zero, $t0\n        j return\n\n    return:\n        jr $ra\n</code></pre>"},{"location":"3_Core/Computer_Architecture/Lab/08_Procedures_2/","title":"08 Procedures 2","text":""},{"location":"3_Core/Computer_Architecture/Lab/08_Procedures_2/#question-1","title":"Question 1","text":"<p>Implement a function to find exponent of a number</p> <pre><code>.data\n\n.text\nmain:\naddi $a0, $zero, 2\naddi $a1, $zero, 0\n\njal power\n\ndisplay:\nadd $a0, $zero, $v0\naddi $v0, $zero, 1\nsyscall\n\nexit:\naddi $v0, $zero, 10\nsyscall\n\npower:\n    beq $a1, 0, zero_case\n\n  addi $sp, $sp, -8\n    sw $ra, 4($sp)\n    sw $a0, 0($sp)\n\n    beq $a1, 1, one_case\n\n    mult $v0, $a0\n    mflo $v0\n    jr $ra\n\n    addi $a1, $a1, -1\n    jal power\n\n    zero_case:\n        addi $v0, $zero, 1\n        jr $ra\n\n    one_case:\n        lw $ra, 4($sp)\n        lw $v0, 0($sp)\n        addi $sp, $sp, 8\n\n        add $v0, $zero, $a0\n        jr $ra\n</code></pre>"},{"location":"3_Core/Computer_Architecture/Lab/08_Procedures_2/#question-2","title":"Question 2","text":""},{"location":"3_Core/Computer_Architecture/Lab/09_Floating_Point/","title":"09 Floating Point","text":""},{"location":"3_Core/Computer_Architecture/Lab/09_Floating_Point/#1-area-of-circle","title":"1: Area of circle","text":"<pre><code>.data\npi: .double 3.1415926535897924 \nr: .float 2.2\n\n.text\nmain:\nldc1 $f2, pi\nlwc1 $f4, r\ncvt.d.s $f4, $f4\n\nmul.d $f12, $f4, $f4    ## r^2\nmul.d $f12, $f2, $f12   ## pi r^2\n\naddi $v0, $zero, 3\nsyscall\n\n## exit\naddi $v0, $zero, 10\nsyscall\n</code></pre>"},{"location":"3_Core/Computer_Architecture/Lab/09_Floating_Point/#2-convert-from-f-to-c","title":"2: Convert from F to C","text":"<pre><code>.data \nf: .float 98.6\n\n.text\nmain:\nli.s $f2, 5.0\nli.s $f3, 9.0\n\nlwc1 $f4, f\nli.s $f5, 32.0\n\ndiv.s $f6, $f2, $f3 ## 5/9\nsub.s $f7, $f4, $f5 ## f - 32\n\nmul.s $f12, $f6, $f7    ## (5/9)*(f-32)\n\naddi $v0, $zero, 2\nsyscall\n\n## exit\naddi $v0, $zero, 10\nsyscall\n</code></pre>"},{"location":"3_Core/Computer_Architecture/Lab/09_Floating_Point/#3-value-of-ax2-bx-c-for-inputted-x","title":"3: Value of \\(ax^2 + bx + c\\) for inputted \\(x\\)","text":"<pre><code>.data \n\n.text\nmain:\n\n## read x\naddi $v0, $zero, 6\nsyscall\n\nmov.s $f2, $f0 ## x\nmul.s $f3, $f0, $f0 ## x^2\n\nli.s $f4, 1.0 ## a\nli.s $f4, 1.0 ## b \nli.s $f5, 1.0 ## c\n\nmul.s $f6, $f4, $f3 ## ax^2\nmul.s $f7, $f5, $f2 ## bx\n\nadd.s $f12, $f6, $f7 ## ax^2 + bx\nadd.s $f12, $f12, $f5 ## ax^2 + bx + c\n\n## Print\naddi $v0, $zero, 2\nsyscall\n\n## exit\naddi $v0, $zero, 10\nsyscall\n</code></pre>"},{"location":"3_Core/Computer_Networks/","title":"Computer Networks","text":"<p>Unfortunately, the notes could\u2019ve been better. Need help improving this.</p>"},{"location":"3_Core/Computer_Networks/01_Introduction/","title":"Introduction","text":""},{"location":"3_Core/Computer_Networks/01_Introduction/#data-communication","title":"Data Communication","text":"<p>Exchange of data b/w devices via transmission medium, where data is information presented in form agreed by involved parties. Termed from \u2018telecommunication\u2019 - communication at a distance</p>"},{"location":"3_Core/Computer_Networks/01_Introduction/#components-of-data-communication","title":"Components of Data Communication","text":"<ul> <li>Message</li> <li>Sender</li> <li>Sending protocol</li> <li>Medium</li> <li>Receiver</li> <li>Receiving protocol</li> </ul>"},{"location":"3_Core/Computer_Networks/01_Introduction/#node","title":"Node","text":"<p>Device capable of sending/receiving data to/from other notes on network</p>"},{"location":"3_Core/Computer_Networks/01_Introduction/#network","title":"Network","text":"<p>Set of devices connected by communication links</p>"},{"location":"3_Core/Computer_Networks/01_Introduction/#purpose","title":"Purpose","text":"<p>Share resourcces</p> <ul> <li>File sharing</li> <li>Hardware sharing</li> <li>Application sharing: Client/server apps</li> <li>Network graming</li> <li>User Commuication</li> <li>Voice over IP (VoIP): allows calls over traditional IP rather than by traditional PTSN</li> </ul>"},{"location":"3_Core/Computer_Networks/01_Introduction/#distance-based-classification","title":"Distance-Based Classification","text":"Range Example LAN Short Wifi MAN Specific area (city, campus) Cable TV WAN Long The Internet"},{"location":"3_Core/Computer_Networks/01_Introduction/#parts-of-network","title":"Parts of Network","text":"Part Role \u2018The Internet\u2019 Router Connect internet to \u2018The Internet\u2019has intelligence(represented using \\(\\otimes\\)) Firewall Rules to adhere on which messages to be allowed Switch Helps form a LAN (Local Area Network)No of ports will always be \\(2^n\\) File Server Database Server File Server WiFi Access Point kinda like a wireless switchconnected to wired swetch <p>Links between one/more routers should be a \u2018dedicated link\u2019</p>"},{"location":"3_Core/Computer_Networks/01_Introduction/#transmission-modes-media","title":"Transmission Modes &amp; Media","text":"Type Medium Range Requires Example Wired Twisted Pair Cables Short Landline, Ethernet cable Coaxial Cables Long Fibreoptic Cables Very Long Wireless(Frequency bands) Radio Waves Long Omni-directional antenna Car radio Micro Waves Long Uni-directional Microwave antennaLon Etisalat connection tower Infrared Waves Short Bluetooth <p>Wireless can - Infrastructure-Based: Mobile Network - Infrastructure-less: Bluetooth</p>"},{"location":"3_Core/Computer_Networks/01_Introduction/#idk","title":"IDK","text":""},{"location":"3_Core/Computer_Networks/01_Introduction/#transmission-modes","title":"Transmission Modes","text":"Direction Order Example Simplex Uni Car Radio Half-Duplex Bi Sequential(one direction after the other) Walkie-Talkie Full-Duplex Bi Simultaneous(both directions at the same time) Telephone"},{"location":"3_Core/Computer_Networks/01_Introduction/#line-configurations","title":"Line Configurations","text":"Example Point-to-Point Connection from ISP to home router Multi-Point Multiple devices connected to a single home router"},{"location":"3_Core/Computer_Networks/01_Introduction/#topology","title":"Topology","text":"<p>Arrangement of nodes in a network</p> Bus Ring Star Mesh Hybrid Arrangement Sequential Each node connected to 2 adjacent nodes Nodes directly connected to a central \u2018controller\u2019 Every device connected to every other devicein point-point manner Combination of star and bus Working Devices collectively help transfer data b/w pointsTerminators stop signals after reaching end of wire,to prevent signal bounce Token-Passing(Token: Message which gives priority to a station to use ring)- Data hops from one device to another until it reaches its destination- Each device communicates its routing info to every other connected device- Each device then determines either passes/keep received data Device Used TapDrop line Repeater Hub/Switch/Router Advantage SimpleCheapEasy installationNode failure does not affect others Easier to manageEasier to locate defective node/cable problemGreat for transmitting signals over long distances on a LANHandles high-volume network trafficEnables reliable communication - Good for modern networks- Low startup costs- Easy to manage- Easy to expand- Great availability of equipment- Scalable- High security Highest redundancyLow failure chanceLow trafficEasy fault identificationRobust Disadvantage Not fault-tolerantProne to congestionNo security ExpensiveSingle point of failureRequires more cable &amp; network equipment at startFewer equipment optionsFewer options for high-speed upgradesOnly one station can send messageRequires tokensRequires multiple repeatersNo security Single point of failure - If hub fails, everything failsPossible congestion at hubRequires more cables than bus Expensive(Many cables, I/O port, connections) Same as star Method Half-Duplex Simplex Duplex? Example Ethernet Between ISP routers Duplex/Half-Duplex links 1 0 \\(n\\) \\(\\frac{n(n-1)}{2}\\) Simplex links 0 1 \\(2n\\) \\(n(n-1)\\) Diagram"},{"location":"3_Core/Computer_Networks/01_Introduction/#network-devices","title":"Network Devices","text":"End Points PCs, Servers, Printers, etc Interconnections Media, ConnectorsNIC(Network Interface Card)/LAN Card/Ethernet Card) Bridge (not used anymore) Switches Connects endpoints to LANMulti-Port Bridge Router Connect multiple LANs to form internetworksChooses best path between LAN &amp; WAN Repeater Repeats Token in a round-robin fashionHelps overcome signal attenuation Hub Device without any intelligenceMulti-port repeaterNot used much anymoreIt will just broadcast every packet, as it cannot select devices."},{"location":"3_Core/Computer_Networks/01_Introduction/#network-rules","title":"Network Rules","text":""},{"location":"3_Core/Computer_Networks/01_Introduction/#protocol","title":"Protocol","text":"<p>Consists rules for the following aspects</p> Aspect Meaning Syntax Format of data Semantics Meaning of each section of bits Timing Timing and speed of data transfer"},{"location":"3_Core/Computer_Networks/01_Introduction/#the-internet","title":"The Internet","text":"<p>Network of networks, consisting of</p> <ul> <li>Connected computing devices</li> <li>communication links</li> <li>Routers</li> <li>Protoctols</li> <li>Communication infrasture for distributed applications</li> <li>Communication services</li> </ul>"},{"location":"3_Core/Computer_Networks/01_Introduction/#standard","title":"Standard","text":"<p>Collection of protocols agreed by organizations, such as ITU, IEEE</p> De Facto Standards De Jure Standards Approved by organizations Adopted through widespread use <p>For eg</p> <ul> <li>Wired LAN uses standard <code>IEEE 802.3</code></li> <li>WiFi (WirelessFidelity) uses standard <code>802.11</code></li> </ul>"},{"location":"3_Core/Computer_Networks/01_Introduction/#internet-standards","title":"Internet Standards","text":"<ul> <li>Internet draft</li> <li>RFC (Request for Comment)</li> </ul>"},{"location":"3_Core/Computer_Networks/01_Introduction/#models","title":"Models","text":"Model Example Client-Server 1 Client1 Server WWWEmail Peer-to-Peer End devices use each other\u2019s resources TorrentingTeleconferencing"},{"location":"3_Core/Computer_Networks/01_Introduction/#types-of-services","title":"Types of Services","text":"Connection-Oriented Connection-Less Stages 1. Set up connection2. Receive acknoledgement3. Send data4. Receive acknowlegment5. Repeat steps 3-4 Send data Reliable \u2705 \u274c Flow Control \u2705 \u274c Congestion Control \u2705 \u274c Speed Slower Faster Example Protocol TCP(Transmission Control Protocol) UDP(User Datagram Protocol) Example Applications HTTP (WWW)FTP (File Transfer Protocol)Telnet (Remote LoginSMTP (Simple Mail Transfer Protocol) Streaming mediaTeleconferencingInternet telephony"},{"location":"3_Core/Computer_Networks/01_Introduction/#switchingrouting-mechanism","title":"Switching/Routing Mechanism","text":"Circuit Switching Packet Switching Type Physical Logical Dedicated circuit per call: telephone net Data sent in discrete \u2018chunks\u2019Each packet uses full link bandwidth Steps - Establish physical connection- Network resources divided into pieces- Pieces allocated to calls- Data Transmission- Teardown - Split data into packets- Transmit packets one hope at a time- Packet reaches receiver Resource reservation \u2705 \u274c Resources allocated Fixed On-Demand (Dynamic) Advantages Line efficiency (Single link can be shared by multiple packets)Data rate conversionPackets are accepted even when network is busy (delayed, but still accepted)Priorities can be set Disadvantage Resource piece idle if not used by owning call (no sharing)Call setup required Connection Type Connection-oriented Connection-less (Virtual Circuit Approach)Connection-oriented (Datagram Approach) Total resource demandcan exceed  available? \u274c \u2705 CongestionControl? \u274c \u2705 Performanceguaranteed? \u2705 \u274c"},{"location":"3_Core/Computer_Networks/01_Introduction/#resource-division","title":"Resource Division","text":"<ol> <li>Frequency division</li> <li>Time division</li> <li>Code division</li> </ol>"},{"location":"3_Core/Computer_Networks/01_Introduction/#tdm","title":"TDM","text":"<p>Time Division Multiplexing</p> <p></p>"},{"location":"3_Core/Computer_Networks/01_Introduction/#subnet-mask","title":"Subnet Mask","text":"<p>This is the value to perform <code>and</code> operation</p> <p>To get the value, just make the network bits of the IP address as 1s and host bits as 0s</p>"},{"location":"3_Core/Computer_Networks/02_Layers/","title":"02 Layers","text":"<p>Task of moving information b/w computers over the network is divided into smaller and more manageable problems.</p> <p>Each problem is considered as a different layer in the network, which reduces complexity.</p> <p>Each layer</p> <ul> <li>provides service to layer above &amp; belo</li> <li>communicates with the same layer\u2019s software or hardware on other computer</li> </ul> <p>There are 2 network standards</p>"},{"location":"3_Core/Computer_Networks/02_Layers/#iso-osi-standard","title":"ISO OSI Standard","text":"<p>International Organization of Standardization-Open System Interconnection</p> <p>The upper 3 layers of the OSI model (application, presentation and session\u2014Layers 7, 6 and 5) are orientated more toward services to the applications</p> <p>Lower 4 layers (transport, network, data link and physical \u2014Layers 4, 3, 2, and 1) are concerned with the flow of data from end to end through the network.</p> <p></p> Type Layer Description PDU Device/Example Address Delivery Protocols TransmissionMode LineConfiguration ServiceType Logical Application Provides  network-access services to user Data/Page WhatsappBrowserMail client HTTPFTPSMTPSNMPDNSNFSTelnetDHCP Presentation Data/File formatData TranslationProtocol conversionSyntax &amp; SemanticsCompression/DecompressionEncryption/Decryption Data/Page SSLTLS Session Session creation, maintainence, terminationDialogue control &amp; synchronization b/w 2 end systemsToken ManagementPassword ValidationLogical connection requestSynchronization &amp; checkpointing of pages Data/Page PPTPSIPSAPNetBIOS Half-duplexFull-duplex Transport Ensuring reliable data exchange mechanismError control (only end-systems: source-dest)Flow controlConnection controlService point addressingSegmentation/Re-assembly into/from a packet Segment Port(identifies process/service) Process-to-Process TCPUDP Multiplex ConnectionlessConnection-oriented Hardware Network Inter-NetworkingRouting algoIP addressingCongestion handlingPacketizingFragmenting Packet/Datagram Router IP Host-to-Host IPv4, IPv6IPSecICMPIGPEGPOGHPRARPARP Data Link Ensuring reliable communication over physical layer\u2018Framing\u2019/ReassemblingError control (router &amp; end-system: source-dest + each hop)Error correction/handlingCorruption detection/correctionFlow control (pacing b/w adjacent sending &amp; receiving nodes)Access controlLAN formationPhysical addressing &amp; matching Frame BridgesSwitches MAC Hop-to-Hop Delivery ATMSLIPFrameRelayPPP SimplexHalf-DuplexFull-Duplex Point-to-PointBroadcast Physical Convert signal b/w digital &amp; analogEncryption &amp; decryptionRepresentation of bitsData rateSynchronization of bitsEncodingModulationLine ConfigurationTransmission mediumTransmission modeTopology Bitstream/Raw Data HubRepeater USBBluetooth Connection-Oriented(most reliable layer)"},{"location":"3_Core/Computer_Networks/02_Layers/#pdu","title":"PDU","text":"<p>Protocol data unit</p> <p>PDU\u2019s are used for peer-to-peer contact between corresponding layers</p>"},{"location":"3_Core/Computer_Networks/02_Layers/#packet","title":"Packet","text":"H3(Header) Data Source IP addressDestination IP address"},{"location":"3_Core/Computer_Networks/02_Layers/#frame","title":"Frame","text":"H2(Header of layer 2) Data T2(Trailer of layer 2) Source MAC AddressDestination MAC Address(found through Hop-to-Hop Delivery) Usually a parity"},{"location":"3_Core/Computer_Networks/02_Layers/#analogy","title":"Analogy","text":"<p>12 kids in Ann\u2019s house sending letters to 12 kids in Bill\u2019s house:</p> <ul> <li>hosts = houses</li> <li>processes = kids</li> <li>app messages = letters in envelopes</li> </ul> <p>transport protocol = Ann\u2019 multiplexing and Bill\u2019 demultiplexing to in-house siblings</p> <p>network-layer protocol = postal service</p>"},{"location":"3_Core/Computer_Networks/02_Layers/#tcpip","title":"TCP/IP","text":"<p>Transmission Control Protocol with inter-networking protocol</p> <ul> <li>Application</li> <li>Transport</li> <li>Network</li> <li>Data Link</li> <li>Physical</li> </ul>"},{"location":"3_Core/Computer_Networks/02_Layers/#osi-vs-tcpip","title":"OSI vs TCP/IP","text":"OSI TCP/IP No of Layers 7 5 Transport Layer Connection-oriented/Connection-less Connection-oriented/Connection-less Network layer Connection-oriented Connection-less Delivery model \u2018Best\u2019 \u2018Best-effort\u2019"},{"location":"3_Core/Computer_Networks/02_Layers/#addresses","title":"Addresses","text":"Address Size (in Bits) Denotion Example Separator Connect devicein ___ network Set during Fixed Administered by Portable Specific Port 16 Decimal 753(0-1024 are reserved) (none; it is a single no) IP/Logical/Host 32 Decimal 192.168.22.5 Dot different Connectionto network \u274c \u274c(address depends on connected IP subnet) MAC(Medium Access Control)/Ethernet/LAN/Physical/Link 4824 Vendor Code,24 Serial No) Hexadecimal AA.F0.C1.E2.77.51 Colon (Linux)Hyphen (Windows) same Devicemanufacture \u2705(usually burnt into NIC ROM;sometimes software-configurable) IEEE(Manufacturer buys portion of MAC address space for uniqueness) \u2705(LAN card can be moved) <ul> <li>MAC address is like Social Security Number</li> <li>IP address is like postal address</li> </ul>"},{"location":"3_Core/Computer_Networks/02_Layers/#idk","title":"idk","text":"<p>The physical addresses will change from hop to hop, but the logical and port addresses usually remain the same. Huh???</p>"},{"location":"3_Core/Computer_Networks/02_Layers/#ipv4","title":"IPv4","text":"Class Byte 1 (Decimal) Byte 1 (Binary) A 0-127 0\u2026 B 128-191 10\u2026 C 192-223 110\u2026 D 224-299 1110\u2026 E 240-255 1111\u2026 <p>Network ID is the first IP address, for eg: <code>10.0.0.0, 20.0.0.0</code>. This is used to refer to all devices in a network.</p> <p>Only end-devices and routers require IP address, as they belong to network layer.</p>"},{"location":"3_Core/Computer_Networks/02_Layers/#protocols","title":"Protocols","text":"Layer Protocol Full Form Details Network IP Internet Protocol Network ICMP Internet Control Message Protocol <code>ping</code> command uses this Network IGMP Internet Group Message Protocol Network +Data Link(Hybrid) ARP Address resolution protocol Convert ip address to mac address Network +Data Link(Hybrid) RARP Reversed Address resolution protocol Convert mac address to ip address(Only required when connecting to a network for the first time)"},{"location":"3_Core/Computer_Networks/03_Performance/","title":"03 Performance","text":""},{"location":"3_Core/Computer_Networks/03_Performance/#network-criteria","title":"Network Criteria","text":"<ul> <li>Fault Tolerance</li> <li>Scalability</li> <li>QoS (Quality of Service)</li> <li>High Throughput</li> <li>High Bandwidth</li> <li>Low Latency</li> <li>Security</li> </ul>"},{"location":"3_Core/Computer_Networks/03_Performance/#performance-criteria","title":"Performance Criteria","text":"Bandwidth Max number of bits transferrable per unit time(In analog world, it is the range of accepted frequencies) Throughput Actual number of bits transferred per unit time Latency/Delay Duration to send info &amp; its earliest possible reception End-to-EndDelay Duration to transmit packet along its entire path- Created by application- Handed over to OS- Passed to NIC- Encoded, transmitted over a physical medium- Received by intermediate device (switch, router)- Analyzed, retransmitted over another medium, etc. Round-Trip-Time Duration to send and receive acknowledge"},{"location":"3_Core/Computer_Networks/03_Performance/#types-of-delays","title":"Types of Delays","text":"Delay Duration of Formula Transmission Placing bits onto transmission mediaum \\(\\frac{\\text{Size}}{\\text{Bandwidth}}\\) Propagation Travel for a bit from one end of medium to other \\(\\frac{\\text{Distance}}{\\text{Speed}}\\) Processing Error verificationRouting decision, ie- analyze packet header- decide where to send packet No of entries inrouting tableImplementatio of data structuresHardware specs Buffer/Queuing Packet to wait until it is transmitted Traffic intensityType of traffic <p>Latency = \\(\\sum\\) all the above delays</p>"},{"location":"3_Core/Computer_Networks/03_Performance/#mediums","title":"Mediums","text":"Medium Speed\u00a0(m/s) Vacuum \\(3 \\times 10^8\\) Cable \\(2.3 \\times 10^8\\) Fiber \\(2 \\times 10^8\\)"},{"location":"3_Core/Computer_Networks/04_Data_Link_Layer/","title":"04 Data Link Layer","text":"<p>It is a combination of hardware, software, and firmware (software for hardware)</p> <p>It is implemented in NIC and attaches into host\u2019s system buses</p>"},{"location":"3_Core/Computer_Networks/04_Data_Link_Layer/#sublayers","title":"Sublayers","text":"<p>The data link may be further divided into sublayers, which is explained in detail in Ethernet</p>"},{"location":"3_Core/Computer_Networks/04_Data_Link_Layer/#flow-control","title":"Flow Control","text":"<p>Handles mismatch b/w sender\u2019s and receiver\u2019s speed</p> Control Method Type Meaning Feedback-Based(More common) Explicit Permission required from receiver Rate-Based Implicit Limit sending rate"},{"location":"3_Core/Computer_Networks/04_Data_Link_Layer/#error-types","title":"Error Types","text":"Type No of Bits Consecutive Bits? Single-Bit 1 Multiple-Bit &gt;1 \u274c Burst &gt;1 \u2705"},{"location":"3_Core/Computer_Networks/04_Data_Link_Layer/#error-control","title":"Error Control","text":"Error detection codes Detect error Error/Forwardcorrection codes(FEC) Detect &amp; correct errorUse in wireless networks Retransmission/Automatic Repeat Request(ARQ) Used along with error detection/correctionBlock of data with error discardedTransmitter retransmits that block of data"},{"location":"3_Core/Computer_Networks/04_Data_Link_Layer/#redundancy","title":"Redundancy","text":"<p>Redundant bits added to data to detect &amp; correct errors</p> <pre><code>flowchart LR\n\nsubgraph s[Sender's Encoder]\nm1[Message] --&gt;\nGenerator --&gt;\na[Message &amp;&lt;br/&gt;Redundancy]\nend\n\nsubgraph r[Receiver's Decoder]\nd[Received&lt;br/&gt;Data] --&gt;\nc[Checker] --&gt;\n|Accept| m2[Message]\nend\n\na --&gt;\n|Unreliable&lt;br/&gt;Transmission| d\n\nc --&gt; |Discard| Lost</code></pre>"},{"location":"3_Core/Computer_Networks/04_Data_Link_Layer/#coding","title":"Coding","text":"<p>Process of adding redundancy for error detection/correction</p> <p>Error-detecting code can detect \u2028only types of errors for which it is designed; other types of errors may remain undetected. There is no way to detect every possible error</p> Code Steps Redundant bits Total bits\\(n\\) Memoryless? Block Divide data into set of \\(k\\)-bit blocks (called datawords) Extra info attached to each blockCombined blocks called codewords \\(r\\) \\(k+r\\) \u2705 Convolutional Treats data a series of bitsComputes code over continuous series \u274c(Code depends on current &amp; previous i/p) <p></p> <pre><code>flowchart TB\n\nd1[\"Dataword&lt;br/&gt;a3 a2 a1 a0&lt;br/&gt;&lt;br/&gt;(k bits)\"] --&gt;\nc1 &amp; g[\"Generator&lt;br/&gt;(r bits)\"]\n\ng --&gt;\nc1[\"Codeword&lt;br/&gt;a3 a2 a1 a0 &lt;span style='color:red'&gt;p0&lt;/span&gt;&lt;br/&gt;&lt;br/&gt;(n bits)\"]</code></pre>"},{"location":"3_Core/Computer_Networks/04_Data_Link_Layer/#code-rate","title":"Code Rate","text":"\\[ = \\frac{k}{n} \\] Code Rate \\(\\implies\\) Error Correcting Capability Bandwidth Efficiency \\(\\uparrow\\) \\(\\downarrow\\) \\(\\uparrow\\) \\(\\downarrow\\) \\(\\uparrow\\) \\(\\downarrow\\)"},{"location":"3_Core/Computer_Networks/04_Data_Link_Layer/#error-detection-methods","title":"Error Detection Methods","text":"<p>If syndrome = 0 at the receiver, there is no error</p> Simple parity check Horizontal &amp; VerticalParity check CRC(Cyclic Redundancy Check) Checksum Use an odd/even parity bit Use parity bit vertically and horizontally Add \\(r\\) zeros to right of dividend, where \\(r=\\)no of redundant bits = length of divisor - 1Long division using XOR (used in network layer)Find sum of digitsIf overflow, perform paddingTake 1s complement Errors detectable \\(\\{1, 3, \\dots, 2n+1 \\}\\)(odd no of errors) \\(\\{1, 2, 3, 5, 6, 7, \\dots \\} \\implies R - \\{4n\\}\\) All All Can correct error? \u274c(error can be in any positionincluding parity bit itself)"},{"location":"3_Core/Computer_Networks/04_Data_Link_Layer/#simple-parity","title":"Simple Parity","text":"Parity Parity bit = 0 means dataword has Odd Odd number of ones Even Even number of ones"},{"location":"3_Core/Computer_Networks/04_Data_Link_Layer/#mac-layer-throughput","title":"Mac Layer Throughput","text":"<p>Number of bits sent by MAC (Data Link) layer in given period of time</p> \\[ \\begin{aligned} \\text{Throughput} = \\frac{\\text{Payload}}{\\text{Total Time}} \\end{aligned} \\]"},{"location":"3_Core/Computer_Networks/04_Data_Link_Layer/#control-frame","title":"Control Frame","text":"<p>Frames that only contain headers/trailers, and no payload</p>"},{"location":"3_Core/Computer_Networks/05_Multiple_Access/","title":"05 Multiple Access","text":""},{"location":"3_Core/Computer_Networks/05_Multiple_Access/#access-protocols","title":"Access Protocols","text":"Random-Access/Contention Controlled-Access Channelization No station is superior to anotherNo station permits another station to send at the same timeNode with packet transmits at full channel data rateAll transmission on shared channel Collisions Moderate Little-to-none Throughput for smaller networks Low High Throughput for larger networks High Low Easy to maintain? \u2705 \u274c Commonly-used? \u2705 \u274c(Hard to control large networks) Example ALOHACSMACSMA/CDCSMA/CA ReservationPollingToken-Passing FDMATDMACDMA"},{"location":"3_Core/Computer_Networks/05_Multiple_Access/#collision","title":"Collision","text":"<p>When 2 nodes transmit concurrently</p>"},{"location":"3_Core/Computer_Networks/05_Multiple_Access/#carrier-sensing","title":"Carrier-Sensing","text":"<p>When the energy level is higher than usual, that means that there is a collision</p> <p></p> <p>However, this method may not suitable for wireless transmission, due to energy loss.</p>"},{"location":"3_Core/Computer_Networks/05_Multiple_Access/#persistence-methods","title":"Persistence Methods","text":"1-persistent Non-persistent \\(p\\) - persistent Default persistent method Probabilistic mixture of 1-persistent &amp; non-persistentAssume channels are slottedOne slot = contention period (one RTT)Used when time slot duration \\(\\ge\\) max \\(T_P\\) Steps 1. Sense channel2. if idle, transmit immediately3. If busy, keep listening 1. Sense channel2. If idle, transmit immediately3. If busy, wait random amount of time and sense channel - When station ready to send, it senses the channel- If channel is idle, transmits with probability \\(pp\\)- If channel is busy, station waits until next slot.- With probability \\(q=l-p\\), the station then waits for beginning of next slot - If next slot also idle, either transmit/wait again with probabilities \\(pp\\) &amp; \\(q\\) - Process repeated till either frame transmitted/another station starts transmitting- If another station  transmits, station waits random amount of time &amp; starts again If collision occurs Wait ranom amount of time &amp; start over Wait random amount of time &amp; start over Diagram"},{"location":"3_Core/Computer_Networks/06_Random_Access/","title":"06 Random Access","text":"<p>Let</p> Symbol Meaning \\(T_\\text{fr}\\) Time to transmit a frame \\(T_p\\) Propagation Delay \\(G\\) Average no of frames requested per frame-time \\(S\\) Throughput(Number of packets successfully transmitted per packet time) \\(V\\) Vulnerable TimeTime bracket for potential collision"},{"location":"3_Core/Computer_Networks/06_Random_Access/#protocols","title":"Protocols","text":"Pure ALOHA Slotted ALOHA CSMA CSMA/CD CSMA/CA IDK Carrier Sense Multiple AccessListen before transmissionNode does not send if another node already sendingUses persistence methods Collision DetectionListen to channel while packet being sentNode stops sending if \\(\\exists\\) interference Assumptions Stations trying to transmit follow Poisson Distribution All frames are of same sizeTime divided into equal slots (time to transmit a frame)Nodes start transmission only at start of slotIf 2/more nodes transmit, all nodes detect collision Constant length packetsNo errors, except ones caused by collisionsEach host can sense transmissions of all other hostsPropagation delay is small compared to transmission delay 1. Check line is quiet2. Detect collision ASAP3. If collision detected, stop transmission; wait random time and start over Preferred for Wired Networks Wired Networks (not used; always used with CD or CA) Slow Wired Networks(since efficiency reduces for faster networks; as bandwidth increases, collisions increase) Wireless Networks (WLAN)(since all signals are broadcasted and collisions cannot be detected) MinimumFrameLength Frame length such that \\(T_t &gt; 2 \\times T_P\\) \\(V\\) \\(2 \\times T_\\text{fr}\\) \\(T_\\text{fr}\\) \\(T_P\\) \\(S\\) \\(G \\times e^{-2G}\\) \\(G \\times e^{-G}\\) \\(G_\\text{max}\\) \u00bd 1 \\(S_\\text{max}\\) 0.184 0.368 Flowchart"},{"location":"3_Core/Computer_Networks/06_Random_Access/#csmacd","title":"CSMA/CD","text":"\\[ \\begin{aligned} B &amp;= \\frac{\\text{PD}}{\\text{TD}} \\\\ &amp;= \\frac{ \\frac{\\text{Distance}}{\\text{Speed}} }{ \\frac{\\text{Data Size}}{\\text{Bandwidth}} }\\\\ &amp;= \\frac{\\text{Distance} \\times \\text{Bandwidth}}{\\text{Speed} \\times \\text{Data Size}} \\end{aligned} \\] \\[ \\begin{aligned} \\text{Throughput } E &amp;= \\frac{1}{1+kB} &amp; (k \\in [1, 10]) \\\\ &amp;= \\frac{1}{1+k \\left(  \\frac{\\text{Distance} \\times \\text{Bandwidth}}{\\text{Speed} \\times \\text{Data Size}} \\right)} \\\\ \\implies E &amp;\\propto \\frac{1}{\\text{Bandwidth}} \\end{aligned} \\]"},{"location":"3_Core/Computer_Networks/06_Random_Access/#csmaca","title":"CSMA/CA","text":"\\[ \\begin{aligned} \\text{Maximize Size of contention window} &amp;= 15 \\times \\text{RTT} \\\\ &amp;= 30 \\times T_P \\end{aligned} \\]"},{"location":"3_Core/Computer_Networks/06_Random_Access/#dcf","title":"DCF","text":"<p>Distributed Coordination Function</p> <p>DCF sublayer uses CSMA </p> <ul> <li> <p>if station has frame to send, it listens to medium</p> </li> <li> <p>if medium idle, station may transmit</p> </li> <li>else waits until current transmission complete </li> </ul> <p>No collision detection possible due to wireless network</p> <p>DCF includes delays that act as a priority scheme</p> <p>Combination of</p> Full Form Contains CSMA/CA RTS Request/Ready to Send Duration required for channel CTS Clear to Send MAC address NAV Network Allocation Vector DIFS Domething InterFrame Space SIFS Something InterFrame Space"},{"location":"3_Core/Computer_Networks/06_Random_Access/#steps","title":"Steps","text":"<p>When a station wants to transmit data</p> <ul> <li> <p>It sends an RTS packet to the intended receiver</p> </li> <li> <p>The RTS packet contains the length of the data that needs to be transmitted</p> </li> <li> <p>Any station other than the intended recipient hearing RTS defers transmission for a time duration equal to the end of the corresponding CTS reception</p> </li> <li>The receiver sends back CTS packet back to sender if it is available to receive</li> <li> <p>The CTS packet contains the length of the data that original sender wants to transmit </p> </li> <li> <p>Any station other than the original RTS sender, hearing CTS defers transmission until the data is sent. </p> </li> <li>The original sender upon reception of the CTS, starts transmitting. </li> </ul>"},{"location":"3_Core/Computer_Networks/06_Random_Access/#flowchart","title":"Flowchart","text":""},{"location":"3_Core/Computer_Networks/06_Random_Access/#timeline-diagram","title":"Timeline Diagram","text":"Vertical Format Horizontal Format"},{"location":"3_Core/Computer_Networks/06_Random_Access/#wireless-channel-problems","title":"Wireless Channel Problems","text":"Hidden Terminal Problem Exposed Terminal Problem Description Two nodes hidden from each other transmit complete frames to base station Disadvantage Wasted bandwidth for long duration Solution Small reservation packets: RTS+CTSNodes track reservation interval with internal NAV Diagram"},{"location":"3_Core/Computer_Networks/07_Controlled_Access/","title":"07 Controlled Access","text":"<p>(please complete from slides)</p>"},{"location":"3_Core/Computer_Networks/07_Controlled_Access/#reservation-method","title":"Reservation Method","text":"<p>Priority is given to each station</p> <p>Reservation frames are used by stations to \u2018reserve access\u2019. Size of reservation frame (in bits) will be equal to number of stations in the network.</p> <ul> <li>Stations take turns transmitting a single frame at a full rate \\(R\\) bps</li> <li>Transmissions are organized into variable length cycles</li> <li>Each cycle begins with a reservation interval that consists of \\(N\\) minislots. One minislot for each of the \\(N\\) stations.</li> <li>When a station needs to send a data frame, it makes a reservation in its own minislot.</li> <li>By listening to the reservation interval, every station knows which stations will transfer frames, and in which order.</li> <li>The stations that made reservations can send their data frames after the reservation frame</li> </ul> <p></p>"},{"location":"3_Core/Computer_Networks/07_Controlled_Access/#polling","title":"Polling","text":"Primary Station Secondary Station Example Servers Clients Permission to select \u2705 \u274c <p>Polling by primary station keeps switching between secondary stations at a certain polling rate.</p> <p>Stations take turns accessing the medium</p>"},{"location":"3_Core/Computer_Networks/07_Controlled_Access/#types","title":"Types","text":"Centralized Polling Distributed Polling - One device is assigned as primary station and the others as secondary stations- All data exchanges are done through the primary- When the primary has a frame to send it sends a select frame that includes the address of the intended secondary- When the primary is ready to receive data it send a Poll frame for each device to ask if it has data to send or not. If yes, data will be transmitted otherwise NAK (Negative AcKnowledgement)\u00a0is sent.- Polling can be done in order (Round-Robin) or based on predetermined order - No primary and secondary- Stations have a known polling order list which is made based on some protocol- station with the highest priority will have the access right first, then it passes the access right to the next station (it will send a pulling message to the next station in the pulling list), which will passes the access right to the following next station, \u2026"},{"location":"3_Core/Computer_Networks/07_Controlled_Access/#token-passing","title":"Token-Passing","text":"<p>Requires ring topology</p> <pre><code>flowchart TB\nstart([Start]) --&gt;\ntoken[\"Wait for&lt;br/&gt;Token\"] --&gt;\nct[/\"Capture&lt;br/&gt;Token\"/] --&gt;\ndfts{\"Data frame&lt;br/&gt;to send?\"}\n\ndfts --&gt;\n|Yes| send[/\"Send&lt;br/&gt;Frame\"/] --&gt;\nate{\"Allocated Time&lt;br/&gt;Expired\"} --&gt;\n|No| dfts\n\ndfts --&gt; |No| e\nate --&gt; |Yes| e\n\n\ne((\" \")) --&gt;\nrt[\"Release Token&lt;br/&gt;(Next station takes token)\"] --&gt;\nstop([Stop])</code></pre>"},{"location":"3_Core/Computer_Networks/08_Ethernet/","title":"08 Ethernet","text":"<p>Protocol for connecting multiple computer systems to form a LAN, with protocols to</p> <ul> <li>control passing of information</li> <li>avoid simultaneous transmission by multiple systems</li> </ul>"},{"location":"3_Core/Computer_Networks/08_Ethernet/#ieee-8023-data-link-layer-sublayers","title":"IEEE 802.3 Data Link Layer Sublayers","text":"Datalink Sublayer Tasks Name of frame Implementation Protocol LLC(Logical Link Control) Error ControlFlow ControlInterconnectivity b/w data link layer of different LANsMultiplex multiple network layer protocols in frame IEEE 802.3 Software CRC (error-correction)ARQ MAC(Medium Access Control) FramingMAC AddressingMedium Access Control IEEE 802.2 Hardware Token-Passing (Wired Token Ring)CSMA/CD (Wired other)CSMA/CA with NAV (Wireless)"},{"location":"3_Core/Computer_Networks/08_Ethernet/#diagram","title":"Diagram","text":""},{"location":"3_Core/Computer_Networks/08_Ethernet/#domains","title":"Domains","text":"Domain Associated with Number Broadcast Router Connection No of switches connected to router Collision Switch Connection No of half-duplex links connected to switch Collision occurs as switches are not as intelligent as routers"},{"location":"3_Core/Computer_Networks/08_Ethernet/#topology","title":"Topology","text":"Bus Star Active switch in center Collision domain All nodes in same collision domain Each spoke runs separate Ethernet protocol Collisions Prevented? \u274c \u2705"},{"location":"3_Core/Computer_Networks/08_Ethernet/#normal-ethernet-operation","title":"Normal Ethernet Operation","text":"Receiver receives frame with Matching destination address Data sent to network layer Broadcast address (e.g. ARP packet) Data sent to network layer Neither of the above Discard frame"},{"location":"3_Core/Computer_Networks/08_Ethernet/#types","title":"Types","text":"Type Speed Connection Reliable? Chance ofdropping frames AccessProtocol Standard 10 Mbps Connectionless \u274c High CSMA/CD Fast 100 Mbps Gigabit 1 Gbps Ten-Gigabit 10 Gbps"},{"location":"3_Core/Computer_Networks/08_Ethernet/#standard-ethernet-implementations","title":"Standard Ethernet Implementations","text":"Implementation Topology Transmission Medium 10Base5 Bus Thick coaxial 10Base2 Bus Thin coaxial 10Base-T Star UTP(Unshielded-Twisted-Pair) 10Base-F Star Fiber"},{"location":"3_Core/Computer_Networks/08_Ethernet/#steps-of-routing-to-another-lan","title":"Steps of Routing to another LAN","text":"<p>Assuming A has all the required addresses already, and wants to send a message to B via R.</p> <ol> <li>Create packet in Network Layer with</li> <li>Source address = A\u2019s IP address</li> <li>Destination address = B\u2019s IP address</li> <li>Create frame in Datalink Layer with </li> <li>Source address = A\u2019s MAC address</li> <li>Destination address = R\u2019s receiving terminal MAC address</li> <li>A sends message to R</li> <li>R receives message</li> <li>R processes and removes frame in Datalink Layer</li> <li>R processes packet in Network Layer</li> <li>R forwards packet with the same source and destination as before in the Network layer</li> <li>R creates frame in the Network layer with</li> <li>Source address = R\u2019s sending terminal MAC address</li> <li>Destination address = B\u2019s MAC address</li> </ol>"},{"location":"3_Core/Computer_Networks/08_Ethernet/#ethernet-switch","title":"Ethernet Switch","text":"<ul> <li>Examines incoming frame\u2019s MAC address</li> <li>Selectively forwards frame to one/more outgoing links when frame is to be forwarded on segment</li> <li>Uses CSMA/CD to access segment</li> <li>Buffers packets</li> </ul> <p>Every host has dedicated &amp; direction connection to switch</p> <p>Each link connected to switch is its own collision domain; hosts transmitting simultaneously does not affect other transmissions if they are on different link.</p>"},{"location":"3_Core/Computer_Networks/08_Ethernet/#characteristics","title":"Characteristics","text":"<ul> <li>Transparent: Hosts are unaware of presence of switches</li> <li>Plug-and-Play device: No configuration required by network admin</li> <li>Self-Learning mechanism</li> </ul>"},{"location":"3_Core/Computer_Networks/08_Ethernet/#types_1","title":"Types","text":"Cut-through Store-and-forward switch Begins forwarding dataafter examining only first part of header entire data Retransmission Time \\(&lt; T_t\\) \\(= T_t\\)"},{"location":"3_Core/Computer_Networks/08_Ethernet/#switch-table","title":"Switch Table","text":"<p>Helps switch data from source to destination</p> Host MAC Address Interface to reach host TTL"},{"location":"3_Core/Computer_Networks/08_Ethernet/#self-learning","title":"Self-Learning","text":"<ol> <li> <p>Check if receiver exists in switch table</p> </li> <li> <p>If yes, go to step 5</p> </li> <li> <p>\u2018Flood\u2019 (broadcast) message to all stations</p> </li> <li> <p>Update switch table with receiver\u2019s entry</p> </li> <li>Send to receiver</li> </ol>"},{"location":"3_Core/Computer_Networks/08_Ethernet/#interconnected-switches","title":"Interconnected Switches","text":"<p>Works using the same self-learning process</p> <p></p>"},{"location":"3_Core/Computer_Networks/08_Ethernet/#switches-vs-router","title":"Switches vs Router","text":"Switch Router Store &amp; Forward? \u2705 \u2705 Layer Data Link Network Examine Data link layer headers Network layer headers Understand addresses MAC IP Forwarding Table? \u2705 \u2705 Learning Method Flooding learning Routing algorithms"},{"location":"3_Core/Computer_Networks/08_Ethernet/#vlan","title":"VLAN","text":"<p>Virtual Local Area Network</p> <p>Allows us to divide a LAN without any additional switches</p> <p>VLAN can be defined using one of the following techniques</p> <ul> <li>Switch port</li> <li>MAC addresses of endpoints</li> </ul>"},{"location":"3_Core/Computer_Networks/08_Ethernet/#advantages","title":"Advantages","text":"<p>VLAN helps overcome the following</p> <ul> <li>Improve traffic isolation: frames by default can only travel within their own VLAN</li> <li>Dynamic membership: ports can be dynamically assigned among VLANs</li> <li>Efficient use of switches</li> <li>Management of users</li> <li>Forwarding between VLANS</li> <li>Address Security, privacy and efficiency issues. Data link layer broadcast traffic (ARP, DHCP, unknown location of destination MAC address) need not cross entire LAN.</li> </ul>"},{"location":"3_Core/Computer_Networks/08_Ethernet/#port-based-vlan","title":"Port-Based VLAN","text":"Details Trunk Port connected to routerTraffic isolationsomething else No of usable ports \\(n-1\\)(Trunk port unusable) Actual connections Behaves as"},{"location":"3_Core/Computer_Networks/08_Ethernet/#trunk-port","title":"Trunk Port","text":"<p>Carries frames between VLANS defined over multiple physical switches. Frames forwarded over multiple switches must carry VLAN ID info as well, and hence uses IEEE 802.1Q Frame.</p> <p></p>"},{"location":"3_Core/Computer_Networks/08_Ethernet/#ethernetieee-8021-frame","title":"Ethernet/IEEE 802.1 Frame","text":"<p>All sizes shown in Bytes</p> Size Minimum 64 Bytes Maximum 1518 Bytes Preamble SFD(Start Frame Delimiter) Dest MACAddress Source MACAddress Type Payload(Data &amp; Padding) CRC 7 Bytes 1 byte 6 Bytes 6 Bytes 2 Bytes \\([46, 1500]\\) Bytes 4 Bytes Alternating 1/01010\u20261010 10101010**11** Type of Data Cyclic Redundancy Check Part of physical layer header(Processed at physical layer) Part of physical layer header(Processed at physical layer) 0800 \u2013&gt; IPv40806 \u2013&gt; ARP Frame8100 \u2013&gt; IEEE 802.1Q Frame86DD \u2013&gt; IPv6 Error -&gt; Frame dropped Synchronizes sender &amp; receiver clock rates Signals the beginning of frame"},{"location":"3_Core/Computer_Networks/08_Ethernet/#example-of-multiple-frames","title":"Example of multiple frames","text":""},{"location":"3_Core/Computer_Networks/08_Ethernet/#receiver-address-type","title":"Receiver Address Type","text":"Type Receiver Address Value Unicast LSB of first byte = 0 Multicast LSB of first byte = 1 Broadcast All bits are 1 <p>LSB = Least Significant Bit</p>"},{"location":"3_Core/Computer_Networks/08_Ethernet/#ieee-8021q-frame","title":"IEEE 802.1Q Frame","text":"<p>Adds/removes additional header fields for frames forwarded between trunk ports</p> <p>(Empty cells of the following table means that they are the same as regular Ethernet frame)</p> Preamble SFD(Start Frame Delimiter) DestAddress SourceAdd Tag Protocol Identifier Tag Control Info Type Data &amp; Padding CRC 2B 12bits VLAN ID field3bits field like IP TOS Recomputed CRC 81-100 <p></p>"},{"location":"3_Core/Computer_Networks/08_Ethernet/#vlan-frame","title":"VLAN Frame","text":"Preamble SFD(Start Frame Delimiter) Dest MACAddress Source MACAddress Tag Type Payload(Data &amp; Padding) CRC <p>Tag is </p>"},{"location":"3_Core/Computer_Networks/09_ARP/","title":"09 ARP","text":"<p>Address Resolution Protocol</p> <p>Obtain MAC Address from IP address</p> <p>It is \u2018plug-and-play\u2019, as nodes create their ARP tables w/o intervention from network admin</p>"},{"location":"3_Core/Computer_Networks/09_ARP/#arp-procedure","title":"ARP Procedure","text":"<p>Let\u2019s say node A wants to send message to node B</p> <ol> <li>Check ARP Table for B\u2019s MAC address using IP address.</li> <li>If B found, A sends message to B. Stop</li> <li>Else, A broadcasts ARP query packet, containing B's IP address</li> <li>dest MAC address = \\(FF \\ FF \\ FF \\ FF \\ FF \\ FF\\)</li> <li>all nodes on LAN receive ARP query</li> <li>B receives ARP packet, replies to A with its (B's) MAC address    frame sent to A\u2019s MAC address (unicast)</li> <li>A adds B\u2019s mac address to its ARP table</li> <li>Go to step 1</li> </ol> ARP Message Type Type ARP Request Broadcast ARP Reply Unicast"},{"location":"3_Core/Computer_Networks/09_ARP/#arp-table","title":"ARP Table","text":"<p>Each IP node on LAN has ARP table, which contains</p> IP Address MAC Address TTL Time To LiveTime after which address mapping will be forgottenUsually 20min"},{"location":"3_Core/Computer_Networks/09_ARP/#arp-packet","title":"ARP Packet","text":"<p>Destination address of the frame will be all 1s, as this will be broadcast</p> <p></p> <p></p> Meaning Value Size Hardware Type Type of network on which ARP is running Ethernet -&gt; 1 16 bits Hardware Length Length of physical address in bytes Ethernet -&gt; 6 8 bits Protocol Length Length of logical address in bytes 8 bits Protocol Type Type of IP used IPv4 -&gt; \\(0800_H\\) 16 bits Operation Type of ARP Packet Request -&gt; 1Reply -&gt; 2 16 bits Sender MAC address 6 bytes for ethernet Sender IP address 4 bytes for IP Target MAC address Request (all 0s as MAC address unknown)Reply (returned MAC address) 6 bytes for ethernet Target IP address 4 bytes for IP"},{"location":"3_Core/Computer_Networks/09_ARP/#target-ip-address","title":"Target IP address","text":"Sender Receiver Network Value of Target IP address Host Host Same Same as destination IP address in IP packet Host Host Different IP address of router Router Host Same Same as destination IP address in IP packet Router Host Different IP address of router"},{"location":"3_Core/Computer_Networks/10_Network_Layer/","title":"10 Network Layer","text":""},{"location":"3_Core/Computer_Networks/10_Network_Layer/#summary","title":"Summary","text":""},{"location":"3_Core/Computer_Networks/10_Network_Layer/#network-layer-source","title":"Network Layer @ Source","text":""},{"location":"3_Core/Computer_Networks/10_Network_Layer/#network-layer-router","title":"Network Layer @ Router","text":""},{"location":"3_Core/Computer_Networks/10_Network_Layer/#network-layer-destination","title":"Network Layer @ Destination","text":""},{"location":"3_Core/Computer_Networks/10_Network_Layer/#packetization","title":"Packetization","text":"<p>Each packet contains a portion of user data plus some control info (routing info, etc)</p> <p>Size of packet determined by network and its governing protocol</p> <p>Packets are received, buffered (stored briefly), then and past on to the next node. This is called as \u2018Store-and-forward\u2019</p>"},{"location":"3_Core/Computer_Networks/10_Network_Layer/#packet-switching-methods","title":"Packet-Switching Methods","text":"<p>Context already given in Introduction</p> Datagram Approach Virtual-Circuit Approach Connection Type Connection-less Connection-oriented Each packet contains Destination address Virtual-circuit ID(implemented in Data Link Layer) Efficiency Better than circuit switched network(channel is always occupied) Delay &gt; Circuit Switching &amp; Virtual-Circuit Switch keeps info about connection state \u274c Each packet is treated independently dependently Path Each node chooses the next node Same path as previous packet of same packet stream(Node does not make any routing decision) Packets can take any possible route based on the link availability \u2705 \u274c Capacity Guaranteed \u2705 \u274c Packets always arrive in order \u274c \u2705(packets  may arrive with different delays  if resource allocation is on demand) Packets not lost \u274c(due to lack of resources; upper layers ask for retransmission) Re-order packets and recover from missing packets Receiver Example The Internet X.25, Frame Relay, ATM Routing Table Single Channel Multiple Channels Delays \\(3 T + 3 \\tau + w_1 + w_2\\) \\(\\text{delay}_\\text{tot} = \\text{delay}_\\text{trans} + \\text{delay}_\\text{prop} + \\text{setup delay} + \\text{teardown delay}\\)"},{"location":"3_Core/Computer_Networks/10_Network_Layer/#ip-addressing","title":"IP Addressing","text":"<p>Each IP address is unique and only defines 1 connection to the Internet. Two devices on the internet can never have the same address at the same time. (referring to IP Public addresses)</p>"},{"location":"3_Core/Computer_Networks/10_Network_Layer/#types-of-addresses","title":"Types of Addresses","text":"Network Address Host Address"},{"location":"3_Core/Computer_Networks/10_Network_Layer/#types-of-addressing","title":"Types of Addressing","text":"Classful Classless Entire range of IP addresses is classified into different classesEach class is divided into a fixed number of blocks with fixed size Variable length blocks No of blocks must be power of 2 Beginning address must be divisible by no of addresses(If block has less than 256 addresses, we need to check only the rightmost byte)(If block has less than 65,536 addresses, we need to check only the two rightmost bytes, and so on.) Inflexible Inefficient(wasted IP addresses)"},{"location":"3_Core/Computer_Networks/10_Network_Layer/#classful-addressing","title":"Classful Addressing","text":""},{"location":"3_Core/Computer_Networks/10_Network_Layer/#classes","title":"Classes","text":"A B C D E Network &amp; Host Part N.H.H.H N.N.H.H N.N.N.H -(Not for commercial use) -(Not for commercial use; only for experimentation) Starting bit(s) \\(0\\) \\(10\\) \\(110\\) \\(1110\\) \\(1111\\) Range Start \\(1.0.0.0\\) \\(128.0.0.0\\) \\(192.0.0.0\\) \\(224.0.0.0\\) \\(240.0.0.0\\) Range End \\(127.255.255.255\\) \\(191.255.255.255\\) \\(223.255.255.255\\) \\(239.255.255.255\\) \\(255.255.255.255\\) Casting Uni-Cast Uni-Cast Uni-Cast Multi-Cast Default Mask(number to AND IP address with to get netid) \\(255.0.0.0\\) \\(255.255.0.0\\) \\(255.255.255.0\\) No of Network Blocks \\(2^{8-1} - 1\\)(1 bit for class)(1 block for private address) \\(2^{16-2}-16\\)(2 bits for class)(16 blocks for private address) \\(2^{24-3}-256\\)(3 bits for class)(256 blocks for private address) 1 No of Hosts \\(2^{24} \\textcolor{hotpink}{-2}\\) \\(2^{16} \\textcolor{hotpink}{-2}\\) \\(2^{8} \\textcolor{hotpink}{-2}\\) Subnetting Possible? \u2705 \u2705 \u274c Supernetting Possible? \u274c \u274c \u2705 <ul> <li> <p>\\(\\textcolor{hotpink}{-2}\\) is because</p> </li> <li> <p>Network address: all host fields = 0</p> </li> <li>Network broadcast address: all host fields = 1</li> <li>\\(0.0.0.0 \\iff 0.255.255.255\\) is a special block not belonging to any class</li> <li>\\(127.0.0.0 \\iff 127.255.255.255\\) is a special block belonging to class A, but it is reserved for loopback(<code>localhost</code>): the computer to refer to itself</li> </ul> <p>The outside world recognizes the network via network address, not the individual host-IPs</p>"},{"location":"3_Core/Computer_Networks/10_Network_Layer/#categories-of-addresses","title":"Categories of Addresses","text":"<p>Packet with loopback address does not leave the device (will not reach the network)</p>"},{"location":"3_Core/Computer_Networks/10_Network_Layer/#private-addresses","title":"Private Addresses","text":"<p>Assigned for private use and not recognized globally, used in</p> <ul> <li>Isolation</li> <li>Connection with network address translation</li> </ul> <p></p>"},{"location":"3_Core/Computer_Networks/10_Network_Layer/#modified-networks","title":"Modified Networks","text":"Subnetting Supernetting Split large network into smaller networks Combine smaller network into 1 larger network Subnet Address borrows bits from hostid netid Rules No of modified nets must be power of 2 No of modified nets must be power of 2 Blocks must be contiguous in address  space 3<sup>rd</sup> byte of first address in supernet must be evenly divisible by number of blocks of supernet"},{"location":"3_Core/Computer_Networks/10_Network_Layer/#subnetting","title":"Subnetting","text":"<p>Network is divided into several smaller groups, with their own subnet address depending on the hierarchy of subnetting, but still appearing as a single network to the rest of the Internet</p> <p>Hierarchy changes from <code>netid:hostid</code> to <code>netid:subnetid:hostid</code></p> <p>Only the network administrator knows about the network address and subnet address but router does not</p> <p>2 routers</p> <ul> <li>External router has routing table based on network addresses</li> <li>Internal router has routing table based on subnetwork addresses.</li> </ul>"},{"location":"3_Core/Computer_Networks/10_Network_Layer/#classless-addressing","title":"Classless Addressing","text":""},{"location":"3_Core/Computer_Networks/10_Network_Layer/#slash-notation","title":"Slash Notation","text":"<p>Also called as CIDR (classless inter-domain routing) Notation</p> <p></p> <p>Where \\(n=\\) no of 1s in mask (starting from the left side)</p>"},{"location":"3_Core/Computer_Networks/10_Network_Layer/#routing","title":"Routing","text":"<p>Determine the most optimal path for packet to take from source to destination</p> <p>Only possible to pick most optimality with global knowledge about network</p>"},{"location":"3_Core/Computer_Networks/10_Network_Layer/#objective","title":"Objective","text":"<ul> <li>Minimize number of hops</li> <li>Minimize end-to-end delay</li> <li>Maximize available bandwidth</li> </ul>"},{"location":"3_Core/Computer_Networks/10_Network_Layer/#criteria-for-routing-algo","title":"Criteria for Routing Algo","text":"<ol> <li>Correctness: correct route and accurate delivery</li> <li>Robustness: adaptive to changes of network topology, in case of node/link failure &amp; varying traffic load</li> <li>Cleverness: ability to detour congestion links &amp; determine connectivity of network</li> <li>Efficiency: rapid finding of route &amp; minimization of control messages</li> </ol>"},{"location":"3_Core/Computer_Networks/10_Network_Layer/#classification-of-routing-algo","title":"Classification of Routing Algo","text":"Static Dynamic Compute route Manually Automatic When Prior During Based on TopologyLink Capacity Life of routing table entry Long Variable Change of routing table entry Fixed Variable Advantage Simple Adaptive Disadvantage Not scalableNot dynamicCannot react to n/w failures, traffic load changes, n/w size increase Complex Suitable for Small &amp; fixed topology networks"},{"location":"3_Core/Computer_Networks/10_Network_Layer/#routing-table","title":"Routing Table","text":"<p>Store path information, so that each node knows how to forward packets</p> <p>Datagram approach &amp; Virtual Circuit method have different types. Refer Packet-Switching Methods for the diagram</p>"},{"location":"3_Core/Computer_Networks/10_Network_Layer/#routing-graph","title":"Routing Graph","text":"<p>Graphical representation of network with</p> <ul> <li>vertices: router nodes</li> <li>edges: links</li> <li>Cost: time delay, monetary cost, congestion level</li> </ul>"},{"location":"3_Core/Computer_Networks/10_Network_Layer/#routing-algorithms","title":"Routing Algorithms","text":"Flooding Shortest Path Routing Link State Routing(OSPF: Open Shortest Path First) Distance Vector Routing(RIP: Routing Information Protocol) Type Static Static Dynamic Dynamic Steps Packet sent by node to every neighborIncoming packets retransmitted on every link (except incoming link)Eventually \\(\\ge 1\\) copies arrive at destinatinEach packet is uniquely number, so duplicates are discardedNodes can remember packets already forwarded to keep network load in boundsCan include hop count in packets Solve single-source shortest path problem using Dijkstra\u2019s AlgoProduces \u201ctree\u201d of routes from source to all pointsConstruct forwarding table containing next hop Each router reliably floods information about its neighbors to every other routerEach router independently calculates the shortest path from itself to every other router, using Dijkstra\u2019s Algo Each router only knows links to neighbors; does not flood entire networkEach router has provisional shortest path (reach B with cost 11 via next hop router D)Routers exchange this information only with neighborsUpdate best path using info from neighborsBellman-Ford Algo Advantages No network info requiredRobust: All possible routes are triedCan be used for virtual circuit: At least one packet will have taken minimum hop count routeAll nodes are visited: Useful to distribute information Diagram Works well for Large networks Smaller networksMax hop limit of 15"},{"location":"3_Core/Computer_Networks/11_IP_Protocol/","title":"11 IP Protocol","text":"<p>Inter-Networking Protocol</p> <p>Responsible for node-to-node transmission</p> <p>Unreliable: Packets might be lost, corrupted, duplicated, or delivered out of order</p>"},{"location":"3_Core/Computer_Networks/11_IP_Protocol/#ipv4-packet-format","title":"IPv4 Packet Format","text":"Meaning Size (bits) Value Vers Version of IP protocol 4 Hlen Header length w/o options 4 Hlen=5 : 20bytesHlen=15: 60bytes TOS Type Of Service(Used for QoS priority) 8 Total Length Length of packet in bytes, including header &amp; payload 16 TTL Time To LiveSpecified how long packet is allowed to remain on InternetPrevents infinite loopsRouters decrement by 1When TTL=0, router discards datagram 8 Protocol Specifies format of payloadIdentify Transport Layer protocol used (TCP/UDP) 8 TCP=6UDP=17ICMP=1IGMP=2(administered by central authority to guarantee agreement) Source IP address 32 Dest IP address 32 Options Mainly used to record a router, timestamps, or specify routing Variable Header Checksum Error control Identification Copied into fragment, allows dest to know which fragments belong to which packet 16 Fragmentation Offset Specifies offset in original packet of data being carried in current fragment 13 Multiple of 8 bytes Flags Control fragmentation 3 - Reserved: 0<sup>th</sup> bit- Don\u2019t fragment: (1<sup>st</sup> bit)   - D=1 Don\u2019t fragment   - D=0 Can fragment- More fragments (LSB)   - M=1: More fragments incoming  - M=0: This is last fragment of packet"},{"location":"3_Core/Computer_Networks/11_IP_Protocol/#ip-fragmentation","title":"IP Fragmentation","text":"<p>Every network has its own MTU (Maximum Transmission Unit). This is the largest size of packet that can be put on the network.</p> <p>For eg, Ethernet is 1500 Bytes</p> <p>What makes fragmentation tricky is that we don\u2019t know the MTU of all networks in advance</p>"},{"location":"3_Core/Computer_Networks/11_IP_Protocol/#reassembly","title":"Reassembly","text":"End Nodes Better Avoids unnecessary workIf any fragment is missing, discard entire packet Intermediate Nodes Dangerous Hard to determine how much buffer space required by routersUnreliable when routes in network changes <p>Final destination host reusables original packet from fragments (if none of them are lost) with the following steps</p> <ol> <li>Check if first fragment has offset field = 0</li> <li>Divide length of first fragment by 8; this value should be equal to offset of 2<sup>nd</sup> fragment</li> <li>Divide the total length of the first and second fragment by 8; this value should be equal to offset of 3<sup>rd</sup> fragment</li> <li>Continue process, until fragment with more bit value = 0 is reached</li> </ol> <p></p> <p></p>"},{"location":"3_Core/Computer_Networks/11_IP_Protocol/#fragmentation-types","title":"Fragmentation Types","text":"<p>IP protocol uses non-transparent fragmentation</p> Transparent Fragmentation Non-Transparent Fragmentation Steps - Router breaks large packet into fragments- All fragments sent to same exit router- Reassemble fragments before forwarding to next network - Router breaks large packet into fragments- Packet fragments not reassembled at intermediate routers- Each fragment is treated as independent packet by routers- Fragments reassembled at final destination host Advantages Multiple exit routers can be usedHigher throughput Disadvantages All packets must be routed via same exit routerExit router must know when all pieces have been receivedEither \u2018count\u2019 field or \u2018end of packet\u2019 field must be stored in each packetLarge overhead: Large packet may fragmented &amp; reassembled repeatedly When a large packet is fragmented, overhead increasesEach fragment must have a header (min 20 bytes)"},{"location":"3_Core/Computer_Networks/11_IP_Protocol/#ipv6","title":"IPv6","text":"<p>(Version 5 was allocated to experimental Internet Stream Protocol)</p> <p>IPv6 has 128 bits, represented as 8 groups of 4 hex digits each</p> <p>Eg: \\(FEDC:BA98:7654:3210:FEDC:BA98:7664:3210\\)</p>"},{"location":"3_Core/Computer_Networks/11_IP_Protocol/#goals","title":"Goals","text":"<ul> <li>Providing improved security. </li> <li>Authentication Header</li> <li>Encrypted Security Payload Header.</li> <li>Reduction in size of Routing Tables</li> <li>Providing for a single, unique address assignment to mobile hosts.</li> <li>Providing support for new as well as older versions of the IP</li> </ul>"},{"location":"3_Core/Computer_Networks/11_IP_Protocol/#benefits","title":"Benefits","text":"<ul> <li>Increases address space</li> <li>Efficient addressing &amp; routing topology</li> <li>Network address is not required (restores end-to-end IP addressing)</li> </ul>"},{"location":"3_Core/Computer_Networks/11_IP_Protocol/#packet","title":"Packet","text":""},{"location":"3_Core/Computer_Networks/11_IP_Protocol/#base-header","title":"Base Header","text":""},{"location":"3_Core/Computer_Networks/11_IP_Protocol/#mobile-ip","title":"Mobile IP","text":"<p>Addressing is the main problem in mobile communication</p> <p>Regular IP addressing is based on the assumption that a host is stationary</p> <ul> <li>Routers use hierarchical structure of IP address to route packet</li> <li>Address is valid only when devices attached to network; if network changes, address is no longer valid</li> </ul> <p>When a host moves from one network to another, IP addressing structure needs to be modified</p> <p></p> <p></p> <p>There are 3 options to deal with device changing networks</p>"},{"location":"3_Core/Computer_Networks/11_IP_Protocol/#change-the-address","title":"Change the address","text":"<ul> <li>DHCP Protocol</li> <li>Limitations</li> <li>Configuration files need to be changed</li> <li>Each time computer moves from one network to another, it must be rebooted</li> <li>DNS tables need to be revised so that every other host in the Internet is aware of change</li> <li>If host roams from one network to another during transmission, data exchange will be interruted<ul> <li>Since port &amp; IP address of client &amp; server must remain constant for duration of connection</li> </ul> </li> </ul>"},{"location":"3_Core/Computer_Networks/11_IP_Protocol/#combination-of-2-addresses-to-identify-device","title":"Combination of 2 addresses to identify device","text":"<p>Host has</p> <ul> <li>Home address: original address</li> <li>Care-of-address: temporary address   (Associate host with foreign network)</li> <li>When host moves from one network to another, care-of-address changes</li> <li>Mobile host receives its care-of-address during agent-discovery &amp; registration</li> </ul> <p></p>"},{"location":"3_Core/Computer_Networks/11_IP_Protocol/#agent-discovery","title":"Agent Discovery","text":"<ol> <li>Home Agent\u2019s and Foreign Agent\u2019s broadcast their presence on each network to which they are attached; Beacon messages via ICMP Router Discovery Protocol (IRDP)</li> <li>Mobile Node\u2019s listen for advertisement and then initiate registration</li> </ol> <p>Thus,</p> <ul> <li>Foreign Agent is now aware of mobile</li> <li>Home Agent knows location of mobile</li> </ul>"},{"location":"3_Core/Computer_Networks/11_IP_Protocol/#registration","title":"Registration","text":"<ol> <li>When Mobile Node is away, it registers its COA with its Home Agent, usually through Foreign Agent with strongest signal</li> <li>Registration control messages are sent via UDP to well-known port</li> </ol>"},{"location":"3_Core/Computer_Networks/11_IP_Protocol/#tables-maintained","title":"Tables Maintained","text":""},{"location":"3_Core/Computer_Networks/11_IP_Protocol/#mobility-binding-table","title":"Mobility Binding Table","text":"<p>Maintained on Home Agent</p> <p>Maps Mobile Node\u2019s home address with its current care-of-address</p> Home Address Care-Of-Address Lifetime (sec)"},{"location":"3_Core/Computer_Networks/11_IP_Protocol/#visitor-list","title":"Visitor List","text":"<p>Maintained on Foreign Agent serving the Mobile Node</p> <p>Maps Mobile Nodes\u2019s home address to its MAC address &amp; Home Address</p> Home Address Home Agent Address Media Address Lifetime (sec)"},{"location":"3_Core/Computer_Networks/11_IP_Protocol/#indirect-triangle-routing","title":"Indirect (Triangle) Routing","text":"<p>Mobile Node uses 2 addresses</p> <ul> <li>Home address, used by correspondent (mobile location is transparent to correspondent)</li> <li>Care of Address, used by Home Agent to forward packets to mobile</li> </ul> <p>Foreign agent functions may be done by mobile itself</p> <p></p>"},{"location":"3_Core/Computer_Networks/11_IP_Protocol/#data-transfer-tunnelling","title":"Data Transfer Tunnelling","text":""},{"location":"3_Core/Computer_Networks/11_IP_Protocol/#problems-with-mobile-ip","title":"Problems with Mobile IP","text":""},{"location":"3_Core/Computer_Networks/11_IP_Protocol/#double-crossing","title":"Double Crossing","text":""},{"location":"3_Core/Computer_Networks/11_IP_Protocol/#triangle-routing","title":"Triangle routing","text":"<p>Packet travel as two sides of triangle</p> <p></p>"},{"location":"3_Core/Computer_Networks/12_Transport_Layer/","title":"12 Transport Layer","text":"<p>Transport Protocols run on end-devices</p> TCP UDP Connection setupCongestion ControlFlow Control No-frills extension of best-effort IPDelay guaranteesBandwidth guarantees"},{"location":"3_Core/Computer_Networks/12_Transport_Layer/#port-addressing","title":"Port Addressing","text":"<p>\\([0, 65535]\\)</p> <ul> <li> <p>Ephemeral (short-lived) port number</p> <ul> <li>\\(\\ge 1,023\\)</li> <li>used for client</li> </ul> </li> <li> <p>Well-known port number</p> <ul> <li>Universal port numbers</li> <li>used for servers</li> </ul> </li> </ul> <p></p>"},{"location":"3_Core/Computer_Networks/12_Transport_Layer/#icann","title":"ICANN","text":"<p>Internet Corporation for Assigned Names and Numbers (ICANN) Ranges</p> <p></p> <ul> <li>Well-known ports : 0 ~ 1,023<ul> <li>Assigned and controlled by ICANN</li> </ul> </li> <li>Registered ports : 1,024 ~ 49,151<ul> <li>Not assigned and controlled by ICANN</li> </ul> </li> <li>Can be registered with ICANN to prevent duplication</li> <li>Dynamic ports : 49,152 ~ 65,535<ul> <li>Can be used as temporary or private port number</li> </ul> </li> </ul>"},{"location":"3_Core/Computer_Networks/12_Transport_Layer/#socket-address","title":"Socket Address","text":"<p>Combination of IP &amp; Port address</p> <p>Eg: \\(200.23.56.8:69\\)</p>"},{"location":"3_Core/Computer_Networks/12_Transport_Layer/#mux-demux","title":"Mux &amp; Demux","text":"Multiplexing Demultiplexing Entity accepts items from  more than one source Entity deliver items to more  than one source"},{"location":"3_Core/Computer_Networks/12_Transport_Layer/#delivery","title":"Delivery","text":"<p>Delivery of items from a producer to a consumer</p> <pre><code>- Pushing : sender delivers items whenever they produced\n- Pulling : producer delivers the items after the consumer has requested\n</code></pre> <p></p>"},{"location":"3_Core/Computer_Networks/12_Transport_Layer/#flow-control","title":"Flow Control","text":"<p>Balance between production and consumption rates</p> <p>A buffer is a set of memory locations that can hold packets at the sender and receiver</p> <p>Normally we use two buffers for flow control</p> <ol> <li>at sending transport layer</li> <li>at receiving transport layer</li> </ol> <p></p>"},{"location":"3_Core/Computer_Networks/12_Transport_Layer/#error-control","title":"Error Control","text":"<ol> <li>Detect and discard corrupted segments</li> <li>Keep track of lost and discarded segments and resend them</li> <li>Recognize duplicate segments and discard them</li> <li>Buffer out-of-order segments until the missing segments arrive</li> </ol>"},{"location":"3_Core/Computer_Networks/12_Transport_Layer/#sequence-number","title":"Sequence Number","text":"<p>Each segment to holds a sequence number</p> <p>Sequence Number helps decide </p> <ul> <li>Which packet is to be resent</li> <li>Which packet is a duplicate</li> <li>Which packet has arrived out of order</li> </ul> <p>Sequence numbers can be repeated</p> <ul> <li>\\(0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7, 0, 1, \\dots\\)</li> </ul>"},{"location":"3_Core/Computer_Networks/12_Transport_Layer/#sequence-number-limit","title":"Sequence Number Limit","text":"<p>If the header of the frame allows \\(m\\) bits for sequence number, the sequence numbers range from \\(0 \\iff 2m \u2013 1\\)</p> <p>The values are modulo \\(2^m\\)</p> \\[ \\text{Final seq no} = \\text{Actual Seq Number } \\% \\ 2^m \\] <p>For m = 3, sequence numbers are: 0, 1, 2, 3, 4, 5, 6, 7.</p>"},{"location":"3_Core/Computer_Networks/12_Transport_Layer/#acknowledgement","title":"Acknowledgement","text":"<p>The receiver side can send an acknowledgement for each or a collection of segments that have been received correctly</p> <p>There can be +ve and -ve ack</p>"},{"location":"3_Core/Computer_Networks/12_Transport_Layer/#combination-of-flow-error-control","title":"Combination of Flow &amp; Error Control","text":"<p>Flow control requires the use of two buffers, one at the sender site and the other at the receiver site. </p> <p>The error control requires the use of sequence and acknowledgment numbers by both sides. </p> <p>These two requirements can be combined by using two numbered buffers, one at the sender, one at the receiver</p>"},{"location":"3_Core/Computer_Networks/12_Transport_Layer/#sliding-window","title":"Sliding Window","text":"Linear Format Circular Format"},{"location":"3_Core/Computer_Networks/12_Transport_Layer/#incomplete","title":"Incomplete","text":"<p>Some contributor, please complete using the ppt below</p>"},{"location":"3_Core/Computer_Networks/13_Application_Layer/","title":"13 Application Layer","text":"<p>Applications are the entities that communicate with each other to exchange services</p> <p>To make any use of the Internet, application programs should run on the two endpoints of a network connection</p> <ul> <li>\u201cClient\u201d applications request service</li> <li>\u201cServer\u201d applications provide service</li> </ul> <p>A socket is one end of an inter-process communication channel.</p>"},{"location":"3_Core/Computer_Networks/13_Application_Layer/#client-server-model","title":"Client Server Model","text":"<p>Many-to-One</p> <p></p> Server Client Run all the time (i.e. infinite)Provide service to any clientUsually specialize in providing certain type of service, e.g. MailListen to a well-known port and passively open connection. Run when needed, then terminate (i.e. finite)Actively open TCP or UDP connection with Server\u2019s socket <ul> <li>Client needs to know existence &amp; address of server </li> <li>However, the server does not need to know the existence or address of the client prior to the connection</li> <li>Once a connection is established, both sides can send and receive information</li> </ul>"},{"location":"3_Core/Computer_Networks/13_Application_Layer/#connection-establishment","title":"Connection Establishment","text":"<ul> <li>Both client and server will construct a socket</li> <li>The process to establish a socket on the client side is different from the process to establish a socket on the server side</li> </ul>"},{"location":"3_Core/Computer_Networks/13_Application_Layer/#p2p-peer-to-peer-model","title":"P2P (Peer-to-Peer) Model","text":"<p>Every node in the network acts alike</p> <p></p>"},{"location":"3_Core/Computer_Networks/13_Application_Layer/#advantages","title":"Advantages","text":"<ul> <li>No central point of failure</li> <li> <p>Scalability</p> </li> <li> <p>Since every peer is alike, it is possible to add more peers to the system and scale to larger networks</p> </li> </ul>"},{"location":"3_Core/Computer_Networks/13_Application_Layer/#disadvantages","title":"Disadvantages","text":"<ul> <li>Decentralized coordination; How to keep global state consistent?</li> <li> <p>All nodes may not be equal</p> </li> <li> <p>Computing power</p> </li> <li>Bandwidth</li> </ul>"},{"location":"3_Core/Computer_Networks/13_Application_Layer/#http","title":"HTTP","text":"<p>HyperText Transfer Protocol</p> <ul> <li>HTTP 1.0: RFC 1945</li> <li>HTTP 1.1: RFC 2068 (persistent TCP)</li> </ul> <p>Used for Client-Server model</p> <ul> <li>Client: Browser request &amp; receive Web objects</li> <li>Server: Web server sends objects in response to requests</li> </ul>"},{"location":"3_Core/Computer_Networks/13_Application_Layer/#properties","title":"Properties","text":"<ul> <li>Uses TCP</li> <li></li> <li>\u201cStateless\u201d</li> <li>A \u2018state\u2019 is information kept in memory     of a host, server or router to reflect     past events: such as routing tables,     data structures or database entries</li> <li>HTTP server maintains no information about past client requests</li> <li>Protocols that maintain \u201cstate\u201d are complex!<ul> <li>history (state) is maintained</li> <li>if server/client crashes, views of \u201cstate\u201d may be inconsistent, must be reconciled</li> <li>state is added via \u2018cookies\u2019</li> </ul> </li> </ul>"},{"location":"3_Core/Computer_Networks/13_Application_Layer/#steps","title":"Steps","text":""},{"location":"3_Core/Computer_Networks/13_Application_Layer/#types","title":"Types","text":"Non-Persistent HTTP Persistent HTTP without Pipelining Persistent HTTP with Pipelining Max no of objects sent over TCP connection 1 Multiple Multiple Used in HTTP Version HTTP/1.0 HTTP/1.1 Categories Steps Server leaves connection open after sending responseSubsequent HTTP messages  between same client/server sent over open connectionClient issues new request only when previous response has been receivedOne RTT for each referenced object Server leaves connection open after sending responseSubsequent HTTP messages  between same client/server sent over open connectionClient sends requests as soon as it encounters a referenced object1 RTT for all referenced objects Total Response Time(\\(n =\\) no of objects) \\(n (2 \\ \\text{RTT} + \\text{Transmit Time})\\)- one RTT to initiate TCP connection- one RTT for HTTP request &amp; first few bytes of HTTP response to return- file transmission time \\(n (1 \\ \\text{RTT} + \\text{Transmit Time})\\) \\(1 \\ \\text{RTT} + n(\\text{Transmit Time})\\) Disadvantages - requires 2 RTTs per object- OS overhead for each TCP connection- browsers often open parallel TCP connections to fetch referenced object Non-Persistent &amp; Non-Parallel Non-Persistent &amp; Parallel"},{"location":"3_Core/Computer_Networks/13_Application_Layer/#http-messages","title":"HTTP Messages","text":"HTTP Request HTTP Response Format Example"},{"location":"3_Core/Computer_Networks/13_Application_Layer/#headers","title":"Headers","text":""},{"location":"3_Core/Computer_Networks/13_Application_Layer/#responses","title":"Responses","text":""},{"location":"3_Core/Computer_Networks/13_Application_Layer/#http-methods","title":"HTTP Methods","text":""},{"location":"3_Core/Computer_Networks/14_WiFi/","title":"WiFi Bands","text":"2.4GHz 5GHz Slower Faster Term Meaning SSID Service Set IDentifier RSSI Received Signal Strength Indicator Less negative, the better"},{"location":"3_Core/Computer_Networks/Practicals/","title":"Index","text":""},{"location":"3_Core/Computer_Networks/Practicals/#tools","title":"Tools","text":"<ul> <li>Java</li> <li>Wireshark</li> <li></li> </ul>"},{"location":"3_Core/Computer_Networks/Practicals/01_Network_Commands/","title":"01 Network Commands","text":"Command Usage Displays Full Form UnixAlternative <code>getmac</code> <code>getmac</code> Get mac address of current machine <code>ipconfig</code> <code>ipconfig</code> <code>ipconfig</code> computer\u2019s IP details <code>ping</code> <code>ping www.google.ae</code> Check validity of connection between 2 machinesSends 4 packets to the other machine and checks how many of those packets reachedRount-trip timeTime to live <code>netstat</code> <code>netstat</code><code>netstat -r</code> Connection informationRouting table Network Statistics <code>route</code> <code>route</code> Routing table <code>netstat -nr</code> <code>arp</code> <code>arp -a</code> ARP cache of current machine Address Resolution Protocol <code>hostname</code> <code>hostname</code> Gives the machine name <code>nslookup</code> <code>nslookup</code><code>nslookup www.google.ae</code> Look and diagnose the DNS of a location Name System <code>tracert</code> <code>tracert www.google.ae</code> Shows the RTT from source and destination node, and also all the intermediary nodes Trace Root <code>traceroute</code> <code>pathping</code> <code>pathping www.google.ae</code> Combination of <code>ping</code> and <code>tracert</code>"},{"location":"3_Core/Computer_Networks/Practicals/02_03_CISCO_Packet_Tracer/","title":"02 03 CISCO Packet Tracer","text":"<p>This is pretty simple, actually</p> <ol> <li>Add end devices</li> <li>Add network devices</li> <li>Add connecting wires</li> <li>Configure IP address</li> <li>Configure Routing</li> <li>Static</li> <li>Dynamic</li> </ol>"},{"location":"3_Core/Computer_Networks/Practicals/04_05_UniThreaded_TCP/","title":"04 05 UniThreaded TCP","text":""},{"location":"3_Core/Computer_Networks/Practicals/04_05_UniThreaded_TCP/#simple-example","title":"Simple Example","text":""},{"location":"3_Core/Computer_Networks/Practicals/04_05_UniThreaded_TCP/#serverjava","title":"<code>Server.java</code>","text":"<pre><code>import java.util.*;\nimport java.io.*;\nimport java.net.*;\n\npublic class Server extends Thread {\n  private ServerSocket server_socket;\n\n  public Server(int port_no) throws IOException {\n    while (true) {\n      try {\n        server_socket = new ServerSocket(port_no);\n\n        System.out.println(\"Waiting for client to connect\");\n\n        Socket server = server_socket.accept();\n        System.out.println(\"Connected to client\");\n\n        DataOutputStream out = new DataOutputStream(server.getOutputStream());\n        String server_message = \"Hi there! Enter something\";\n        out.writeUTF(server_message);\n\n        DataInputStream in = new DataInputStream(server.getInputStream());\n        String client_reply = in.readUTF();\n        System.out.println(\"User replied with: \" + client_reply);\n        server.close();\n      } catch (Exception e) {\n        e.printStackTrace();\n      }\n    }\n  }\n\n  public static void main(String[] args) throws IOException {\n    Thread t1 = new Server(5000);\n    t1.start();\n  }\n}\n</code></pre>"},{"location":"3_Core/Computer_Networks/Practicals/04_05_UniThreaded_TCP/#clientjava","title":"<code>Client.java</code>","text":"<pre><code>import java.util.*;\nimport java.io.*;\nimport java.net.*;\n\npublic class Client\n{\n  public Client(String host, int port) throws IOException\n  {\n    try\n    {\n      Socket client = new Socket(host, port);\n\n      System.out.println(\"Connected to server\");\n\n      DataInputStream in = new DataInputStream(client.getInputStream());\n\n      String server_message = in.readUTF();\n      System.out.println(server_message);\n\n      DataOutputStream out = new DataOutputStream(client.getOutputStream());\n\n      Scanner user_in = new Scanner(System.in);\n      String client_reply = user_in.next();\n      out.writeUTF(client_reply);\n\n      user_in.close();\n      client.close();\n    } catch (Exception e)\n    {\n      e.printStackTrace();\n    }\n  }\n  public static void main(String[] args) throws IOException\n  {\n    new Client(\"localhost\", 5000);\n  }\n}\n</code></pre>"},{"location":"3_Core/Computer_Networks/Practicals/04_05_UniThreaded_TCP/#execution-commands","title":"Execution Commands","text":"<pre><code>javac Server.java\njava Server\n\njavac Client.java\njava Client\n</code></pre>"},{"location":"3_Core/Computer_Networks/Practicals/04_05_UniThreaded_TCP/#output","title":"Output","text":""},{"location":"3_Core/Computer_Networks/Practicals/04_05_UniThreaded_TCP/#server","title":"Server","text":""},{"location":"3_Core/Computer_Networks/Practicals/04_05_UniThreaded_TCP/#client","title":"Client","text":""},{"location":"3_Core/Computer_Networks/Practicals/04_05_UniThreaded_TCP/#calculator","title":"Calculator","text":""},{"location":"3_Core/Computer_Networks/Practicals/04_05_UniThreaded_TCP/#serverjava_1","title":"<code>Server.java</code>","text":"<pre><code>import java.util.*;\nimport java.io.*;\nimport java.net.*;\n\npublic class Server extends Thread {\n    private ServerSocket server_socket;\n\n    public Server(int port_no) throws IOException {\n        while (true) {\n            try {\n                server_socket = new ServerSocket(port_no);\n\n                System.out.println(\"Waiting for client to connect\");\n\n                Socket server = server_socket.accept();\n                System.out.println(\"Connected to client\");\n\n                DataOutputStream out = new DataOutputStream(server.getOutputStream());\n                String server_message = \"Hi there! Enter something\";\n                out.writeUTF(server_message);\n\n                DataInputStream in = new DataInputStream(server.getInputStream());\n\n                int num1 = Integer.parseInt(in.readUTF());\n                String op = in.readUTF();\n                int num2 = Integer.parseInt(in.readUTF());\n\n                String client_reply = \"Received: \" + num1 + op + num2;\n                System.out.println(client_reply);\n\n                int result = 0;\n\n                switch(op)\n                {\n                    case \"+\": {result = num1 + num2; break;}\n                    case \"-\": {result = num1 - num2; break;}\n                    case \"*\": {result = num1 * num2; break;}\n                    case \"/\": {result = num1 / num2; break;}\n                    case \"^\": {result = num1 ^ num2; break;}\n                    default: {System.out.println(\"Invalid Operator\");}\n                }\n\n                out.writeUTF(Integer.toString(result));\n\n                server.close();\n            } catch (Exception e) {\n        e.printStackTrace();\n            }\n        }\n    }\n\n    public static void main(String[] args) throws IOException {\n        Thread t1 = new Server(args[0]);\n        t1.start();\n    }\n}\n</code></pre>"},{"location":"3_Core/Computer_Networks/Practicals/04_05_UniThreaded_TCP/#clientjava_1","title":"<code>Client.java</code>","text":"<pre><code>import java.util.*;\nimport java.io.*;\nimport java.net.*;\n\npublic class Client {\n    public Client(String host, int port) throws IOException\n  {\n    try\n    {\n      Socket client = new Socket(host, port);\n\n      System.out.println(\"Connected to server\");\n\n      DataInputStream in = new DataInputStream(client.getInputStream());\n\n      String server_message = in.readUTF();\n      System.out.println(server_message);\n\n      DataOutputStream out = new DataOutputStream(client.getOutputStream());\n\n      Scanner user_in = new Scanner(System.in);\n\n      int num1 = user_in.nextInt();\n      String op = user_in.next();\n      int num2 = user_in.nextInt();\n\n      out.writeUTF(Integer.toString(num1));\n      out.writeUTF(op);\n      out.writeUTF(Integer.toString(num2));\n\n\n      String result = in.readUTF();\n      System.out.println(result);\n\n      user_in.close();\n      client.close();\n    } catch (Exception e)\n    {\n      e.printStackTrace();\n    }\n  }\n\n    public static void main(String[] args) throws IOException {\n        new Client(args[0], args[1]);\n    }\n}\n</code></pre>"},{"location":"3_Core/Computer_Networks/Practicals/04_05_UniThreaded_TCP/#execution-commands_1","title":"Execution Commands","text":"<pre><code>javac Server.java\njava Server 5000\n\njavac Client.java\njava Client localhost 5000\n</code></pre>"},{"location":"3_Core/Computer_Networks/Practicals/04_05_UniThreaded_TCP/#output_1","title":"Output","text":""},{"location":"3_Core/Computer_Networks/Practicals/04_05_UniThreaded_TCP/#server_1","title":"Server","text":""},{"location":"3_Core/Computer_Networks/Practicals/04_05_UniThreaded_TCP/#client_1","title":"Client","text":""},{"location":"3_Core/Computer_Networks/Practicals/04_05_UniThreaded_TCP/#working-with-telnet","title":"Working with Telnet","text":"<ol> <li>no need to implement your own client</li> <li>change server\u2019s parameters as command-line arguments (using <code>args[0], ...</code>)</li> <li>Need to use <code>BufferedReader</code> and <code>PrintWriter</code> as the encoding used Telnet is different</li> </ol> <pre><code>import java.util.*;\nimport java.io.*;\nimport java.net.*;\n\npublic class Server extends Thread {\n  private ServerSocket server_socket;\n\n  public Server(int port_no) throws IOException {\n    while (true) {\n      try {\n        server_socket = new ServerSocket(port_no);\n\n        System.out.println(\"Waiting for client to connect\");\n\n        Socket server = server_socket.accept();\n        System.out.println(\"Connected to client\");\n\n        PrintWriter out = new PrintWriter(server.getOutputStream());\n        String server_message = \"Hi there! Enter something\";\n        out.print(server_message);\n\n        BufferedReader in = new BufferedReader(new InputStreamReader(\n            server.getInputStream()\n        ));\n        String client_reply = in.readLine();\n\n        System.out.println(\"User replied with: \" + client_reply);\n        server.close();\n      } catch (Exception e) {\n        e.printStackTrace();\n      }\n    }\n  }\n\n  public static void main(String[] args) throws IOException {\n    Thread t1 = new Server(5000);\n    t1.start();\n  }\n}\n</code></pre> <pre><code>javac Server.java\njava Server 5000\ntelnet localhost 5000\n</code></pre>"},{"location":"3_Core/Computer_Networks/Practicals/06_Multithreaded_TCP/","title":"06 Multithreaded TCP","text":""},{"location":"3_Core/Computer_Networks/Practicals/06_Multithreaded_TCP/#serverjava","title":"<code>Server.java</code>","text":"<pre><code>// import java.util.*;\nimport java.io.*;\nimport java.net.*;\n\npublic class Server {\n    private ServerSocket server_socket;\n\n    public Server(int port_no) throws IOException {\n        server_socket = new ServerSocket(port_no);\n\n        System.out.println(\"Waiting for client to connect\");\n\n        while (true) {\n            try {\n                Socket connection = server_socket.accept();\n                System.out.println(\"Connected to client\");\n\n                ClientHandler client_handler = new ClientHandler(connection);\n                client_handler.start();\n            } catch (Exception e) {\n        e.printStackTrace();\n            }\n        }\n    }\n\n    public static void main(String[] args) throws IOException {\n        new Server(5000);\n    }\n}\n\nclass ClientHandler extends Thread {\n\n    public ClientHandler(Socket connection) throws IOException {\n        try {\n            DataOutputStream out = new DataOutputStream(connection.getOutputStream());\n            String server_message = \"Hi there! Enter something\";\n            out.writeUTF(server_message);\n\n            DataInputStream in = new DataInputStream(connection.getInputStream());\n\n            int num1 = Integer.parseInt(in.readUTF());\n            String op = in.readUTF();\n            int num2 = Integer.parseInt(in.readUTF());\n\n            String client_reply = \"Received: \" + num1 + op + num2;\n            System.out.println(client_reply);\n\n            int result = 0;\n\n            switch (op) {\n                case \"+\": {\n                    result = num1 + num2;\n                    break;\n                }\n                case \"-\": {\n                    result = num1 - num2;\n                    break;\n                }\n                case \"*\": {\n                    result = num1 * num2;\n                    break;\n                }\n                case \"/\": {\n                    result = num1 / num2;\n                    break;\n                }\n                case \"^\": {\n                    result = num1 ^ num2;\n                    break;\n                }\n                default: {\n                    System.out.println(\"Invalid Operator\");\n                }\n            }\n\n            out.writeUTF(Integer.toString(result));\n\n            in.close();\n            out.close();\n            connection.close();\n\n        } catch (Exception e) {\n      e.printStackTrace();\n        }\n    }\n}\n</code></pre>"},{"location":"3_Core/Computer_Networks/Practicals/06_Multithreaded_TCP/#clientjava","title":"<code>Client.java</code>","text":"<pre><code>import java.util.*;\nimport java.io.*;\nimport java.net.*;\n\npublic class Client {\n    public Client(String host, int port) throws IOException\n  {\n    try\n    {\n      Socket client = new Socket(host, port);\n\n      System.out.println(\"Connected to server\");\n\n      DataInputStream in = new DataInputStream(client.getInputStream());\n\n      String server_message = in.readUTF();\n      System.out.println(server_message);\n\n      DataOutputStream out = new DataOutputStream(client.getOutputStream());\n\n      Scanner user_in = new Scanner(System.in);\n\n      int num1 = user_in.nextInt();\n      String op = user_in.next();\n      int num2 = user_in.nextInt();\n\n      out.writeUTF(Integer.toString(num1));\n      out.writeUTF(op);\n      out.writeUTF(Integer.toString(num2));\n\n\n      String result = in.readUTF();\n      System.out.println(result);\n\n      user_in.close();\n      client.close();\n    } catch (Exception e)\n    {\n      e.printStackTrace();\n    }\n  }\n\n    public static void main(String[] args) throws IOException {\n        new Client(\"localhost\", 5000);\n    }\n}\n</code></pre>"},{"location":"3_Core/Computer_Networks/Practicals/07_HTTP/","title":"07 HTTP","text":""},{"location":"3_Core/Computer_Networks/Practicals/07_HTTP/#initial-steps","title":"Initial Steps","text":"<ol> <li>Go to Windows Features</li> <li>Turn on <code>Internet Information Services</code></li> <li>Host the web page</li> <li>Go to <code>Windows (C:)&gt;inetpub&gt;wwwroot</code></li> <li>Add a file called <code>index.html</code></li> <li> <p>Type your html code</p> </li> <li> <p>Perform Execution</p> </li> </ol>"},{"location":"3_Core/Computer_Networks/Practicals/07_HTTP/#clientjava","title":"<code>Client.java</code>","text":"<pre><code>import java.util.*;\nimport java.io.*;\nimport java.net.*;\n\npublic class Client\n{\n    public static void main(String[] args) throws IOException\n    {\n        String server = args[0];\n        int port = Integer.parseInt(args[1]);\n\n        try\n        {\n            Socket socket = new Socket(server, port);\n\n            DataOutputStream request = new DataOutputStream(socket.getOutputStream());\n            request.writeUTF(\"\\n\");\n\n            DataInputStream response = new DataInputStream(socket.getInputStream());\n\n            String response_text = response.readUTF();\n            while(response_text != null)\n            {\n                System.out.println(response_text);\n                response_text = response.readUTF();\n            }\n\n            request.close();\n            response.close();\n            socket.close();\n        } catch (Exception e)\n        {\n            e.printStackTrace();\n        }\n    }\n}\n</code></pre>"},{"location":"3_Core/Computer_Networks/Practicals/07_HTTP/#execution","title":"Execution","text":"<pre><code>javac Client.java\njava Client localhost /\n</code></pre>"},{"location":"3_Core/Computer_Networks/Practicals/08_UDP/","title":"08 UDP","text":"<p>This is not required for evaluation.</p>"},{"location":"3_Core/Computer_Networks/Practicals/08_UDP/#baseserverjava","title":"<code>BaseServer.java</code>","text":"<pre><code>import java.io.*;\nimport java.net.*;\n\npublic class udpBaseServer\n{\n  public static void main(String[] args) throws IOException\n  {\n    // Step 1 : Create a socket to listen at port 1234\n    DatagramSocket ds = new DatagramSocket(1234);\n    byte[] receive = new byte[65535];\n\n    DatagramPacket DpReceive = null;\n    while (true)\n    {\n\n      // Step 2 : create a DatgramPacket to receive the data.\n      DpReceive = new DatagramPacket(receive, receive.length);\n\n      // Step 3 : revieve the data in byte buffer.\n      ds.receive(DpReceive);\n\n      System.out.println(\"Client:-\" + data(receive));\n\n      // Exit the server if the client sends \"bye\"\n      if (data(receive).toString().equals(\"bye\"))\n      {\n        System.out.println(\"Client sent bye.....EXITING\");\n        break;\n      }\n\n      // Clear the buffer after every message.\n      receive = new byte[65535];\n    }\n  }\n\n  // A utility method to convert the byte array data into a string representation.\n  public static StringBuilder data(byte[] a)\n  {\n    if (a == null)\n      return null;\n    StringBuilder ret = new StringBuilder();\n    int i = 0;\n    while (a[i] != 0)\n    {\n      ret.append((char) a[i]);\n      i++;\n    }\n    return ret;\n  }\n}\n</code></pre>"},{"location":"3_Core/Computer_Networks/Practicals/08_UDP/#baseclientjava","title":"<code>BaseClient.java</code>","text":"<pre><code>import java.io.*;\nimport java.net.*;\nimport java.util.*;\n\npublic class BaseClient\n{\n  public static void main(String args[]) throws IOException\n  {\n    Scanner sc = new Scanner(System.in);\n\n    // Step 1:Create the socket object for carrying the data.\n    DatagramSocket ds = new DatagramSocket();\n\n    InetAddress ip = InetAddress.getLocalHost();\n    byte buf[] = null;\n\n    // loop while user not enters \"bye\"\n    while (true)\n    {\n      String inp = sc.nextLine();\n\n      // convert the String input into the byte array.\n      buf = inp.getBytes();\n\n      // Step 2 : Create the datagramPacket for sending the data.\n      DatagramPacket DpSend = new DatagramPacket(buf, buf.length, ip, 1234);\n\n      // Step 3 : invoke the send call to actually send the data.\n      ds.send(DpSend);\n\n      // break the loop if user enters \"bye\"\n      if (inp.equals(\"bye\"))\n        break;\n    }\n  }\n}\n</code></pre>"},{"location":"3_Core/Computer_Networks/Practicals/09_Wireshark/","title":"09 Wireshark","text":"<p>Packet Analyzer</p>"},{"location":"3_Core/Computer_Networks/Practicals/09_Wireshark/#steps","title":"Steps","text":"<ol> <li>Select the network</li> <li>Click capture packets</li> <li>Make a request on your browser</li> <li>Stop capture</li> <li>Filter by <code>http</code></li> <li>Analyze each packet\u2019s 5 layers\u2019</li> </ol>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/","title":"Design &amp; Analysis of Algorithms","text":"<p>Taught by Dr. Siddhaling</p> <p>This course provides a solid conceptual foundation in Computer Networks, emphasizing design aspects and the latest advancements in the field. It prepares students for more rigorous and specialized studies in related areas. By the end of the course, students will be equipped to analyze, design, troubleshoot, and maintain various network types, as well as initiate network-oriented software development.</p> <p>Key Learning Objectives:</p> <ul> <li>Understand fundamental concepts and architecture of computer networks.</li> <li>Analyze and design network protocols and services.</li> <li>Troubleshoot and maintain commonly used network types.</li> <li>Begin developing network-oriented software solutions.</li> </ul> <p>Topics Covered:</p> <ul> <li>Introduction to the uses of computer networks and network hardware (the network edge, core, and access networks).</li> <li>Theoretical basis for data communication, including Fourier analysis and maximum data rates.</li> <li>Internet Service Providers (ISPs), internet backbones, and delay/loss in packet-switched networks.</li> <li>Network software, protocol hierarchies, and service models (OSI and TCP/IP).</li> <li>Data Link Layer services, error detection and correction techniques, and multiple access protocols (TDM, ALOHA, CSMA).</li> <li>Link Layer addressing, including MAC addresses, ARP, DHCP, and Ethernet.</li> <li>Wireless networks, including Wi-Fi and 802.11 architecture.</li> <li>Network Layer topics such as virtual circuits, routers, and routing algorithms (RIP, OSPF, BGP).</li> <li>Transport Layer principles, including TCP, UDP, flow control, and socket programming.</li> <li>Application Layer protocols like HTTP, FTP, SMTP, and DNS.</li> <li>An overview of network security and cryptography.</li> <li>Machine Learning applications for networking.</li> </ul> <p>Make sure to first read Introduction to Algorithms</p> <p>Whatever code/algorithm/complexity is missing, refer DSA Notes</p>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/01_Divide_%26_Conquer/","title":"01 Divide & Conquer","text":"<ol> <li>Divide problem into 2/more smaller sub-problems (Divide)</li> <li>Solve sub-problems recursively</li> <li>Obtain sol to original problem by combining sub-solutions (Conquer)</li> </ol>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/01_Divide_%26_Conquer/#general-form","title":"General Form","text":"<pre><code>Algorithm divide_and_conquer(a, p, r)\n{\n    if(p&lt;r)\n    {\n        part1 = divide_and_conquer(a, p, q_1)\n        part2 = divide_and_conquer(a, q1 + 1, q2)\n        ...\n        partn = divide_and_conquer(a, qn + 1, r)\n\n        return combine(part1, part2, ..., partn)\n    }\n    else\n    {\n        return solution\n    }\n}\n</code></pre>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/01_Divide_%26_Conquer/#algorithms","title":"Algorithms","text":"\\(T(n)\\) \\(O()\\) Binary Search \\(1 \\cdot T(n/2) + 1\\) \\(O(\\log n)\\) Min-Max Algorithm \\(2 \\cdot T(n/2) + 1\\) \\(O(n)\\) Merge Sort \\(2 \\cdot T(n/2) + n\\) \\(O(n \\log n)\\) Quick Select Quick Sort \\(2 \\cdot T(n/2) + n\\)\\(T(n-1) + n\\) \\(O(n \\log n)\\)Worst-Case \\(O(n^2)\\) \\(n!\\) \\(T(n-1) + 1\\) Tower of Hanoi \\(2 T(n-1) + 1\\) Example \\(3T(n/4) + n^2\\) <p>Derive the complexity for the above using</p> <ul> <li>Substitution method</li> <li>Recursion tree</li> <li>Master\u2019s Theorem</li> </ul>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/01_Divide_%26_Conquer/#min-max","title":"Min-Max","text":"<pre><code>Algorithm min(a, p, r)\n{\n  if (p&lt;r)\n  {\n    q = (p+r)/2;\n\n    min1 = min(a, p, q);\n    min2 = min(a, q+1, r);\n\n    return argin(min1, min2);\n  }\n  else\n  {\n      return a[p];\n  }\n}\n\n// We could use brute-force method. It is O(n) as well, but it worse.\n\nAlgorithm min_max(a)\n{\n  min = max = a[0];\n\n  for i=1 to n\n    if a[i] &lt; min\n        min = a[i]\n    else if a[i] &gt; max\n        max = a[i]\n\n  return min\n}\n</code></pre>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/01_Divide_%26_Conquer/#quick-select","title":"Quick Select","text":"<p>The algorithm is quick select algorithm, based on the Quick-Sort algorithm.</p> <p>It returns \\(k^\\text{th}\\) smallest element in S</p> <pre><code>Select (k, S)\n{\n  pick pivot in S\n\n  partition S into L, E, Q such that:\n    max(L) &lt; pivot // L contains all elements smaller than pivot\n    E = {pivot}\n    pivot &lt; min(G) // G contains all elements greater than pivot\n\n  if k \u2264 length(L) // Searching for item \u2264 pivot.\n    return Select(k, L)\n  else if k \u2264 length(L) + length(E) // Found\n    return pivot\n  else  // Searching for item \u2265 pivot.\n    return Select(k - length(L) - length(E), G)\n}\n</code></pre>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/01_Divide_%26_Conquer/#time-complexity","title":"Time Complexity","text":"<ul> <li>Worst-Case \\(O(n^2)\\)</li> <li>Average-Case \\(O(n)\\)</li> </ul>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/01_Divide_%26_Conquer/#others","title":"Others","text":"<p>Refer DSA</p>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/01_Divide_%26_Conquer/#masters-theorem","title":"Master\u2019s Theorem","text":"<p>Consider a recurrence relation</p> \\[ T(n) = a T \\left( \\frac{n}{b} \\right) + n^d \\] \\(d \\ \\_\\_\\_ \\ \\log_b a\\) \\(T(n)\\) \\(&gt;\\) \\(n^d\\) \\(=\\) \\(n^d \\log n\\) \\(&lt;\\) \\(n^{\\log_b a}\\)"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/02_Greedy/","title":"02 Greedy","text":"<p>A greedy algorithm solves problems by making the choice that appear to be the best at that particular moment.</p>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/02_Greedy/#prims-algorithm","title":"Prim\u2019s Algorithm","text":"<p>Refer Discrete Structures</p> <ul> <li>With cost matrix \\(O(n^2)\\)</li> <li>With heaps \\(O\\Bigg( (n + |E|) \\log n \\Bigg)\\)</li> </ul> <p>I didn\u2019t understand the code implementation</p> <p></p>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/02_Greedy/#knapsack-problem","title":"Knapsack Problem","text":"<p>There exists items, with each having</p> <ul> <li>profit \\(p_i\\)</li> <li>weight \\(w_i\\)</li> </ul> <p>There also exists a limit \\(m\\) which is the max weight you can pick</p>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/02_Greedy/#goal","title":"Goal","text":"<p>maximize profit \\(P\\), while adhering to the limit.</p> \\[ \\begin{aligned} \\text{max } P = &amp; \\sum p_i x_i \\\\ \\text{such that} &amp; \\sum w_i x_i \\le m \\end{aligned} \\]"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/02_Greedy/#steps","title":"Steps","text":"<ol> <li>Convert list of \\(p_i\\) and \\(w_i\\) into a new list of \\(p_i/w_i\\)</li> <li>Sort in descending order</li> </ol>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/02_Greedy/#algorithm","title":"Algorithm","text":"<pre><code>// this is for x_i = 0 or 1 (binary)\n\nAlgorithm GreedyKnapsack(p, w, m, n)\n{\n    Input\n        - Profit array (sorted)\n        - Weights array (sorted)\n        - max capacity (knapsack size)\n        - no of objects\n\n    for i=1 to n\n        x[i] = 0\n\n    remaining_cap = m\n\n    for i=1 to n\n        if(w[i] &lt; remaining_cap)\n            x[i] = 1\n            remaining_cap -= w[i]\n        else\n            break\n\n    if (i&lt;=n)\n        x[i] = remaining_cap/w[i]\n}\n</code></pre>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/02_Greedy/#types","title":"Types","text":"Type Example 0/1(Discrete) An item can be- not taken- completely taken Phone Fractional(Continous) An item is can be- not taken- partially taken- completely taken Juice"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/02_Greedy/#tree-vertex-splitting","title":"Tree Vertex Splitting","text":"<p>Directed and weighted binary tree</p> <p>Nice video</p>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/02_Greedy/#steps_1","title":"Steps","text":"<ol> <li>Traverse from bottom</li> <li>Split a node from the tree if \\(d(u) &gt; \\delta\\)</li> <li>\\(d(u) = 0\\) for leaves</li> <li>\\(d(u) = \\underset{v \\in c(u)}{\\max} \\{ d(v) + w(u, v) \\}\\)</li> <li>Traverse upward</li> </ol>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/02_Greedy/#example-with-delta5","title":"Example with \\(\\delta=5\\)","text":"<pre><code>flowchart TB\n\nsubgraph After\ndirection TB\na1((1)) --&gt; a2((2i)) &amp; a3((3))\na2o((2o)) --&gt; a4i((4i))\na4o((4o)) --&gt; a7((7)) &amp; a8((8))\n\na3 --&gt; a5((5)) &amp; a6i((6i))\na6o((6o)) --&gt; a9((9)) &amp; a10((10))\nend\n\nsubgraph Before\ndirection TB\n1((1))\n2((2))\n3((3))\n4((4))\n5((5))\n6((6))\n7((7))\n8((8))\n9((9))\n10((10))\n\n1 --&gt;|4| 2\n2 --&gt; 0(( ))\n2 --&gt;|2| 4\n4 --&gt;|1| 7\n4 --&gt;|4| 8\n\n1 --&gt;|2| 3\n3 --&gt;|1| 5\n3 --&gt;|3| 6\n6 --&gt;|2| 9\n6 --&gt;|3| 10\nend</code></pre> <pre><code>Algorithm TVS(T, w, delta)\n{\n    if(T != 0)\n    {\n        d[T] = 0\n\n        for each child v of T\n        {\n            TVS(v, w, delta)\n            d[T] = argmax{d[T], d[v] + w(T, v)}\n        }\n\n        if(\n            T is not root\n            and\n            d[T] + w(parent(T), T) &gt; delta\n        )\n        {\n            write(T)\n            d[T] = 0\n        }\n    }\n}\n</code></pre>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/02_Greedy/#job-scheduling","title":"Job Scheduling","text":"<p>Similar to Knapsack problem</p> <p>There exists jobs, with each having</p> <ul> <li>profit \\(p_i\\)</li> <li>deadline \\(w_i\\)</li> </ul> <p>To complete each job, it only takes 1 unit of time</p> <p>Also</p> <ul> <li>it is not necessary to complete all jobs</li> <li>a job can only be taken once</li> </ul>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/02_Greedy/#goal_1","title":"Goal","text":"<p>maximize profit \\(P\\), while adhering to the limit. $$ \\begin{aligned} \\text{max } P = &amp;\\sum p_i x_i \\ \\text{such that } &amp; d_i \\text{ is not violated} \\end{aligned} $$</p>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/02_Greedy/#step","title":"Step","text":"<ol> <li>Sort the jobs in descending order based on profit</li> </ol>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/02_Greedy/#algorithm_1","title":"Algorithm","text":"<pre><code>Algo JS(arr, t):\n\n    // length of array\n    n = len(arr)\n\n        sort_desc(arr)\n\n    // To keep track of free time slots\n    result = [False] * t\n\n    // To store result (Sequence of jobs)\n    job = ['-1'] * t\n\n    // Iterate through all given jobs\n    for i in range(len(arr)):\n\n        // Find a free slot for this job\n        // (Note that we start from last possible slot)\n        for j in range(min(t-1, arr[i][1] - 1), -1, -1):\n            // Free slot found\n            if result[j] is False:\n                result[j] = True\n                job[j] = arr[i][0]\n                break\n\n    // print the sequence\n    print(job)\n</code></pre>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/03_Dynamic_Programming/","title":"03 Dynamic Programming","text":"<p>Difference b/w divide &amp; conquer and dynamic programming is  - divide &amp; conquer combines the solutions of the subproblems to find solution of main problem - dynamic programming uses the result of the subproblems to find optimum solution of main problem</p> <p>Difference b/w greedy method and dynamic programming is</p> <ul> <li>greedy method first makes a choise that appears best at the time, and then solves a resulting subproblem</li> <li>dynamic programming solves all subproblems, and then selects one that helps to find the optimum solution</li> </ul>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/03_Dynamic_Programming/#principle-of-optimality","title":"Principle of Optimality","text":"<p>Optimal sequence of decisions has the property that whatever the initial state and decision are, the remaining decisions must constitute an optimal decision sequence with regard to the state resulting from the first decision.</p>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/03_Dynamic_Programming/#all-pair-shortest-path","title":"All Pair Shortest Path","text":"\\[ \\begin{aligned} A^k(i, j) = \\text{argmin} &amp; \\{ \\\\ &amp; A^{k-1} (i, j), \\\\ &amp; A^{k-1} (i, k) + A^{k-1} (k, j) \\\\ &amp;\\} \\end{aligned} \\] <p>Try to find the path between pairs of points either directly/through another intermediate point.</p> <p>Time complexity \\(= O(n^3)\\)</p> <pre><code>Algorithm AllPaths(cost, n)\n{\n    for k=1 to n // taking every node as intermediary\n        for i=1 to n\n            for j=1 to n\n                a[i][j] = argmin(\n                    cost[i][j],\n                    cost[i][k] + cost[k][j]\n                )\n\n    return a\n}\n</code></pre>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/03_Dynamic_Programming/#single-source-shortest-path","title":"Single Source Shortest Path","text":"<p>Also called as Bellman Ford algorithm</p> <p>Similar to All Pair Shortest Path, but only from a single source to every other point.</p> <pre><code>Algorithm BellmanFord(v, cost)\n{\n    for i=1 to n\n        dist[i] = cost[v][i]\n\n    for k=1 to n\n        for i=1 to n\n            for j=1 to n\n                a[i] = argmin(\n                    a[i],\n                    cost[i][k] + cost[k][j]\n                )\n\n    return a\n}\n</code></pre> <p>Time complexity \\(= O(n^3)\\)</p>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/04_Backtracking/","title":"04 Backtracking","text":"<p>A general algorithm that</p> <ul> <li>incrementally builds candidates to the solutions</li> <li>backtracks/abandons candidate \\(c\\) once it is found that \\(c\\) cannot help attain valid solution</li> </ul> <p>It is an important tool for solving constraint satisfaction problems, such as crosswords, verbal arithmetic, Sudoku, and many other puzzles.</p>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/04_Backtracking/#types","title":"Types","text":"<ul> <li>Find a path to success</li> <li>Find all paths to success</li> <li>Find the best path to success</li> </ul>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/04_Backtracking/#pseudocode","title":"Pseudocode","text":"<pre><code>Backtrack(x)\n    if x is not a solution\n        return false\n    if x is a new solution\n        add to list of solutions\n\n    backtrack(expand x)\n</code></pre>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/04_Backtracking/#terminology","title":"Terminology","text":"Term Meaning Problem State Node in Depth-First Search Tree State Space Set of all paths from root \\(\\to\\) other nodes Solution States Problem states for which path from root node defines a tuple in solution spaceIn variable tuple size formulation tree, all nodes are solution statesIn fixed tuple size formulation tree, only leaf nodes are solution states Answer states Solution states for which path from root defines a tuple that is a member of set of solutionsThese states implicit constraints State Space Tree Tree organization of solution space Static trees/Fixed-Size Independent of problem instance being solved Dynamic trees/Variable-Sized Dependent of problem instance being solved Live node Generated node whose all children have not been generated yet E-Node Live nodes whose children are currently being generated/explored Dead node Generated node that is not to be expanded any further Promising Node Node can lead to feasible solution Non-Promising Node Node cannot lead to feasible solution Pruned state space tree State space tree consisting of expanded nodes"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/04_Backtracking/#backtracking-tree-structure","title":"Backtracking Tree Structure","text":"<pre><code>flowchart TB\nstart(( )) --&gt;\n1(( )) &amp; 2(( )) &amp; 3(( ))\n\n1 --&gt; 4(( )) &amp; 5\n5(( )) --&gt; 6(( )) &amp; 7(( ))\n\n2 --&gt; 8(( )) &amp; 9(( )) &amp; 12(( ))\n3 --&gt; 10(( )) &amp; 11(( ))\n\nclassDef red fill:red,stroke:red\nclassDef green fill:green,stroke:green\n\nclass 4,6,7 red\nclass 8 green</code></pre>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/04_Backtracking/#sudoku","title":"Sudoku","text":"<p>Obviously, the brute force approach would be randomly try all combinations of solutions, but the probability of getting the answer right in just a few attempts is very low, and hence, it is very computationally expensive.</p>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/04_Backtracking/#n-queens-problem","title":"N Queens Problem","text":"<p>Place \\(n\\) queen pieces on a chess board such that</p> <ul> <li>none of them can attack eachother</li> <li>no two queens in same row, same column or diagonal</li> </ul>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/04_Backtracking/#steps","title":"Steps","text":"<ol> <li>Each recursive call attempts to place a queen in a specific column.</li> <li>For a given call, the state of the board from previous placements is known (i.e. where are the other queens?)</li> <li>Current step backtracking: If a placement within the column does not lead to a solution, the queen is moved \"down\" the column (to the next row)</li> <li>Previous step backtracking: When all rows in a column have been tried, the call terminates and backtracks to the previous call (in the previous column)</li> <li>If a queen cannot be placed into column \\(i\\), do not try to place one in column \\((i+1)\\), rather, backtrack to column \\((i-1)\\) and move the queen that had been placed there. Using this approach we can reduce the number of potential solutions even more</li> </ol>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/04_Backtracking/#algorithm","title":"Algorithm","text":"Clash Possible if Row Won\u2019t happen cuz every iteration goes to next row implicitly Column If a queen is placed in column i, no other queen is placed in the same column Diagonal If two queens are placed at positions \\((i,j)\\) and \\((k,l)\\), then they are on the same diagonal only if\\(\\vert j \u2013 l\\vert  = \\vert i \u2013 k \\vert\\) <pre><code>Algorithm NQueens(k, n)\n{\n    for i=1 to n\n    {\n        if Place(k, i)\n        {\n            x[k] = i\n            if(k=n)\n                write(x[1:n])\n            else\n                NQueens(k+1, n)\n        }\n    }\n}\n\nAlgorithm Place(k, i)\n{\n    for j=1 to k-1\n    {\n        if (\n            x[j] == i // same col\n            or\n            abs(x[j]-i) == abs(j-k) // diagonal\n    )\n        return false\n    }\n\n    return true\n}\n</code></pre>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/04_Backtracking/#time-complexity","title":"Time Complexity","text":"Approach Complexity Brute Force \\(n^n\\) Refactored Brute Force(modify to prevent from checking queens occupying same rows) \\(n!\\) Backtracking \\(2^n\\)"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/04_Backtracking/#4x4-example","title":"4x4 Example","text":"<p>Only has 2 solutions</p> <ul> <li>\\((2, 4, 1, 3)\\)</li> <li>\\((3, 1, 4, 2)\\)</li> </ul> <p></p> <p></p> <p>where</p> <ul> <li>level 1, 2, 3, 4 correspond to queen 1, 2, 3, 4 respectively</li> <li>value at each node represents the position of the corresponding queen</li> </ul>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/04_Backtracking/#sum-of-subsets","title":"Sum of Subsets","text":"<p>From a given set, find subset of numbers whose sum adds up to a given number</p> <p>Given n positive integers \\(\\{w_1, \\dots, w_n\\}\\) and a positive integer \\(S\\). Find all subsets of \\(w1, \\dots wn\\) that sum to \\(S\\). Let \\(\\{x_1, x_2, x_3, \\dots, x_n\\}\\) takes value either 0 or 1.</p> <ul> <li>If \\(x_i = 1\\) then \\(w_i\\) is chosen</li> <li>If \\(x_i=0\\) then \\(wi\\) is not chosen</li> </ul>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/04_Backtracking/#using-bfs","title":"Using BFS","text":"<p>Edges from level \\(i\\) nodes to level \\(i+1\\) nodes are labeled with the value of \\(x_i\\) which is either zero or one. </p> <p>All paths from the root to a leaf node define the solution space. </p> <p>The left subtree of the root defines all subsets containing \\(w_1\\) while the right subtree defines all subsets not containing \\(w_1\\) etc. </p> <p>Variable-Sized tree with 24 leaf nodes which represent 16 possible tuples.</p> <p></p>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/04_Backtracking/#using-dfs","title":"Using DFS","text":"<p>Solution states are the leaf nodes </p> <p>The state space tree organization described here will be called static/Fixed sized tees. </p> <p>The tree organizations are independent of the problem instance being solved. Either fixed sized or variable sized tree can be opted. </p> <p>For some problems it is advantageous to use different tree organizations for different problem instances</p> <p></p>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/04_Backtracking/#algorithm_1","title":"Algorithm","text":"<pre><code>Algorithm SumOfSub(s, k, r)\n{\n    // generate left child\n    x[k] = 1\n\n    if s+w[k] == m\n        write(x[1:k])\n\n    else if s+w[k]+w[k+1] &lt;= m\n        SumOfSub(s+w[k], k+1, r-w[k])\n\n    // generate right child\n    if (\n        s+r-w[k] &gt;= m\n        and\n        s+w[k+1] &lt;= m\n    )\n    {\n        x[k] = 0\n        SumOfSub(s, k+1, r-w[k])\n    }\n}\n</code></pre>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/04_Backtracking/#complexity","title":"Complexity","text":"Brute Force \\(2^n\\) Backtracking \\(2^n\\), but faster"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/05_Branch_%26_Bound/","title":"05 Branch & Bound","text":"<p>Algorithm that is used for solving combinatorial optimization problems, which usually have exponential time complexity.</p> <p>Algorithm that is used for solving combinatorial optimization problems, which usually have exponential time complexity.</p> <p>All children of the E-node are generated before any other live node can become the E-node</p> <p>Bounding functions are used to help avoid the generation of sub trees that do not contain an answer node</p>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/05_Branch_%26_Bound/#types","title":"Types","text":"<ol> <li>DFS (Stack/FIFO)</li> <li>BFS (Queue/LIFO)</li> <li>Least cost branch and bound</li> </ol> <p>In both LIFO and FIFO types, the selection rule for the next E-node is rigid, resulting in exploring maximum of all the E-nodes consuming more time.</p>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/05_Branch_%26_Bound/#least-cost-branch-bound","title":"Least Cost Branch &amp; Bound","text":"<p>Selection of E-node is based on a cost function</p> <p>Not all the children of a node is generated; only nodes with least cost are expanded</p> <p>If we are close to the solution, first explore that path completely, instead of partially exploring other paths.</p>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/05_Branch_%26_Bound/#cost-function","title":"Cost Function","text":"<p>If Cost measure is used</p> <ul> <li>search would always generate the minimum number of nodes </li> <li>only nodes to become E-nodes are the nodes on the path from the root to the nearest answer node</li> </ul> <p>The difficulty with using the \"ideal\" cost functions is that computing the cost of a node will usually involve a search of the subtree X for an answer node.</p> <p>Hence, by the time the cost of a node is determined, that subtree has been searched and there is no need to explore X again. </p> <p>For this reason, search algorithms usually rank nodes based only on an estimated value, \u011d(), of their cost.</p>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/05_Branch_%26_Bound/#formula","title":"Formula","text":"\\[ \\hat c(x) = f\\Big(h(x)\\Big) + \\hat g(x) \\] <p>where</p> <ul> <li>\\(f(x)\\) is a non-decreasing function</li> <li>\\(h(x)\\) is cost reaching \\(x\\) from root</li> <li>\\(\\hat g(x)\\) is estimated additonal effort  to reach answer node from \\(x\\)</li> <li>\\(\\hat g(y) \\le \\hat g(x)\\), if \\(y\\) is a child of \\(x\\)</li> </ul> <pre><code>flowchart TB\nr((Root)) --&gt;\n|\"h(x)\"| x((x)) --&gt;\n|\"\u011d(x)\"| a((Answer&lt;br/&gt;Node ))</code></pre>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/05_Branch_%26_Bound/#cases-of-formula","title":"Cases of Formula","text":"Case \\(f\\Big(h(x)\\Big) =0\\) DFS \\(f\\Big(h(x)\\Big) \\ne 0\\) Favor node close to the root over a node which is many levels below, thus reducing possibility of unproductive deep searches into tree \\(f\\Big(h(x)\\Big) =\\) level of node \\(x\\) \\(\\hat g (x) = 0\\) BFS"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/05_Branch_%26_Bound/#cases-of-cost","title":"Cases of Cost","text":"\\(x\\) is answer node Subtree \\(x\\) has answer node \\(\\implies\\) \\(c(x)\\) \u2705 N/A Cost of reaching \\(x\\) from root \u274c \u2705 Cost of mininum cost answer node in subtree \\(x\\) \u274c \u274c \\(\\infty\\)"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/05_Branch_%26_Bound/#optimization-vs-decision-problem","title":"Optimization vs Decision Problem","text":"Optimization Decision Problem Value Type Continuous Binary Example Finding the value of shortest path Finding whether the shortest path in a graph is of length 20"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/05_Branch_%26_Bound/#backtracking-vs-branch-bound","title":"Backtracking vs Branch &amp; Bound","text":"Parameter Backtracking Branch and Bound Approach Find all possible solutions available to a problem.When a wrong choice is made, perform backtracking Once a better optimal solution is obtained than a pre-solution leads to, it abandons that pre-solution Traversal DFS DFSBFS Function Feasibility function Bounding function Problems Decision Problem Optimization Problem Searching Until solution is obtained Tree need to be searched completely Efficiency better worse"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/05_Branch_%26_Bound/#4-queens","title":"4 Queens","text":""},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/05_Branch_%26_Bound/#fifo","title":"FIFO","text":""},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/05_Branch_%26_Bound/#lifo","title":"LIFO","text":""},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/05_Branch_%26_Bound/#least-cost","title":"Least Cost","text":""},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/06_Approximation_Algorithms/","title":"06 Approximation Algorithms","text":"<p>Algorithm that generates a feasible solution with value close to the optimal solution.</p> <p>To reduce an time complexity of solving an optimization problem</p> <ol> <li>We remove the requirement that the algorithm must always generate an optimal solution</li> <li>We use an \u2018Probabilistically Good Algorithm\u2019, that almost always generates optimal solution</li> </ol>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/06_Approximation_Algorithms/#np-problem","title":"NP Problem","text":"<p>A problem is an NP (nondeterministic polynomial time) problem if it is solvable in polynomial time by a nondeterministic Turing machine</p> <p>A P-problem whose solution time is bounded by a polynomial is always also NP</p> <p>eg: 0/1 knapsack, traveling salesperson</p>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/06_Approximation_Algorithms/#terminology","title":"Terminology","text":"Symbol Meaning \\(p\\) NP problem \\(I\\) Instance of \\(P\\) \\(\\overset{\\star} F (I)\\) Opitmal solution to \\(I\\) \\(\\hat F(I)\\) Approximate solution to \\(I\\) \\(A\\) Algorithm that generates feasible solution to every \\(I\\) of \\(P\\) \\(\\hat F(I) &lt; \\overset{\\star} F (I)\\) Maximization problem \\(\\hat F(I) &gt; \\overset{\\star} F (I)\\) Minimization problem"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/06_Approximation_Algorithms/#types-of-approximation-algorithms","title":"Types of Approximation Algorithms","text":"Type Meaning Application Absolute \\(\\vert \\hat F(I) - \\overset{\\star} F (I) \\vert \\le k\\), for some constant \\(k\\)for every \\(I\\) of \\(P\\) Planar Graph ColoringMaximum Programs Stored Problem \\(f(n)\\) \\(\\frac{\\vert \\hat F(I) - \\overset{\\star} F (I)  \\vert}{\\overset{\\star} F (I)} \\le f(n)\\), for \\(\\overset{\\star} F (I) &gt; 0\\)for every \\(I\\) of \\(P\\) \\(\\epsilon\\) \\(f(n)\\) approximation algo for which \\(f(n) \\le \\epsilon\\), where \\(\\epsilon\\) is some constant"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/06_Approximation_Algorithms/#planar-graph-coloring","title":"Planar Graph Coloring","text":"<p>Coloring vertices of a graph, such that no two adjacent vertices have the same color</p> <p>Goal: Minimize no of colors used</p> <p>A graph is planar if it can be represented by a drawing in the plane such that no edges cross.</p> <p>The maximum no of colors required for a planar graph is 4</p> <pre><code>Algorithm Acolor(V, E)\n{\n  if V = null\n    return 0\n  else if E = null\n    return 1\n  else if (G is bipartite)\n    return 2\n  else\n    return 4\n}\n</code></pre> <p>Time Complexity: \\(O(|V|+|E|)\\), which is the time taken to check if graph is bipartite</p>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/06_Approximation_Algorithms/#maximum-program-stored-problem","title":"Maximum Program Stored Problem","text":"<p>Consider</p> <ul> <li>\\(n\\) programs</li> <li>two disks with storage capacity of \\(L\\) each. </li> <li>list \\(l\\) where \\(l_i\\) is the storage required to store program \\(i\\)</li> </ul>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/06_Approximation_Algorithms/#goal","title":"Goal","text":"<p>Determine max no of programs that can be stored on the disks, without splitting a program over the disks.</p>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/06_Approximation_Algorithms/#solution","title":"Solution","text":"<ol> <li>Sort \\(l\\) in ascending order (to maximize count; in knapsack, we don\u2019t maximize count, we maximize profit)</li> <li>Keep placing elements</li> </ol>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/06_Approximation_Algorithms/#algorithm","title":"Algorithm","text":"<pre><code>Algorithm Pstore(l, n, L) \n{ \n    // sort l in ascending order\n\n    i = 1\n    for j=1 to 2\n    {\n        sum = 0; // amount (part) of disk j already assigned \n    while (sum + l[i]) &lt;= L\n    { \n        write (\"store program\", i, \"on disk\", j)\n        sum = sum + l[i]\n        i = i + 1\n        if i &gt; n\n        return\n    }\n  }\n}\n</code></pre>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/06_Approximation_Algorithms/#time-complexity","title":"Time Complexity","text":"<p>\\(O(n) + \\underbrace{O(n \\log n)}_{\\text{Sorting}} = O(n \\log n)\\)</p> <p>The most optimal algorithm will have exponential time.</p>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/07_Network_Flow/","title":"07 Network Flow","text":"<p>blah</p>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/07_Network_Flow/#properties","title":"Properties","text":"Condition Meaning Mathematical representation Capacity \\(0 \\le \\text{flow}_e \\le \\text{capacity}_e\\) \\(0 \\le f(e) \\le c_e, \\ \\forall e \\in E\\) Conservation Inflow = Outflow @ every node \\(\\sum\\limits_\\text{Inflow} f(e) = \\sum\\limits_\\text{Outflow} f(e), \\ \\forall e \\in E\\)"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/07_Network_Flow/#algorithm","title":"Algorithm","text":"<ol> <li>Find all paths from source to destination</li> <li>The maximum flow is limited by the bottleneck, which is the lowest value in a path, ie \\(\\text{argmin} (c_e), \\ e \\in P\\)</li> </ol>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/07_Network_Flow/#residual-graph","title":"Residual Graph","text":"<p>Indicates how much more flow is allowed in each edge in the network graph</p> Path Direction of flow in residual graph Unused Same Partially used Used will be reversedUnused will be same Completely used Reversed"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/08_Parallelization/","title":"08 Parallelization","text":""},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/08_Parallelization/#multi-threading","title":"Multi-Threading","text":"<p>Ability of a single core of a CPU to have concurrent threads of execution, supported by the OS</p> <p>The threads share the resources of the single/multiple cores, including computing units, cache, and TLB (translation look-aside buffer)</p>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/08_Parallelization/#real-life-examples","title":"Real-Life Examples","text":"<ul> <li>Web Browsers</li> <li>Download multiple files at the same time</li> <li>One website blocking downloads will not affect other downloads</li> <li>Web Servers</li> <li>A thread web server handles each request with a new thread</li> <li>There is a thread pool and each time a new request comes in, it is assigned to a thread from the thread pool</li> </ul>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/08_Parallelization/#parallelization","title":"Parallelization","text":"<p>Multi-threading a single algorithm so that some of its instructions can be executed simultaneously</p> <p>This can be applied to scheduling &amp; managing multiple algorithms, each running concurrently on their own threads and possibly sharing resources.</p>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/08_Parallelization/#types-of-multi-threading","title":"Types of Multi-Threading","text":"Static Dynamic Type of Control Explicit Implicit Real Control Programmer Concurrency Platform(maps concurrency opportunities specified by programmer to actual static threads) Flexible \u274c \u2705"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/08_Parallelization/#dynamic-multi-threading","title":"Dynamic Multi-Threading","text":"Keyword Meaning <code>parallel</code> Add to loop construct such as <code>for</code>Indicates each iteration can be executed in parallel <code>spawn</code> Create a parallel subprocess, then keep executing current process <code>sync</code> Wait here until all active parallel threads created by this instance of the program finish <p><code>parallel</code> and <code>spawn</code> are not compulsory to be followed; they do not force parallelism, they just say that this is permissible. A scheduler will make the decision concerning allocation to processors.</p> <p>However, if parallelism is used, sync must be respected. For safety, there is an explicit sync at the end of every procedure.</p>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/08_Parallelization/#example-fibonacci","title":"Example: Fibonacci","text":"<pre><code>F(0)\u00a0= 0\nF(1)\u00a0= 1\nF(i)\u00a0= F(i-1)\u00a0+ F(i-2), for\u00a0i\u00a0\u2265 2\n</code></pre>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/08_Parallelization/#non-parallel","title":"Non-Parallel","text":"<pre><code>fib(n):\n    if n &lt;= 1\n        return n\n    else\n        x = fib(n-1)\n        y = fib(n-2)\n        return x + y\n</code></pre> \\[ \\begin{aligned} T(n) &amp;= T(n\u00a0\u2212 1) + T(n\u00a0\u2212 2) + \\theta(1) \\\\ &amp;= \\theta(F_n) \\end{aligned} \\] <p>This grows exponentially, this is not very efficient.</p>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/08_Parallelization/#parallel","title":"Parallel","text":"<pre><code>fib(n):\n    if n &lt;= 1\n        return n\n    else\n        x = spawn fib(n-1)\n        y = fib(n-2)\n        sync\n        return x + y\n</code></pre> <p><code>sync</code> is required to avoid <code>x</code> and <code>y</code> getting summed before <code>x</code> is computed. Even if there wasn\u2019t an explicitly-mentioned <code>sync</code>, every function does an implicit <code>sync</code> and then only terminates</p>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/08_Parallelization/#modelling-dynamic-multi-threading","title":"Modelling Dynamic Multi-Threading","text":"<p>Consider a computation DAG (Directed Acyclic Graph) \\(G=(V, E)\\)</p> <ul> <li>Vertices represent instructions/strand</li> <li>A strand is a sequence of non-parallel instructions</li> <li>A strand with multiple successors means that all but one of them must have spawned.</li> <li>A strand with multiple predecessors means that they join at a sync statement.</li> <li>Edges represent dependencies between edges</li> <li>\\((u, v) \\in E\\) means \\(u\\) must execute before \\(v\\)<ul> <li>This means that \\(u\\) and \\(v\\) are in series</li> </ul> </li> <li>All \\((u, v) \\not \\in E\\) means that they are \\(u\\) and \\(v\\) are logically parallel</li> </ul> <p>We assume an ideal parallel computer with sequentially-consistent memory: it behaves as if instructions are executed sequentially in some full ordering consistent with orderings within each thread (consistent with partial-ordering of computation DAG)</p> <p></p> <p>Rounded rectangles are not part of the formal model, but they help organize the visualization by collecting together all strands for a given call</p> <p>The colors are specific to this example and indicate the corresponding code: black indicates that the strand is for lines 1-3; grey for line 4; and white for lines 5-6</p> <p></p>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/09_NP/","title":"09 NP","text":""},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/09_NP/#problems","title":"Problems","text":"P NP NP Complete NP Hard Can be solved using __ algorithm Deterministic Non-Deterministic Non-Deterministic Non-Deterministic Time Complexity (Solving) Polynomial Exponential Exponential Exponential Time Complexity (Verification) Polynomial Polynomial Exponential Type of problem Any Any Decision Problem Any IDK Problem L \u00a0is NP Complete if- L is in NP- Every problem in NP is reducible to L in polynomial time Example Determination of hamiltonian cycleDetermination of satisfaction level of boolean formula Circuit SatisfactoryVertex CoverHalting problems"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/09_NP/#problem-properties","title":"Problem Properties","text":""},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/09_NP/#reducible","title":"Reducible","text":"<p>\\(L_1 \\underset{\\text{reduces to}}{\\propto} L_2 \\iff\\) there is a way to solve \\(L_1\\) using a deterministic algorithm that solves \\(L_2\\) in polynomial time</p> <p>This is transitive, ie, \\(L_1 \\propto L_2, L_2 \\propto L_3 \\implies L_1 \\propto L_3\\)</p>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/09_NP/#polynomially-equivalent","title":"Polynomially-Equivalent","text":"<p>\\(L_1\\) and \\(L_2\\) are polynomially-equivalent \\(\\iff (L_1 \\propto L_2) \\land (L_2 \\propto L_1)\\)</p> <p>To show that a problem \\(L_2\\) is NP-Hard, it is sufficient to show that \\(L_1 \\propto L_2\\), where \\(L_1\\) is a problem already known to be NP-Hard.</p> <p>Due to transitivity, if satisfiability \\(\\propto L_1\\) and \\(L_1 \\propto L_2 \\implies\\) Satisfiability \\(\\propto L_2\\)</p> <p>To show that an NP-Hard decision problem is NP-Complete, we just need to find a polynomial time non-deterministic algorithm for it.</p>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/09_NP/#p-np-algorithms","title":"P &amp; NP Algorithms","text":"P Algorithm NP Algorithm Deterministic Non-Deterministic Polynomial Polynomial Used to solve exponential problems"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/09_NP/#non-deterministic-algorithms","title":"Non-Deterministic Algorithms","text":"<code>choice(S)</code> Arbitrarily choose one of the elements in set \\(S\\) <code>failure()</code> Signals unsuccessful completion <code>success()</code> Signals successful completion"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/09_NP/#non-deterministic-search-algorithm","title":"Non-Deterministic Search Algorithm","text":"<pre><code>Algo Nsearch(a, n, key)\n{\n    c = Choice([1, n]);\n\n    if (a[c] == key)\n    success()\n    else\n    failure();\n}\n</code></pre>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/09_NP/#non-deterministic-sort-of-ve-integers","title":"Non-Deterministic Sort of +ve integers","text":"<pre><code>Algo Nsort(a, n)\n\n    for i=1 to n\n        b[i] = 0\n\n    // randomly place elements\n    for i=1 to n\n        c = choice([1, n])\n        if b[c] != 0 // element already not there\n            b[c] = a[i]\n        else // element exists at same position\n            failure()\n\n    // verify order\n    for i=1 to n\n        if b[i] &gt; b[i+1]\n            failure()\n\n    sucess()\n</code></pre>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/09_NP/#non-deterministic-binary-knapsack","title":"Non-Deterministic Binary Knapsack","text":"<pre><code>Algo NDBK(profits, weights, min_profit, max_weight)\n    taken_profit = 0\n    taken_weight = 0\n\n    for i=1 to n do\n        c = choice(0, 1)\n\n        if c == 1 then\n      taken_profit += profits[i]\n      taken_weight += weights[i]\n\n  if (\n    (taken_profit &gt;= min_profit) &amp;&amp;\n    (taken_weight &lt;= max_weight)\n  )\n    success()\n  else\n    failure()\n</code></pre>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/09_NP/#clique-problem","title":"Clique Problem","text":"<p>A clique is complete subgraph of a given graph, ie, every node of the subgraph is connected to each other.</p> <pre><code>Algo clique(a) // adjacency matrix\n    s = [] // subgraph\n\n    // randomly add elements\n    for i=1 to a.length\n        c = choice(0, 1)\n\n        if c==1\n            s.add(\n                (i, j)\n            )\n\n    // verify subgraph\n    for i=1 to a.length\n        for j=1 to a.length\n            i not connected to j then\n                failure()\n\n    success()\n</code></pre>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/09_NP/#satisfiability","title":"Satisfiability","text":"<pre><code>Algo sat(E, n)\n    for i=1 to n do\n        x[i] = choice(false, true)\n\n    if E(x[1], ..., x[n]) then\n        success()\n    else\n        failure()\n</code></pre>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/09_NP/#np-hard-graph-problems","title":"NP-Hard Graph Problems","text":"<p>To prove that a problem \\(L_2\\) is NP-hard</p> <ol> <li>Pick a problem \\(L_1\\) already known to be NP-hard</li> <li>Show how to obtain (in polynomial deterministic time) an instance \\(I'\\) of \\(L_2\\) from any instance \\(I\\) of \\(L_1\\) such that from the solution of \\(I'\\), we can determine (in polynomial deterministic time) the solution to instance \\(I\\) of \\(L_1\\)</li> <li>Conclude from 2. that \\(L_1 \\propto L_2\\)</li> <li>Conclude from 1 &amp; 3 and transitivity that \\(L_2\\) is NP-hard</li> </ol>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/09_NP/#clique-decision-problem","title":"Clique Decision Problem","text":""},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/09_NP/#node-cover-decision-problem","title":"Node Cover Decision Problem","text":""},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/09_NP/#chromatic-number-decision-problem","title":"Chromatic Number Decision Problem","text":""},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/10_Linear_Programming/","title":"10 Linear Programming","text":"<p>If we can specify the objective as a linear function of variables, and if we can specify the constraints on resources as equalities/inequalities, we have a linear programming problem</p> <p>I already did optimization course, so I\u2019ve skipped a lot.</p>"},{"location":"3_Core/Design_%26_Analysis_of_Algorithms/10_Linear_Programming/#conversions","title":"Conversions","text":"<ul> <li>Convert from min problem to max problem</li> <li>Negate the coefficients in objective function</li> <li>If some of the variables do not have \\(\\ge 0\\) constraints</li> <li>Add \\(\\ge 0\\) constraints \\(x'_j &gt; 0, x''_j &gt; 0\\)</li> <li>Replace \\(x_j \\to x'_j - x''_j\\)</li> <li>Replace \\(c_j x_j \\to c_j x'_j - c_j x''_j\\)</li> <li>Replace \\(a_{ij} x_j \\to a_{ij} x'_j - a_{ij} x''_j\\)</li> <li>Any feasible solution obtained corresponds to<ul> <li>\\(\\bar x_j = \\hat x'_j - \\hat x''_j\\)</li> <li>\\(\\hat x'_j = \\bar x_j, \\hat x''_j = 0, \\text{ if }\\bar x_j \\ge 0\\)</li> <li>\\(\\hat x''_j = \\bar x_j, \\hat x'_j = 0, \\text{ if }\\bar x_j \\le 0\\)</li> </ul> </li> <li>Convert constraint \\(\\ge k \\implies \\le -k\\)</li> <li>Convert constraint \\(= k \\implies \\le k \\ \\&amp; \\ge k\\)</li> <li>Add slack</li> </ul>"},{"location":"3_Core/Immunology/","title":"Immunology","text":"<p>Taught by Dr. Mainak Dutta</p> <p>(add description)</p>"},{"location":"3_Core/Operating_Systems/","title":"Operating Systems","text":"<p>Taught by Dr. Angel Arul Jothi</p> <p>This course introduces the fundamental concepts of operating systems (OS), focusing on their roles, structures, and management techniques. Students will examine the evolution of OS, process management, and synchronization, as well as memory and storage management strategies.</p>"},{"location":"3_Core/Operating_Systems/#key-learning-objectives-include","title":"Key learning objectives include:","text":"<ul> <li>Understanding the purpose and architecture of operating systems (OS operations and computing environments).</li> <li>Exploring the evolution of operating systems (batch processing, multiprogramming, time-sharing).</li> <li>Investigating special-purpose operating systems (real-time systems and multimedia applications).</li> <li>Examining OS structures and services (system calls, debugging, and system boot processes).</li> <li>Analyzing process management (process control blocks, states, and inter-process communication).</li> <li>Understanding scheduling algorithms (FCFS, SJF, priority scheduling, and round-robin).</li> <li>Exploring process synchronization (critical section problems and synchronization resources).</li> <li>Examining deadlock conditions and strategies (preemptable resources and deadlock detection).</li> <li>Analyzing memory management techniques (fixed and dynamic partitioning).</li> <li>Understanding virtual memory management (paging concepts and demand paging).</li> <li>Investigating page replacement algorithms (FIFO, LRU, and optimal algorithms).</li> <li>Exploring secondary storage structures and I/O scheduling (disk management and RAID).</li> <li>Understanding file system implementation (allocation methods and free space management).</li> <li>Examining multimedia operating systems (design considerations for multimedia applications).</li> </ul> <p>This course provides students with essential theoretical knowledge and practical understanding of operating systems, preparing them for further studies and careers in computer science and software development.</p>"},{"location":"3_Core/Operating_Systems/01_Intro/","title":"01 Intro","text":""},{"location":"3_Core/Operating_Systems/01_Intro/#software","title":"Software","text":"<p>a set of programs</p>"},{"location":"3_Core/Operating_Systems/01_Intro/#types-of-softwares","title":"Types of Softwares","text":"Application Softwares System Softwares Purpose Helps users with specific applications - Provides environment for application software- Manage hardware resources Example Word, Paint - Operating System- Emulator- Language Translators (Compiler, Interpreters, Assemblers)- Linker, Loader"},{"location":"3_Core/Operating_Systems/01_Intro/#operating-system","title":"Operating System","text":"<p>System software that acts as an interface between users and the hardware resources of a computing system.</p> <ul> <li>Resource allocator<ul> <li>Keeps track of occupied/empty portions of the primary and secondary memory</li> <li>Status of I/O Devices</li> </ul> </li> <li>control program</li> <li>Kernel of a computing system   Kernel is the most important part of any computing system/programming environment<ul> <li>Kernel is system software which is part of operating system</li> <li>Kernel provides interface between hardware and software components</li> </ul> </li> </ul> \\[ \\fbox{System, Application Software $\\fbox{ Operating System $\\fbox{Kernel $\\fbox{H/W Resources}$}$}$} \\]"},{"location":"3_Core/Operating_Systems/01_Intro/#types-of-operating-system","title":"Types of Operating System","text":"<ul> <li>Mobile</li> <li>Personal Computer</li> <li>Real Time</li> <li>Distributed</li> </ul>"},{"location":"3_Core/Operating_Systems/01_Intro/#hardware-resources","title":"Hardware Resources","text":"<ul> <li>Memory<ul> <li>Primary is volatile</li> <li>Secondary is non-volatile</li> </ul> </li> <li>Processor</li> <li>I/O Devices</li> </ul>"},{"location":"3_Core/Operating_Systems/01_Intro/#example","title":"Example","text":"<pre><code>flowchart LR\n\ngc[Google Chrome] --&gt;\nCPU((CPU)) --&gt;\nScreen[Output Screen]\n\nHD[(Hard Disk)] --&gt;\n|Operating System &lt;br/&gt; loads| pm[[Primary Memory]]</code></pre>"},{"location":"3_Core/Operating_Systems/01_Intro/#device","title":"Device","text":"<pre><code>flowchart LR\nOS &lt;--&gt;\ndd[Device Driver] &lt;--&gt;\ndc[Device Controller] &lt;--&gt;\nDevice</code></pre>"},{"location":"3_Core/Operating_Systems/01_Intro/#controller","title":"Controller","text":"<ul> <li>Data Register/Local buffer</li> <li>Command Register</li> </ul>"},{"location":"3_Core/Operating_Systems/01_Intro/#driver","title":"Driver","text":"<ul> <li>Interface between OS &amp; Device</li> <li>Understands the device controller &amp; device</li> </ul>"},{"location":"3_Core/Operating_Systems/01_Intro/#daemons","title":"Daemons","text":"<p>Background system processes, that are not in direct control of user</p>"},{"location":"3_Core/Operating_Systems/01_Intro/#interrupts","title":"Interrupts","text":"<p>Asynchoronous request to the CPU, handled by the OS using the ISR (Interrupt Service Routine).</p> <p>They can be initiated anytime without reference to the system clock.</p> <p>Could be hardware-generated or software-generated.</p>"},{"location":"3_Core/Operating_Systems/01_Intro/#isr-ivt","title":"ISR &amp; IVT","text":"<p>ISR and IVT(Interrupt Vector Table) are stored in fixed location in memory.</p> <p>ISR is accessed using IVT, which contains the starting address of the ISRs.</p> \\[ \\begin{aligned} &amp;\\text{Starting address of ISR in IVT}\\\\ &amp; = \\text{Starting address of IVT in memory } \\\\ &amp; \\quad + ( \\text{type} \\times \\text{no. of loc to store address of 1 ISR} ) \\end{aligned} \\] <p>8086 has 256 vectored interrupts</p> <p>Each ISR requires 4 bytes</p> <ul> <li>2 bytes of IP   followed by</li> <li>2 bytes of CS</li> </ul> <p>If the starting address of IVT is \\(\\text{00000_H}\\), then the ending address is \\(\\text{003FF_H}\\)</p> <p></p>"},{"location":"3_Core/Operating_Systems/01_Intro/#graph","title":"Graph","text":"<p>(take from slides)</p>"},{"location":"3_Core/Operating_Systems/01_Intro/#interrupt-latency","title":"Interrupt latency","text":"<p>Time taken to service an interrupt</p>"},{"location":"3_Core/Operating_Systems/01_Intro/#advantages","title":"Advantages","text":"<ul> <li>Save processor time<ul> <li>Processor resources are very valuable, as it can be used for some other task</li> </ul> </li> <li>Avoid polling<ul> <li>Going around asking I/O devices; wasting processor time</li> </ul> </li> </ul> <pre><code>flowchart LR\n\nsubgraph Write Operation\ndirection LR\nup2[User Program] --&gt;\nOS2[OS] --&gt;\ndd2[Device Driver] --&gt;\ndc2[Device Controller] &lt;--&gt;\nod2[Output Device]\n\ndc2 --&gt;|Status| dd2\nend\n\nsubgraph Read Operation\ndirection LR\nup[User Program] --&gt;\nOS --&gt;\ndd[Device Driver] --&gt;\ndc[Device Controller] &lt;--&gt;\nid[Input Device]\n\ndc --&gt;|Interrupt| dd\nend</code></pre>"},{"location":"3_Core/Operating_Systems/01_Intro/#program-counter","title":"Program Counter","text":"<p>Similar to instruction pointer of x86</p> <ul> <li>Initially points to 1<sup>st</sup> instruction</li> <li>Subsequently, points to the address of the next instruction to be executed</li> </ul>"},{"location":"3_Core/Operating_Systems/01_Intro/#interrupt-handling","title":"Interrupt Handling","text":"<ol> <li>An interrupt that occurs in between an instruction can only happen after fetch, decode, execute, write-back of that instruction is first complete.</li> <li> <p>When interrupt occurs, we need to push the following values into stack</p> <ul> <li> <p>PC so that we can return to the same point after finishing the interrupt</p> </li> <li> <p>CPU state  Contents of all CPU and flag registers</p> </li> <li>Service the interrupt using ISR</li> <li>Restore processor state</li> <li>Load the saved return address into the program counter</li> <li>Resume interrupted computation</li> </ul> </li> </ol>"},{"location":"3_Core/Operating_Systems/01_Intro/#storage-of-multi-byte-data","title":"Storage of Multi-Byte data","text":"<p>Little Endian = Lower Byte is stored first then Higher Byte.  This is what 8086 uses</p> <p>Big Endian = Higher Byte is stored first then Lower Byte</p>"},{"location":"3_Core/Operating_Systems/01_Intro/#storage-structure","title":"Storage Structure","text":"<ul> <li>Bit (0/1) [most basic unit]</li> <li>Byte = 8 bits</li> <li>Word = Group of bytes</li> </ul> Bytes KB \\(2^{10}\\) MB \\(2^{20}\\) GB \\(2^{30}\\) TB \\(2^{40}\\) PB \\(2^{50}\\)"},{"location":"3_Core/Operating_Systems/01_Intro/#types-of-storage-devices","title":"Types Of Storage Devices","text":"<pre><code>flowchart TB\nMemory --&gt;\ncpu[CPU Registers] &amp; Cache &amp; Primary &amp; Secondary &amp; Tertiary\n\nPrimary[\"Primary&lt;br/&gt;(Volatile)\"] --&gt; RAM\nSecondary[\"Secondary&lt;br/&gt;(Non-Volatile)\"] --&gt; HDD &amp; SSD\nTertiary[\"Tertiary&lt;br/&gt;(Non-Volatile Backup)\"] --&gt; CD &amp; DVD &amp; Backup</code></pre> Feature Order Speed Reg &gt; Cache &gt; PM &gt; SM &gt; TM Cost Reg &gt; Cache &gt; PM &gt; SM &gt; TM Access Time Reg &lt; Cache &lt; PM &lt; SM &lt; TM Size Reg &lt; Cache &lt; PM &lt; SM &lt; TM <pre><code>flowchart TB\n\nsubgraph While Using\nCPU2[CPU] --&gt;|1| PM2[PM] --&gt;|2| CPU2\nend\n\nsubgraph First Time\ndirection LR\n\nCPU1[CPU]\nCPU1 --&gt;\n|1| PM1[PM] --&gt;\n|2| SM1[(SM)]\n\nSM1 --&gt;|3|PM1\n\nPM1 --&gt;|4| CPU1\nend</code></pre>"},{"location":"3_Core/Operating_Systems/01_Intro/#booting","title":"Booting","text":"<p>Process of loading OS Kernel into the primary memory</p>"},{"location":"3_Core/Operating_Systems/01_Intro/#steps","title":"Steps","text":"<ul> <li>Starting address of Bootstrap program is stored into the Program Counter</li> <li>Bootstrap loader loads the OS using Boostrap program</li> </ul>"},{"location":"3_Core/Operating_Systems/01_Intro/#bootstrap-program","title":"Bootstrap Program","text":"<ul> <li>In Intel architecture, it\u2019s called as BIOS(Basic Input Output System)</li> <li>In Unix architecture, it\u2019s called as GRUB(GRand Unified Bootloader)</li> <li>In Android, LK(Little Kernel)</li> </ul>"},{"location":"3_Core/Operating_Systems/01_Intro/#tasks","title":"Tasks","text":"<ol> <li>Run POST(Power-On Self Test) diagnostics</li> <li>Initialize and check peripheral devices</li> <li>Initializes other aspects of the system, such as the registers</li> <li>Locates and loads the kernel</li> </ol>"},{"location":"3_Core/Operating_Systems/01_Intro/#rom","title":"ROM","text":"<p>Read-Only Memory</p>"},{"location":"3_Core/Operating_Systems/01_Intro/#eeprom","title":"EEPROM","text":"<p>Electrically Erasable Programmable Read-Only Memory</p> <p>It is firmware (combination of hardware and software that can hold code)</p> <p>Bootstrap program is burnt into EEPROM</p> <p>System Programs loaded to PM, System process, Deamons, Program in its execution, Printer network, Background Process</p> <p>In linux, the first program is <code>systemd</code></p>"},{"location":"3_Core/Operating_Systems/01_Intro/#software-interrupt","title":"Software Interrupt","text":"<p>Trap/exception</p> Source/Cause Example Errors - Divide by zero- Access to illegal parts of memory System call When user actions requires something like input/output <pre><code>flowchart LR\n\nUP[User Program]\nOS\nTasks\n\nsubgraph Tasks\n    direction LR\n    io[I/O Management]\n    Memory\nend\n\nUP --&gt;|Request| OS --&gt; Tasks</code></pre>"},{"location":"3_Core/Operating_Systems/01_Intro/#programming-types","title":"Programming Types","text":"<p>Uni was before.</p> Uni Multi Primary memory parts - OS area- User area can be used by only 1 program - OS area- User area contains multiple programs Advantage When one program is waiting for I/O, the OS sends off another program to the CPU. Disadvantage This has improper utilization of system resources, especially when I/O devices are being used. Malicious programs can affect the other program\u2019s segments; before it was even possible for them to affect the OS"},{"location":"3_Core/Operating_Systems/01_Intro/#modes-of-operations","title":"Modes of Operations","text":"<p>This mode bit will change continuously.</p> Mode Bit Mode Computer is executing 0 Kernel/Supervisor/Priveledged OS code/System call 1 User User code"},{"location":"3_Core/Operating_Systems/01_Intro/#privileged-execution-of-instruction","title":"Privileged Execution of Instruction","text":"<p>Privilege signifies the access level of a program</p> <p>I/O, memory, timer, CPU, and Interrupts are privileged</p> <p>Privilege level varies from</p> <ul> <li>0 (high privilege)</li> <li>3 (low privilege)</li> </ul> <p>If a program trying to access memory is privileged, then it is checked if it is in Kernel mode or User mode.</p> <p>If it is in user mode, a trap is generated.</p>"},{"location":"3_Core/Operating_Systems/01_Intro/#lab-code","title":"Lab code","text":"<p>Lab</p>"},{"location":"3_Core/Operating_Systems/01_Intro/#i-missed-something-sep-14-1st-hour","title":"I missed something (Sep 14 1<sup>st</sup> Hour)","text":""},{"location":"3_Core/Operating_Systems/02_Functions_of_OS/","title":"02 Functions of OS","text":""},{"location":"3_Core/Operating_Systems/02_Functions_of_OS/#functions-of-os","title":"Functions of OS","text":""},{"location":"3_Core/Operating_Systems/02_Functions_of_OS/#process-management","title":"Process Management","text":"<ul> <li>Job scheduling<ul> <li>Picks processes from job pool to be placed in PM</li> <li>Process = Program in its execution</li> </ul> </li> <li>CPU Scheduling<ul> <li>Pick a process from PM and allocate to CPU</li> </ul> </li> <li>Process Synchronization</li> <li>Deadlock Handling</li> </ul>"},{"location":"3_Core/Operating_Systems/02_Functions_of_OS/#memory-management","title":"Memory Management","text":"<ul> <li>Allocate/Deallocate PM to processes</li> <li>Keep track of allocated and unallocated parts of PM</li> </ul>"},{"location":"3_Core/Operating_Systems/02_Functions_of_OS/#storage-management","title":"Storage Management","text":"<ul> <li>File System Management</li> <li>Mass Storage/Disk Management</li> <li>I/O Management</li> </ul>"},{"location":"3_Core/Operating_Systems/02_Functions_of_OS/#protection-and-security","title":"Protection and Security","text":""},{"location":"3_Core/Operating_Systems/03_Evolution/","title":"03 Evolution","text":""},{"location":"3_Core/Operating_Systems/03_Evolution/#early-computers","title":"Early Computers","text":"<p>Something compiler</p> <p>Aim was to optimize CPU time</p>"},{"location":"3_Core/Operating_Systems/03_Evolution/#batch-processing-systems","title":"Batch Processing Systems","text":"<p>Shared computer systems</p>"},{"location":"3_Core/Operating_Systems/03_Evolution/#steps","title":"Steps","text":"<ul> <li>Operator hired</li> <li>Jobs with similar needs were batched together and run though computer, to reduce setup time</li> </ul>"},{"location":"3_Core/Operating_Systems/03_Evolution/#problems","title":"Problems","text":"<ul> <li>Stopped jobs (normal/abnormal)</li> <li>Dump (Log file)   Status of the memory is stored into a text file</li> <li>Load device with next job</li> <li>Restart computer</li> </ul>"},{"location":"3_Core/Operating_Systems/03_Evolution/#issues","title":"Issues","text":"<p>(i missed this)</p> <p>CPU Burst</p> <p>I/O Burst</p>"},{"location":"3_Core/Operating_Systems/03_Evolution/#solution","title":"Solution","text":"<ul> <li>Perform CPU execution and I/O concurrently</li> <li>Multi-programming and Time Sharing<ul> <li>Multi-programming \\(\\ne\\) Multi-Processing</li> <li>Multi-Programming means using 1 CPU and performing multiple programs concurrently</li> <li>Multi-Processing means using multiple CPUs</li> </ul> </li> </ul>"},{"location":"3_Core/Operating_Systems/03_Evolution/#time-sharingmulti-tasking-systems","title":"Time Sharing/Multi-Tasking Systems","text":"<p>Extension of Multiprogramming</p>"},{"location":"3_Core/Operating_Systems/03_Evolution/#time-multiplexing","title":"Time Multiplexing","text":"<p>CPU switch between the programs kept in PM</p> <p>This is done by dividing CPU time into Time Quanta(fixed intervals)</p> <p>Involves timer = counters + clock</p>"},{"location":"3_Core/Operating_Systems/03_Evolution/#space-multiplexing","title":"Space Multiplexing","text":"<p>After partitioning primary memory into OS and User area, the User Area is then further partitioned for multiple users</p>"},{"location":"3_Core/Operating_Systems/03_Evolution/#advantages","title":"Advantages","text":"<ul> <li>Supported user interaction</li> <li>Improved response time</li> </ul>"},{"location":"3_Core/Operating_Systems/03_Evolution/#difference-from-multi-programming","title":"Difference from Multi-Programming","text":"<p>A single CPU switches between multiple users, in a way that every user feels as if they are using the CPU.</p>"},{"location":"3_Core/Operating_Systems/03_Evolution/#steps_1","title":"Steps","text":"<ol> <li>P1 runs to completion</li> <li>P1 requests I/O</li> <li>P1 has not completed the execution, but the time quanta expired</li> <li>Timer interrupt occurs (as the down-counting timer is over), the CPU switches to another user</li> </ol>"},{"location":"3_Core/Operating_Systems/03_Evolution/#time-delay","title":"Time Delay","text":"<p>Time between switching between 2 users</p> <p>= Count value * Time Period</p>"},{"location":"3_Core/Operating_Systems/03_Evolution/#features","title":"Features","text":"<ul> <li>Job, CPU Scheduling</li> <li>Memory Management</li> <li>Resonable response time<ul> <li>Using Virtual memory</li> <li>Swap/Roll</li> </ul> </li> <li>File system and disk management</li> <li>job sync and communication</li> <li>Deadlocks handling</li> <li>protection and security</li> </ul>"},{"location":"3_Core/Operating_Systems/03_Evolution/#swaproll","title":"Swap/Roll","text":"Swap-in/Roll-In Swap-out/Roll-out Moving partially executed program from ___ memory secondary \\(\\to\\) primary primary \\(\\to\\) secondary Basically like loading in"},{"location":"3_Core/Operating_Systems/03_Evolution/#multi-tasking","title":"Multi-Tasking","text":"<p>Each program of the same user is a \u2018task\u2019. The processor switches between each task.</p>"},{"location":"3_Core/Operating_Systems/03_Evolution/#computer-system","title":"Computer System","text":"Single-Processor System Multi-Processor System Total No of chips 1 Multiple Total No of cores 1 Multiple <p>Core = CPU, Register, Local Cache</p>"},{"location":"3_Core/Operating_Systems/03_Evolution/#multi-processor-system","title":"Multi-Processor System","text":"Traditional Modern(Multi-Core) Each processor has ___ core 1 Multiple Processors connected to shared memory unit, shared I/O, shared clock, and other shared system resources All shared resources for the cores are within the CPU, thereby improving performance <ul> <li>i3 has dual-core</li> <li>i5, i7 have quad-core</li> </ul>"},{"location":"3_Core/Operating_Systems/03_Evolution/#multi-processing","title":"Multi-Processing","text":"<p>Concurrent execution using multiple processors</p>"},{"location":"3_Core/Operating_Systems/03_Evolution/#increased-throughput","title":"Increased Throughput","text":"<p>Execution of programs is distributed to multiple processors, hence more output</p> <p>Expected increase is the number of additional processors, but this is not real. Check Amdahl's law in CA</p>"},{"location":"3_Core/Operating_Systems/03_Evolution/#improved-reliability","title":"Improved Reliability","text":"<p>Even if one processor fails, complete failure is avoided. However there will be obvious delay.</p>"},{"location":"3_Core/Operating_Systems/03_Evolution/#useful-for-testing-fault-tolerance","title":"Useful for testing fault Tolerance","text":"<p>2 systems perform the same task and the results will be checked.</p> <p>However, increased redundancy and consumption of resources.</p>"},{"location":"3_Core/Operating_Systems/03_Evolution/#desktop-os","title":"Desktop OS","text":"<p>WindowsOS, MacOS, Linux</p>"},{"location":"3_Core/Operating_Systems/03_Evolution/#mobile-os","title":"Mobile OS","text":"<p>Android, iOS</p>"},{"location":"3_Core/Operating_Systems/03_Evolution/#embedded-systems","title":"Embedded Systems","text":"<p>Washing machine, dishwasher</p>"},{"location":"3_Core/Operating_Systems/03_Evolution/#realtime-os","title":"Realtime OS","text":"<p>More complex, Dynamic and have time constraints                  </p> <p>eg: Industrial control systems, Weapon systems</p>"},{"location":"3_Core/Operating_Systems/03_Evolution/#distributed-systems","title":"Distributed Systems","text":"<p>Multiple hardware devices are networked together.</p> <p>Each device runs a subset of the \u2018distributed OS\u2019</p> <p>When a process executes, the process is split into subprocesses which is sent to different nodes.</p>"},{"location":"3_Core/Operating_Systems/03_Evolution/#advantages_1","title":"Advantages","text":"<p>Some more points are there</p> <ul> <li>Data access</li> <li>Special h/w requirements</li> <li>Load balancing</li> </ul>"},{"location":"3_Core/Operating_Systems/03_Evolution/#definitions","title":"Definitions","text":""},{"location":"3_Core/Operating_Systems/03_Evolution/#throughput","title":"Throughput","text":"<p>No of tasks completed</p>"},{"location":"3_Core/Operating_Systems/03_Evolution/#turn-around-time","title":"Turn around time","text":"<p>Time between starting of execution of a program and its completion</p>"},{"location":"3_Core/Operating_Systems/03_Evolution/#questions","title":"Questions","text":"<ol> <li>Steps when a user double clicks MS Powerpoint icon in a Windows Desktop<ul> <li>CPU recognizes user click (Hardware Interrupt), after execution of current instruction</li> <li>Mode changes to kernel mode</li> <li>Identifies <code>.exe</code> file in secondary memory</li> <li>OS loads program into primary memory</li> <li>Allocate the CPU</li> <li>Hardware interrupt to display on screen</li> </ul> </li> <li>Important activities/functions done by OS<ul> <li>Functions of OS</li> </ul> </li> <li>Suppose a computer system has 10000 bytes of memory available. Out of this, the OS occupies 5000bytes. Now it is required to run a program whose size is 7000bytes. Is this possible?<ul> <li>No, it is not possible in basic computers</li> <li>Every program has to be loaded into primary memory for its execution</li> <li>The memory is split into 2 parts</li> <li>OS segment</li> <li>Program segment (for loading the program)</li> <li>As only 5000bytes is available for the program segment, we cannot load this program, and hence we cannot run it</li> <li>However, we can sort this issue using virtual memory</li> </ul> </li> <li>What is meant by dual mode of operation in intel CPUs<ul> <li>To ensure better security and stricter access priveleges, the CPU has 2 modes of operations</li> <li>Instructions by the user are done in user mode, with lower priveleges</li> <li>Instructions by the user are done in kernel mode, with higher priveleges</li> </ul> </li> <li>Can you guess in what mode OS program is run and in what mode user program is run?<ul> <li>Kernel mode</li> <li>User mode</li> </ul> </li> <li>What is the difference between hardware and software interrupt in a computer system? Can you give examples of each type.<ul> <li>Hardware interrupt are interrupts involving I/O devices. eg: keyboard input</li> <li>Software interrupts do not include I/O devices. eg: Mathematical errors, rejected priviledged instructions, Exceptions</li> </ul> </li> <li>Why do you require a bootstrap loader?<ul> <li>POST Diagnostics</li> <li>Detect and initialize devices</li> <li>Initialize registers and primary memory</li> <li>Load the kernel into primary memory</li> </ul> </li> <li>Using the Interrupt Vector Table (of an 8086 processor operating in real mode) shown below, determine the address of the ISR of a device with interrupt vector 42H. </li> </ol> <p>Answer \\(= \\text{4D6EA_H}\\) 9. Differentiate between multi-programming &amp; multi-processing      - Multi-programming means running multiple programs, using only 1 processor      - Multi-processing means running one or more programs, using multiple processors</p>"},{"location":"3_Core/Operating_Systems/03_Evolution/#system-call","title":"System Call","text":"<p>It is the way for user instructions to perform priviledged instructions.</p>"},{"location":"3_Core/Operating_Systems/03_Evolution/#virtual-memory","title":"Virtual Memory","text":"<p>Combines primary and secondary memory into a single \u2018logical memory\u2019</p> <p>Load only the required part of a program into primary memory.</p>"},{"location":"3_Core/Operating_Systems/04_Process_Management/","title":"04 Process Management","text":""},{"location":"3_Core/Operating_Systems/04_Process_Management/#process","title":"Process","text":"<p>Program in execution</p> <p>Smallest unit of work</p>"},{"location":"3_Core/Operating_Systems/04_Process_Management/#types","title":"Types","text":"<ul> <li>User processes</li> <li>System processes</li> </ul>"},{"location":"3_Core/Operating_Systems/04_Process_Management/#example","title":"Example","text":"<pre><code>flowchart TB\nP1 --&gt; P2 &amp; P3\n\nP3 --&gt; P4</code></pre>"},{"location":"3_Core/Operating_Systems/04_Process_Management/#interpretations","title":"Interpretations","text":"<ul> <li>\\(P_1\\) spawns \\(P_2\\) and \\(P_3\\), and is their parent</li> <li>\\(P_2\\) and \\(P_3\\) are children of \\(P_1\\)</li> </ul>"},{"location":"3_Core/Operating_Systems/04_Process_Management/#program-vs-process","title":"Program vs Process","text":"Program Process Active? \u274c (Passive) \u2705 Requires resources? \u274c \u2705 Segments required Text (Code) Text (Code)DataStackHeap Type of memory required Secondary Primary"},{"location":"3_Core/Operating_Systems/04_Process_Management/#registers","title":"Registers","text":"Register Pointer? Stores Base register \u2705 Starting address of process in Primary Memory Limit register Length of the process Flag register GPRsGeneral purpose Registers PCProgram Counter \u2705 Address of next instruction to be executed Index pointer \u2705"},{"location":"3_Core/Operating_Systems/04_Process_Management/#memory-layout-of-a-process","title":"Memory Layout of a Process","text":"Segment Stores Text Code Data Global variables Stack - Function Calls- Return address arguments- Local variables Heap - Dynamic data- Data structures in memory"},{"location":"3_Core/Operating_Systems/04_Process_Management/#process-state-transition-diagram","title":"Process State Transition Diagram","text":""},{"location":"3_Core/Operating_Systems/04_Process_Management/#5-state","title":"5-State","text":"<p>In very early systems that didn\u2019t have virtual memory</p> <pre><code>flowchart LR\nNew --&gt;\n|Admit| Ready --&gt;\n|Dispatch| Run --&gt;\n|Exit| Terminate\n\nRun ----&gt; |1. Timer Interrupt&lt;br /&gt;2. Higher-Priority Process pre-empts| Ready\n\nRun --&gt;\n|I/O&lt;br/&gt;Request| b[Blocked/&lt;br/&gt;Wait] --&gt;\n|I/O&lt;br/&gt;Complete| Ready</code></pre> <p>Jobs are picked by job scheduler and CPU scheduler</p>"},{"location":"3_Core/Operating_Systems/04_Process_Management/#new-to-ready","title":"New \\(\\to\\) Ready","text":"<p>Promoting a program into a process has some work associated with it for the OS</p> <ul> <li>Creates PCB</li> <li>Address space</li> </ul> <p>(I missed some points)</p>"},{"location":"3_Core/Operating_Systems/04_Process_Management/#7-state-virtual-memory","title":"7-State (Virtual Memory)","text":"<p>Further explanation has been uploaded on LMS</p> <p>Similar to 5-State, but has 2 more states</p> <ul> <li>Blocked suspended: Wait for I/O or event; kept in secondary memory</li> <li>Ready Suspended: Wait for CPU but kept in secondary memory</li> </ul>"},{"location":"3_Core/Operating_Systems/04_Process_Management/#equivalent-names","title":"Equivalent Names","text":"<ul> <li>Activate = Swap-in</li> <li>Suspend = Swap-out</li> </ul> <pre><code>flowchart LR\nws[Wait&lt;br/&gt;Suspended]\nrs[Ready&lt;br/&gt;Suspended]\n\nNew -----&gt;\n|Admit| Ready --&gt;\n|Dispatch| Run --&gt;\n|Exit| Terminate\n\nRun ---&gt; |1. Timer Interrupt&lt;br /&gt;2. Higher-Priority Process pre-empts| Ready\n\nRun --&gt;\n|I/O&lt;br/&gt;Request| b[Blocked/&lt;br/&gt;Wait] --&gt;\n|I/O&lt;br/&gt;Complete| Ready\n\nrs --&gt;|Activate| Ready --&gt;|Suspend| rs\n\nws --&gt;|I/O&lt;br/&gt;Complete| rs\n\nws --&gt; |Activate| b --&gt; |Suspend| ws\n\nNew --&gt;|Preloading| rs</code></pre>"},{"location":"3_Core/Operating_Systems/04_Process_Management/#wait-to-wait-suspended-state","title":"Wait \\(\\to\\) Wait Suspended State","text":"<ol> <li>In case all processes are in wait state (let\u2019s say all are waiting for I/O)</li> <li>OS shifts some waiting processes into wait suspended queue (in secondary memory)</li> <li>Then, the OS brings in programs from the Job Queue (in secondary memory), and loads them into primary memory.</li> <li>There is ready and wait processes, but there isn\u2019t enough primary memory available</li> <li>So the OS shifts some waiting processes into wait suspended queue (in secondary memory) to free up some primary memory</li> </ol>"},{"location":"3_Core/Operating_Systems/04_Process_Management/#run-to-ready-suspended","title":"Run \\(\\to\\) Ready Suspended","text":"<ul> <li>Processes in the ready or running state but are swapped out of main memory and placed in the disk </li> <li>The process will transition back to ready state whenever the process is again brought onto the main memory</li> </ul>"},{"location":"3_Core/Operating_Systems/04_Process_Management/#new-to-ready_1","title":"New \\(\\to\\) Ready","text":"<p>PCB address space</p> <p>Just-in Time process</p>"},{"location":"3_Core/Operating_Systems/04_Process_Management/#new-to-ready-suspended","title":"New \\(\\to\\) Ready Suspended","text":"<p>Pre-emptive computations are performed</p> <p>PCB, address space are generated before-hand</p> <p>Some designers think it is necessary, others argue otherwise</p>"},{"location":"3_Core/Operating_Systems/04_Process_Management/#pcbtcb","title":"PCB/TCB","text":"<p>Process/Task Control Block</p> <p>It stores the Context of a Process</p> <p>Whenever CPU requires details on any process, it will refer to PCB</p>"},{"location":"3_Core/Operating_Systems/04_Process_Management/#context-of-a-process","title":"Context of a Process","text":"<ul> <li>State</li> <li>Process id \\(\\to\\) unique number</li> <li>PC</li> <li>Register Contents<ul> <li>Flag</li> <li>Index Pointer</li> <li>GPRs (General Purpose Registers)</li> </ul> </li> <li>CPU Scheduling info<ul> <li>Priority no</li> <li>Scheduling info</li> <li>Pointer to scheduling queues</li> </ul> </li> <li>Memory management info<ul> <li>Base register</li> <li>Limit register</li> <li>Virtual memory</li> <li>Page Table, Segment Table</li> </ul> </li> <li>Accounting info<ul> <li>Amt of CPU Time used</li> </ul> </li> <li>I/O Info<ul> <li>I/O devices alloted</li> <li>List of open file(s)</li> </ul> </li> </ul>"},{"location":"3_Core/Operating_Systems/04_Process_Management/#context-switching","title":"Context Switching","text":"<p>Whenever there is a switch from one process to another, the OS</p> <ol> <li>saves/stores the context of the previous process</li> <li>loads/restores the context of the current process</li> </ol> <p>The \u2018process\u2019 can be a program/interrupt service routine; this interrupt can be Maskable/Non-Maskable Interrupt</p>"},{"location":"3_Core/Operating_Systems/04_Process_Management/#context-switch-time","title":"Context-Switch Time","text":"<p>This is an overhead, as it is not \u2018useful work\u2019 and is just \u2018book-keeping\u2019.</p> <p>Depends on</p> <ul> <li>architecture of the system</li> <li>amount of \u2018context\u2019 information that is required to be stored/loaded</li> </ul>"},{"location":"3_Core/Operating_Systems/04_Process_Management/#process-scheduler","title":"Process Scheduler","text":"Job Scheduler CPU Scheduler Mid-Term Type Long-Term Short-Term Medium-Term Executing Frequency \\(\\downarrow\\) \\(\\uparrow\\)(CPU Burst is faster) Task Selects processes to be loaded into Ready Queue Allocates process from ready queue \\(\\to\\) CPU Swap-InSwap-Out Goal Ensure good mix of CPU-Bound and I/O-Bound operationsControls degree of multi-programming Ensure max CPU-utilization Modify degree of multiprogramming"},{"location":"3_Core/Operating_Systems/04_Process_Management/#medium-term-scheduler-flowchart","title":"Medium Term Scheduler Flowchart","text":""},{"location":"3_Core/Operating_Systems/04_Process_Management/#degree-of-multi-programming","title":"Degree of multi-programming","text":"<p>No of programs kept in primary memory</p>"},{"location":"3_Core/Operating_Systems/04_Process_Management/#cpu-io-bound","title":"CPU-I/O Bound","text":"CPU-Bound I/O-Bound Process spends most of its time with CPU Process spends most of its time in I/O operations Short burst time Long burst time"},{"location":"3_Core/Operating_Systems/04_Process_Management/#overhead","title":"Overhead %","text":"<p>% time spent on scheduling decision</p> \\[ \\begin{aligned} &amp;\\text{Overhead} \\% \\\\ &amp;= \\frac{\\text{Scheduling Decision Time}}{\\text{Process Execution Time} + \\text{Scheduling Decision Time}} \\times 100 \\% \\end{aligned} \\] <p>We want to minimize this.</p>"},{"location":"3_Core/Operating_Systems/04_Process_Management/#queues","title":"Queues","text":"<p>Each queue has queue header containing pointers to the first and last PCBs of list</p>"},{"location":"3_Core/Operating_Systems/04_Process_Management/#job-queue","title":"Job Queue","text":"<p>set of all processes in system</p> <p>stored in secondary memory</p> <p></p>"},{"location":"3_Core/Operating_Systems/04_Process_Management/#ready-queue","title":"Ready Queue","text":"<p>Processes that are ready to execute</p> <p>Stored in primary memory</p>"},{"location":"3_Core/Operating_Systems/04_Process_Management/#device-queue","title":"Device Queue","text":"<p>Set of processes waiting for I/O device</p>"},{"location":"3_Core/Operating_Systems/04_Process_Management/#queuing-diagram","title":"Queuing Diagram","text":"<p>Represents queues, resources and the corresponding flows</p> <p>Very similar to state transition diagram, but the focus is on the queues</p> <pre><code>flowchart LR\njq[[Job Queue]] --&gt;\n|Admit| rq[[Ready Queue]] --&gt;\n|Dispatch| p[(CPU)] --&gt;\n|Release| r(( ))\n\np --&gt; |Time-Out| rq\n\np --&gt;\n|I/O| bq[[Device Queue]] --&gt;\nrq</code></pre>"},{"location":"3_Core/Operating_Systems/05_CPU_Scheduling/","title":"05 CPU Scheduling","text":""},{"location":"3_Core/Operating_Systems/05_CPU_Scheduling/#cpu-scheduling","title":"CPU Scheduling","text":"<p>Note that in this course, we are neglecting the effect of context switch. In reality, context switch time is around \\(5 \\micro s\\)</p> <ul> <li>Wait \\(\\to\\) Ready<ul> <li>Higher priority process enters Ready Queue</li> </ul> </li> <li>Current Process in the CPU goes to wait state</li> <li>Current process terminates</li> <li>Current process is timed-out</li> </ul> <pre><code>flowchart LR\nNew --&gt;\nReady --&gt;\nRunning --&gt;\nTerminates\n\nRunning --&gt;\nWaiting --&gt;\nReady</code></pre>"},{"location":"3_Core/Operating_Systems/05_CPU_Scheduling/#pre-emptive-scheduling","title":"Pre-Emptive Scheduling","text":"<p>Re-evaluate the schedule every time a new process enters the ready queue.</p> <p>Usually you evaluate the schedule every time a process completes its quanta(will be explained below)/or completely executes</p>"},{"location":"3_Core/Operating_Systems/05_CPU_Scheduling/#dispatcher","title":"Dispatcher","text":"<p>Gives control of CPU to the process selected by CPU-Scheduler</p>"},{"location":"3_Core/Operating_Systems/05_CPU_Scheduling/#functions","title":"Functions","text":"<ul> <li>Context Switch</li> <li>Switch to the user mode</li> <li>Jumping to correct location of user program\u2019s for restarting it</li> </ul>"},{"location":"3_Core/Operating_Systems/05_CPU_Scheduling/#dispatch-latency","title":"Dispatch Latency","text":"<p>Time between stopping one process and starting another process, including context switch time</p>"},{"location":"3_Core/Operating_Systems/05_CPU_Scheduling/#sheduling-criteria","title":"Sheduling Criteria","text":"Criteria Goal is to max/min? CPU UtilizationAmount of time CPU is used \\(\\uparrow\\) Throughput \\(\\uparrow\\) Response Time \\(\\downarrow\\)"},{"location":"3_Core/Operating_Systems/05_CPU_Scheduling/#times","title":"Times","text":"Time Meaning Formula Arrival Time Time at which process arrived (duh) Burst Time Duration of CPU execution required for a process End Time/Completion Time Time at which process has completed entire execution (not duration)\ud83d\udca1 Tip for questions- Find the latest instance of the process in the Gantt chart- That end time is the end time of the process Turnaround Time Duration to complete execution, ie time between job submitted and final result obtained- Waiting time to enter memory- Waiting time in ready queue- Time to execute- Time to perform I/O operations EndTime - ArrivalTime Wait Time(to be minized) Duration spent waiting for complete execution TurnAroundTime - BurstTime Average Wait Time Avg of wait times of all processesWe can reduce this by shifting the more time-consuming processes to later"},{"location":"3_Core/Operating_Systems/05_CPU_Scheduling/#cpu-scheduling-algorithms","title":"CPU Scheduling Algorithms","text":""},{"location":"3_Core/Operating_Systems/05_CPU_Scheduling/#fcfs","title":"FCFS","text":"<p>First-Come-First-Serve</p> <p>Simple</p> <p>Ready Queue = FIFO Queue</p>"},{"location":"3_Core/Operating_Systems/05_CPU_Scheduling/#properties","title":"Properties","text":"<ul> <li>Non-interactive<ul> <li>Not applicable for time-sharing systems</li> </ul> </li> <li>Non-Preemptive Scheduling</li> <li>High average waiting time</li> <li>Convey effect?   Many times resources are idle<ul> <li>CPU</li> <li>I/O devices</li> </ul> </li> </ul>"},{"location":"3_Core/Operating_Systems/05_CPU_Scheduling/#sjf","title":"SJF","text":"<p>Shortest Job First</p> <p>Pick the process with the lowest CPU remaining burst time (at the moment of scheduling)</p>"},{"location":"3_Core/Operating_Systems/05_CPU_Scheduling/#types","title":"Types","text":"<ul> <li> <p>**Preemptive **(SRTF)   Shortest remaining time first (makes sense for partially-executed programs)</p> <ul> <li>Reschedule every time a new task enters ready queue</li> </ul> </li> <li> <p>Non-Preemptive</p> <ul> <li>Only prioritize once a process is completed</li> </ul> </li> </ul>"},{"location":"3_Core/Operating_Systems/05_CPU_Scheduling/#priority-scheduling","title":"Priority Scheduling","text":"<p>Processes are scheduled based on a priority number alone (Burst time not involved in the scheduling calculation)</p> <p>Priority number is assigned to every process, which is an integer value within a fixed range</p> <ul> <li>Some implementations take \\(0\\) as highest priority</li> <li>Some implementations take \\(0\\) as low priority</li> </ul> <p>In this course, smallest number \\(=\\) highest priority, unless specified otherwise.</p> <p>Equal priority processes are scheduled in FCFS order</p>"},{"location":"3_Core/Operating_Systems/05_CPU_Scheduling/#types_1","title":"Types","text":"<ul> <li>Preemptive</li> <li>Non-Preemptive</li> </ul>"},{"location":"3_Core/Operating_Systems/05_CPU_Scheduling/#priority","title":"Priority","text":"<p>In our questions, it will be given.</p> <p>However, in real world, it is based on</p> <pre><code>flowchart TB\ns((Start)) --&gt;\nInternal &amp; External\n\nInternal --&gt;\nResources --&gt;\nc[CPU Time] &amp; o[Open File] &amp; Memory\n\nExternal --&gt;\nImportance &amp; f[\"Funds&lt;br&gt;(Payment for Cloud-Computing)\"]</code></pre>"},{"location":"3_Core/Operating_Systems/05_CPU_Scheduling/#starvation","title":"\u274c Starvation","text":"<p>Low priority processes may never execute</p>"},{"location":"3_Core/Operating_Systems/05_CPU_Scheduling/#solution-aging","title":"Solution: Aging","text":"<p>Increase the priority of processes, as time progresses</p>"},{"location":"3_Core/Operating_Systems/05_CPU_Scheduling/#round-robin","title":"Round Robin","text":"<p>Basically pre-emptive FCFS</p> <p>Used in Time Sharing systems</p> <p>Also called as \u2018Fair-Share Scheduling\u2019</p> <p>CPU Time is divided into small units, called as time slice/quanta</p> <p>Ready Queue is actually a circular queue</p>"},{"location":"3_Core/Operating_Systems/05_CPU_Scheduling/#working","title":"Working","text":"<ol> <li>Scheduler runs</li> <li>Picks process at the <code>Head</code> of the queue</li> <li>Dispatcher runs</li> <li>Timer is set (equal to time quanta)</li> <li>Timer interrupt (time-out/time quanta expires)</li> <li>Process moves to the <code>Tail</code> of the queue</li> <li><code>Head</code> pointer now points to the next process</li> </ol> <p>If there are \\(n\\) processes in the ready queue and the time quantum is \\(q\\), the each process gets \\(\\frac{1}{n}\\) of the CPU time in chunks of atmost \\(q\\) time units at once.</p> <p>No process waits more than \\((n-1) q\\) time units</p> <ul> <li>\u274c Higher average waiting time than SJF</li> <li>\u2705 Lower response time than SJF</li> </ul>"},{"location":"3_Core/Operating_Systems/05_CPU_Scheduling/#value-of-q","title":"Value of \\(q\\)","text":"<p>We need to choose \\(q\\) in a reasonable way</p> \\(q\\) Behaves like Solution Too large FCFS Dec \\(q\\) Too small too many context switches Inc \\(q\\)\\(q\\) must be large with respect to context switch timeotherwise overhead is too high; more time will be spent on context switching rather than actual execution"},{"location":"3_Core/Operating_Systems/05_CPU_Scheduling/#example","title":"Example","text":"<p>Assume time quantum is 20ms, and all the processes arrive at time 0.</p> Process Burst Time Turn-Around Time WT \\(P_1\\) \\(53\\) 134 81 \\(P_2\\) \\(17\\) 37 20 \\(P_3\\) \\(68\\) 162 94 \\(P_4\\) \\(24\\) 121 97 Avg \\(\\to\\) 113.5 73 <pre><code>gantt\ndateFormat x\naxisFormat %L\ntitle CPU\n\nP1: 0, 20\nP2: 20, 37\nP3: 37, 57\nP4: 57, 77\n\nP1: 77, 97\nP3: 97, 117\nP4: 117, 121\n\nP1: 121, 134\nP3: 134, 162</code></pre>"},{"location":"3_Core/Operating_Systems/05_CPU_Scheduling/#multi-level-queuing","title":"Multi-Level Queuing","text":"<ol> <li>Maintain one ready queue for each type of process</li> <li> <p>Process are usually classified into different types, using one of the following methods</p> <ul> <li> <p>Method 1</p> </li> <li> <p>foreground process</p> </li> <li>interactive</li> <li> <p>require quick response time</p> </li> <li> <p>background process</p> </li> <li> <p>batch process</p> </li> <li> <p>Method 2</p> </li> <li> <p>Real-Time Process</p> </li> <li> <p>System Process</p> </li> <li> <p>Interactive Process</p> </li> <li> <p>Batch Process</p> </li> <li>Assign fixed priorities to each queue</li> <li>Apply an appropriate within-queue scheduling algo</li> <li>Apply an appropriate between-queue scheduling algo</li> <li>Fixed-priority pre-emptive scheduling</li> <li>Entry of a new process into a higher priority queue will cause pre-emption</li> <li>The higher priority ones have short burst time, so no need to worry about user delay</li> <li>Low-priority queues do not execute unless higher priority queues are empty</li> <li> <p>Hence, there is possibility of starvation of batch processes</p> </li> <li> <p>Time Slice Scheduling</p> </li> <li>Round Robin between queues</li> <li>Each group is given a certain amount of CPU Time</li> <li>For eg</li> <li>Real-Time = 40% of CPU Time</li> <li>System = 30% of CPU Time</li> <li>Interactive = 20% of CPU Time</li> <li>Batch = 10% of CPU Time</li> </ul> </li> </ol>"},{"location":"3_Core/Operating_Systems/05_CPU_Scheduling/#multilevel-feedback-queue","title":"Multilevel Feedback Queue","text":"<p>Move between queues</p> <p>Kind of pre-emptive scheduling, as process from low priority queue is pre-empted when a process enters any of the high priority queues</p>"},{"location":"3_Core/Operating_Systems/05_CPU_Scheduling/#aim","title":"Aim","text":"<p>Separate processes based on CPU Burst Time</p>"},{"location":"3_Core/Operating_Systems/05_CPU_Scheduling/#working_1","title":"Working","text":"<p>Favors (gives high priority to) I/O-bound and interactive processes</p> <ul> <li>Small CPU burst time \\(\\to\\) High priority</li> <li>Long CPU burst time \\(\\to\\) Low priority</li> </ul> <p>In case of starvaton, process from low priority queue moves to a high priority queue (Aging)</p> <p>In case of any pre-emption, the round robin timer won\u2019t reset</p> <ul> <li>For eg, if a process in \\(Q_1\\) has executed for 4ms out of 8ms time quanta, and it gets pre-empted by a process in \\(Q_0\\)</li> <li>When we get back to \\(Q_1\\), we continue from 4ms (not reset to 0)</li> </ul>"},{"location":"3_Core/Operating_Systems/05_CPU_Scheduling/#components","title":"Components","text":"<p>3 Queues</p> <pre><code>flowchart LR\n\nStart((Start))\nFinish((Finish))\n\nStart --&gt;\nq0[[\"Q0&lt;br&gt;(q = 8ms)\"]] --&gt;\n|\"Incomplete&lt;br /&gt;\u274c\"| q1[[\"Q1&lt;br /&gt;(q = 16 ms)\"]] --&gt;\n|\"Incomplete&lt;br /&gt;\u274c\"| q2[[\"Q2&lt;br /&gt;FCFS\"]]\n\nq0 &amp; q1 &amp; q2 --&gt;|\"Complete&lt;br /&gt;\u2705\"| Finish\n\nclassDef queue fill: teal, color: orange, font-weight: bold\nclass q0,q1,q2 queue</code></pre>"},{"location":"3_Core/Operating_Systems/06_Realtime_CPU_Scheduling/","title":"06 Realtime CPU Scheduling","text":""},{"location":"3_Core/Operating_Systems/06_Realtime_CPU_Scheduling/#realtime-systems","title":"Realtime Systems","text":"<p>Realtime processes exist along normal processes, and their tasks are of higher priority \\(\\implies\\) Latency should be minimized</p> <p>In this course, we are assuming that all realtime tasks are periodic, ie task repeats itself at regular intervals of time</p>"},{"location":"3_Core/Operating_Systems/06_Realtime_CPU_Scheduling/#types-of-realtime-systems","title":"Types of Realtime Systems","text":"Soft Realtime System Hard Realtime System Strict deadline constraints? \u274c \u2705 Deadline miss leads to Degradation in performance failure/destruction Bounded Latency? \u274c \u2705 Example Streaming Video Robots in medical treatmentAutomated chemical plantAuto-missile system"},{"location":"3_Core/Operating_Systems/06_Realtime_CPU_Scheduling/#types-of-latency","title":"Types of Latency","text":"<ul> <li>Interrupt latency</li> <li>Dispatch latency</li> <li>Event latency</li> </ul>"},{"location":"3_Core/Operating_Systems/06_Realtime_CPU_Scheduling/#terms","title":"Terms","text":"Term Symbol Meaning Execution Time \\(t\\) Time taken for a process to complete execution Time Period \\(p\\) The interval at which the process has to repeat itself(not like time quanta in Round Robin) Deadline \\(d\\) Time constraint for execution time \\(:d \\le p\\)In this course, we are assuming that \\(d = p\\) Processor Utilization \\(U\\) Fraction of utilization of available processor resources\\(U = \\sum\\limits_{i=1}^n \\frac{t}{p}\\), where \\(n=\\) number of tasks"},{"location":"3_Core/Operating_Systems/06_Realtime_CPU_Scheduling/#scheduling-algorithms","title":"Scheduling Algorithms","text":"<p>Algorithms to complete a set of \\(n\\) tasks, using a single processor, such that</p> <ul> <li>\\(d=p\\)</li> <li>\\(t =\\) constant</li> </ul> <p>CPU utilization is not always 100%. It is bounded to a limit, based on number of tasks in system</p> RMS/RMA EDF Full Form Rate-Monotic Scheduling Algorithm Earliest Deadline First Priority-Based \u2705 \u2705 Priority Type Static Dynamic High Priority for task with __ Shortest Period Earliest Deadline Schedulability Condition(s) - Test of Schedulability- Test of Maximum CPU Utilization Bound - Test of Schedulability (Necessary &amp; Sufficient Condition) Requirement Tasks must announce their deadlines to scheduler, when it becomes runnable"},{"location":"3_Core/Operating_Systems/06_Realtime_CPU_Scheduling/#3-cases","title":"3 Cases","text":"\\(U_\\text{tot}\\) RMS Schedulable? EDF Schedulable? \\(&gt; 1\\) \u274c \u274c \\(\\le U_\\text{max}\\) \u2705 \u2705 \\(U_\\max &lt; U_\\text{tot} &lt; 1\\) Inconclusive(Draw Realtime Scheduling Gantt Chart) \u2705"},{"location":"3_Core/Operating_Systems/06_Realtime_CPU_Scheduling/#testing-methods","title":"Testing Methods","text":"Test Check Test of Schedulability \\(U_\\text{tot} \\le 1\\) Test of Maximum CPU Utilization Bound(aka upper bound of schedulability test) \\(U_\\text{tot} \\le n(2^\\frac{1}{n} - 1)\\) Realtime Scheduling Gantt Chart Scheduling the task set using a Gantt chart(If the total time to plot is not given, plot till the LCM of the periods of the processes.)"},{"location":"3_Core/Operating_Systems/07_Process_Synchronization/","title":"07 Process Synchronization","text":""},{"location":"3_Core/Operating_Systems/07_Process_Synchronization/#types-of-processes","title":"Types of Processes","text":"Independent process Co-operating process Affect other processes \u274c \u2705 Affected by other processes \u274c \u2705 Share code/data with other processes \u274c \u2705"},{"location":"3_Core/Operating_Systems/07_Process_Synchronization/#process-synchronization","title":"Process Synchronization","text":"<p>is used for orderly execution of cooperating processes that share a logical address space, in concurrent or parallel execution</p>"},{"location":"3_Core/Operating_Systems/07_Process_Synchronization/#types-of-execution","title":"Types of Execution","text":"Execution Type Meaning Sequential One after another Asynchronous (Not in the course) Concurrent Multiple tasks start, run, and complete in overlapping time periods, in no specific order Parallel Multiple tasks or subtasks of the same task that literally run at the same time on a hardware with multiple computing resources like multi-core processor"},{"location":"3_Core/Operating_Systems/07_Process_Synchronization/#producer-consumer-bounded-buffer-problem","title":"Producer-Consumer / Bounded-Buffer Problem","text":"Parts Role Producer Process Produce an item Consumer Process Consume an item Bounded-Buffer Contains produced items (values, records)Implemented as Circular Array of fixed size"},{"location":"3_Core/Operating_Systems/07_Process_Synchronization/#bounded-buffer-variables","title":"Bounded-Buffer Variables","text":"Variable Stores Initial Value in location of next item to be written by producer 0 out location of next item to be read by consumer 0 counter number of elements in buffer 0"},{"location":"3_Core/Operating_Systems/07_Process_Synchronization/#counter-updation","title":"<code>counter</code> Updation","text":"Occurance Counter value Production counter++ Consumption counter--"},{"location":"3_Core/Operating_Systems/07_Process_Synchronization/#counter-cases","title":"<code>counter</code> Cases","text":"<code>counter</code> Value __ is Blocked because buffer is Problem \\(= 0\\) Consumer empty Busy Waiting Buffer_Size Producer full(buffer content has not yet been consumed)"},{"location":"3_Core/Operating_Systems/07_Process_Synchronization/#interlockedinterleaved-schedule","title":"Interlocked/Interleaved Schedule","text":""},{"location":"3_Core/Operating_Systems/07_Process_Synchronization/#busy-waiting","title":"Busy Waiting","text":"<p>consumes CPU cycles but no work is done</p>"},{"location":"3_Core/Operating_Systems/07_Process_Synchronization/#race-condition","title":"Race Condition","text":"<p>Situtation when several processes access and manipulate shared data concurrently.</p> <p>Final value of shared data depends on the final write operation</p> <p>To prevent race condition, concurrent processes must be synchronized</p>"},{"location":"3_Core/Operating_Systems/07_Process_Synchronization/#critical-section-problem","title":"Critical-Section problem","text":"<p>\\(n\\) cooperating processes all competing to use some shared data</p>"},{"location":"3_Core/Operating_Systems/07_Process_Synchronization/#critical-section","title":"Critical-Section","text":"<p>Section where shared data is accessed/modified.</p> <p>For producer-consumer problem, <code>counter++</code> and <code>counter--</code> are the critical section</p>"},{"location":"3_Core/Operating_Systems/07_Process_Synchronization/#requirement","title":"Requirement","text":"<p>Avoid data inconsistency</p> <p>When one process is ints critical section, another process should not be alowed to enter its critical section</p>"},{"location":"3_Core/Operating_Systems/07_Process_Synchronization/#task","title":"Task","text":"<p>Design protocol/algorithms which cooperating processes can use to cooperate, ensuring that the requirement is satisfied</p>"},{"location":"3_Core/Operating_Systems/07_Process_Synchronization/#structure","title":"Structure","text":"<p>Assume a process \\(P\\) is executing indefinitely</p> <pre><code>do\n{\n  entry_section\n    critical_section\n  exit_section\n    remainder_section\n} while (1);\n</code></pre> Section Role Entry Gain access to criticial section/resourceEnsures that only a selected number of processes are in its critical section Critical Code that affects common memory location Exit Relinquish access to criticial section/resourceAllows other process to enter their critical section Remainder"},{"location":"3_Core/Operating_Systems/07_Process_Synchronization/#solution","title":"Solution","text":"Solution Mutual Exclusion If process \\(P_i\\) is executing in its critical section, no other processes can be executing in their critical sections Progress If no process is executing in its critical section, and \\(\\exists\\) some processes that wish to enter their critical section, then the selection of the processes that will enter the critical section next cannot be postponed indefinitely Bounded waiting Bound must exist on the number of times that other processes are allowed to enter their critical sections after a process has made a request to enter its critical section before that request is granted"},{"location":"3_Core/Operating_Systems/07_Process_Synchronization/#semaphores","title":"Semaphores","text":"<p>Synchronization Tool/Construct</p>"},{"location":"3_Core/Operating_Systems/07_Process_Synchronization/#purpose","title":"Purpose","text":"<ul> <li>Solve critical section problem</li> <li>Guard access to shared resource</li> </ul>"},{"location":"3_Core/Operating_Systems/07_Process_Synchronization/#parts","title":"Parts","text":"<ul> <li> <p>Value</p> </li> <li> <p>Queue of process that are waiting on it</p> </li> <li> <p>2 operations/methods associated with it   atomic/indivisible - These operations cannot be interrupted</p> </li> </ul> Operation <code>wait(s)</code> <code>signal(s)</code> Alternate Name P(s) V(s) Purpose used by process to gain access to critical section/access to shared resource used by process to inform that it has completed access of critical region/using the shared resource Steps - dec value of semaphore- check safety for process to enter critical section - inc value of semaphore- check if semaphore value \\(\\le 0\\), or if any process is waiting WhenOutput  = Yes Process enters critical region WhenOutput  = No Process added to queue of process waiting for this semaphore <p>The number of process that are waiting in the semaphore queue</p> \\[ = |\\text{Semaphore Value}| \\\\ \\Big(\\text{Value} \\iff &lt; 0 \\Big) \\]"},{"location":"3_Core/Operating_Systems/07_Process_Synchronization/#implementation-of-counting-semaphore","title":"Implementation of Counting Semaphore","text":"<pre><code>typedef struct\n{\n  int value;\n  Queue of processes;\n} Sephamore;\n</code></pre> <p>Assume 2 operations</p> <ul> <li><code>block()</code> suspends the process that invokes it<ul> <li>Moves the process from run state \\(\\to\\) blocked/wait state</li> <li>Control is transferred to CPU scheduler, which then schedules another process instead</li> </ul> </li> <li><code>wakeup(P)</code> resumes the execution of a blocked process \\(P\\)<ul> <li>Moves the process from blocked/wait state \\(\\to\\) ready state</li> </ul> </li> </ul> <pre><code>P1\n{\n  wait(s);\n  access printer;\n  signal(s);\n}\n\nP2\n{\n  wait(s);\n  access printer;\n  signal(s);\n}\n</code></pre>"},{"location":"3_Core/Operating_Systems/07_Process_Synchronization/#wait-operation","title":"<code>wait()</code> Operation","text":"<pre><code>void wait(Semaphore s)\n{\n  s.value--;\n\n  if(s.value &lt; 0)\n  {\n    // Semaphore is unavailable\n    // process cannot access critical region\n\n    add this process to s.queue;\n    block();\n  }\n\n  // Semaphore available\n  // Process gains access to critical region\n}\n</code></pre>"},{"location":"3_Core/Operating_Systems/07_Process_Synchronization/#for-binary-semaphore","title":"For binary semaphore","text":"<pre><code>void wait(Semaphore s)\n{ \n  if(s.value == 1)\n  {\n        // Semaphore available\n        // Process gains access to critical region\n\n\n    s.value = 0;\n  }\n  else\n  {\n    // Semaphore is unavailable\n    // process cannot access critical region\n\n    add this process to s.queue;\n    block();\n  }\n}\n</code></pre>"},{"location":"3_Core/Operating_Systems/07_Process_Synchronization/#signal-operation","title":"<code>signal()</code> Operation","text":"<pre><code>void signal(Semaphore s)\n{\n  s.value++;\n  if(s.value &lt;= 0)\n  {\n    // processes are waiting\n    // pick up a process\n\n    remove process from s.queue;\n    wakeup(P);\n  }\n\n  // no processes are waiting for this semaphore\n\n}\n</code></pre> <pre><code>void signal(Semaphore s)\n{\n  if( s.queue.isempty()  )\n  {\n    // no processes are waiting for this semaphore\n      s.value = 1;\n  }\n  else\n  {\n    // processes are waiting\n    // pick up a process\n\n    remove process from s.queue;\n    wakeup(P);      \n  }\n\n}\n</code></pre>"},{"location":"3_Core/Operating_Systems/07_Process_Synchronization/#solution-for-bounded-buffer-problem-using-semaphores","title":"Solution for bounded buffer problem using semaphores","text":"<p>Use 3 semaphores</p> Semaphore Initial value Producer produces a process 0 means 1 means \\(n\\) means Empty Keep track of free buffers \\(n\\) <code>wait()</code> Buffer full Buffer empty Full Keep track of full buffers 0 <code>signal()</code> Buffer empty Buffer full Mutex**Mut**ual-**ex**clusion Ensures exclusive access to buffer pool (binary) Access to buffer pool not possible Access to buffer pool possible N/A <p>This solution prevents Busy Waiting</p>"},{"location":"3_Core/Operating_Systems/07_Process_Synchronization/#producer-process","title":"Producer process","text":"<pre><code>do\n{\n  // produce an item in next_produced\n\n  wait(empty);\n  // check if buffer pool has empty buffer. if yes, then proceed else it waits.\n  // avoids busy waiting\n\n  wait(mutex); // gain access to buffer pool\n\n  // add next_produced to buffer\n\n  signal(mutex); // gives control to buffer pool\n  signal(full); // increment value of full\n\n} while (1);\n</code></pre>"},{"location":"3_Core/Operating_Systems/07_Process_Synchronization/#consumer-process","title":"Consumer Process","text":"<pre><code>do\n{\n  // item next_consumed;\n\n  wait(full);\n  // check if buffer is empty. if true, process blocks\n  // avoids busy waiting\n\n  wait(mutex); // exclusive access buffer pool\n\n  // remove item from buffer to nextconsumed\n\n  signal(mutex);\n  signal(empty); // increment value of empty\n\n  // consume item in next_consumed\n\n} while(1);\n</code></pre>"},{"location":"3_Core/Operating_Systems/07_Process_Synchronization/#readers-writers-problem","title":"Readers-Writers Problem","text":"Meaning Reader Process Reads from shared resource Writer Process Reads and/or writes from shared resource"},{"location":"3_Core/Operating_Systems/07_Process_Synchronization/#problem-statement","title":"Problem Statement","text":"<p>Multiple readers can access shared resource</p> <p>Only 1 writer can have exclusive access to shared resource</p> Process 1 Process 2 Read \u2705 Read Read \u2705 Read Write \u274c Write \u2705 Write Read \u274c Write Write \u274c"},{"location":"3_Core/Operating_Systems/07_Process_Synchronization/#variables","title":"Variables","text":"<code>readcount</code> <code>mutex</code> <code>rw_mutex</code> data type integer binary semaphore binary semaphore Purpose Keep track of number of readers accessing shared resource @ a timeGive access to first readerRelinquish access after last reader Control access to <code>readcount</code> Control access to shared resource Used by N/A Readers 1<sup>st</sup> reader - gain exclusive access <code>wait(rw_mutex)</code>Last reader - relinquish access <code>signal(rw_mutex)</code>All writers <code>wait(rw_mutex)</code>, <code>signal(rw_mutex)</code> initial value 0 1 1 when a reader/writer accesses the shared resource inc 0 0 when a reader/writer finishes reading dec 1 1"},{"location":"3_Core/Operating_Systems/07_Process_Synchronization/#writer-process","title":"Writer Process","text":"<pre><code>do\n{\n  // entry section\n  wait(rw_mutex);   // gain exlclusive access by this writer process to shared resource\n\n  // critical section\n  // perform read and/or write operation\n\n\n  // exit section\n  signal(rw_mutex); // relinquish access by this writer process to shared resource\n\n} while(1);\n</code></pre>"},{"location":"3_Core/Operating_Systems/07_Process_Synchronization/#reader-process","title":"Reader Process","text":"<pre><code>do\n{\n  wait(mutex); // exclusive access to readcount\n\n  readcount++;\n  if(readcount == 1) // 1st reader\n    wait(rw_mutex); // gain exclusive access by all readers to shared resource\n\n    signal(mutex); // relinquish access to readcount\n\n  // read from shared resource\n\n  wait(mutex); // exclusive access to readcount\n\n  readcount--; \n  if(readcount == 0) // last reader\n    signal(rw_mutex); // relinquish access by all readers to shared resource\n  signal(mutex); // relinquish access to readcount\n\n} while(1);\n</code></pre>"},{"location":"3_Core/Operating_Systems/08_Deadlock/","title":"08 Deadlock","text":""},{"location":"3_Core/Operating_Systems/08_Deadlock/#types-of-resources","title":"Types of Resources","text":"Preemptable Resource Non-Preemptable Resource Can be removed from process without causing computation fail Once allotted to a process, it cannot be removed from a process unless the process relinquishes the resource by itself Example Primary Memory can be swapped out Printers"},{"location":"3_Core/Operating_Systems/08_Deadlock/#necessary-conditions-for-deadlocks","title":"Necessary conditions for Deadlocks","text":"Condition Meaning Mutual-exclusion Only one process can use a resource at a time Hold &amp; Wait A process holding at least one resource is waiting to acquire additional resources held by other processes (hence causing a wait) Non-Preemptive resources A resource can be released only voluntarily by the process holding it, after that process has completed its task. Circular Wait There exists a set {P0, P1, \u2026, Pn} of waiting processes such that P0 is waiting for a resource that is held by P1, P1 is waiting for a resource that is held by P2, \u2026, Pn\u20131 is waiting for a resource that is held by Pn, and Pn is waiting for a resource that is held by P0."},{"location":"3_Core/Operating_Systems/08_Deadlock/#rag","title":"RAG","text":"<p>Resource Allocation Graph</p> <p>Directed graph that how processes and resources are related</p> Symbol Meaning Circle Process Rectangle with circles Resource Type with instances Edges Request Edge (process \\(P_i\\) requests resource \\(R_j\\)) \\(P_i \\to R_j\\) Assignment Edge (resource \\(R_j\\) is alloted to \\(P_i\\)) \\(R_j \\to P_i\\)"},{"location":"3_Core/Operating_Systems/08_Deadlock/#checking-deadlock","title":"Checking deadlock","text":"Cycle exists? Instances of each resource type \\(\\implies\\) Deadlock exists? \u274c N/A \u274c \u2705 1 \u2705 \u2705 Multiple Inconclusive"},{"location":"3_Core/Operating_Systems/08_Deadlock/#disadvantage","title":"Disadvantage","text":"<p>Inconclusive for the 3<sup>rd</sup> case</p>"},{"location":"3_Core/Operating_Systems/08_Deadlock/#bankers-algo-for-deadlock-detection","title":"Banker\u2019s Algo for Deadlock Detection","text":"<p>It is an algorithm which is implemented as a system process in the OS</p> <p>In this section, processes are referring to user processes</p> <p>Let</p> <ul> <li>\\(n =\\) No of processes</li> <li>\\(m =\\) No of resources</li> </ul> Dimension Meaning Initial value MaxMatrix \\(n \\times m\\) Maximum resource requirement of each type by every process AllocationMatrix \\(n \\times m\\) Total number of resources of each type that is currently allocated to each process NeedMatrix \\(n \\times m\\) Denotes the number of resources of each resource type that are yet to be allocated to a process AvailableVector \\(m\\) Total no of instances of each resource type that is available WorkVector \\(m\\) No of instances of each resource type that is currently available Work = Available FinishVector \\(n\\) Denotes the completion status of each process False RequestVector for process \\(i\\) \\(m\\) No of instances of each type of resource that process \\(i\\) requests \\[ \\text{Need}[i][j] = \\text{Max}[i][j] - \\text{Allocation}[i][j] \\\\ i \\in [1, n],\\\\ j \\in [1, m] \\]"},{"location":"3_Core/Operating_Systems/08_Deadlock/#vector-comparison","title":"Vector Comparison","text":"<p>Let \\(X, Y\\) be 2 vectors</p> \\[ X \\le Y \\iff X[i] \\le Y[i], \\\\ \\forall i \\in \\text{len}(X) = \\text{len}(Y) \\\\ (0, 0, 0) \\le (0, 0, 1) \\\\(0, 1, 0) \\not \\le (0, 0, 1) \\] <p>Every element of \\(X\\) should be smaller than/equal to every corresponding element of \\(Y\\)</p>"},{"location":"3_Core/Operating_Systems/08_Deadlock/#algorithm","title":"Algorithm","text":"<ol> <li> <p>Initialize all vectors</p> </li> <li> <p>Find process \\(i\\) with</p> </li> <li> <p>Finish[i] = <code>False</code></p> </li> <li>Need[i] \\(\\le\\) Work</li> </ol> <p>If we can\u2019t find, go to step 4</p> <ol> <li> <ul> <li>Work = Work + Allocation[i]</li> <li>Finish[i] = True</li> <li>Go to step 2</li> </ul> </li> <li> <p>If finish[i]==True \\(\\forall i\\), the system is in a safe state</p> </li> </ol>"},{"location":"3_Core/Operating_Systems/08_Deadlock/#safe-sequence","title":"Safe Sequence","text":"<p>You may get multiple valid safe sequences for the same list of process</p>"},{"location":"3_Core/Operating_Systems/08_Deadlock/#resource-request-algorithm","title":"Resource-Request Algorithm","text":"<p>Consider a Request[i] vector for process \\(P_i\\)</p> <ol> <li> <p>Check if Request[i] \\(\\le\\) Need[i]</p> <ul> <li>if true, got to step 2</li> <li>else, raise error condition that \\(P_i\\) has requested more than it needs</li> </ul> </li> <li> <p>Check if Request[i] \\(\\le\\) Available[i]</p> </li> <li> <p>if true, go to step 3</p> </li> <li> <p>else, \\(P_i\\) must wait, as resources are not available</p> </li> <li> <p>Pretend to allocate requested resources to \\(P_i\\), by modifying the states as follows</p> </li> </ol> <pre><code>Available -= Request[i]\nAllocation[i] += Request[i]\nNeed[i] -= Request[i]\n</code></pre> <ol> <li> <p>Run the safety algo to check if the system is in a safe state</p> <ul> <li>If safe, resources are allocated to \\(P_i\\)</li> <li>else, \\(P_i\\) must wait and the old resource-allocated state is restored</li> </ul> </li> </ol>"},{"location":"3_Core/Operating_Systems/08_Deadlock/#deadlock-handling","title":"Deadlock Handling","text":"Deadlock Avoidance Deadlock Prevention Deadlock Detection &amp; Recovery"},{"location":"3_Core/Operating_Systems/08_Deadlock/#deadlock-detection","title":"Deadlock Detection","text":"<p>Almost exactly the same as Banker\u2019s; just that this has request instead of need.</p> <ol> <li> <p>Initialization</p> </li> <li> <p>Let <code>work = available</code></p> </li> <li> <p>Check if \\(\\text{allocation}[i] \\ne 0\\)</p> <ol> <li>true \\(\\implies\\) set finish[i] = false</li> <li>false \\(\\implies\\) set finish[i] = true</li> </ol> </li> <li> <p>Find \\(i\\) such that</p> </li> <li> <p>Finish[i] = false</p> </li> <li>request[i] \\(\\le\\) work</li> </ol> <p>If not found, go to step 4</p> <ol> <li> <p>Work = Work + Allocation    Finish[i] = true    Go to step 2</p> </li> <li> <p>If finish[i] == false, for some \\(i \\implies\\) system is in deadlock state    or, in other words, there is no deadlock if</p> <ul> <li>all the finish[i] = false \\(\\forall i\\)</li> <li>we are able to derive a safe sequence with all the processes, then there is no deadlock</li> </ul> </li> </ol>"},{"location":"3_Core/Operating_Systems/09_Memory_Management/","title":"09 Memory Management","text":""},{"location":"3_Core/Operating_Systems/09_Memory_Management/#address","title":"Address","text":"Address Logical (Virtual) Address generated by CPU Physical Address seen by memory unit(one loaded into memory address register)"},{"location":"3_Core/Operating_Systems/09_Memory_Management/#address-space","title":"Address Space","text":"Address Space Logical Set of all logical address Physical Set of all physical address"},{"location":"3_Core/Operating_Systems/09_Memory_Management/#types-of-memory-allocation","title":"Types of Memory Allocation","text":"Type 1 chunk of consecutive locations are required? Contiguous \u2705 Non-contiguous \u274c <pre><code>flowchart TB\n\nma[Memory Allocation] --&gt;\nc &amp; nc[Non-Contiguous]\nc[Contiguous] --&gt;\nspa[Single Partion Allocation] &amp; mpa[Multiple partition allocation]\n\nmpa --&gt;\nfsp &amp; dp[Dynamic Partioning]\n\nfsp[Fixed Size Partioning] --&gt;\nes[Equal Sized] &amp; us[Unequal Sized]</code></pre>"},{"location":"3_Core/Operating_Systems/09_Memory_Management/#multiple-partition-allocation","title":"Multiple Partition Allocation","text":"<p>When a partition is free, a process is selected from input queue and is loaded to the free partition</p>"},{"location":"3_Core/Operating_Systems/09_Memory_Management/#fixed-size-partitions","title":"Fixed Size Partitions","text":"<p>Internal fragmentation = unused memory within a partition</p> <p></p>"},{"location":"3_Core/Operating_Systems/09_Memory_Management/#equal-fixed-size-partitions","title":"Equal Fixed Size Partitions","text":"<p>\u274c Program may be much smaller or larger than the size of the partitions</p> <p>\u274c Internal fragmentation = large</p>"},{"location":"3_Core/Operating_Systems/09_Memory_Management/#unequal-fixed-size-partitions","title":"Unequal Fixed Size Partitions","text":"<p>\u274c Program may be much smaller or larger than the size of the partitions</p> <p>\u2705 Internal fragmentation is lower than equal partitioning</p>"},{"location":"3_Core/Operating_Systems/09_Memory_Management/#dynamic-partitions","title":"Dynamic Partitions","text":"<p>Partitions are created dynamically as process enter and leave main memory</p> <p>Hole is the block of available memory</p> <p>\u274c Causes external fragmentation (multiple holes generated within the memory)</p> <p></p> <p></p>"},{"location":"3_Core/Operating_Systems/09_Memory_Management/#allocation-algorithms","title":"Allocation Algorithms","text":"Meaning First-fit Allocate the first hole that is big enough to accomodate Next-fit Allocate the hole immediately after the previously alloted location(Subtype of first-fit) Best-fit Allocate the smallest hole that is big enough to accomodate Worst-fit Allocate the largest hole available"},{"location":"3_Core/Operating_Systems/09_Memory_Management/#compactiondefragmentation","title":"Compaction/Defragmentation","text":"<p>Computationally-expensive, but allows us to reduce external fragmentation</p> <p>Move</p> <ul> <li>allotted partitions beside each other</li> <li>holes beside each other</li> </ul>"},{"location":"3_Core/Operating_Systems/09_Memory_Management/#paging","title":"Paging","text":"<p>non-contiguous memory allocation technique</p> <p>To run a program of size \\(n\\) pages, we need to find \\(n\\) free frames and load the pages of the program into the frames</p> <p>used to map/translate logical to physical address</p> <p>Every process has a page table</p> <p>frame size = page size \\(\\in 2^n, n \\in Z\\)</p>"},{"location":"3_Core/Operating_Systems/09_Memory_Management/#page-table","title":"Page Table","text":"<p>Indexed by page number</p> <p>Stores the frame number/base address of each page in physical memory</p> <p>No of entries = no of pages</p> Page number(acts as index) Frame no/base address 0 0 1 4 2 2 3 7"},{"location":"3_Core/Operating_Systems/09_Memory_Management/#formulae","title":"Formulae","text":"\\[ \\begin{aligned} &amp; \\text{Total no of logical address (space size) of the paging} \\\\ &amp;= \\text{No of pages } \\times \\text{ Page Size} \\\\ &amp; \\text{Total Physical memory size} \\\\ &amp;= \\text{No of frames} \\times \\text{Frame Size} \\end{aligned} \\] \\[ \\begin{aligned} \\text{Logical memory size} &amp;= 2^\\text{Size of logical address bus} \\\\ \\implies \\text{Size of logical address bus} &amp;= \\log_2 (\\text{Logical memory size}) \\\\ \\text{Physical memory size} &amp;= 2^\\text{Size of physical address bus} \\\\ \\implies \\text{Size of physical address bus} &amp;= \\log_2 (\\text{Physical memory size}) \\end{aligned} \\]"},{"location":"3_Core/Operating_Systems/09_Memory_Management/#address-translation","title":"Address Translation","text":"<p>(Diagram from slides)</p> <ol> <li>Given logical address</li> <li>Get page no and page offset</li> </ol> Page No Page Offset Bits to locate base address of each page in physical memory Bits to combine with base address to define the physical address <ol> <li> <p>Get frame no (stored in the page we just accessed)</p> </li> <li> \\[    \\text{Base address} = \\text{Frame no} \\times \\text{Frame size}    \\] </li> <li> <p>Frame Offset = page offset</p> </li> </ol> Frame No Frame Offset Bits to locate base address of each frame in physical memory Bits to combine with base address to define the physical address <ol> <li> \\[    \\text{Physical address} = \\text{Base Address} + \\text{Frame Offset}    \\] </li> </ol>"},{"location":"3_Core/Operating_Systems/09_Memory_Management/#i-missed-something","title":"I Missed Something","text":""},{"location":"3_Core/Operating_Systems/09_Memory_Management/#tlb","title":"TLB","text":"<p>Small, fast-lookup hardware cache called associative memory or translation look-aside buffers (TLBs) are used for implementing Page Table.</p> <p>64 or 128 locations/entries</p> <p>Each entry has 2 parts</p> <ul> <li>key(tag)</li> <li>value</li> </ul> <p>When presented with an item, it is compared with all keys simulatneously for a match</p> <p>If a key is matched, corresponding value is returned</p> <p>Costly but faster</p> <p>TLB Hit, TLB Miss</p>"},{"location":"3_Core/Operating_Systems/09_Memory_Management/#effective-memory-access-time","title":"Effective Memory Access Time","text":"\\[ \\text{EMET } = (\\text{ TLB Hit Ratio } \\times \\text{ Hit Time }) + (\\text{ TLB Miss Ratio } \\times \\text{ Miss Time})} \\] \\[ \\begin{aligned} \\text{Hit Time} &amp;= \\text{Time to search TLB } + &amp; \\text{Time to access memory} \\\\ \\text{Miss Time} &amp;= \\text{Time to search TLB } + &amp; \\textcolor{hotpink}{2 \\times} \\text{Time to access memory} \\\\ \\end{aligned} \\]"},{"location":"3_Core/Operating_Systems/09_Memory_Management/#memory-protection","title":"Memory Protection","text":"<p>We achieve this using memory protection bit in the page table (not TLB)</p> Memory Protection Bit Read-only 0 Read-write 1 <p>Implementation of page table will now be</p> Frame No Memory Protection Bit 0 2 0 1 4 1 2 3 0 3 5 1 <p>Whenever we have a write operation, the OS checks the memory protection bit in the page table</p> <p>If we try to write into a read-only page of the process, we receive a trap (software interrupt) by the OS</p>"},{"location":"3_Core/Operating_Systems/10_Virtual_Memory/","title":"10 Virtual Memory","text":"<p>Only the required page of a process is required to be brought to physical memory; when a page is not required for execution, it is not brought in</p>"},{"location":"3_Core/Operating_Systems/10_Virtual_Memory/#main-usecases","title":"Main usecases","text":"<p>Rarely used features/functions and data structures</p>"},{"location":"3_Core/Operating_Systems/10_Virtual_Memory/#advantages","title":"Advantages","text":"<ul> <li>Size of process is not limited by the size of physical memory (primary)</li> <li>Increased degree of multiprogramming (no of programs that can exist in the primary memory at the same time)</li> <li>Increased CPU utilization</li> <li>Reduced I/O wrt a process, by eliminating unnecessary \u2018swapping-in\u2019</li> </ul>"},{"location":"3_Core/Operating_Systems/10_Virtual_Memory/#page-fault","title":"Page Fault","text":"<p>When a process tries to access a page that exists in memory, execution continues as normal</p> <p>Otherwise, if the process tries to access a page that is marked invalid, this means that the corresponding page is missing</p> <ul> <li>software interrupt (trap) is created</li> <li>bring in required page from secondary memory</li> <li>Store into free frame/Page Replacement</li> <li>Reset page table</li> <li>Restart execution</li> </ul>"},{"location":"3_Core/Operating_Systems/10_Virtual_Memory/#page-replacement","title":"Page Replacement","text":"<p>If there are no free frames, we need to replace the frame in a manner that would reduce future page faults.</p> <p></p>"},{"location":"3_Core/Operating_Systems/10_Virtual_Memory/#algorithms","title":"Algorithms","text":"Algo Replace frame that is Avoids B\u00e9l\u00e1dy's Anomaly FIFO (First in First out) oldest \u274c Optimal least likely to be used in the future \u2705 LRU Least Recently-Used \u2705"},{"location":"3_Core/Operating_Systems/10_Virtual_Memory/#beladys-anomaly","title":"B\u00e9l\u00e1dy's Anomaly","text":"<p>In computer storage, the phenomenon in which having more page frames can cause more page faults for first-in first-out page replacement algorithm</p>"},{"location":"3_Core/Operating_Systems/Lab/01_Reading_Files/","title":"01 Reading Files","text":""},{"location":"3_Core/Operating_Systems/Lab/01_Reading_Files/#return-value","title":"Return Value","text":"Return Value Read Successful? Return Value \\(&gt;= 0\\) \u2705 no of bytes read \\(&lt; 0\\) \u274c <pre><code>#include&lt;stdio.h&gt;\n#include &lt;sys/types.h&gt;\n#include &lt;sys/stat.h&gt;\n#include &lt;fcntl.h&gt;\n#include &lt;string.h&gt;\n#include &lt;errno.h&gt;\n#include &lt;unistd.h&gt;\n\nint main(int argc, char *argv[])\n{\n  int fd;\n  char content[100]=\"\\0\";\n  fd = open(argv[1], O_RDONLY);\n  if(fd &lt; 0)\n  {\n    printf(\"File could not be opened.\\n\");\n    return 1;\n  }\n  else\n  {\n    read(fd, content, sizeof(content)-1);\n    write(1, content, sizeof(content)-1);\n  }\n  return 0;\n}\n</code></pre> <ul> <li><code>argc</code> stands for argument count and argv stands for argument values</li> <li><code>open</code>: Used to Open the file for reading, writing or both.<ul> <li><code>int open (const char* Path, int flags [, int mode ]);</code></li> </ul> </li> <li>Flags<ul> <li><code>O_RDONLY</code>: read only</li> <li><code>O_WRONLY</code>: write only</li> <li><code>O_RDWR</code>: read and write,  </li> <li><code>O_CREAT</code>: create file if it doesn\u2019t exist</li> <li><code>O_EXCL</code>: prevent creation if it already exists</li> </ul> </li> </ul>"},{"location":"3_Core/Operating_Systems/Lab/02_Processes_1/","title":"02 Processes 1","text":""},{"location":"3_Core/Operating_Systems/Lab/02_Processes_1/#unix-process-creation","title":"UNIX Process Creation","text":"<p>If \\(P_1\\) spawns \\(P_2\\)</p> <ul> <li>\\(P_1\\) is parent</li> <li>\\(P_2\\) is child</li> </ul>"},{"location":"3_Core/Operating_Systems/Lab/02_Processes_1/#fork","title":"<code>fork()</code>","text":"<p>function using which new processes become child processes of the caller</p> <ul> <li>No parameters</li> <li>returns 0 to child process</li> <li>returns process ID of the child to the parent</li> </ul> <p>Both parent and child will immediately execute after the <code>fork()</code></p> <p>UNIX makes an exact copy of the parent\u2019s Stack, Heap, Data, and Code in another sequence of memory locations.</p> <p>Any change of variables by parent process won\u2019t affect the child process\u2019 values, and vice-versa</p> <p>If there are \\(n\\) <code>fork()</code> one after each other,</p> <pre><code>void main()\n{\n  fork(); // 1\n  fork(); // 2\n  ...;\n  fork(); // n\n}\n</code></pre> <ul> <li>Total number of processes \\(= 2^n\\)</li> <li>Total number of child processes \\(= 2^n - 1\\)</li> </ul>"},{"location":"3_Core/Operating_Systems/Lab/02_Processes_1/#code-1","title":"Code 1","text":"<p>The output after <code>fork()</code></p> <pre><code>#include &lt;unistd.h&gt;\n#include &lt;stdio.h&gt;\n\nvoid main()\n{\n  int pid;\n  pid = fork();\n\n  print(\"Hello\\n\");\n}\n</code></pre> <pre><code>Hello\nHello\n</code></pre>"},{"location":"3_Core/Operating_Systems/Lab/02_Processes_1/#concept-2","title":"Concept 2","text":"<p>The order of execution may be</p> <ul> <li>Parent then child</li> <li>or, Child then parent</li> </ul> <p>This depends on the system</p> <p>Value of <code>pid</code></p> <ul> <li>Child (=0)</li> <li>Parent (&gt;0)</li> <li>Unsuccessful (&lt;0)</li> </ul> <pre><code>#include &lt;unistd.h&gt;\n#include &lt;stdio.h&gt;\n\nvoid main()\n{\n  int pid;\n  pid = fork();\n\n  if(pid == 0)\n    printf(\"Child\\n\");\n  else if(pid &gt; 0)\n    printf(\"Parent\\n\");\n  else\n    printf(\"Unsuccessful Fork\");\n}\n</code></pre>"},{"location":"3_Core/Operating_Systems/Lab/02_Processes_1/#concept-3","title":"Concept 3","text":"<p>Let\u2019s say both the parent and child get the same variable, then</p> <ul> <li>Changes in parent process will only affect the value in the parent process</li> <li>Changes in child process will only affect the value in the child process</li> </ul> <p>This is because each process has its own address space; any modifications will be independent of each other</p> <pre><code>#include &lt;unistd.h&gt;\n#include &lt;stdio.h&gt;\n\nvoid main()\n{\n  int a = 0;\n\n  int pid;\n  pid = fork();\n\n  if(pid == 0)\n  {\n    a = a+10;\n    printf(\"Child %d\\n\", a);\n  } else if(pid &gt; 0)\n  {\n    a = a+5;\n    printf(\"Parent\\n\");\n  } else\n    printf(\"Unsuccessful Fork\");\n}\n</code></pre> <pre><code>Parent 5\nChild 10\n(or)\nChild 10\nParent 5\n</code></pre>"},{"location":"3_Core/Operating_Systems/Lab/03_Processes_2/","title":"03 Processes 2","text":""},{"location":"3_Core/Operating_Systems/Lab/03_Processes_2/#getpid","title":"<code>getpid()</code>","text":"<p>Used to display the id of the process that invokes it</p>"},{"location":"3_Core/Operating_Systems/Lab/03_Processes_2/#getppid","title":"<code>getppid()</code>","text":"<p>Get parent\u2019s process ID</p>"},{"location":"3_Core/Operating_Systems/Lab/03_Processes_2/#waitnull","title":"<code>wait(NULL)</code>","text":"<p>parent process waits for completion of any one of its children</p> <p>Parent initially moves to wait state, then comes to ready queue</p>"},{"location":"3_Core/Operating_Systems/Lab/03_Processes_2/#dependencies","title":"Dependencies","text":"<ul> <li><code>#include &lt;sys/types.h&gt;</code></li> <li><code>#include &lt;sys/wait.h&gt;</code></li> </ul>"},{"location":"3_Core/Operating_Systems/Lab/03_Processes_2/#return-value","title":"return value","text":"<p>pid of the terminated child</p>"},{"location":"3_Core/Operating_Systems/Lab/03_Processes_2/#exit-status-of-child","title":"exit status of child","text":"<p>integer value</p> <ul> <li>+ve: sucessful termination</li> <li>-ve : unsuccessful termination</li> </ul> <pre><code>pid_t wait(int *status);\n\nint status;\npid = wait(&amp;status);\n</code></pre>"},{"location":"3_Core/Operating_Systems/Lab/03_Processes_2/#code-1","title":"Code 1","text":"<pre><code>#include &lt;stdio.h&gt;\n#include &lt;unistd.h&gt;\n\n#include &lt;sys/types.h&gt;\n#include &lt;sys/wait.h&gt;\nvoid main()\n{\n  int rv, a;\n  rv = fork();\n\n  if(rv == 0)\n  {\n        printf(\"Hello\\n\");\n    printf(\"Child PID is %d\\n\", getpid());\n    printf(\"My parent's PID is %d\\n\", getppid());\n  }\n  else if(rv &gt; 0)\n  {\n    a = wait(NULL);\n    printf(\"Parent PID is %d\\n\", getpid());\n    printf(\"Parent: The child that terminated is %d\\n\", a);\n  }\n  else\n  {\n    printf(\"Unsuccessful\");\n  }\n}\n</code></pre> <pre><code>Hello\nChild PID is 1676\nMy parent's PID is 1672\nParent PID is 1672\nParent: The child that terminated is 1676\n</code></pre>"},{"location":"3_Core/Operating_Systems/Lab/04_Processes_3/","title":"04 Processes 3","text":""},{"location":"3_Core/Operating_Systems/Lab/04_Processes_3/#execlp","title":"<code>execlp()</code>","text":"<p>replaces the data &amp; text region of the calling process with the new data of program</p> <pre><code>execlp(program path, exec_name, arg_1, arg_2, ... , NULL);\n</code></pre> <p><code>NULL</code> is always the last parameter of <code>execlp()</code></p> <p><code>execlp()</code> will return to the child process only in case of error. It will go back to the parent process regardless. </p> <p>Hence, in the following example, \u201cblah blah\u201d will not execute.</p> <pre><code>#include &lt;stdio.h&gt;\n#include &lt;unistd.h&gt;\n#include &lt;sys/types.h&gt;\n#include &lt;sys/wait.h&gt;\n\nvoid main()\n{\n  int rv;\n  rv = fork();\n\n  if(rv == 0)\n  {\n    printf(\"I am a child process\\n\");\n    execlp(\"ls\", \"ls\", NULL);\n    printf(\"blah blah\"); \n  }\n  else if(rv &gt; 0)\n  {\n    wait(NULL);\n    printf(\"Hi. I am the parent\\n\");\n  }\n  else\n  {\n    printf(\"Unsuccessful\");\n  }\n}\n</code></pre> <pre><code>I am a child process\na.out  main.c\nHi. I am the parent\n</code></pre>"},{"location":"3_Core/Operating_Systems/Lab/04_Processes_3/#code-2-long-listing","title":"Code 2: Long Listing","text":"<pre><code>#include &lt;stdio.h&gt;\n#include &lt;unistd.h&gt;\n#include &lt;sys/types.h&gt;\n#include &lt;sys/wait.h&gt;\n\nvoid main()\n{\n  int rv;\n  rv = fork();\n\n  if(rv == 0)\n  {\n    printf(\"I am a child process\\n\");\n    execlp(\"ls\", \"ls\", \"-l\", NULL);\n    printf(\"blah blah\");\n  }\n  else if(rv &gt; 0)\n  {\n    wait(NULL);\n    printf(\"Hi. I am the parent\\n\");\n  }\n  else\n  {\n    printf(\"Unsuccessful\");\n  }\n}\n</code></pre> <pre><code>I am a child process\ntotal 24\n-rwxr-xr-x 1 runner3 runner3 16864 Oct 20 05:55 a.out\n-rwxrwxrwx 1 root    root      371 Oct 20 05:55 main.c\nHi. I am the parent\n</code></pre>"},{"location":"3_Core/Operating_Systems/Lab/04_Processes_3/#code-3-without-null","title":"Code 3: Without <code>NULL</code>","text":"<p>Gives error: missing sentinel in functional call</p> <p>This causes blah blah to be displayed</p> <pre><code>#include &lt;stdio.h&gt;\n#include &lt;unistd.h&gt;\n#include &lt;sys/types.h&gt;\n#include &lt;sys/wait.h&gt;\n\nvoid main()\n{\n  int rv;\n  rv = fork();\n\n  if(rv == 0)\n  {\n    printf(\"I am a child process\\n\");\n    execlp(\"ls\", \"ls\", \"-l\");\n    printf(\"blah blah\");\n  }\n  else if(rv &gt; 0)\n  {\n    wait(NULL);\n    printf(\"Hi. I am the parent\\n\");\n  }\n  else\n  {\n    printf(\"Unsuccessful\");\n  }\n}\n</code></pre> <pre><code>main.c: In function \u2018main\u2019:\nmain.c:14:5: warning: missing sentinel in function call [-Wformat=]\n   14 |     execlp(\"ls\", \"ls\", \"-l\");\n      |     ^~~~~~\nI am a child process\nblah blahHi. I am the parent\n</code></pre>"},{"location":"3_Core/Operating_Systems/Lab/04_Processes_3/#code-4-executing-another-program","title":"Code 4: Executing another program","text":""},{"location":"3_Core/Operating_Systems/Lab/04_Processes_3/#samplec","title":"<code>sample.c</code>","text":"<pre><code>#include &lt;stdio.h&gt;\nvoid main()\n{\n  printf(\"Hi there!\");\n  something\n}\n</code></pre> <pre><code>cc sample.c a.out\ncc sample.c -o sample.o\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/","title":"Principles of Programming Languages","text":"<p>Taught by Dr. B. Vijayakumar</p> <p>This course explores the key features and principles of programming languages, introducing main programming paradigms. Students will study semantics related to control abstraction, data types, scope, parameter passing, and concurrency, along with runtime environments, including global and local data, function call stacks, and threading.</p> <p>The curriculum covers functional programming with lambda calculus, logic programming principles, and scripting as a paradigm, along with domain-specific languages. The programming languages included in this course are C, C++, Python, Prolog, and LISP. </p>"},{"location":"3_Core/Principles_of_Programming_Languages/01_Intro/","title":"01 Intro","text":""},{"location":"3_Core/Principles_of_Programming_Languages/01_Intro/#programming-lanugage","title":"Programming Lanugage","text":"<p>Tool for humans to communicate with a computer system</p> <p>We should try to generalize a programming language, to ensure compatibility.</p>"},{"location":"3_Core/Principles_of_Programming_Languages/01_Intro/#properties","title":"Properties","text":"<ul> <li>Turing Complete   Must be able to express anything computable </li> <li>Must be able to implement on existing hardware platforms</li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/01_Intro/#what-makes-a-good-language","title":"What makes a good language?","text":"<ul> <li>Syntax close to what we are used to</li> <li>Readability</li> <li>Expressibility   For eg, <code>while</code> loop instead of <code>if</code> and <code>goto</code></li> <li>Reliability: Error Checking<ul> <li>Detect use of initialized variables</li> <li>Type checking</li> <li>Array Bounds (C vs Java)</li> <li>Testing Support</li> </ul> </li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/01_Intro/#paradigms","title":"Paradigms","text":"Imperative Declarative Programmer tells __ to do how what Focus of programming language Do exactly what is told Do what is meant Classifications - Procedural: Fortran, C- Object-Oriented: C++, Java- Scripting: Python, Javascript, Bash(Shell) - Functional: LISP/Scheme/Haskel, symbolic data processing- Logic: Prolog, Logic Reasoning- SQL: mySQL, PostgreSQL, Cassandra, NoSQL - Sequential- Concurrent"},{"location":"3_Core/Principles_of_Programming_Languages/01_Intro/#why-do-we-usually-use-imperative-languages","title":"Why do we usually use imperative languages?","text":"<p>As we mainly use von Neumann machines, which is the class of stored program digital computer.</p> <p>Imperative languages give control over moving data and program instructions between registers in CPU and Memory location</p>"},{"location":"3_Core/Principles_of_Programming_Languages/01_Intro/#von-neumann-architecture","title":"von Neumann Architecture","text":"<p>There is no distinction between code and data, as they are stored in the same memory</p>"},{"location":"3_Core/Principles_of_Programming_Languages/01_Intro/#structured-programming","title":"Structured Programming","text":"<p>Dijkstra</p> <p>Also referred to as \u2018goto-less\u2019 programming</p> <ul> <li>Single entry, Single exit sequence</li> <li>Selection</li> <li>Repetition</li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/01_Intro/#generations","title":"Generations","text":"LanguageGeneration Description LanguageTranslatorrequired Example Performance Machine This is what your machine understands None \ud83d\udd25 Assembly Mnemonics (names and symbols) Assembler x86MIPS \ud83d\udd25 High-Level - Machine independence- Semantics not limited to machine architecture- Human-Friendly- Data/Control Flow abstractions- Availability of libraries- Consistency check (data types) CompilerInterpreter CC++FortranPascal \ud83d\udd25 4<sup>th</sup> Gen Interpreter Python \ud83d\udc0c 5<sup>th</sup> Gen Just-in-Time Compiler Julia \ud83d\udd25 <p>In the present-day, compiler-generated code is faster than human-written assembly code; it was not the case before</p>"},{"location":"3_Core/Principles_of_Programming_Languages/01_Intro/#bugs","title":"Bugs","text":"<p>Programming testing can be used to show the presence of bugs, but never their absence!</p> <p>~ Dijkstra</p>"},{"location":"3_Core/Principles_of_Programming_Languages/01_Intro/#solution","title":"Solution","text":"<ul> <li>Organize code modularly, such that each part can be understood</li> <li>Alpha Testing by company testers</li> <li>Beta Testing by end-users</li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/01_Intro/#early-programming-languages","title":"Early Programming Languages","text":"Year Language Full Form Creator Purpose Focus Still Used? 1954 Fortran Formula Translator J Backus (IBM) Numerical Computing Efficiency \u2705 1958 ALGOL Algorithmic Language J Backus (IBM)F Bauer (TU) Algo \\(\\to\\) Programs ClarityElegance \u274c 1958 LISP List Processor J McCarthy (MIT) Symbolic Computing Abstractions \u2705 1959 COBOL Common Business Oriented Language G Hopper (US Navy) Data Processing (Payroll) \u2018English-like\u2019 \u2705"},{"location":"3_Core/Principles_of_Programming_Languages/01_Intro/#scripting-language-vs-programming-language","title":"Scripting Language vs Programming Language","text":"<p>Programming languages</p> <ul> <li>follow EBNF(Extended Bacus Normal Form)</li> <li>have CFG(Context-Free Grammar)</li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/01_Intro/#language-translators","title":"Language Translators","text":"Compiler Interpreter Virtual Machine Speed Faster Slower Error Detection Harder Easier Code Processing Method One-Go Line-by-Line Translation Once Every execution Distributes Executable Executable not possible Development time Slower Faster Typing Static/Dynamic Dynamic Target Program Machine CodePortable Machine Code (Java Byte Code)C Code \u274cDoes not generate target programexecutes instructions directly Example CC++Java JavascriptPython JavaPython"},{"location":"3_Core/Principles_of_Programming_Languages/01_Intro/#compiled-code","title":"Compiled Code","text":"<pre><code>flowchart LR\n\nsubgraph \"Once\"\n  sp2[Source Program] --&gt; Compiler\nend\n\nCompiler --&gt;  tp2\n\nsubgraph \"Every Execution\"\n    tp2[Target Program]\n    i2[/Input/] --&gt; tp2 --&gt; o2[/Output/]\nend</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/01_Intro/#interpreted-code","title":"Interpreted Code","text":"<pre><code>flowchart LR\n\nsubgraph \"Every Execution\"\n    direction LR\n  sp2[Source Program] --&gt; Interpreter\n  i1[/Input/] --&gt; Interpreter --&gt; o2[/Output/]\nend</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/01_Intro/#virtual-machines","title":"Virtual Machines","text":"<pre><code>flowchart LR\n\nsubgraph \"Once\"\n  sp2[Source Program] --&gt; Compiler --&gt; ip[\"Byte Code&lt;br /&gt;(Intermediate Program)\"]\nend\n\nip --&gt; vm[Virtual Machine]\n\nsubgraph \"Every Execution\"\n    i2[/Input/] --&gt; vm --&gt; o2[/Output/]\nend</code></pre> <p>combine the plus points of compiler/interpreter</p> <p>Interpreting high level code is slow</p>"},{"location":"3_Core/Principles_of_Programming_Languages/01_Intro/#java","title":"Java","text":"<pre><code>javac Prog.java\njava Prog\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/01_Intro/#python","title":"Python","text":"<pre><code>python prog.py\n</code></pre> <ul> <li>First time the program is run, it is interpreted</li> <li>In subsequent runs, the cached byte code <code>.pyc</code> is executed</li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/01_Intro/#ram","title":"RAM","text":"<p>Random Access Memory</p> <p>Stores program instructions, addresses, and immediate values</p>"},{"location":"3_Core/Principles_of_Programming_Languages/01_Intro/#shell-scripts","title":"Shell Scripts","text":"<pre><code>ls | sort | more\n</code></pre> <p>View resources consumption by various users</p> <pre><code>ipcs\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/02_Language_Evaluation/","title":"02 Language Evaluation","text":""},{"location":"3_Core/Principles_of_Programming_Languages/02_Language_Evaluation/#qualities-of-a-language","title":"Qualities of a Language","text":"Quality Meaning Readability Ease with humans can read and understand programs Writability Ease with humans can write programs Reliability Program performs to its specifications under all conditions Cost-efficiency - Efficiency of Training programmers- Efficiency of writing, compiling, executing, reading programs- Efficiency of computation and development- Monetary cost of compilers, license- Maintenance cost, due to reliability- Portability (standardization of the language)- Generality (applicability to a wide range of use-cases/applications)"},{"location":"3_Core/Principles_of_Programming_Languages/02_Language_Evaluation/#features","title":"Features","text":"Characteristic AffectsReadability? AffectsWritability? AffectsReliability? Simplicity \u2705 \u2705 \u2705 Orthogonality \u2705 \u2705 \u2705 Data Types &amp; Structures \u2705 \u2705 \u2705 Syntax Design \u2705 \u2705 \u2705 Abstraction \u2705 \u2705 Expressivity \u2705 \u2705 Type Checking \u2705 Exception Handling \u2705 Restricted Aliasing \u2705"},{"location":"3_Core/Principles_of_Programming_Languages/02_Language_Evaluation/#simplicity","title":"Simplicity","text":""},{"location":"3_Core/Principles_of_Programming_Languages/02_Language_Evaluation/#fewer-features-and-basic-constructs","title":"Fewer features and basic constructs","text":"<p>The main cause of readability issues is because program author uses a subset different from what the reader is familiar with</p>"},{"location":"3_Core/Principles_of_Programming_Languages/02_Language_Evaluation/#fewer-feature-multiplicity","title":"Fewer Feature Multiplicity","text":"<p>Feature Multiplicity = Ability to do the same operation in different ways</p> <pre><code>count = count + 1;\ncount += 1;\ncount++;\n++count;\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/02_Language_Evaluation/#fewer-operator-overloading","title":"Fewer Operator Overloading","text":"<p>Ambiguity arises due to ability of operator to perform multiple operations</p>"},{"location":"3_Core/Principles_of_Programming_Languages/02_Language_Evaluation/#orthogonality","title":"Orthogonality","text":"<p>Constructs in programming languages should be independent of each other; should not be redundant. Every combination of features should be meaningful.</p> <p>Any operation has minimal undesired side effects.</p> <p>Orthogonality is the property that means \"Changing A does not change B\".</p> <p>An example of an orthogonal system would be a radio, where changing the station does not change the volume and vice-versa.</p> <p>A non-orthogonal system would be like a helicopter where changing the speed can change the direction.</p> <p>In programming languages this means that when you execute an instruction, nothing but that instruction happens (which is very important for debugging).</p>"},{"location":"3_Core/Principles_of_Programming_Languages/02_Language_Evaluation/#data-types-structures","title":"Data Types &amp; Structures","text":"<p>For eg, the existence of <code>boolean</code> data type in a programming language is important, as otherwise we have to use integers (which make the program less clear)</p>"},{"location":"3_Core/Principles_of_Programming_Languages/02_Language_Evaluation/#syntax-design","title":"Syntax Design","text":""},{"location":"3_Core/Principles_of_Programming_Languages/02_Language_Evaluation/#identifiers","title":"Identifiers","text":"<p>Names for variables, functions, arrays, structures, etc</p>"},{"location":"3_Core/Principles_of_Programming_Languages/02_Language_Evaluation/#rules","title":"Rules","text":"<ul> <li>Starting character must be alphabet/underscore</li> <li>Other characters can be<ul> <li>Alphabet</li> <li>Underscore</li> <li>Digits</li> </ul> </li> <li>Max Length = 31 characters</li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/02_Language_Evaluation/#key-words","title":"Key Words","text":"<p><code>while</code>, <code>for</code>, <code>class</code></p> <p>Most programming languages use braces for pairing control structers</p>"},{"location":"3_Core/Principles_of_Programming_Languages/02_Language_Evaluation/#semantics-should-follow-syntax","title":"Semantics should follow Syntax","text":"<p>Semantics = meaning of your code; basically the logic</p> <p>For eg, use of <code>static</code>, <code>extern</code> in C</p>"},{"location":"3_Core/Principles_of_Programming_Languages/02_Language_Evaluation/#form-and-meaning","title":"Form and Meaning","text":"<p>Self-descriptive constructs and meaningful keywords</p>"},{"location":"3_Core/Principles_of_Programming_Languages/02_Language_Evaluation/#abstraction","title":"Abstraction","text":"<p>Ability to define and use complex structures/operations in ways that allow details to be ignored</p>"},{"location":"3_Core/Principles_of_Programming_Languages/02_Language_Evaluation/#process-abstraction","title":"Process Abstraction","text":"<p>Functions/Sub-routines for codes that will be required multiple times</p>"},{"location":"3_Core/Principles_of_Programming_Languages/02_Language_Evaluation/#data-abstraction","title":"Data Abstraction","text":"<p>Ability to create own data structures, such as binary tree using pointers/integers</p>"},{"location":"3_Core/Principles_of_Programming_Languages/02_Language_Evaluation/#expressivity","title":"Expressivity","text":"<p>Set of relatively convenient ways of specifying operations</p>"},{"location":"3_Core/Principles_of_Programming_Languages/02_Language_Evaluation/#examples","title":"Examples","text":"<ul> <li><code>count++</code> instead of <code>count = count + 1</code></li> <li><code>for</code> instead of <code>while</code></li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/02_Language_Evaluation/#type-checking","title":"Type Checking","text":"<p>Testing for type errors - ensuring that operands of an operator are of compatible type</p> <p>Run-Time type checking is expensive, hence compile-time type checking is preferred</p> Statically-Typed Languages Dynamically-Typed Languages CC++C#JavaFortran Objective-CGroovyJavascriptLISPLuaPHPPrologPythonRubySmalltalkTCL"},{"location":"3_Core/Principles_of_Programming_Languages/02_Language_Evaluation/#exception-handling","title":"Exception Handling","text":"<p>Intercept run-time errors and take corrective measures</p>"},{"location":"3_Core/Principles_of_Programming_Languages/02_Language_Evaluation/#restricted-aliasing","title":"Restricted Aliasing","text":"<p>Aliasing = Presence of multiple names for the same memory location</p> <p>Changing value pointed by one pointer changes the value pointed by another pointer to the same location.</p> <p>\u274c In C, union members and pointers may be set to point to the same variable.</p>"},{"location":"3_Core/Principles_of_Programming_Languages/02_Language_Evaluation/#grep","title":"Grep","text":"<p>**G**et **re**gular **e**x**p**ression</p>"},{"location":"3_Core/Principles_of_Programming_Languages/03_Language_Description/","title":"03 Language Description","text":""},{"location":"3_Core/Principles_of_Programming_Languages/03_Language_Description/#syntax-vs-semantics","title":"Syntax vs Semantics","text":"Syntax Semantics Structure of expressions, statements, program units Meaning of expressions, statements, program units - Lexical layer- Grammar layer"},{"location":"3_Core/Principles_of_Programming_Languages/03_Language_Description/#grammar","title":"Grammar","text":"<p>Means of</p> <ul> <li>describing a language: all possible legally-correct programs</li> <li>analyzing a sentence: check if program is valid</li> <li>derive a sentence: program</li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/03_Language_Description/#notations","title":"Notations","text":"Example Easy to decode during left-right scan Parenthesis-free Operands of each operator can be found unambiguously Can be evaluated with Prefix \\(+ ab\\) \u2705 \u2705 \u2705 - Tree- Stack Postfix \\(ab +\\) \u274c \u2705 \u2705 - Tree- Queue Infix \\(a+b\\) \u274c \u274c \u274c - Tree Mixfix If \\(a&gt;b\\) then \\(a\\) else \\(b\\)- <code>if</code>, <code>then</code>, <code>else</code> are keywords- \\(a, b\\) are components of expression \u274c \u274c \u274c"},{"location":"3_Core/Principles_of_Programming_Languages/03_Language_Description/#infix-notation","title":"Infix Notation","text":""},{"location":"3_Core/Principles_of_Programming_Languages/03_Language_Description/#associativity","title":"Associativity","text":"<ul> <li>Left: \\(+, -, *, /\\)</li> <li>Right: \\(=,\\) ^</li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/03_Language_Description/#precedence","title":"Precedence","text":"<p>BODMAS</p> <ol> <li>Brackets</li> <li>Orders (Powers, Indices, Roots)</li> <li>Division</li> <li>Multiplication</li> <li>Addition</li> <li>Subtraction</li> </ol>"},{"location":"3_Core/Principles_of_Programming_Languages/03_Language_Description/#abstact-syntax-tree","title":"Abstact Syntax Tree","text":"<p>identifies meaningful components of each construct in the language</p>"},{"location":"3_Core/Principles_of_Programming_Languages/03_Language_Description/#ab-ab-ab","title":"\\(+ab, \\ a+b, \\ ab+\\)","text":"<p>all have the same abstract syntax tree</p> <pre><code>flowchart TB\n+ --&gt; a &amp; b</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/03_Language_Description/#if-a-b-then-a-else-b","title":"if \\(a &gt; b\\), then \\(a\\), else \\(b\\)","text":"<pre><code>flowchart TB\nstmt --&gt; if &amp; e1 &amp; then &amp; stmt1 &amp; else &amp; stmt2\n\ne1 --&gt; x[\"a&gt;b\"]\nstmt1 --&gt; a\nstmt2 --&gt; b</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/03_Language_Description/#lexical-syntax","title":"Lexical Syntax","text":"<p>group characters of source program into meaningful sequence (omitting blank spaces and comments) and generate tokens/terminals</p>"},{"location":"3_Core/Principles_of_Programming_Languages/03_Language_Description/#syntax-of-token","title":"Syntax of token","text":"<p>"},{"location":"3_Core/Principles_of_Programming_Languages/03_Language_Description/#token-classes","title":"Token Classes","text":"Class Example Keyword <code>if</code>, <code>else</code>, <code>while</code> Operator \\(+ - * /\\) Identifier(Variables) max, a, total Constant(Numeric/String constant) pi Punctuation Marks \\(, ; () [] \\{ \\}\\) Number \\(1 \\ 2 \\ 3 \\ \\dots\\)"},{"location":"3_Core/Principles_of_Programming_Languages/03_Language_Description/#example","title":"Example","text":"<p>\\(a*b-4\\) has the following tokens"},{"location":"3_Core/Principles_of_Programming_Languages/03_Language_Description/#cfg","title":"CFG","text":"<p>Context-Free Grammar</p> <p>Used to specify grammar of non-regular expressions (refer Theory of Computation)</p>"},{"location":"3_Core/Principles_of_Programming_Languages/03_Language_Description/#parts","title":"Parts","text":"<ul> <li>Set of tokens/terminals</li> <li>Set of non-terminals</li> <li>Set of rules/productions   Each production has a<ul> <li>non-terminal on its left hand-side</li> <li>\\(::=\\) or \\(\\rightarrow\\) on right-hand side</li> <li>String of terminals/non-terminals on right-hand side</li> </ul> </li> <li>A Non terminal is chosen as the starting non-terminal   Unique starting terminal is called starting symbol</li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/03_Language_Description/#bnf","title":"BNF","text":"<p>Bacus Naur Form</p> <p>used to specify grammar</p>"},{"location":"3_Core/Principles_of_Programming_Languages/03_Language_Description/#parts_1","title":"Parts","text":"<ul> <li>Terminal appear as keyword, opeator, identifiers, constant, punctuation mark</li> <li>Non-terminals are enclosed between \\(&lt; &gt;\\)   Eg: &lt; fraction &gt;</li> <li>Productions<ul> <li>Read \\(::=\\) as \u2018can be\u2019</li> <li>Read \\(|\\) or</li> </ul> </li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/03_Language_Description/#example_1","title":"Example","text":"<ul> <li>&lt; fraction &gt; \\(::=\\) &lt; digit &gt; | &lt; digit &gt;&lt; fraction &gt;   Fraction can be digit or a digit followed by fraction</li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/03_Language_Description/#cfg-for-real-number-bnf","title":"CFG for real number BNF","text":"<ul> <li>&lt; real-number &gt; \\(::=\\)  . &lt; fraction &gt; <li>&lt; integer_part &gt; \\(::=\\) &lt; digit &gt; |  &lt; digit &gt; <li>&lt; fraction &gt; \\(::=\\) &lt; digit &gt; | &lt; digit &gt;&lt; fraction &gt;</li> <li>&lt; digit &gt; \\(::=\\) 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 9</li>"},{"location":"3_Core/Principles_of_Programming_Languages/03_Language_Description/#parse-tree","title":"Parse Tree","text":"<p>Also called as Concrete Syntax Tree</p> <p>Tree built using starting non-terminal and productions</p> <p>Construction of a parse tree is called parsing</p> <p>Generates the string formed by reading terminals at its leaves from left to right</p> <p>A string is only in a language if it is generated by some parse tree</p>"},{"location":"3_Core/Principles_of_Programming_Languages/03_Language_Description/#rules","title":"Rules","text":"<ul> <li>Root is labeled with the starting non terminal</li> <li>Each non-leaf is labeled with a non terminal</li> <li>Each leaf is labeled with a terminal</li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/03_Language_Description/#example_2","title":"Example","text":"<p>\\(123.789\\)</p> <pre><code>flowchart TB\nrn[Real Number]\n\nrn --&gt; i1[Integer-Part] &amp; . &amp; f1[Fraction]\ni1 --&gt; d1[Digit] &amp; i2[Integer-Part]\ni2 --&gt; d2[Digit] &amp; i3[Integer-Part]\ni3 --&gt; d3[Digit]\n\nf1 --&gt; d7[Digit] &amp; f2[Fraction]\nf2 --&gt; d8[Digit] &amp; f3[Fraction]\nf3 --&gt; d9[Digit]\n\nd1 --&gt; 1\nd2 --&gt; 2\nd3 --&gt; 3\nd7 --&gt; 7\nd8 --&gt; 8\nd9 --&gt; 9</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/03_Language_Description/#ambiguous-grammar","title":"Ambiguous Grammar","text":"<p>A grammar for a language is syntactically ambiguous, if some string in its language has more than one parse tree.</p>"},{"location":"3_Core/Principles_of_Programming_Languages/03_Language_Description/#dangling-else","title":"Dangling Else","text":"<p><code>if E1 then if E2 then S1 else S2</code> has 2 parse trees with the following grammar</p> <ul> <li>S \\(::=\\) if E then S</li> <li>S \\(::=\\) if E then S else S</li> </ul> <p></p>"},{"location":"3_Core/Principles_of_Programming_Languages/03_Language_Description/#math","title":"Math","text":"<p>\\(2+3*5\\) has 2 parse trees with the following grammar</p> <ul> <li>&lt; expr &gt; \\(::=\\) &lt; expr &gt; &lt; op &gt; &lt; expr&gt; | &lt; digit &gt;</li> <li>&lt; op &gt; \\(::=\\) + | - | * | /</li> <li>&lt; digit &gt; \\(::= 0 | 1 | 2 | \u2026. | 9\\)</li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/03_Language_Description/#derivations","title":"Derivations","text":"<p>Text version of Parse Tree</p> Top-Down Derivation Bottom-Up Derivation Start from Starting symbol and derive the sentence. Start from the sentence and reach the start symbol. Replace the LHS of a production by RHS Replace the RHS of a production by the LHS Leftmost Derivation Rightmost Derivation Leftmost nonterminal is replaced repeatedly Rightmost nonterminal is replaced repeatedly"},{"location":"3_Core/Principles_of_Programming_Languages/03_Language_Description/#example_3","title":"Example","text":"<p>Real Number</p> <p>real-number \\(\\implies\\) integer-part . fraction \\(\\implies\\) integer-part digit.fraction</p>"},{"location":"3_Core/Principles_of_Programming_Languages/03_Language_Description/#grammar-for-expressions","title":"Grammar for Expressions","text":"<ul> <li>Left-Recursive Grammars handle </li> <li>Right-Recursive Grammars handle right-associativity</li> </ul> Left-Recursive Right-Recursive Handles Left-associativity Right-associativity Example \\(L ::= L + \\text{num} \\ L \u2013 \\text{num} \\vert  \\text{num}\\) \\(R ::= \\text{num} + R \\vert  \\text{num} \u2013 R  \\vert \\text{num}\\)"},{"location":"3_Core/Principles_of_Programming_Languages/03_Language_Description/#operator-precedence","title":"Operator Precedence","text":"<p>(Low to High)</p> Operator Meaning \\(=\\) Assignment \\(\\vert  \\vert\\) Logical or \\(\\&amp;\\&amp;\\) Logical and \\(\\vert\\) inclusive or ^ exclusive or \\(\\&amp;\\) and \\(== \\ \\ \\ne\\) equality \\(&lt; \\ \\le \\ \\ge \\ &gt;\\) relational \\(&lt;&lt; \\ \\ &gt;&gt;\\) shift \\(+ -\\) additive \\(* / \\%\\) multiplicative \\(\\uparrow\\) exponentiation"},{"location":"3_Core/Principles_of_Programming_Languages/03_Language_Description/#operator-association","title":"Operator Association","text":"Association Left \\(+ - */\\) Right \\(= \\ \\uparrow\\)"},{"location":"3_Core/Principles_of_Programming_Languages/03_Language_Description/#extended-bnf","title":"Extended BNF","text":"<ul> <li>Braces \\(\\{ \\}\\) represent zero/more repetitions</li> <li>Brackets \\([]\\) represent optional contruct</li> <li>Vertical bar \\(|\\) represents a choice</li> <li>Parentheses \\(()\\) are used for grouping</li> </ul> \\[ \\begin{aligned} E &amp;::= T \\Big\\{ (+ | -) \\ T \\Big\\} \\\\ T &amp;::= F \\Big\\{ (* | /) \\ F \\Big\\} \\\\ F &amp;::= ( E ) | \\text{Name} | \\text{Number} \\end{aligned} \\] <ul> <li>\\(E =\\) Expression</li> <li>\\(T =\\) Term</li> <li>\\(F =\\) Factor</li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/03_Language_Description/#syntax-chart","title":"Syntax Chart","text":"<p>The above can be represented as \u2b07</p> <p></p>"},{"location":"3_Core/Principles_of_Programming_Languages/04_Structured_Programming/","title":"04 Structured Programming","text":""},{"location":"3_Core/Principles_of_Programming_Languages/04_Structured_Programming/#structured-programming-context","title":"Structured Programming Context","text":"<p>CA sequential computation consists of sequence of actions. These computations are dynamic occur during program execution</p> <p>Program text is static</p> <p>It is essential that the program text represents the computation that occurs when the program runs</p>"},{"location":"3_Core/Principles_of_Programming_Languages/04_Structured_Programming/#design-principles-of-imperative-languages","title":"Design Principles of Imperative Languages","text":"<ol> <li>Structure of program text should help understand what program does. Reliability of structured programs make them easier to modify for efficiency</li> <li>Language must allow underlying machine to be used efficiently</li> </ol>"},{"location":"3_Core/Principles_of_Programming_Languages/04_Structured_Programming/#structured-control-flow","title":"Structured Control Flow","text":"<p>A program is structured if the flow of control through the program is evident from the syntactic structure of the program text.</p> <p>This is done by making structured statements single-entry, single-exit</p>"},{"location":"3_Core/Principles_of_Programming_Languages/04_Structured_Programming/#sequential-statements","title":"Sequential Statements","text":"<p>sequence of statements</p> <p>A compound statement is a block of code, which is a grouped statement enclosed in some manner, such as <code>{...}</code>, <code>begin ... end</code></p>"},{"location":"3_Core/Principles_of_Programming_Languages/04_Structured_Programming/#selectionconditional-statements","title":"Selection/Conditional Statements","text":""},{"location":"3_Core/Principles_of_Programming_Languages/04_Structured_Programming/#if-else","title":"<code>if ... else</code>","text":"<ul> <li><code>if &lt;exp1&gt; then &lt;stmt1&gt;</code></li> <li><code>if &lt;exp1&gt; then &lt;stmt1&gt; else &lt;stmt2&gt;</code></li> <li>Nested conditionals</li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/04_Structured_Programming/#switchcase","title":"<code>switch...case</code>","text":"<pre><code>switch(exp)\n{\n  case const1:\n    ; // stmt1\n    break;\n  case const2:\n    ; // stmt2\n    break;\n  default:\n    ; // default stmt\n}\n</code></pre> <p>Properties</p> <ul> <li>Case constants can appear in any order</li> <li>Case constants need not be consecutive</li> <li>Several case constants can select the same sub-statement</li> <li>Case constants must be distinct to avoid ambiguity</li> </ul> <p>Notes</p> <ul> <li>Pascal gives error if none of the cases are selected</li> <li>Pascal uses <code>else</code> case instead of <code>default</code> case</li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/04_Structured_Programming/#if-else-vs-switch-case","title":"<code>if ... else</code> vs <code>switch ... case</code>","text":"<p>Case statements are preferred over <code>if ... else</code> when we have adjacent conditionals. Otherwise, use <code>if \u2026 else</code></p> <pre><code>// \u2705\nswitch(s)\n{\n  case 1:\n  case 2:\n  case 3:\n}\n\n// \u274c\nswitch(s)\n{\n  case 1:\n  case 2000:\n  case 30000:\n}\n\n// \u2705\nif (s == 1)\nelse if (s == 2000)\nelse if (s == 30000)\n</code></pre> <p>When using cases, a jump table is created, which contains entry \\(i\\) that is machine instruction to jump to code for case \\(i\\). The no of entries in the jump table \\(= \\max - \\min + 1\\). However, only the cases that are in the our code are actually used.</p> <p>Compiler uses jump table if atleast half of the entries are used. Else, the compiler uses a hash table instead.</p>"},{"location":"3_Core/Principles_of_Programming_Languages/04_Structured_Programming/#iterativeloops-statements","title":"Iterative/Loops Statements","text":"<ul> <li>Definite Loop   Known number of iterations</li> <li>Indefinite Loop   Number of iterations is only known at run time</li> <li>Infinite loop   Loop keeps iterating</li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/04_Structured_Programming/#while","title":"While","text":"<p><code>while &lt;exp&gt; do &lt;stmt</code></p> <p></p>"},{"location":"3_Core/Principles_of_Programming_Languages/04_Structured_Programming/#do-while","title":"Do-While","text":"<p><code>repeat &lt;stmt&gt; until &lt;exp&gt;</code></p>"},{"location":"3_Core/Principles_of_Programming_Languages/04_Structured_Programming/#for","title":"For","text":"<p><code>for &lt;id&gt; = &lt;exp&gt; to &lt;exp&gt; do &lt;stmt&gt;</code></p> <p>Components</p> <ul> <li>index/iterative variable</li> <li>Step</li> <li>Limit</li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/04_Structured_Programming/#handling-special-cases","title":"Handling Special Cases","text":"<ul> <li><code>break</code></li> <li><code>continue</code></li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/04_Structured_Programming/#return-statements","title":"Return Statements","text":"<p><code>return &lt;exp&gt;</code></p> <p>Sends control from a procedure back to a caller carrying the value of the expression. If return is not inside a procedure the program halts.</p> <p>Break vs Return</p> <ul> <li>Break : control goes out of a loop</li> <li>Return : control goes out of a procedure</li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/04_Structured_Programming/#goto-statements","title":"Goto Statements","text":"<p>Basically unconditional jump</p> <pre><code>goto L\n  ...\nL: &lt;stmt&gt;\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/05_Types_Data_Representation/","title":"05 Types Data Representation","text":""},{"location":"3_Core/Principles_of_Programming_Languages/05_Types_Data_Representation/#data-type-definition","title":"Data Type Definition","text":"<p>Collection of dat objects, having a set of predefined operations</p> <ul> <li>Descriptor: Collection of attributes for a variable</li> <li>Object: Instance of a user-defined/abstract data type</li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/05_Types_Data_Representation/#primitive-data-types","title":"Primitive Data Types","text":""},{"location":"3_Core/Principles_of_Programming_Languages/05_Types_Data_Representation/#void","title":"<code>void</code>","text":""},{"location":"3_Core/Principles_of_Programming_Languages/05_Types_Data_Representation/#boolean","title":"<code>boolean</code>","text":"<p>true/false</p> <p>represented as a byte</p> <ul> <li>Could be represented as bit</li> <li>but accessing a single bit is not as efficient as accessing a byte (as it is the smallest addressable unit of memory)</li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/05_Types_Data_Representation/#char","title":"<code>char</code>","text":""},{"location":"3_Core/Principles_of_Programming_Languages/05_Types_Data_Representation/#int","title":"<code>int</code>","text":""},{"location":"3_Core/Principles_of_Programming_Languages/05_Types_Data_Representation/#float","title":"<code>float</code>","text":""},{"location":"3_Core/Principles_of_Programming_Languages/05_Types_Data_Representation/#double","title":"<code>double</code>","text":""},{"location":"3_Core/Principles_of_Programming_Languages/05_Types_Data_Representation/#table","title":"Table","text":"<p>No need to study this; just for reference</p> <p></p>"},{"location":"3_Core/Principles_of_Programming_Languages/05_Types_Data_Representation/#derived-data-types","title":"Derived Data Types","text":""},{"location":"3_Core/Principles_of_Programming_Languages/05_Types_Data_Representation/#array","title":"Array","text":"<ul> <li>Collection of elements of the same data type</li> <li>Linear data structure</li> </ul> <p>Implementing arrays require high compile-time effort. Code to allow accessing of array elements must be generated during compilation. At run time, this code must be executed to produce element addresses.</p> <p>Dynamic allocation of arrays allows to choose an array length at runtime. Java has built-in dynamic arrays.</p>"},{"location":"3_Core/Principles_of_Programming_Languages/05_Types_Data_Representation/#alternative-names","title":"Alternative Names","text":"Dimension of array Alternative Name 1 Vector 2 Matrix"},{"location":"3_Core/Principles_of_Programming_Languages/05_Types_Data_Representation/#rowcolumn-major","title":"Row/Column Major","text":"<p>Most programming languages use row major</p> Major Address of \\((i, j)\\)th element Row \\(B + W[C(i \u2013 L_r) + (j \u2013 L_c)]\\) Column \\(B + W[(i \u2013 L_r) + R(j \u2013 L_c)]\\) Variable Meaning \\(B\\) Base address \\(W\\) Width of every blockSize of every element(bytes) \\(L_r\\) index of 1<sup>st</sup> row, if in-between(assume 0 if not specified) \\(L_c\\) index of 1<sup>st</sup> col(assume 0 if not specified) \\(R\\) total number of rows \\(C\\) total number of columns"},{"location":"3_Core/Principles_of_Programming_Languages/05_Types_Data_Representation/#structures","title":"Structures","text":"<ul> <li>Collection of elements of one/more data types</li> <li>Non-linear data structure</li> <li>Another way to think of it: Represent a record of different attributes</li> </ul> <pre><code>struct Book\n{\n  char title[50];\n  int year;\n};\n\nvoid main()\n{\n    struct Book b1;\n\n  strcpy(b1.title, \"Blah Blah\");\n  b1.year = 2020;\n}\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/05_Types_Data_Representation/#union","title":"union","text":"<p>Data type in C that allows to store different data types in the same memory location</p> <pre><code>union Book\n{\n  char title[50];\n  int year;\n}\n</code></pre> <p>Union is very similar to structure, but the memory implementation is different; both <code>title</code> and <code>year</code> will be stored together in one block, rather than different blocks.</p> <p>The size of the block will be the size of the largest data type. So, the value of the block will be the latest value we stored. So we won\u2019t get the correct expected output like structure.</p> <p></p>"},{"location":"3_Core/Principles_of_Programming_Languages/05_Types_Data_Representation/#enum","title":"Enum","text":"<p>Assign names to integral constants</p> <pre><code>enum week {\n  Mon, // 0\n  Tue, // 1\n  Wed, // 2\n  Thu, // 3\n  Fri, // 4\n  Sat, // 5\n  Sun  // 6\n}\n\nvoid main()\n{\n  enum week d1;\n  d1 = Wed;\n  printf(\"%d\", day);\n}\n\n// Output\n// 2\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/05_Types_Data_Representation/#pointer","title":"Pointer","text":"<p>Variable containing address to memory location</p> <pre><code>char *p;\nint *p;\nfloat *p;\ndouble *p;\n</code></pre> <pre><code>void main()\n{\n  int var = 20;\n\n  int *ip;\n  ip = &amp;var;\n\n  printf(\n    \"Address of var variable: %x\\n\",\n    &amp;var\n  ); \n  printf(\n    \"Address stored in ip variable: %x\\n\",\n    ip\n  ); \n\n  printf(\n    \"Value of *ip variable: %d\\n\",\n    *ip\n  );\n}\n\n// Output\n// Address of var variable: bffd8b3c \n// Address stored in ip variable: bffd8b3c \n// Value of *ip variable: 20 \n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/05_Types_Data_Representation/#null-pointer","title":"Null Pointer","text":"<p>Pointer pointing to \u2018nothing\u2019 (empty location in memory)</p> <p>Value of pointer is 0</p> <pre><code>int *p = NULL;\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/05_Types_Data_Representation/#issues-with-pointers-and-dynamic-allocation","title":"Issues with Pointers and Dynamic Allocation","text":"Dangling Pointer Memory Leak Meaning Pointer is pointing to- invalid memory- memory not owned by program Allocated memory is not de-allocated, after utilization Occurs when Referencing object is deleted/de-allocated, without changing the value of pointers Programmer does not de-allocate memory, if the PL doesn\u2019t have automatic garbage collection. Solution Set the pointer as null pointer when not required anymore. Have a counter variable to track no of- Dynamic allocations- De-allocationsAt the end, both counters should be equal <pre><code>void main()\n{\n    int *p = NULL;\n  p = malloc( // or calloc\n    sizeof(int) * 5\n  ); // dynamic array that can contain 5 integers\n\n  free(p); // Without This =&gt; Memory Leak\n  *p = NULL; // without this =&gt; Dangling Pointer\n}\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/05_Types_Data_Representation/#subrange-type","title":"Subrange Type","text":"<p>Limits the range of values a variable can take, by defining a subset of the values of a particular type. Code is inserted by compiler to restrict assingments to subrange values</p> <p>Example</p> <pre><code>type\ndigit = 0..9;\nletter = 'A'..'Z';\n\nvar\nnum: digit;\nalpha: letter;\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/05_Types_Data_Representation/#types-error-checking","title":"Types &amp; Error Checking","text":""},{"location":"3_Core/Principles_of_Programming_Languages/05_Types_Data_Representation/#purpose","title":"Purpose","text":"<ul> <li>Avoid errors in programs</li> <li>Analyze safety of program (eg bufferoverflow)</li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/05_Types_Data_Representation/#variable-bindings","title":"Variable Bindings","text":"<p>Associate a property with a variable</p> <p>Binding can be</p> <ul> <li>static (early)</li> <li>dynamic (late)</li> </ul> Language Variables Values Compiled Fixed Dynamic Interpreted Changeable Dynamic"},{"location":"3_Core/Principles_of_Programming_Languages/05_Types_Data_Representation/#types-systems","title":"Types Systems","text":"<ul> <li>Set of rules for associating a type with expressions</li> <li>Used for detecting invalid operations on incompatible types   For eg<ul> <li>pointer + int = \u2705</li> <li>pointer + pointer = \u274c</li> </ul> </li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/05_Types_Data_Representation/#type-checking","title":"Type Checking","text":"<ul> <li>Uses property of function</li> <li>Function maps element of one set to another set</li> <li> <p>For eg: Arithmetic operations are functions</p> <ul> <li>If \\(E, F\\) are <code>int</code>, then \\(E+F\\) will also be <code>int</code></li> </ul> </li> <li> <p>Strongly-typed language is one that allows only safe expressions.</p> </li> <li> <p>Type-Safe Program is one that executes without any type errors</p> </li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/05_Types_Data_Representation/#static-vs-dynamic-typing","title":"Static vs Dynamic Typing","text":"Type-Checking When? Execution Time Example Static Compile-Type \\(\\downarrow\\) C, C++ Dynamic Run-Time \\(\\uparrow\\) Python"},{"location":"3_Core/Principles_of_Programming_Languages/05_Types_Data_Representation/#type-conversion","title":"Type Conversion","text":"Conversion By Automatic? Alternate Name Example Implicit Language Translator \u2705 Coercion <code>double x = 3;</code> Explicit User \u274c Casting <code>double x = (double) 5;</code>"},{"location":"3_Core/Principles_of_Programming_Languages/05_Types_Data_Representation/#type-name-equivalence","title":"Type Name &amp; Equivalence","text":"EquivalenceType Meaning Example Structural SE1 Type is structurally-equivalent to itself \\(\\text{char} \\equiv \\text{char}\\) SE2 2 types are structurally-equivalent, if they are formed using the same construtor to structurally-equivalent types \\(\\text{char} \\equiv \\text{char} \\implies \\text{char}[10] \\equiv  \\text{char}[10]\\) SE3 One data type is declared using <code>typedef</code> with the other <code>typedef X Y</code> \\(\\implies X \\equiv Y\\) RestrictedType Pure Name Type name is equivalent to itself \\(\\text{char} \\equiv \\text{char}\\) Transitive Name <code>typedef X int; typedef Y X</code> \\(\\implies X \\equiv Y \\equiv \\text{int}\\) TypeExpression Type name is equivalent only to itselfApply same constructor to equivalent expressions <p>Notes</p> <ul> <li>C uses SE for all types, except structures</li> <li>(Somethign about circular types - I didn\u2019t understand)</li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/05_Types_Data_Representation/#type-error","title":"Type Error","text":"<p>If a function/operation expects an argument of type \\(T\\), but is supplied with argument of type \\(S\\), such that \\(S \\not \\equiv T\\)</p>"},{"location":"3_Core/Principles_of_Programming_Languages/05_Types_Data_Representation/#polymorphism","title":"Polymorphism","text":"<p>assuming different forms</p> Binding Type When Example Static Compilation - Operator Overloading- Function Overloading Dynamic Execution Function Overriding"},{"location":"3_Core/Principles_of_Programming_Languages/06_Procedures/","title":"06 Procedures","text":"<p>Named block of code. When the name is called, the body is executed.</p>"},{"location":"3_Core/Principles_of_Programming_Languages/06_Procedures/#types-of-procedures","title":"Types of Procedures","text":"FunctionProcedures ProperProcedures Extend built-in__ of language Operators Actions/statements Return single value - Set variables/perform output Placement Within-Expression Atomic Statements <code>r * sin(angle)</code> <code>read(ch)</code> Called byfunction \u2705 \u274c Called byprocedure \u2705 \u2705 PL C/C++ Pascal"},{"location":"3_Core/Principles_of_Programming_Languages/06_Procedures/#procedure-definition","title":"Procedure Definition","text":"<ul> <li>Procedure Name</li> <li>Code body</li> <li>Formal Parameters</li> <li>Return type</li> </ul> <pre><code>function square(x:integer):integer\nbegin\n    square := x*x\nend\n</code></pre> <pre><code>int square(int x)\n{\n  return x*x;\n}\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/06_Procedures/#note","title":"Note","text":"<p>C does not allows procedure bodies to be nested, so uses control links only. So calling a procedure means that procedure will be</p> <ul> <li>Within same procedure   (or)</li> <li>Outside all procedures</li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/06_Procedures/#procedure-calls","title":"Procedure Calls","text":"<pre><code>procedure_name(parameters)\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/06_Procedures/#procedure-activation","title":"Procedure Activation","text":"<p>Execution of procedure body</p> <p>Layout of activation is known at run-time</p> <p>Frame is put on the stack and storage within is accessed relative to the frame pointer.</p> <p>The values of variables in an activation are accessed as</p> <ul> <li>Frame Pointer + Displacement (or offset)</li> <li>Displacement is calculated at run-time</li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/06_Procedures/#benefits-of-procedures","title":"Benefits of Procedures","text":"<ul> <li>Procedure Abstraction</li> <li>Hides implementation details</li> <li>Program Legibility</li> <li>Better maintainence</li> <li>Better modularity</li> <li>Code re-usability (user-defined and from libraries)</li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/06_Procedures/#parameter-passing-methods","title":"Parameter Passing Methods","text":"Call by value Call by reference Passed Absolute valueVariable AddressPointer Changes to formal parameter in the function affects the actual parameter? \u274c \u2705 Same memory location for Formal and actual parameter \u274c \u2705 Default in PL C Pascal"},{"location":"3_Core/Principles_of_Programming_Languages/06_Procedures/#scope","title":"Scope","text":""},{"location":"3_Core/Principles_of_Programming_Languages/06_Procedures/#scope-of-variable","title":"Scope of Variable","text":"<p>Part of program where use of variable refers to its declaration</p> Static/Lexical Dynamic Binding occurs during Compilation Execution ProgrammerComprehensibility Easy Difficult Example CPascalJavaPearl (<code>my</code>) PythonLISPPearl (<code>local</code>) Rules A variable always refers to its top-level environment Variable declared within a block are not in the scope outside the block Global identifier refers to the identifier associated with the most recent environment Variables outside the block are visible unless overridden The occurrence of a identifier is searched in the most recent binding Binding of variable can be determined by program text, independent of run-time function call stack Each time a new function is executed, a new scope is pushed onto the stack. Compiler first searches in the current block, then in global variables The compiler first searches the current block and then successively all the calling functions. ### Scope Rules <p>Visibility rules for names in a PL; names could denote procedures, types, constants, variables</p>"},{"location":"3_Core/Principles_of_Programming_Languages/06_Procedures/#macros","title":"Macros","text":"<ul> <li>Procedure body is substituted at every point of call</li> <li>Actual parameters are substituted for the formals   Different from call by value/reference</li> </ul> <p>Dynamic scoping</p> <pre><code>#define pi 3.14\n</code></pre> <p>Every occurance of <code>pi</code> is replace with \\(3.14\\) by the compiler.</p> <pre><code>#define product(x, y) x*y\n</code></pre> <p>Every call of <code>product(x, y)</code> is replaced with <code>x*y</code></p>"},{"location":"3_Core/Principles_of_Programming_Languages/06_Procedures/#runtime-memory-model","title":"Runtime Memory Model","text":"<p>Layout of executable file</p> Segment Stores Code Machine code of program Static Data live throughout program execution- Global vars- Constants Stack Local variables of procedureProcedure activation records (C, Pascal) Space reclaimed when procedure terminatesRelative Address of variable are same Heap Dynamic memory allocationProcedure activation records Activation records stay here as long as they are needed <p></p>"},{"location":"3_Core/Principles_of_Programming_Languages/06_Procedures/#procedure-activations","title":"Procedure Activations","text":"<p>I didn't understand this</p> <p></p>"},{"location":"3_Core/Principles_of_Programming_Languages/06_Procedures/#activation-record","title":"Activation Record","text":"<p>Activation records on stack are called as stack frame</p>"},{"location":"3_Core/Principles_of_Programming_Languages/06_Procedures/#sections","title":"Sections","text":"Link Type Represents ___ environment of procedure Access Static Points to activation record for run-time caller defining Control Dynamic implement statically-scope languages calling"},{"location":"3_Core/Principles_of_Programming_Languages/06_Procedures/#special-registers","title":"Special Registers","text":"Register Full-Form Pointer to PC Program Counter Next instruction to be executed SP Stack Pointer Last location allocated on call stack FP Frame Pointer Current activation record to allow access to local variable AP Argument Pointer Current argument/parameter"},{"location":"3_Core/Principles_of_Programming_Languages/06_Procedures/#variables","title":"Variables","text":"<p>Storage is allocated at compile time</p> <p>Languages without recursive procedure treat all variables as static.</p> Static Local Lifetime Entire program Within procedure activation Retain values between activations? \u2705 \u274c(Bound to distinct storage in each activation) <p>Example for Static variable declaration in C <pre><code>static int count = 0;\nint count = 0; // variables declared outside main() are static by default\n</code></pre></p>"},{"location":"3_Core/Principles_of_Programming_Languages/06_Procedures/#activation-tree","title":"Activation Tree","text":"<p>Tree representing procedure activations of program</p> <p>For eg: Trees for merge sort, quicksort in DSA</p>"},{"location":"3_Core/Principles_of_Programming_Languages/06_Procedures/#garbage-collection","title":"Garbage Collection","text":"<p>Technique to reclaim storage that is no longer needed</p>"},{"location":"3_Core/Principles_of_Programming_Languages/06_Procedures/#recursion","title":"Recursion","text":"<p>Also called as multiple activation</p> <p>Recursive procedure is one that can be activated by its own body.</p> Traditional Recursion Tailed Recursion Nothing to do after the function returns, except return its value Compiler replaces caller with callee Last statement in the body of procedure is recursive call \u274c \u2705 Steps - Perform recursive calls first- Take the return value of the recursive call- Calculate the result - Perform your calculations first- Execute the recursive call- Passing results of current step to next Result of calculation obtained only after returning from every recursive call Re-useStack Frame \u274c(Nested Stack Frames) \u2705 Efficient \u274c \u2705 <pre><code>// Traditional\nfactorial(n) {\n    if (n == 0)\n    return 1;\n\n  return n * factorial(n - 1);\n}\n\n// Tailed\nfactorial1(n, accumulator)\n{\n  if (n == 0)\n    return accumulator;\n\n  return factorial1(n - 1, n * accumulator);\n}\n\nfactorial(n)\n{\n  return factorial1(n, 1);\n}\n</code></pre> <p>Converting a tailed-recursion logic into an equivalent control-flow logic is called as Tail-recursion elimination. This can be done using <code>goto</code></p>"},{"location":"3_Core/Principles_of_Programming_Languages/07_OOP/","title":"07 OOP","text":"<p>We did an entire course on this, so not much details have been written here. If any doubts, refer to <code>Sem 3 &gt; OOP</code> notes</p>"},{"location":"3_Core/Principles_of_Programming_Languages/07_OOP/#oop","title":"OOP","text":"<p>Object-Oriented Programming</p> <ul> <li>Classes are collection of properties/functions</li> <li>Objects are instances of that class</li> <li>Subclass, Superclass</li> <li>Method: Procedure body implementing operation</li> <li>Message: Procedure call; request to execute method</li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/07_OOP/#purpose-of-oop","title":"Purpose of OOP","text":"<ul> <li>Data abstraction</li> <li>Data encapsulation</li> <li>Data hiding</li> <li>Polymorphism</li> <li>Inheritance   <code>is-a</code> relationships</li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/07_OOP/#classes","title":"Classes","text":"<ul> <li>Class declaration and definition</li> <li>Access specifiers</li> <li>Constructors</li> <li>Destructors (<code>~Class()</code>)</li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/07_OOP/#binary-scope-resolution-operator","title":"Binary scope resolution operator","text":"<p>used for defining member function outside class</p> <pre><code>return_type ClassName::function_name()\n{\n\n}\n\n// constructor\nLine::Line() {\n   ;\n}\n\n// destructor\nLine::~Line() {\n   ;\n}\n\n// regular functions\nvoid Line::setLength() {\n   ;\n}\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/07_OOP/#inheritance","title":"Inheritance","text":"<ul> <li>Base/Parent/Super class</li> <li>Derived/Child/Sub class</li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/07_OOP/#purpose","title":"Purpose","text":"<p>It allows a class to __ properties/functions of another class</p> <ul> <li>Re-use</li> <li>Extend</li> <li>Modify</li> </ul> <pre><code>class derived_class:access_specifier base_class {\n\n};\n\nclass Rectangle: public Shape {\n   public:\n      int func() { \n        ;\n      }\n};\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/07_OOP/#access-specifiers","title":"Access Specifiers","text":"<ul> <li>Public</li> <li>Protected</li> <li>Private</li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/07_OOP/#types","title":"Types","text":"<ul> <li>Single</li> <li>Multiple</li> <li>Multi-Level</li> <li>Hierarchical</li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/07_OOP/#polymorphism","title":"Polymorphism","text":""},{"location":"3_Core/Principles_of_Programming_Languages/07_OOP/#function-overloading","title":"Function Overloading","text":"<p>based on different argument list</p>"},{"location":"3_Core/Principles_of_Programming_Languages/07_OOP/#function-overriding","title":"Function overriding","text":"<p>Derived class function definition overrides parent class definition</p>"},{"location":"3_Core/Principles_of_Programming_Languages/07_OOP/#operator-overloading","title":"Operator Overloading","text":"<p>Overloaded operators are functions with special names.</p> <pre><code>class className {\n  public:\n    returnType operator symbol (arguments) {\n        ;\n    } \n};\n</code></pre> <pre><code>class Person {\n  int age;\n  Person()\n  {\n    age = 0;\n  }\n  void operator ++ () {\n    ++age;\n  }\n};\n\nvoid main() {\n  Person p;\n\n  ++p;\n  // Calls ++ ()\" that I defined\n}\n</code></pre> <p>Following operators cannot be overloaded</p> <ul> <li>\\(::\\)</li> <li>\\(.*\\)</li> <li>\\(?:\\)</li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/07_OOP/#virtual-function","title":"Virtual Function","text":"<p>Declared using <code>virtual</code> in base class</p> <p>Member function of base class that is overriden by derived class</p> <p>When you refer to a derived class object using a pointer or a reference to the base class, you can call a virtual function for that object and execute the derived class\u2019s version of the function.</p> <p>Used to achieve runtime polymorphism</p> <p>Useful when you want a function to exist in the base class, but have no meaningful definition in the base class.</p>"},{"location":"3_Core/Principles_of_Programming_Languages/07_OOP/#mutable","title":"<code>mutable</code>","text":"<p>\u2026 is used for making data member of a object that is declared as a constant, to be changeable</p>"},{"location":"3_Core/Principles_of_Programming_Languages/07_OOP/#explanation","title":"Explanation","text":"<p>If we have an object that is declared as a constant, then by default, all the data members of this object now become constant. However, if you want one/more specific data members to be changeable, use <code>mutable</code> next to the data member in the class defintion.</p> <pre><code>class Student\n{\n  public:\n    x = 0;\n    mutable y = 0;\n};\n\nvoid main()\n{\n  const Student s;\n\n  s.x = 10; // not possible\n  s.y = 10; // possible due to mutable keyword\n}\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/07_OOP/#order-of-invokation-of-nested-objects","title":"Order of invokation of nested objects","text":"<ol> <li>Nested object constructor</li> <li>Outer object constructor</li> <li>Outer object destructor</li> <li>Nested object destructor</li> </ol>"},{"location":"3_Core/Principles_of_Programming_Languages/08_Python/","title":"08 Python","text":""},{"location":"3_Core/Principles_of_Programming_Languages/08_Python/#characteristics-of-scripting-languages","title":"Characteristics of Scripting Languages","text":"<ul> <li>Both Batch and Interactive use</li> <li>Economy of Expression</li> <li>Lack of declarations; simple scoping rules</li> <li>Flexible dynamic typing</li> <li>Easy access to other programs</li> <li>Sophisticated Pattern matching</li> <li>High-level data types</li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/08_Python/#python","title":"Python","text":"<p>is a general-purpose interpreted, interactive, object-oriented, and high-level programming language.</p> <p>It was created by Guido van Rossum during 1985-1990.</p> <p>Philosophy: Easy-to-read, easy-to-learn</p>"},{"location":"3_Core/Principles_of_Programming_Languages/08_Python/#variables","title":"Variables","text":"<p>No explicit declaration; declaration happens the first time you initialize variable</p>"},{"location":"3_Core/Principles_of_Programming_Languages/08_Python/#data-types","title":"Data Types","text":"<pre><code>x = True\nx = 5 ## int\nx = 5.4 ## float\nx = 'a' #string\nx = 1+10j ## complex number\nx = \"Ahmed Thahir\" ## string\nx = (\"a\", \"b\", \"c\") ## tuple \nx = [\"a\", \"b\", \"c\"] ## list\nrange(5) ## range from 0-4\nx = { ## dict\n  \"a\": \"x\",\n  \"b\": \"y\"\n}\nx = {\"a\", \"b\", \"c\"}\nx = frozenset({\"a\", \"b\", \"c\"}) ## frozenset\nx = b\"Hello\" ## bytes\nx = bytearray(5) ## bytearray\nx = memoryview(bytes(5)) ## memoryview\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/08_Python/#idk","title":"Idk","text":"<pre><code>x = str(3)\nx = int(3)\nx = float(3)\n\nprint(x)\nprint(type(x))\n\nfruits = [\"apple\", \"banana\", \"cherry\"]\nx, y, z = fruits\nprint(x, y, z)\n## apple banana cherry\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/08_Python/#conditional-statements","title":"Conditional Statements","text":"<pre><code>if x &lt; 10 or y &lt; 20:\n  print(1)\nelif x &gt; 20 and y &lt; 20:\n  print(2)\nelse\n    print(3)\n\nprint(1) if x &gt; 10 else print(2) if x&gt;20 else print(3)\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/08_Python/#loops","title":"Loops","text":"<pre><code>i = 0\nwhile i &lt; 10:\n  if(i==3):\n    continue\n\n  if(i==5):\n    break\n\n  print(i)\n  i += 1\n</code></pre> <pre><code>for i in range(5):\n  print(i)\n\nfor i in range(1, 5, 3): ## starting, ending, updation\n  print(i)\n\nfor i in [3, 5, 7]:\n  print(i)\n\nfor ch in \"Hello\":\n  print(ch)\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/08_Python/#pass-vs-comments","title":"Pass vs Comments","text":"<code>pass</code> <code>#comment</code> No operation CPU instruction(NOP) Skipped over <pre><code>def func(test):\n    ## comment\n  pass\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/08_Python/#list","title":"List","text":"<pre><code>a = [10, 20, 30, 40, 50]\n\nprint(a)\nprint(a[0])\nprint(a[-1])\n\nprint(a[3:5])\n## prints list with elements of index 3 and 4\n## tip to remember: no of elements = 5-3 = 2\n\nif 10 in a:\n  print(\"Yes\")\nif \"hi\" in a:\n  print(\"Yes\")\n</code></pre> <pre><code>a.append(\"hi\")\na.clear()\nb = a.copy()\na.count(\"hi\")\na.extend(another_list)\na.index(\"hi\")\n\na.insert(\"hi\", 3)\na.pop(3) ## index\na.remove(\"hi\") ## element\na.reverse()\na.sort()\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/08_Python/#list-comprehension","title":"List Comprehension","text":"<pre><code>new_list = [x for x in fruits if x &gt; 10]\nnew_list = [x for x in fruits if \"h\" in x]\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/08_Python/#tuple","title":"Tuple","text":"<p>is non-mutable</p> <pre><code>a = [10, 20, 30, 40, 50]\n\nprint(a)\nprint(a[0])\nprint(a[-1])\n\nprint(a[3:5])\n## prints tuple with elements of index 3 and 4\n## tip to remember: no of elements = 5-3 = 2\n\nif 10 in a:\n  print(\"Yes\")\nif \"hi\" in a:\n  print(\"Yes\")\n</code></pre> <pre><code>a.count(\"hi\")\na.index(\"hi\")\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/08_Python/#modifying-a-tuple","title":"Modifying a tuple","text":""},{"location":"3_Core/Principles_of_Programming_Languages/08_Python/#add-a-tuple","title":"Add a tuple","text":"<pre><code>x = (10, 20, 30, 40, 50)\nx += (60)\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/08_Python/#you-have-to-convert-it-to-a-list-and-then-revert-it-to-a-tuple","title":"You have to convert it to a list and then revert it to a tuple","text":"<pre><code>x = (10, 20, 30, 40, 50)\nx = list(x)\n\nx.append(40)\nx[1] = 100\nx.remove(10)\nx += 50\n\nx = tuple(x)\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/08_Python/#advantages","title":"Advantages","text":"<ul> <li>Better performance than list, as they are immutable</li> <li>Tuple ensures write-protection of elements</li> <li>General convention<ul> <li>Tuples for different data types</li> <li>Lists for similar data types</li> </ul> </li> <li>Tuples that contain immutable elements can be used as key for dictionary</li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/08_Python/#dictionaries","title":"Dictionaries","text":"<p>key-value pairs</p> <p>keys have to be unique</p> <pre><code>student_marks = {\n  \"thahir\": 10,\n  \"ahmed\": 20\n}\n\nstudent_marks = dict(\n  thahir = 10,\n  ahmed = 20\n)\n\nprint(student_marks[\"thahir\"])\nprint(student_marks[\"ahmed\"])\n\nprint(students_marks.get(\"thahir\"))\n\nfor key in students_marks.keys():\n  print(students_marks[key])\n\nfor key, value in student_marks:\n  print(key, value)\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/08_Python/#functions","title":"Functions","text":"<pre><code>student_marks.clear()\ns = student_marks.copy()\n\nnew_dict = dict.fromkeys((\"key1\", \"key2\"), 0)\n## gives a dictionary with the specified keys and specified value\n## {'key1': 0, 'key2': 0, 'key3': 0}\nnew_dict = dict.fromkeys((\"key1\", \"key2\"))\n## {'key1': None, 'key2': None, 'key3': None}\n\nstudent_marks.get(\"thahir\")\nstudent_marks.items() ## list of tuples\nstudent_marks.keys() ## list\nstudent_marks.values() ## list\n\nstudent_marks.pop(\"thahir\")\nstudent_marks.popitem() #remove last inserted key-value pair\n\nx = thisdict.setdefault(\"azhar\", 10)\n## Returns the value of the specified key. If the key does not exist: insert the key, with the specified value\n\nstudent_marks.update({\n  \"thahir\": 100\n})\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/08_Python/#nested","title":"Nested","text":"<pre><code>student_marks = {\n  \"thahir\": {\n    \"math\": 10,\n    \"science\": 20\n  },\n  \"ahmed\": {\n    \"math\": 10,\n    \"science\": 20\n  }\n}\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/08_Python/#set","title":"Set","text":"<p>unordered and mutable collection of elements, which are unique and immutable</p> <pre><code>my_set = {\"thahir\", \"ahmed\", 10}\n\nfor x in my_set:\n  print(x)\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/08_Python/#strings","title":"Strings","text":"<pre><code>a = 'hello world'\na = \"hello world\"\na = \"\"\"\nhello world\n\"\"\"\n\nprint(a)\nprint(a[0])\nprint(a[1:3])\n\nprint(a[-3:-1])\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/08_Python/#functions_1","title":"Functions","text":"<p>Values are passed by value, just like C++. However, pandas dataframes behave like pointers, and hence are passed by reference.</p> <pre><code>def func(name, age):\n  print(name, age)\n\n  return name\n\nname = func(\"ahmed\", 20)\nname = func(age=20, name=\"ahmed\")\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/08_Python/#arbitrary-arguments","title":"Arbitrary Arguments","text":"<p>When you are unsure how many parameters will be passed</p> <pre><code>def func(*args):\n  print(args) ## tuple\n  print(args[0], args[1])\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/08_Python/#default-argument","title":"Default Argument","text":"<pre><code>def func(name, age=20):\n  print(name, age)\n\nfunc(\"ahmed\")\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/08_Python/#keyword-arguments","title":"Keyword Arguments","text":"<pre><code>def func(**kwargs):\n  print(kwargs) ## dictionary\n  print(args[\"name\"], args[\"age\"])\n\nfunc(name = \"thahir\", age = 20)\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/08_Python/#recursion","title":"Recursion","text":"<pre><code>def fact(n):\n  if(n==0):\n    return 1\n  else:\n    return n * fact(n-1)\n\nf = fact(10)\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/08_Python/#classes","title":"Classes","text":"<p>The first argument to any function/constructor in a class is the class object itself</p> <pre><code>class Student:\n  def __init__(self): ## self can be anything (blah, bruh)\n    self.age = 20\n\nclass Student:\n  def __init__(bruh, name, age):\n    bruh.name = name\n    bruh.age = age\n  def print_name(bruh):\n    print(bruh.name)\n\n## object creation\ns = Student(\"thahir\", 20)\n\n## printing\nprint(s.age)\nprint(s.print_name())\n\n## modification\ns.age += 10\n\n## deletion\ndel s.age\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/08_Python/#inheritance","title":"Inheritance","text":"<pre><code>class Student:\n  def __init__(self, name): ## self can be anything (blah, bruh)\n    self.name = name\n    self.age = 20\n\nclass Math_Student(Student):\n  pass\n\nclass Sci_Student(Student):\n  def __init__(self, name):\n    Student.__init__(self, name)\n\nclass Eng_Student(Student):\n  def __init__(self, name):\n    super().__init__(name) ## no need of self\n\n## Multiple inheritance\nclass CS_Student(Sci_Student, Math_Student):\n  def __init__(self, name):\n    Sci_Student.__init__(self, name)\n    Math_Student.__init__(self, name)\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/09_LISP/","title":"09 LISP","text":""},{"location":"3_Core/Principles_of_Programming_Languages/09_LISP/#lisp","title":"LISP","text":"<p>LISt Processing</p> <p>In this course, we will use PICO LISP</p>"},{"location":"3_Core/Principles_of_Programming_Languages/09_LISP/#functional-programming","title":"Functional Programming","text":"<p>is a subset of declarative programming</p> <p>is not tied to von Neumman machine</p> <p>Functional programs do not concern themselves with state and memory locations. They work exclusively with values, expressions and functions that compute values.</p>"},{"location":"3_Core/Principles_of_Programming_Languages/09_LISP/#characteristics","title":"Characteristics","text":"<ul> <li>Simple and concise syntax and semantics</li> <li>Repetition is done through recursion instead of iteration</li> <li>Functions are manipulated easily like any data type</li> <li>Data as functions   We can build a function on-the-fly and execute it</li> <li>Higher order functions   Arguments and results of a function can be functions</li> <li>Lazy evaluation   Expressions are evaluated only when necessary</li> <li>Garbage collection   Dynamic memory that is no longer required is automatically reclaimed by the system</li> <li>Polymorphic types   Functions can work on data of different types</li> <li>Easier mathematical manipulation compared to procedural programming</li> <li>Global assignments are not permitted (side effects are avoided)</li> <li>Easier parallelization   Possibility of performing function evaluation in parallel is inherent in the function definition. Hence, no new language construct is required to express parallelism.</li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/09_LISP/#symbolic-expressions","title":"Symbolic Expressions","text":"Examples Atom happybirthdayyouare20 (numeric atom) Lists Group of atoms(not comma-separated) (happy birthday)(you are 20)"},{"location":"3_Core/Principles_of_Programming_Languages/09_LISP/#lisp-procedures","title":"LISP Procedures","text":"<p>Defined in pre-fix format</p> <p>Invokation consists of </p> <ul> <li>pair of enclosing parentheses <code>(...)</code></li> <li>procedure</li> <li>arguments</li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/09_LISP/#primitives","title":"Primitives","text":"<p>A primitive is an inbuilt procedure such as <code>+, -, *, /</code></p> <pre><code>(+ 3.14 3) ; 6 (rounded-off)\n(- 3.14 3) ; 0 (rounded-off)\n(* 3.14 3) ; 9 (rounded-off)\n(/ 3.14 3) ; 1 (rounded-off)\n(% 3.14 3) ; 0 (rounded-off)\n\n(* (+ 3.14 3) 5) ; 30.0 (inner and outer calculations are rounded-off)\n\n(- -8) ; 8\n\n(max 3 5)   ; 5\n(min 3 5)   ; 3\n(sqrt 4)    ; 2\n(abs -5)    ; 5\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/09_LISP/#procedure-abstraction","title":"Procedure Abstraction","text":"<p>constructing new user-defined procedures by combining existing ones</p> <p>A program is a collection of procedures</p> <p>Defined using <code>de</code> keyword</p> <pre><code>(\n de simple(x)\n x\n)\n\n(simple 2) ; 2\n\n(\n de pow(x n)\n (\n  if(= n 0)\n  1\n  ( ; else\n   *\n   x\n   (pow x (- n 1))\n  )\n )\n)\n\n(pow 2 3) ; 8\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/09_LISP/#side-effect","title":"Side Effect","text":"<p>Anything done by a procedure that persists after it returns its value</p> <p><code>setq</code> function has a side effect that value of a variable changes</p>"},{"location":"3_Core/Principles_of_Programming_Languages/09_LISP/#advanced-primitives","title":"Advanced Primitives","text":""},{"location":"3_Core/Principles_of_Programming_Languages/09_LISP/#quote","title":"<code>quote</code>","text":"<p>Takes 1 argument</p> <p>returns the argument</p> <p>Used for characters; basically for the programming language to know if we are passing a string or a character</p> <p><code>\u201c...\u201d</code> is for strings</p> <pre><code>(quote (A))\n(quote (A B C))\n\n'A\n'(A B C)\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/09_LISP/#setq","title":"<code>setq</code>","text":"<p>Sets a value to a variable</p> <p>2 arguments</p> <pre><code>(setq &lt;variable&gt; &lt;value&gt;)\n\n(setq my_list (1 2 3))\n(setq my_list '(A B C))\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/09_LISP/#car","title":"<code>car</code>","text":"<p>First element of a non-empyt list</p> <p>1 argument</p> <pre><code>(car my_list) ; A\n(car '(A B C)) ; A\n\n(car 'L) ; error, as this is not a list\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/09_LISP/#cdr","title":"<code>cdr</code>","text":"<p>All elements except the first elemnt</p> <p>1 argument</p> <pre><code>(car my_list) ; (B C)\n(car '(A B C)) ; (B C)\n\n(car 'L) ; error, as this is not a list\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/09_LISP/#cons","title":"<code>cons</code>","text":"<p>Create record structure</p> <p>2 arguments</p> <p>Returns a list such that</p> <ul> <li>arg1 is car</li> <li>arg2 is cdr</li> </ul> <pre><code>; dotted pair\n(cons 1 2) ; (1.2)\n(cons 'a 'b); (a.b)\n(cons 1 'b); (1.b)\n\n; regular list\n(cons 'a '(b c d)) ; (a b c d)\n(cons '(a b) '(c d)) ; ((a b) c d)\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/09_LISP/#append","title":"<code>append</code>","text":"<p>2 Arguments</p> <pre><code>(append (1 2) (3 4)) ; (1 2 3 4)\n\n(append '(A B) (3 4)) ; (A B 3 4)\n\n(append (1) (2) (3) 4) ; (1 2 3.4)\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/09_LISP/#list","title":"<code>list</code>","text":"<p>infinite arguments</p> <p>converts a list of arguments to a list</p> <pre><code>(list 1 2 3 4) ; (1 2 3 4)\n(list 'a 'b 'c 'd) ; (a b c d)\n\n(list 'a \"hello world\" (2 3) 'd) ; (a \"hello world\" (2 3) d)\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/09_LISP/#cond","title":"<code>cond</code>","text":"<p>conditional branching</p> <pre><code>(\n cond\n (test_1 action_1)\n (test_2 action_2)\n ...\n (test_n action_n)\n)\n\n; with t\n; t is like else\n; t\u00a0in a clause ensures that the last action is performed if none other would.\n(t(\n cond\n (test_1 action_1)\n (test_2 action_2)\n ...\n (testn action_n)\n (action_default)\n))\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/09_LISP/#dotted-pair","title":"Dotted Pair","text":"<p>created using <code>cons</code></p> <p>neither atoms nor lists</p>"},{"location":"3_Core/Principles_of_Programming_Languages/10_Prolog/","title":"10 Prolog","text":""},{"location":"3_Core/Principles_of_Programming_Languages/10_Prolog/#logic-programming","title":"Logic Programming","text":"<p>Logic program is expressed as a set of atomic sentences(facts) and Horn clauses (rules)</p> <p>We can ask questions in the form of conjectures (like facts, but with variables) to find out what values will make the conjecture true</p> Statement Type of Statement Ramu is a boy Fact Ramu\u2019s father is Kicha Relationship Ramu eats chocolate if chocolates are available and chocolates have nuts Horn Clause(Rule) <p>LIPS: Logical Inferences per second</p>"},{"location":"3_Core/Principles_of_Programming_Languages/10_Prolog/#prolog","title":"Prolog","text":"<p>Consists of a database of predicates composed of facts and rules, involving constants and variables</p>"},{"location":"3_Core/Principles_of_Programming_Languages/10_Prolog/#properties","title":"Properties","text":"<ul> <li>There is no structure imposed on a Prolog program, there is no main procedure, and there is no nesting of definitions. </li> <li>Terminator is <code>.</code></li> <li>Assignment is performed using <code>is</code></li> <li>Program is executed by asking a question using <code>?-</code>, called a query</li> <li>All facts and rules are global in scope and the scope of a variable is the fact or rule in which it appears</li> <li>Facts, rules, and queries are called clauses. </li> </ul> Naming Rule Example Constant(Atoms) NumberString starting with lowercase letter tombilla1x217-33.74 Variable String starting with uppercase letterString starting with <code>_</code> Thahir_thahirX Fact True about some constant Predicate Function result, which can be true/false"},{"location":"3_Core/Principles_of_Programming_Languages/10_Prolog/#relationship","title":"Relationship","text":"<pre><code>father(bukhari, thahir)\nmother(habeeb_fathima, bukhari)\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/10_Prolog/#rules","title":"Rules","text":"Symbol Meaning <code>:-</code> <code>if</code> <code>,</code> <code>and</code> <code>;</code> <code>or</code> <pre><code>grandmother(x, z) :- mother(x, y), parent(y, z)\n\ngrandmother(x, z) :- mother(x, y), ( father(y, z); mother(y, z) )\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/10_Prolog/#query","title":"Query","text":"<p>Queries in Prolog are entered by the user following the  ?- prompt</p> <pre><code>?- grandmother(habeeb_fathima, thahir) % will give Yes\n?- grandmother(What, thahir) % will give habeeb_fathima\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/10_Prolog/#datatypes","title":"Datatypes","text":""},{"location":"3_Core/Principles_of_Programming_Languages/10_Prolog/#complex-term","title":"Complex term","text":"<p>No of arguments in a complex term is called arity</p> <pre><code>position(20, 10)\nemployee(1234, 'Jones', 'James', 1000)\nhide(X, father(father(father(butch))))\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/10_Prolog/#other","title":"Other","text":"<pre><code>% list\n[dog, cat, mouse]\n\n% records/tuples are represented as patterns; elements are accessed by pattern matching\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/10_Prolog/#example-of-a-knowledge-base","title":"Example of a Knowledge Base","text":"<pre><code>loves(vincent, mia).\nloves(marcellus, mia).\nloves(pumpkin, honey_bunny).\nloves(honey_bunny, pumpkin).\n\njealous(X, Y) :- loves(X, Z), loves(Y, Z).\n\n?- loves(X, mia) % gives us who loves mia\n?- jealous(X, Y) % gives us which pairs are jealous\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/10_Prolog/#database","title":"Database","text":"<pre><code>part(p001,nut).\npart(p002,bolt).\n\nsupplier(s001,abc).\nsupplier(s002,def).\n\npart_supplier_cost(p001,s001,0.10).\npart_supplier_cost(p002,s001,0.15).\n\nlist(Pkey):-\n  part(Pkey,Pname),\n  part_supplier_cost(Pkey,Skey,Cost),\n  supplier(Skey,Sname),\n\n  write(Pkey),write(' '),\n  write(Pname),write(' '),\n  write(Skey),write(' '),\n  write(Sname),write(' '),\n  write(Cost),nl.\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/10_Prolog/#type-predicates","title":"Type Predicates","text":"<p><code>=..</code> is used to compose and decompose terms</p>"},{"location":"3_Core/Principles_of_Programming_Languages/10_Prolog/#arithmetic-expression","title":"Arithmetic Expression","text":"<pre><code>?- X is 3*4.\n    X = 12\nyes\n</code></pre> <pre><code>plus(X, Y, Z) :- Z is X + Y.\n\n?- plus(2,3,Z)\nZ=5\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/10_Prolog/#unification","title":"Unification","text":"<p>Two terms unify,</p> <ul> <li>if they are the same term   or</li> <li>if they contain variables that can be uniformly instantiated with terms in such a way that the resulting terms are equal.</li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/10_Prolog/#example","title":"Example","text":"Unify? mia and mia \u2705 42 and 42 \u2705 woman(mia) and woman(mia) \u2705 vincent and mia \u274c woman(mia) and woman(jody) \u274c <pre><code>?- mia = mia.\nyes\n\n?- mia = vincent.\nno \n\n?- mia = X.\nX=mia\nno\n\n?- X=mia, X=vincent.\nno\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/10_Prolog/#comparison-operators","title":"Comparison Operators","text":"Symbol Operation <code>A =:= B</code> A==B(value) <code>A =\\+= B</code> A!=B(value) <code>A &lt; B</code> numeric <code>A @&lt; B</code> String comparison <code>A =&lt; B</code> numeric <code>A @=&lt; B</code> String comparison <code>A &gt; B</code> numeric <code>A @&gt; B</code> String comparison <code>A &gt;= B</code> numeric <code>A @&gt;= B</code> String comparison <pre><code>3 @&lt; 4 % yes\n3 @&lt; a % yes\na @&lt; abc6 % yes\nab\u00a0@&lt; abc % yes\nabcd\u00a0@&lt; ab % no\n\n?- 3&gt;0 % yes\n</code></pre> <p>Value of functor with argument (term) is always more than any numeric and character (or string)</p>"},{"location":"3_Core/Principles_of_Programming_Languages/10_Prolog/#logical-operators","title":"Logical Operators","text":"<pre><code>a :- b.                 % a if b\na :- b,c.               % a if b and c.\na :- b;c.               % a if b or c.\na :- not b.         % a if b fails\na :- b -&gt; c;d.  % a if (if b then c else d)\n</code></pre> Meaning <code>a :- b.</code> a if b <code>a :- b,c.</code> a if b and c <code>a :- b;c.</code> a if b or c <code>a :- not b.</code> a if b fails <code>a :- b -&gt; c;d.</code> a if (if b then c else d)"},{"location":"3_Core/Principles_of_Programming_Languages/10_Prolog/#functions","title":"Functions","text":"<p>Functions are implemented using relations</p> <pre><code>fac(0,1).\nfac(N,F) :- N &gt; 0, M is N - 1, fac(M,Fm), F is N * Fm.\n\n?- fac(5,F).\n120\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/11_Concurrent_Programming/","title":"11 Concurrent Programming","text":""},{"location":"3_Core/Principles_of_Programming_Languages/11_Concurrent_Programming/#concurrent-programming","title":"Concurrent Programming","text":"<p>Making programs having cooperating processes that are set to achieve a common goal</p>"},{"location":"3_Core/Principles_of_Programming_Languages/11_Concurrent_Programming/#ipc","title":"IPC","text":"<p>Inter-Process Communication</p> <p>Transfer data/info between address space, while ensuring protection and isolation</p> IPC Meaning Diagram \u2705Advantage \u274cDisadvantage Pipes(Shared buffer) - Exchange of messages- Named pipes allow processes on different computers to communicate over the network OS in control Overhead Shared Memory One process creates a portion of memory that another process accesses No overhead May require re-writing code Message Queue Ordered list of memory segments where processes store and retrieve data Process Synchronization mutex locksemaphoremonitors Files - Parent process creates 2 files before forking child process- Child inherits file descriptors from parent, and they share the file pointers- Can use one for parent to write and child to read, other for child to write and parent to read <pre><code>ipcs -a ## (linux only)\n</code></pre> <p>Lists all IPC facilities which has read access for the current process. It provides details abou message queue, semaphore, and shared memory.</p> <p>Every IPC facility has unique key and identifier, which is used to identify an IPC facility.</p> <p></p>"},{"location":"3_Core/Principles_of_Programming_Languages/11_Concurrent_Programming/#thread","title":"Thread","text":"<p>Basic unit of CPU utilization</p>"},{"location":"3_Core/Principles_of_Programming_Languages/11_Concurrent_Programming/#components","title":"Components","text":"<ul> <li>thread ID</li> <li>program counter</li> <li>stack</li> <li>set of registers</li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/11_Concurrent_Programming/#advantages-of-multi-threading","title":"Advantages of Multi-Threading","text":"Advantage Description Responsiveness One thread can provide response, while other threads are blocked/slowed down due to intensive operations Resource-sharing Multiple tasks can be performed in a single address space Economy Creating, managing, context-switching between multiple threads is faster than that of multiple processes Scalability Utilization of multiprocessor architecture"},{"location":"3_Core/Principles_of_Programming_Languages/11_Concurrent_Programming/#types-of-threads","title":"Types of Threads","text":"Thread Type Description User Thread Threads that application programmers put in programsWithout kernel support Kernel Thread Supported within kernel of OS <p>In a specific implementation, user threads must be mapped to kernel threads</p> User Thread \\(\\to\\) Kernel Thread Mapping Example One \\(\\to\\) One Windows 95 \\(\\leftrightarrow\\) Windows XPDOS Many \\(\\to\\) One Word Processor Application Many \\(\\to\\) Many LinuxSolaris"},{"location":"3_Core/Principles_of_Programming_Languages/11_Concurrent_Programming/#thread-libraries","title":"Thread Libraries","text":"<p>Provide API for programmers to create and manage threads</p> <p>May be implemented in</p> <ul> <li>user space: no kernel support</li> <li>kernel space: involves system calls, requires kernel with thread library support</li> </ul> <p>3 present-day thread libraries</p> <ul> <li>POSIX Pthreads</li> <li>Win32 Threads</li> <li>Java Threads   Implementation of threads depends on which OS and hardware the JVM (Java Virtual Machine) is running on, ie, Pthreads/Win32</li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/11_Concurrent_Programming/#critical-section-problem","title":"Critical Section Problem","text":"Mutual Exclusion Only one process can execute at a time in their critical section Progress If no process is currently executing in their critical section, and one or more processes want to execute their critical section, then only the processes not in their remainder sections can participate in the decision, and the decision cannot be postponed indefinitely.( i.e. processes cannot be blocked forever waiting to get into their critical sections. ) Bounded Waiting Process requesting entry into their critical section will get a turn eventually, and there is a limit on how many other processes go first"},{"location":"3_Core/Principles_of_Programming_Languages/11_Concurrent_Programming/#mutex-lock","title":"Mutex Lock","text":"<pre><code>do {\n    acquire_lock() // returns when it is safe for process to enter critical section\n\n    critical section\n\n    release_lock() // allow another process to acquire lock\n\n    remainder section\n} while(True);\n</code></pre> <pre><code>acquire_lock(){\n    while(! available){\n        // if not available, then keep waiting until available\n        ; // causes busy waiting to keep checking if available\n    }\n\n    // if available\n    available = False;\n}\n\nrelease_lock(){\n    available = True;\n}\n</code></pre> <p>\u274c Busy loop is used to block processes in the acquire phase. These types of locks are referred to as spinlocks, because the CPU just spins while blocking the process, thus wasting CPU cycles. This is especially bad on single-CPU single-threaded machines, as this blocks the entire computer, and does not allow any process to release the lock.</p> <p>\u2705 Spinlocks do not have overhead of a context switch, so they are effectively used on multi-threaded machines, when it is expected that the lock will be released after a short time.</p>"},{"location":"3_Core/Principles_of_Programming_Languages/11_Concurrent_Programming/#semaphores","title":"Semaphores","text":"<p>Counters to control access to shared resources, used as a locking mechanism to prevent processes from accessing a particular resource while another process is performing operations on it.</p> <p>Actually implemented as sets.</p> <p>Semaphore is actually an old railroad term, referring to the crossroad \u2018arms\u2019 that prevent cars from crossing the tracks at intersections.</p> <ul> <li>If the semaphore is on (the arms are up), then a resource is available (cars may cross the tracks)</li> <li>else (the arms are down), then resources are not available (the cars must wait) </li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/11_Concurrent_Programming/#dining-philosophers-problem","title":"Dining-Philosophers Problem","text":"<p>Consider five philosophers sitting around a table, in which there are five chopsticks evenly distributed and an endless bowl of rice in the center, as shown in the diagram below. ( There is exactly one chopstick between each pair of dining philosophers. )These philosophers spend their lives alternating between two activities: eating and thinking.</p> <ul> <li>When it is time for a philosopher to eat, it must first acquire two chopsticks - one from their left and one from their right.</li> <li>When a philosopher thinks, it puts down both chopsticks in their original locations.</li> </ul> <p></p>"},{"location":"3_Core/Principles_of_Programming_Languages/11_Concurrent_Programming/#solution","title":"Solution","text":"<p>Array of semaphores <code>chopsticks[0 ... (n-1)]</code></p> <p>Here, \\(n=5\\)</p> <pre><code>do {\n    wait(chopstick[i]);\n    wait(chopstick[(i+1)%n]);\n\n    // eat\n\n    signal(chopstick[i]);\n    signal(chopstick[(i+1)%n]);\n\n    // think\n\n} while(true);\n\nvoid wait(s)\n{\n  s--;\n}\n\nvoid signal(s)\n{\n  s++;\n}\n</code></pre> <p>\u274c But suppose that all five philosophers get hungry at the same time, and each starts by picking up their left chopstick. They then look for their right chopstick, but because it is unavailable, they wait for it, forever, and eventually all the philosophers starve due to the resulting deadlock.</p>"},{"location":"3_Core/Principles_of_Programming_Languages/11_Concurrent_Programming/#monitor","title":"Monitor","text":"<p>Monitor is a class in which</p> <ul> <li>all data is private</li> <li>only one method within any given monitor object may be active at a time.</li> <li>methods may only access the shared data within the monitor and any data passed to them as parameters, ie, they cannot access any data external to the monitor</li> </ul> <pre><code>monitor Monitor_Name\n{\n  // shared variable declarations\n\n  procedure P1(){;}\n  procedure P2(){;}\n  ...\n  procedure Pn(){;}\n\n  initialization_code(){;}\n}\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/11_Concurrent_Programming/#data-type-condition","title":"Data Type <code>condition</code>","text":"<p>Variable <code>X</code> of type <code>condition</code> has only two legal operations</p> <ul> <li><code>X.wait()</code><ul> <li>Blocks a process until some other process calls <code>signal()</code></li> <li>adds the blocked process onto a list associated with that condition</li> </ul> </li> <li><code>X.signal()</code><ul> <li>does nothing if there are no processes waiting on that condition</li> <li>Otherwise, it wakes up exactly one process from the condition's list of waiting processes</li> </ul> </li> </ul> <p></p>"},{"location":"3_Core/Principles_of_Programming_Languages/11_Concurrent_Programming/#disadvantage","title":"Disadvantage","text":"<p>If a process P within the monitor issues a <code>signal()</code> that would wake up process Q also within the monitor, then there would be two processes running simultaneously within the monitor, violating the exclusion requirement.</p> <p>Solutions for this</p> <ul> <li>Signal and wait   When process P issues the <code>signal()</code> to wake up process Q, P then <code>waits()</code>, either for Q to leave the monitor or on some other condition</li> <li>Signal and continue   When P issues the <code>signal()</code>, Q <code>waits()</code>, either for P to exit the monitor or for some other condition.</li> </ul>"},{"location":"3_Core/Principles_of_Programming_Languages/11_Concurrent_Programming/#dining-philosophers-solution-using-monitors","title":"Dining Philosophers Solution using Monitors","text":"<p>Philosopher \\(i\\) can set variable <code>state[i] = EATING</code> only if two neighbours are not eating</p> <pre><code>cond = (\n  (state[(i+4) % 5] != EATING) and\n  (state[(i+1) % 5] != EATING)\n)\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/11_Concurrent_Programming/#full-implementation","title":"Full Implementation","text":"<pre><code>monitor DP\n{ \n\u00a0\u00a0\u00a0\u00a0status state[5]; \n\u00a0\u00a0\u00a0\u00a0condition self[5]; \n\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0// Pickup chopsticks \n\u00a0\u00a0\u00a0\u00a0Pickup(int i) \n\u00a0\u00a0\u00a0\u00a0{ \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0// indicate that I\u2019m hungry \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0state[i] = hungry; \n\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0// set state to eating in test() \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0// only if my left and right neighbors\u00a0 \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0// are not eating \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0test(i); \n\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0// if unable to eat, wait to be signaled \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0if (state[i] != eating) \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self[i].wait; \n\u00a0\u00a0\u00a0\u00a0} \n\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0// Put down chopsticks \n\u00a0\u00a0\u00a0\u00a0Putdown(int i) \n\u00a0\u00a0\u00a0\u00a0{ \n\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0// indicate that I\u2019m thinking \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0state[i] = thinking; \n\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0// if right neighbor R=(i+1)%5 is hungry and \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0// both of R\u2019s neighbors are not eating, \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0// set R\u2019s state to eating and wake it up by\u00a0 \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0// signaling R\u2019s CV \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0test((i + 1) % 5); \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0test((i + 4) % 5); \n\u00a0\u00a0\u00a0\u00a0} \n\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0test(int i) \n\u00a0\u00a0\u00a0\u00a0{ \n\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0if (state[(i + 1) % 5] != eating \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0&amp;&amp; state[(i + 4) % 5] != eating \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0&amp;&amp; state[i] == hungry) { \n\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0// indicate that I\u2019m eating \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0state[i] = eating; \n\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0// signal() has no effect during Pickup(), \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0// but is important to wake up waiting \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0// hungry philosophers during Putdown() \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0self[i].signal(); \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0} \n\u00a0\u00a0\u00a0\u00a0} \n\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0init() \n\u00a0\u00a0\u00a0\u00a0{ \n      // Execution of Pickup(), Putdown() and test() \n      // are all mutually exclusive, \n      // i.e. only one at a time can be executing \n      for i = 0 to 4 \n      {\u00a0\n        // Verify that this monitor-based solution is \n        // deadlock free and mutually exclusive in that \n        // no 2 neighbors can eat simultaneously \n        state[i] = thinking; \n      }\n\u00a0\u00a0\u00a0\u00a0} \n} // end of monitor \n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/12_Non_Local_Jump/","title":"12 Non Local Jump","text":"<p>C mechanism to transfer control to any program point higher in the current stack, ie, to go to a function that called the current function.</p> <p>It is a form of \u2018non-local goto\u2019</p> <p>For eg, if we have 3 functions <code>f1(), f2(), f3()</code>, such that <code>f1()</code> invokes <code>f2()</code> and <code>f3()</code>. The calling diagram will be represented as follows</p> <pre><code>flowchart TB\nf1 --&gt; f2 &amp; f3</code></pre> Direction of jump Long jump possible? Higher \\(\\to\\) lower \u274c Lower \\(\\to\\) Higher \u2705 Adjacent \u274c"},{"location":"3_Core/Principles_of_Programming_Languages/12_Non_Local_Jump/#requirements","title":"Requirements","text":"Description Returns <code>setjmp.h</code> Header file N/A <code>jmp_buf jb</code> Store \u2018environment\u2019 of return point.It is a pointer to a structure.Stores- current register content- stack pointer- Program counter N/A <code>int setjmp(jb)</code> Set return point, by saving current state of program execution in <code>jb</code> 0 <code>void longjmp(jb, val)</code> Restores register context from jump buffer envSets function\u2019s return value register to <code>val</code>Jumps to the old PC value stored in jump buffer <code>jb</code> It itself does not return anything, butcauses <code>setjmp()</code> to return \\[ \\text{longjmp}(jb, \\textcolor{hotpink}{k})  \\text{ causes setjmp to return} \\\\ \\text{return val} = \\begin{cases} 1, \\textcolor{hotpink}{k} = 0 \\\\ k, \\textcolor{hotpink}{k} \\ne 0 \\end{cases} \\]"},{"location":"3_Core/Principles_of_Programming_Languages/12_Non_Local_Jump/#example","title":"Example","text":"<pre><code>#include &lt;stdio.h&gt;\n#include &lt;longjmp.h&gt;\n\njmp_buf jb;\n\nvoid f1()\n{\n  printf(\"Entering f1()\");\n\n  f2();\n\n  printf(\"Exiting f1()\");\n}\n\nvoid f2()\n{\n  printf(\"Entering f2()\");\n\n  f3();\n\n  printf(\"Exiting f2()\");\n}\n\nvoid f3()\n{\n  printf(\"Entering f3()\");\n\n  longjmp(buf);\n\n  printf(\"Exiting f3()\");\n}\n\nvoid main()\n{\n  printf(\"Entering main() function\");\n\n  setjmp(jb);\n  f1();\n\n  printf(\"Exiting main() function\");\n}\n</code></pre>"},{"location":"3_Core/Principles_of_Programming_Languages/12_Non_Local_Jump/#output","title":"Output","text":"<pre><code>Entering main() Function\nEntering f1()\nEntering f2()\nEntering f3()\nExiting main() Function\n</code></pre> <p>Notice how the exiting and return statements of <code>f1(), f2(), f3()</code> are skipped.</p>"},{"location":"3_Core/Theory_of_Computation/","title":"Theory Of Computation","text":"<p>Taught by Dr. Santhosh Kumar</p> <p>This course introduces the foundational concepts of formal models of computation and computability. Students will explore a hierarchy of machines and languages, focusing on automata theory, formal languages, and complexity classes.</p> <p>Key learning objectives include:</p> <ul> <li>Understanding foundational concepts and notation (infinite sets, proofs, alphabets, languages).</li> <li>Exploring finite automata and regular expressions (deterministic and non-deterministic automata, closure properties).</li> <li>Investigating context-free grammars and push-down automata (parsing techniques and properties).</li> <li>Examining Turing machines as models of computation (recursive languages and Turing machine extensions).</li> <li>Understanding computability, decidability, and complexity classification (complexity classes P and NP, reductions).</li> </ul> <p>This course equips students with essential theoretical knowledge in computation and problem classification based on computational complexity.</p>"},{"location":"3_Core/Theory_of_Computation/01_Alpha%2C_Strings%2C_Lang/","title":"01 Alpha, Strings, Lang","text":""},{"location":"3_Core/Theory_of_Computation/01_Alpha%2C_Strings%2C_Lang/#model-of-computation","title":"Model of Computation","text":"<ol> <li>FSM(Finite State Machine)    Has Memory/State</li> <li>FSM with Stack</li> <li>Turing Machine    Read/Write capability</li> </ol> <p>A problem that cannot be solved by a Turing Machine is not computable.</p>"},{"location":"3_Core/Theory_of_Computation/01_Alpha%2C_Strings%2C_Lang/#symbols","title":"Symbols","text":"Symbol Meaning Description \\(\\Sigma\\) Set of Alphabet Non-empty finite set of symbolseg: \\(\\{0, 1 \\}\\) String Finite sequence of zero/symbols \\(\\vert  s  \\vert\\) Length of string s \\(\\Sigma^k\\) Set of strings of length \\(k\\) \\(\\Sigma^1\\) Set of strings of length 1 eg: \\(\\{0, 1 \\}\\) \\(\\epsilon\\) Empty String \u201c\u201d \\(\\phi, \\{ \\}\\) Null set(Empty) \\(\\phi \\ne \\epsilon \\ne \\{\\epsilon\\}\\) \\(L\\) Language Finite/countably-infinite set of strings over a finite alphabet"},{"location":"3_Core/Theory_of_Computation/01_Alpha%2C_Strings%2C_Lang/#operations","title":"Operations","text":""},{"location":"3_Core/Theory_of_Computation/01_Alpha%2C_Strings%2C_Lang/#on-strings","title":"On Strings","text":"Operation Representation Description Concatenation \\(S_1 \\cdot S_2\\) \\(S_1\\) followed by \\(S_2\\) Power \\(S^n\\) Concatenation with itself for \\(n\\) times Closure $\\begin{aligned} \\Sigma^+ &amp;= \\Sigma^1 \\cup \\Sigma^2 \\cup \\dots \\ \\Sigma^* &amp;= \\Sigma^0 \\cup \\Sigma^1 \\cup \\Sigma^2 \\cup \\dots \\ &amp;=  { \\epsilon } \\cup \\Sigma^+ \\end{aligned}$ Union of infinite concatenation with itselfAlso called as Kleene Closure/Star"},{"location":"3_Core/Theory_of_Computation/01_Alpha%2C_Strings%2C_Lang/#on-languages","title":"On Languages","text":"Operation Representation Description Union \\(L_1 \\cup L_2\\) Union of both sets Intersection \\(L_1 \\cap L_2\\) Intersection of both sets Concatenation \\(L_1 \\cdot L_2\\) Complement $\\begin{aligned} &amp;L'\\ = &amp;\\Sigma^* - L \\end{aligned}$ Opposite of defined languageSwap all accepting and rejecting states Closure of languages \\(L^*\\) Similar to that of string Power Set of \\(\\Sigma^*\\) $\\begin{aligned} P(S) &amp;= 2<sup>{\\Sigma</sup>*} \\ \\vert  P(S) \\vert  &amp;= 2^{\\vert S \\vert} \\end{aligned}$ Set of all subsets of \\(\\Sigma^*\\) \\[ \\begin{aligned} L = \\phi \\implies L^* &amp;= \\{ \\epsilon \\} \\\\ \\text{How?} \\implies L^* &amp;= L^0 \\cup L^1 \\cup \\dots \\\\ &amp;= \\{ \\epsilon \\} \\cup \\phi \\cup \\dots \\\\ &amp;= \\{ \\epsilon \\} \\end{aligned} \\]"},{"location":"3_Core/Theory_of_Computation/02_DFA/","title":"02 DFA","text":""},{"location":"3_Core/Theory_of_Computation/02_DFA/#automator","title":"Automator","text":"Symbol Meaning Description A Automator State Machine(same as DD) FA Finite Automator Automator with finite no of states DFA Deterministic Finite Automator FA where \\(\\exists\\) next state \\(\\forall\\) states <p>States are the only mechanism for a FA to \u201cremember\u201d what it has seen of input string so far</p>"},{"location":"3_Core/Theory_of_Computation/02_DFA/#dfa","title":"DFA","text":"<ul> <li>Can have more than one accept state</li> <li>Start state can also be an accept state</li> <li>DFA accepts \\(\\epsilon \\iff\\) start state is accepting state</li> </ul> Notation Meaning \\(\\enclose{circle}{q_0}\\) State \\(\\overset{1}{\\longrightarrow}\\) Transition \\(\\delta\\) \\(\\enclose{circle}{q_0} \\overset{1}{\\longrightarrow} \\enclose{circle}{q_1}\\) \\(\\delta(q_0, 1) = q_1\\) \\(\\enclose{circle}{\\enclose{circle}{\\ q_5 \\ }}\\) Accepting State Symbol Meaning \\(Q\\) Set of States \\(q_0\\) Starting State \\(F\\) Set of Final States \\(\\Sigma\\) Alphabet $\\begin{aligned} &amp;\\delta: Q \\times \\Sigma \\to Q \\ &amp; \\delta(\\text{Current State}, \\text{Input}) \\end{aligned}$ State Transition Function $\\begin{aligned} &amp;\\delta^: Q \\times \\Sigma^ \\to Q \\ &amp; \\delta(\\text{Current State}, \\text{Input}) \\end{aligned}$ Extended State Transition Function(Recursive traversal including \\(\\epsilon\\)) \\[ \\begin{aligned} \\delta (q, \\epsilon) &amp;= q \\\\ \\delta^* (q, \\epsilon) &amp;= \\{q, \\ \\dots\\} \\end{aligned} \\] <p>DFA seen as 5-Tuple</p> \\[ \\text{DFA} = D(Q, \\Sigma, \\delta, q_0, F) \\]"},{"location":"3_Core/Theory_of_Computation/02_DFA/#language-of-a-machine","title":"Language of a Machine","text":"<p>If \\(L\\) is the set of all strings that machine \\(M\\) accepts, then we say</p> <ul> <li>\\(M\\) recognizes \\(L\\)</li> <li>\\(L\\) is the language of machine \\(M\\)</li> </ul>"},{"location":"3_Core/Theory_of_Computation/02_DFA/#notes","title":"Notes","text":"<ul> <li>Machine may accept several strings, but recognizes only one language</li> <li>If a machine accepts no strings, it still recognizes one language: empty language \\(\\phi\\)</li> </ul>"},{"location":"3_Core/Theory_of_Computation/02_DFA/#regular-language","title":"Regular Language","text":"<p>A language \\(L\\) is regular if it is recognized by some DFA \\(M\\) \\(\\implies L(M)= L\\)</p> <p>Operations on a regular language gives another regular language(s)</p>"},{"location":"3_Core/Theory_of_Computation/02_DFA/#closure-properties","title":"Closure Properties","text":"<p>Regular languages are closed under these operations</p> <ul> <li>\\(L1 \\cup L2\\)</li> <li>\\(L1 \\cap L2\\)</li> <li>\\(L1 - L2\\)</li> </ul>"},{"location":"3_Core/Theory_of_Computation/02_DFA/#combination-of-machines","title":"Combination of Machines","text":"<p>Rather than deriving machines for everything, we can use the above operations to combine machines.</p> <p>If</p> <ul> <li>\\(Q_1, Q_2\\) are states of \\(M_1, M_2\\)</li> <li>\\(M_3\\) is combined machine and \\(Q_3\\) is the pair of states in \\(M_3\\)</li> </ul> <p>Then</p> <ul> <li>For \\(L1 \\cup L2 \\to F_3 = [F_1 \\times Q_2] \\cup [F_2 \\times Q_1]\\)</li> <li>For \\(L1 \\cap L2 \\to F_3 = F_1 \\times F_2\\)   Cross-Product of 2 DFA</li> <li>Max no of states in \\(Q_3\\) is given by \\(|Q_3|_\\max = |Q_1| \\times |Q_2|\\)</li> </ul>"},{"location":"3_Core/Theory_of_Computation/02_DFA/#dfa-decidable-decision-problem","title":"DFA-Decidable Decision Problem","text":"<p>A decision problem that is solveable by algorithm that</p> <ul> <li>takes input of string of length \\(n\\)</li> <li>uses constant amount of memory</li> <li>runs in exactly \\(n\\) steps</li> </ul>"},{"location":"3_Core/Theory_of_Computation/02_DFA/#regular-expression","title":"Regular Expression","text":"<p>Expression for which a FA exists, which can be used by a regular language.</p>"},{"location":"3_Core/Theory_of_Computation/02_DFA/#dead-state","title":"Dead State","text":"<p>Also called as Sink/Trap state</p> <ul> <li>Once a machine enters a dead state, there is no way out</li> <li>Remaining input are ignored</li> <li>Only has self loop</li> <li>The input string is rejected</li> </ul> <p>In the following diagram, <code>b</code> is the deadstate</p> <pre><code>flowchart LR\na --&gt;|0| b\na --&gt;|1| c\n\nb --&gt;|0/1| b\n\nc --&gt;|0| b\nc --&gt;|1| a\n\nclassDef dead fill:darkred, color: #EEE\nclass b dead</code></pre>"},{"location":"3_Core/Theory_of_Computation/02_DFA/#complement-of-dfa","title":"Complement of DFA","text":"<ul> <li>Accepting states \\(\\to\\) Non-Accepting states</li> <li>Non-Accepting states \\(\\to\\) Accepting states</li> </ul> \\[ \\begin{aligned} L&amp; \\implies M = ( Q, \u03a3, \u03b4, q_0, F ) \\\\ L'&amp; \\implies M' = ( Q, \u03a3, \u03b4, q_0, Q-F ) \\end{aligned} \\]"},{"location":"3_Core/Theory_of_Computation/02_DFA/#reversing-a-regular-expression","title":"Reversing a regular expression","text":"<p>Given a DFA</p> <ol> <li>Reverse all arrows of transitions of the DFA</li> <li>Swap the start state and accepting state    If there are 2 start states, use \\(\\epsilon\\) transition</li> <li>Convert NFA \\(\\to\\) DFA</li> </ol>"},{"location":"3_Core/Theory_of_Computation/02_DFA/#grep","title":"Grep","text":"<p>**G**et **re**gular ex**p**ression</p> <p>utility for identifying regular expressions</p>"},{"location":"3_Core/Theory_of_Computation/03_NFA/","title":"03 NFA","text":"<p>Non-deterministic Finite Automator</p> <p>Finite automator that only shows the transitions required for the pattern to be recognized.</p> <p>For a state \\(q\\) and symbol \\(a \\in \\Sigma\\), NFA can have</p> <ul> <li>no edge leaving \\(q\\) labelled with symbol \\(a\\)   or   multiple edges leaving \\(q\\) labelled with the same symbol \\(a\\)</li> <li>edges leaving \\(q\\) labelled with \\(\\epsilon\\)</li> <li>can take \\(\\epsilon\\)-edge without reading any symbol from input string.</li> </ul>"},{"location":"3_Core/Theory_of_Computation/03_NFA/#advantage-of-nfa","title":"Advantage of NFA \u2705","text":"<ul> <li>Easier to construct than DFAs</li> <li>Easier to combine than DFAs for operations like union, concatenation etc. on regular languages</li> </ul>"},{"location":"3_Core/Theory_of_Computation/03_NFA/#evaluation-of-nfa","title":"Evaluation of NFA","text":""},{"location":"3_Core/Theory_of_Computation/03_NFA/#tracing-steps","title":"Tracing Steps","text":"<ul> <li>NFA splits into multiple copies of itself (threads)<ul> <li>Each copy performs independent and parallel computation</li> </ul> </li> <li>At any instant, NFA will be in a set of states (one/more states)</li> <li>If a copy is in a state and there is no outgoing transition, then the copy dies/crashes<ul> <li>We discard this copy from our evaluation</li> </ul> </li> </ul>"},{"location":"3_Core/Theory_of_Computation/03_NFA/#conclusion","title":"Conclusion","text":"<ul> <li>Accept, if atleast one copy ends in an accept state after reading entire input string</li> <li>Reject, if no copy ends in an accept state after reading entire input string</li> </ul>"},{"location":"3_Core/Theory_of_Computation/03_NFA/#dfa-vs-nfa","title":"DFA vs NFA","text":"DFA NFA \\(\\Sigma\\) consist of only symbolsexcluding \\(\\epsilon\\) \\(\\Sigma_\\epsilon = \\Sigma \\cup \\epsilon\\) \\(\\delta\\) \\(Q \\times \\Sigma \\to Q\\) \\(Q \\times \\Sigma^* \\to P(Q)\\)(Power set of \\(Q\\)) No of states \\(\\uparrow\\) \\(\\downarrow\\) Easy to construct \u274c \u2705 Efficient Execution \u2705 \u274c(due to backtracking) Time Complexity \\(T\\) \\(O(k)\\) \\(O\\Big(k*f(m, n)\\Big)\\) <p>\\(k =\\) length of input string</p>"},{"location":"3_Core/Theory_of_Computation/03_NFA/#notes","title":"Notes","text":"<ul> <li>Both DFA and NFA accept the same class of languages called Regular Languages</li> <li>Every NFA has an equivalent DFA</li> <li>NFAs have the same power as DFAs</li> <li>Parallel (or non deterministic execution) does not add any computation power to NFAs</li> <li>Power of a machine is defined based on the class of languages recognized by the machine.</li> </ul>"},{"location":"3_Core/Theory_of_Computation/03_NFA/#epsilon-closure-of-a-set-of-states","title":"\\(\\epsilon\\)-Closure (of a set of states)","text":"<p>Set of states that can be reached transitively from a state by travelling using 0 or more \\(\\epsilon\\) transitions</p>"},{"location":"3_Core/Theory_of_Computation/03_NFA/#nfa-to-dfa","title":"NFA \\(\\to\\) DFA","text":"<p>For every transition, we will have a set of states</p> <p>These combination of states will be renamed as a single new state.</p>"},{"location":"3_Core/Theory_of_Computation/03_NFA/#nfa-for-operations","title":"NFA for Operations","text":""},{"location":"3_Core/Theory_of_Computation/03_NFA/#union","title":"Union","text":"<p>Let\u2019s say you have 2 NFAs</p> <ul> <li>Take both the NFAs</li> <li>Introduce an extra starting state<ul> <li>Introduce a \\(\\epsilon\\) transition to each of the starting states of the NFAs</li> </ul> </li> <li>Introduce an extra ending state<ul> <li>Introduce a \\(\\epsilon\\) transition from each of the ending states of the NFAs</li> </ul> </li> </ul> <pre><code>flowchart LR\nq0((q0)) --&gt;|\u03b5| NFA1 &amp; NFA2 --&gt;|\u03b5| qf</code></pre>"},{"location":"3_Core/Theory_of_Computation/03_NFA/#intersection","title":"Intersection","text":"<p>No simple method; you have to do cross product</p>"},{"location":"3_Core/Theory_of_Computation/03_NFA/#concatenation","title":"Concatenation","text":"<ul> <li>Draw both the NFAs</li> <li>Connect end state of one NFA to start state of other NFA</li> </ul> <pre><code>flowchart LR\nNFA1 --&gt;|\u03b5| NFA2</code></pre>"},{"location":"3_Core/Theory_of_Computation/03_NFA/#wr","title":"\\(W^R\\)","text":"<p>Reversed word</p> <p>(similar to DFA only; however, here we are given an NFA and are asked for an NFA)</p> <ol> <li>Swap the direction of every transition</li> <li>Swap starting and accepting states</li> </ol>"},{"location":"3_Core/Theory_of_Computation/04_Reg_Ex/","title":"04 Reg Ex","text":""},{"location":"3_Core/Theory_of_Computation/04_Reg_Ex/#regular-language-types","title":"Regular Language Types","text":"RL Description Method Difficulty forHuman Difficulty forComputer Verbal Textual Easy Difficult(Natural Language Processing required) Reg Ex Algebraic Slightly difficult Difficult NFA Diagrammatic Medium Medium DFA Diagrammatic Difficult Easy"},{"location":"3_Core/Theory_of_Computation/04_Reg_Ex/#regular-expression","title":"Regular Expression","text":"<p>are algebraic expression to describe the same class of strings (which can be recognized with finite memory)</p> <p>Denoted as \\(R\\)</p>"},{"location":"3_Core/Theory_of_Computation/04_Reg_Ex/#atomic","title":"Atomic","text":"<ul> <li>\\(R = 0\\) means \\(\\{0 \\}\\)</li> </ul>"},{"location":"3_Core/Theory_of_Computation/04_Reg_Ex/#composite","title":"Composite","text":"<p>Atomic regex with operations (in order of precedence)</p> <ul> <li>Kleene star \\({}^*\\)</li> <li>concatenation \\(\\cdot\\)</li> <li>union: represented with \\(\\cup\\) or \\(|\\) or \\(+\\)</li> </ul> <p>Think similar to BODMAS: ^\\(,\\times , +\\)</p> <p>Other operators</p> <ul> <li>\\(R^+ = RR^*\\)</li> <li>\\(R^k\\) for k -fold concatenation</li> </ul>"},{"location":"3_Core/Theory_of_Computation/05_CFG/","title":"05 CFG","text":"<p>Refer to POPL notes, as this is kinda repeated</p>"},{"location":"3_Core/Theory_of_Computation/05_CFG/#grammar","title":"Grammar","text":"<p>Set of substitution/production rules to derive a string, applied \\(k\\) times</p> <p>Grammar is way to</p> <ul> <li>Describe a language   Represent all possible legal strings</li> <li>Derive a sentence   Generate a legal string of the language</li> <li>Analyze a sentence   Check if given string is valid (opposite of derivation)</li> </ul>"},{"location":"3_Core/Theory_of_Computation/05_CFG/#parse-tree","title":"Parse Tree","text":"<p>Visual representation of derivation of a string, using a grammar</p>"},{"location":"3_Core/Theory_of_Computation/05_CFG/#cfg","title":"CFG","text":"<p>Context-Free Grammar</p> <p>Context-free grammar</p> \\[ \\text{CFG} = G(V, \\Sigma, R, S) \\] \\(V\\) Finite set of variables/non-terminals \\(\\Sigma\\) Finite set of terminals \\(R\\) Finite set of substitution rules \\(S\\) Start symbol <p>\\(V \\cap \\Sigma = \\phi\\)</p>"},{"location":"3_Core/Theory_of_Computation/05_CFG/#cfg-vs-csg","title":"CFG vs CSG","text":"CFG CSG \\(X\\) Single non-terminal \\(\\in (V \\cup \\Sigma)^+\\) Substitution independent of context \\(X\\) appears in \u2705 \u274c"},{"location":"3_Core/Theory_of_Computation/05_CFG/#derivations","title":"Derivations","text":"<p>\\(u \\overset{*}{\\implies} v\\) means \\(u\\) derives \\(v\\)</p> <ul> <li>\\(u = v\\)</li> <li>or   \\(\\exists \\ u_1, u_2, \\dots u_k\\) such that \\(u \\implies u_1 \\implies u_2 \\implies \\dots \\implies v\\)</li> </ul> <p>\\(\\overset{*}{\\implies}\\) denotes a sequence of zero/more single step-derivations</p> <p>Note</p> <p>\\(\\to\\) and \\(\\implies\\) are di\ufb00erent</p> <ul> <li>\\(\\to\\) used in de\ufb01ning rules (productions)</li> <li>\\(\\implies\\) used in derivation</li> </ul>"},{"location":"3_Core/Theory_of_Computation/05_CFG/#cfl","title":"CFL","text":"<p>Context-Free Language</p> <p>Lang which can be generated by a CFG</p> \\[ L(G) = \\{ w \\in \\Sigma^* | S \\overset{*}{\\implies} w \\} \\] \\[ L(G) \\subseteq \\Sigma^* \\] <pre><code>flowchart LR\n\nsubgraph Languages\n  subgraph CFL\n    RL\n  end\nend\n\nclassDef bg2 fill:#222, color:white\nclass Languages bg2</code></pre>"},{"location":"3_Core/Theory_of_Computation/05_CFG/#ambiguous-grammar","title":"Ambiguous Grammar","text":"<p>Multiple parse tree for a single string</p> <p>\\(\\exists\\) multiple interpretations for the string</p> <p>Examples</p> <ul> <li>Arithmetic expressions evaluation without BODMAS</li> <li>Balenced parenthesis evaluation</li> </ul>"},{"location":"3_Core/Theory_of_Computation/06_PDA/","title":"06 PDA","text":""},{"location":"3_Core/Theory_of_Computation/06_PDA/#pda","title":"PDA","text":"<p>Pushdown Automaton</p> <p>NFA + Stack</p> <p>Also represented as NPDA (non-deterministic)</p> \\[ \\text{PDA} \\implies M(Q, \\Sigma, \\Gamma, \\delta, q_0, F) \\] Symbol Meaning \\(Q\\) Set of States \\(\\Sigma\\) Set of input alphabet \\(\\Sigma_\\epsilon = \\Sigma \\cup \\epsilon\\) \\(\\Gamma\\) Set of stack alphabet \\(\\Gamma_\\epsilon = \\Gamma \\cup \\epsilon\\) \\(\\delta\\) Transition Function \\(\\delta: Q \\times \\Sigma_\\epsilon \\times \\Gamma_\\epsilon \\to P(Q \\times \\Gamma_\\epsilon)\\) \\(q_0\\) Start state \\(F\\) Set of accepting states <ul> <li>States \\(Q\\)</li> <li>Input Alphabet \\(\\Sigma\\)</li> <li>Stack Alphabet \\(\\Gamma\\)<ul> <li>$ marks the bottom of stack</li> <li>Symbols are pushed/popped to/from stack</li> </ul> </li> <li>Transitions<ul> <li>If PDA is current in state \\(q_i\\)</li> <li>it reads \\(a \\in \\Sigma_\\epsilon\\) off the stack</li> <li>it pops \\(b \\in \\Gamma_\\epsilon\\) off the stack</li> </ul> </li> </ul>"},{"location":"3_Core/Theory_of_Computation/06_PDA/#conclusion","title":"Conclusion","text":"<p>Same as NFA Conclusion</p>"},{"location":"3_Core/Theory_of_Computation/06_PDA/#note","title":"Note","text":"<p>DPDA means DFA + Stack</p> <ul> <li>Every DPDA has an equivalent PDA</li> <li>Not every PDA has an equivalent DPDA</li> </ul>"},{"location":"3_Core/Theory_of_Computation/06_PDA/#stack-operations","title":"Stack Operations","text":"Operation Representation Pop 0 and Push 1 \\(0 \\to 1\\) Push 0(Pop \\(\\epsilon\\), Push 0) \\(\\epsilon \\to 0\\) Pop 0(Pop 0, Push \\(\\epsilon\\)) \\(0 \\to \\epsilon\\) <pre><code>flowchart LR\nq0((q0)) --&gt;|\"input, push &amp;rarr; pop\"| q1((q1))</code></pre>"},{"location":"3_Core/Theory_of_Computation/06_PDA/#cfl","title":"CFL","text":"<p>Context-Free Language</p> <p>Language that is recognized by a PDA</p> <p>The intersection of 2 context-free languages does not result in another CFL</p> Deterministic CFL Non-Deterministic CFL \\(\\exists\\) DPDA? \u2705 \u274c Example Programming Languages - \\(w w^R\\)- \\(0^n 1^n\\)"},{"location":"3_Core/Theory_of_Computation/06_PDA/#closed-operations","title":"Closed Operations","text":"<p>Let \\(L_1, L_2\\) have grammar \\(G_1 = \\{S_1 \\to A\\}, G_2 = \\{S_2 \\to B\\}\\)</p> Operation New Grammar \\(G_\\text{new}\\) \\(L^*\\) \\(\\{SS \\vert  \\epsilon \\}\\) \\(L_1 \\cdot L_2\\)(concatenation) \\(\\{S_1 \\cdot S_2 \\}\\) \\(L_1 \\cup L_2\\) \\(\\{S_1 \\vert  S_2\\}\\)"},{"location":"3_Core/Theory_of_Computation/06_PDA/#note_1","title":"Note","text":"<p>Using properties, we can say that \\(L=\\{a^n b^n | n \\ge 0, n \\ne 50 \\}\\) is CFL</p> <p>This is because</p> \\[ \\begin{aligned} L =&amp; \\{a^n b^n \\} - \\{a^{50} b^{50} \\} \\\\ =&amp; \\{a^n b^n \\} \\cap \\{a^{50} b^{50} \\}' &amp; &amp; \\Big( A-B = A \\cap B' \\Big) \\\\ =&amp; \\text{CFL} \\cap \\text{RL} &amp;&amp; {\\Big( (RL)' \\to RL \\Big)} \\\\ \\implies &amp; \\text{CFL} \\end{aligned} \\]"},{"location":"3_Core/Theory_of_Computation/06_PDA/#non-closed-operations","title":"Non-Closed Operations","text":"<ul> <li>Intersection is not closed \\(\\implies L_1 \\cap L_2\\) is not always CFL</li> <li>Complement is not closed \\({L_1}'\\) is not always CFL</li> </ul>"},{"location":"3_Core/Theory_of_Computation/06_PDA/#pda-for-cfg","title":"PDA for CFG","text":"<p>Initialize stack as $</p>"},{"location":"3_Core/Theory_of_Computation/06_PDA/#leftmost-derivation","title":"Leftmost Derivation","text":"Top of Stack Input Non-Terminal Pop itPush RHS of rule Terminal Terminal Pop it $ Pop itAccept string <p>As pushing strings instead of a symbol into stack is not possible, we can use another approach, as both are equivalent.</p> <pre><code>flowchart LR\nsubgraph B\n    direction LR\n    p0((q0)) --&gt;\n    |\"a, b &amp;rarr; x\"| p1((q1)) --&gt;\n    |\"&amp;epsilon;, &amp;epsilon; &amp;rarr; y\"| p2((q2)) --&gt;\n    |\"&amp;epsilon;, &amp;epsilon; &amp;rarr; z\"| p3((q3))\nend\n\nsubgraph A\n    direction LR\n    q0((q0)) --&gt;|\"a, b &amp;rarr; xyz\"| q1((q1))\nend</code></pre>"},{"location":"3_Core/Theory_of_Computation/06_PDA/#classic-questions","title":"Classic Questions","text":""},{"location":"3_Core/Theory_of_Computation/06_PDA/#pda-for-0n-1n","title":"PDA for \\(0^n 1^n\\)","text":"<pre><code>flowchart LR\nq0(((q0))) --&gt;\n|\"&amp;epsilon;, &amp;epsilon; &amp;rarr; $\"| q1((q1)) --&gt;\n|\"\n0, &amp;epsilon; &amp;rarr; 0\n(Push)\n\"| q1 --&gt;\n|\"&amp;epsilon;, &amp;epsilon; &amp;rarr; &amp;epsilon;\"| q2((q2)) --&gt;\n|\"\n1, 0 &amp;rarr; &amp;epsilon;\n(Pop)\n\"|q2 --&gt;\n|\"&amp;epsilon;,  $ &amp;rarr; &amp;epsilon;\"| q3(((q3)))</code></pre> <p>The reason why \\(1, 0 \\to \\epsilon\\) for \\(q_2\\to q_3\\) can be replaced by \\(\\epsilon, \\textcolor{hotpink}{\\epsilon} \\to \\epsilon\\) is because the top of stack should be \\(\\textcolor{hotpink}{\\epsilon}\\). The top of stack can be thought as the actual top of stack or an empty string \u201c\u201d \\((\\epsilon)\\)</p> Input \\(\\rightarrow\\) \\(0\\) \\(1\\) \\(\\epsilon\\) Stack Top \\(\\rightarrow\\) \\(0\\) $ \\(\\epsilon\\) \\(0\\) $ \\(\\epsilon\\) \\(0\\) $ \\(\\epsilon\\) \\(q_0\\) {(q_1, $)} \\(q_1\\) \\(\\{(q_1, 0)\\}\\) \\(\\{(q_2, \\epsilon)\\}\\) \\(q_2\\) \\(\\{(q_2, \\epsilon)\\}\\) \\(\\{(q_3, \\epsilon)\\}\\) \\(q_3\\) <p>(Blank entries are \\(\\phi\\))</p>"},{"location":"3_Core/Theory_of_Computation/06_PDA/#pda-for-wwr","title":"PDA for \\(ww^R\\)","text":"<pre><code>flowchart LR\nq0((q0)) --&gt;\n|\"&amp;epsilon;, &amp;epsilon; &amp;rarr; $\"| q1 --&gt;\n|\"\n0, $ &amp;rarr; 0$\n1, $ &amp;rarr; 1$\n0, 0 &amp;rarr; 00\n0, 1 &amp;rarr; 01\n1, 0 &amp;rarr; 10\n1, 1 &amp;rarr; 11\n(Push)\n\"| q1((q1)) --&gt;\n|\"&amp;epsilon;, &amp;epsilon; &amp;rarr; &amp;epsilon;\"| q2 --&gt;\n|\"\n0, 0 &amp;rarr; &amp;epsilon;\n1, 1 &amp;rarr; &amp;epsilon;\n(Pop)\n\"|q2((q2)) --&gt;\n|\"&amp;epsilon;, $ &amp;rarr; &amp;epsilon;\"| q3(((q3)))</code></pre>"},{"location":"3_Core/Theory_of_Computation/06_PDA/#pda-for-ai-bj-ck-i-j-k-ge-0-land-bigij-lor-ik-big","title":"PDA for \\(a^i b^j c^k: (i, j, k \\ge 0) \\land \\Big((i=j) \\lor (i=k) \\Big)\\)","text":"<p>We can simplify the regex: our PDA will accept the following strings</p> <ul> <li>\\(a^i b^i c^*\\)</li> <li>\\(a^i b^* c^i\\)</li> </ul> <pre><code>flowchart LR\nq0(((q0)))\nq1((q1))\nq2((q2))\nq3((q3))\nq4((q4))\nq5((q5))\nq6((q6))\n\nq0 --&gt;\n|\"&amp;epsilon;, &amp;epsilon; &amp;rarr; $\"| q1 --&gt;\n|\"a, &amp;epsilon; &amp;rarr; a\"| q1 --&gt;\n|\"&amp;epsilon;, &amp;epsilon; &amp;rarr; &amp;epsilon;\"| q2 &amp; q4\n\nq2 --&gt;\n|\"b, a &amp;rarr; &amp;epsilon;\"| q2 --&gt;\n|\"&amp;epsilon;, &amp;epsilon; &amp;rarr; &amp;epsilon;\"| q3 --&gt;\n|\"c, &amp;epsilon; &amp;rarr; &amp;epsilon;\"| q3\n\nq4 --&gt;\n|\"b, &amp;epsilon; &amp;rarr; &amp;epsilon;\"| q4 --&gt;\n|\"&amp;epsilon;, &amp;epsilon; &amp;rarr; &amp;epsilon;\"| q5 --&gt;\n|\"c, a &amp;rarr; &amp;epsilon;\"| q5 --&gt;\n|\"&amp;epsilon;, $ &amp;rarr; &amp;epsilon;\"| q6</code></pre>"},{"location":"3_Core/Theory_of_Computation/06_PDA/#pda-for-an-b2n","title":"PDA for \\(a^n b^{2n}\\)","text":"<pre><code>flowchart LR\nq0(((q0)))\nq1((q1))\nq2((q2))\nq3((q3))\nq4(((q4)))\n\nq0 --&gt;\n|\"&amp;epsilon;, &amp;epsilon; &amp;rarr; $\"| q1 --&gt;\n|\"a, &amp;epsilon; &amp;rarr; a\"| q1 --&gt;\n|\"&amp;epsilon;, &amp;epsilon; &amp;rarr; &amp;epsilon;\"| q2 --&gt;\n|\"b, &amp;epsilon; &amp;rarr; &amp;epsilon;\"| q3 --&gt;\n|\"b, a &amp;rarr; &amp;epsilon;\"| q2\n\nq2 --&gt; |\"&amp;epsilon;, $ &amp;rarr; &amp;epsilon;\"| q4</code></pre>"},{"location":"3_Core/Theory_of_Computation/06_PDA/#pda-for-a2n-b3n","title":"PDA for \\(a^{2n} b^{3n}\\)","text":"<pre><code>flowchart LR\nq0(((q0)))\nq1((q1))\nq2((q2))\nq3((q3))\nq4((q4))\nq5((q5))\nq6(((q6)))\n\nq0 --&gt;\n|\"&amp;epsilon;, &amp;epsilon; &amp;rarr; $\"| q1 --&gt;\n|\"a, &amp;epsilon; &amp;rarr; &amp;epsilon;\"| q2 --&gt;\n|\"a, &amp;epsilon; &amp;rarr; a\"| q1 --&gt;\n|\"&amp;epsilon;, &amp;epsilon; &amp;rarr; &amp;epsilon;\"| q3 --&gt;\n|\"b, &amp;epsilon; &amp;rarr; &amp;epsilon;\"| q4 --&gt;\n|\"b, &amp;epsilon; &amp;rarr; &amp;epsilon;\"| q5 --&gt;\n|\"b, a &amp;rarr; &amp;epsilon;\"|q3\n\nq3 ---&gt;\n|\"&amp;epsilon;, $ &amp;rarr; &amp;epsilon;\"| q6</code></pre>"},{"location":"3_Core/Theory_of_Computation/06_PDA/#pda-for-g-s-to-text0ts1-1t0-t-to-1","title":"PDA for \\(G = \\{S \\to \\text{0TS1} | 1T0 , T \\to 1 \\}\\)","text":"<pre><code>flowchart LR\nq0((q0))\nq1((q1))\nq2(((q2)))\n\nq0 --&gt;\n|\"&amp;epsilon;, &amp;epsilon; &amp;rarr; &lt;span style=background:yellow;color:black;font-weight:bold&gt;S$&lt;/span&gt;\"| q1 --&gt;\n|\"\n&amp;epsilon;, S &amp;rarr; 0TS1\n&amp;epsilon;, S &amp;rarr; 1T0\n&amp;epsilon;, T &amp;rarr; 1\n0, 0 &amp;rarr; &amp;epsilon;\n1, 1 &amp;rarr; &amp;epsilon;\n\"| q1 --&gt;\n|\"&amp;epsilon;, $ &amp;rarr; &amp;epsilon;\"| q2</code></pre>"},{"location":"3_Core/Theory_of_Computation/06_PDA/#pda-for-g-s-to-0s1-quad-0n-1n","title":"PDA for \\(G = \\{S \\to 0S1\\} \\quad (0^n 1^n)\\)","text":"<pre><code>flowchart LR\nq0((q0))\nq1((q1))\nq2((q2))\n\nq0 --&gt;\n|\"&amp;epsilon;, &amp;epsilon; &amp;rarr; S$\"| q1 --&gt;\n|\"\n&amp;epsilon;,  S &amp;rarr; 1S1\n0, 0 &amp;rarr; &amp;epsilon;\n1, 1 &amp;rarr; &amp;epsilon;\n\"| q1 --&gt;\n|\"&amp;epsilon;, $ &amp;rarr; &amp;epsilon;\"| q2</code></pre>"},{"location":"3_Core/Theory_of_Computation/07_Turing_Machine/","title":"07 Turing Machine","text":""},{"location":"3_Core/Theory_of_Computation/07_Turing_Machine/#cromsky-hierarchy","title":"Cromsky Hierarchy","text":""},{"location":"3_Core/Theory_of_Computation/07_Turing_Machine/#turing-machine","title":"Turing Machine","text":"<p>NFA + Read/Write Capability</p> <ul> <li>Read/write happens from/to an \u2018infinite tape\u2019</li> <li>Read-write head can move left and right</li> <li> <p>Initially, all cells of the tape have special blank symbol \\(\\square\\), except where the input string exists</p> </li> <li> <p>FSMs always halt after \\(n\\) steps, where \\(n\\) is the length of the input. At that point, they either accept or reject.</p> </li> <li>PDAs don't always halt, but there is an algorithm to convert any PDA into one that does halt.</li> <li>Turing machines can do one of the following</li> <li>Halt and accept</li> <li>Halt and reject</li> <li>Not halt       If a turing machine loops forever \\(\\implies \\not \\exists\\) algorithm to solve the problem</li> </ul> Symbol Meaning \\(Q\\) Finite set of states \\(\\Sigma\\) Finite set of input alphabet \\(\\Gamma\\) Finite set of tape alphabet, such that\\(\\square \\in \\Gamma\\)\\(\\Sigma \\in \\Gamma\\) \\(q_0\\) Start state \\(q_\\text{acc}\\) Accepting state \\(q_\\text{rej}\\) Rejecting state \\(\\delta\\) Transition function\\(Q \\times \\Gamma \\to Q \\times \\Gamma \\times \\text{\\{L, R\\}}\\)"},{"location":"3_Core/Theory_of_Computation/07_Turing_Machine/#transition-in-expression-form","title":"Transition in Expression form","text":"<p>\\(\\delta(q_0, a) = (q_1, b, L)\\) represents a transition with</p> <ul> <li>Current state \\(q_0\\)</li> <li>\\(a\\) is current tape input character</li> <li>New state \\(q_1\\)</li> <li>\\(b\\) is new tape input character</li> <li>\\(L\\) is the direction that read-write head moves</li> </ul>"},{"location":"3_Core/Theory_of_Computation/07_Turing_Machine/#transition-in-diagram-form","title":"Transition in Diagram Form","text":"<p><code>&lt;tape_input&gt;/&lt;tape_write&gt;, &lt;direction&gt;</code></p> <pre><code>flowchart LR\nq0((q0)) --&gt;|1/X, R| q1((q1))</code></pre>"},{"location":"3_Core/Theory_of_Computation/07_Turing_Machine/#uses","title":"Uses","text":"<ul> <li>Language decider/recognizer<ul> <li>Yes/No output</li> <li>Halts for correct input</li> <li>May not halt for wrong inputs</li> </ul> </li> <li>Compute functions<ul> <li>Reverse string</li> <li>Computing systems</li> </ul> </li> </ul>"},{"location":"3_Core/Theory_of_Computation/07_Turing_Machine/#hailstone-sequence","title":"Hailstone Sequence","text":"<p>For example, for a starting number of 7, the sequence is 7, 22, 11, 34, 17, 52, 26, 13, 40, 20, 10, 5, 16, 8, 4, 2, 1, 4, 2, 1, .... Such sequences are called hailstone sequences because the values typically rise and fall, somewhat analogously to a hailstone inside a cloud.</p> <p>Suppose we have</p> <p>(Code not required to be studied)</p> <pre><code>#include &lt;stdio.h&gt;\nint main()\n{\n  unsigned int n;        \n  printf(\"Pl. enter no :\");    \n  scanf(\"%d\", &amp;n);    \n\n  while ( n &gt; 0 )\n  {\n    printf( \"%d \", n);\n    if (n == 1)\n      break;\n    if (!(n &amp; 1))\n      n /= 2;\n    else\n      n = 3*n + 1;\n  }    \n\n  printf(\"Done \\n\");\n  return 0;\n}\n</code></pre> <p>Is there any n for which the program does not terminate, ie does not converge to 1? It is inconclusive, as we do not know.</p> <p>Hence, the turing machine may not halt for this problem.</p>"},{"location":"3_Core/Theory_of_Computation/07_Turing_Machine/#questions","title":"Questions","text":""},{"location":"3_Core/Theory_of_Computation/07_Turing_Machine/#0n-1n-n-ge-1","title":"\\(0^n 1^n, n \\ge 1\\)","text":"<pre><code>flowchart LR\n\nsubgraph Only X\ndirection LR\n\nq0((q0))\nq1((q1))\nq2((q2))\nq3((q3))\nq4((q4))\nq5(((q5)))\n\nq0 --&gt;\n|0/X, R| q1 --&gt;\n|\"\n0/0, R\nX/X, R\n\"| q1 --&gt;\n|1/X, L| q2 --&gt;\n|X/X, L| q2 --&gt;\n|\"0/0, L\"| q3 --&gt;\n|\"0/0, L\"| q3 --&gt;\n|\"X/X, R\"| q0\n\nq2 --&gt;\n|\"&amp;square;/&amp;square;, R\"| q0\n\nq0 ---&gt;\n|\"X/X, R\"| q4 --&gt;\n|\"X/X, R\"| q4 --&gt;\n|\"&amp;square;/&amp;square;, R\"| q5\n\nend\n\nsubgraph Using X &amp; Y\ndirection LR\np0((q0))\np1((q1))\np2((q2))\np3((q3))\np4(((q4)))\n\np0 --&gt;\n|0/X, R| p1 --&gt;\n|\"\n0/0, R\nY/Y, R\n\"| p1 --&gt;\n|1/Y, L| p2 --&gt;\n|\"\n0/0, L\nY/Y, L\n\"| p2 --&gt;\n|X/X, R| p0 --&gt;\n|Y/Y, R| p3 --&gt;\n|Y/Y, R| p3 --&gt;\n|\"&amp;square;/&amp;square;, R\"| p4\nend</code></pre>"},{"location":"3_Core/Theory_of_Computation/07_Turing_Machine/#w-w","title":"\\(w w\\)","text":"<p>Kinda complicated</p>"},{"location":"3_Core/Theory_of_Computation/07_Turing_Machine/#w-wr","title":"\\(w w^r\\)","text":"<p>Kinda complicated</p>"},{"location":"3_Core/Theory_of_Computation/07_Turing_Machine/#balanced-parantheses","title":"Balanced Parantheses","text":"<p>We first look for closing bracket; The opening bracket for a given closed bracket is always the first one on its left; converse statement is not true.</p> <pre><code>flowchart LR\nq0((q0))\nq1((q1))\nq2((q2))\nq3((q3))\nq4(((q4)))\n\nq0 --&gt;\n|\"\n(/(, R\nX/X, R\n\"| q0 --&gt;\n|\")/X, R\"| q1 --&gt;\n|\"X/X, L\"| q1 --&gt;\n|\"(/X, R\"| q0 --&gt;\n|\"&amp;square;/&amp;square;, L\"| q3 --&gt;\n|\"X/X, L\"| q3 --&gt;\n|\"&amp;square;/&amp;square;, R\"| q4\n\nq3 --&gt;|\"(/(, R\"| q2\n\nq1 --&gt;\n|\"&amp;square;/&amp;square;, R\"| q2</code></pre>"},{"location":"3_Core/Theory_of_Computation/07_Turing_Machine/#ww-011011","title":"\\(w\\#w: 011\\#011\\)","text":""},{"location":"3_Core/Theory_of_Computation/07_Turing_Machine/#add-b-to-match-a-such-that-an-bn","title":"Add \\(b\\) to match \\(a\\), such that \\(a^n b^n\\)","text":""},{"location":"3_Core/Theory_of_Computation/07_Turing_Machine/#convert-w-to-w-wr","title":"Convert \\(w \\to w w^r\\)","text":"<p>Pretty complicated</p>"},{"location":"3_Core/Theory_of_Computation/07_Turing_Machine/#multiplication","title":"Multiplication","text":"<p>Pretty complicated</p>"},{"location":"CS_Electives/3D_Printing/","title":"3D Printing","text":""},{"location":"CS_Electives/3D_Printing/#references","title":"References","text":"<ul> <li> https://www.youtube.com/playlist?list=PLGs0VKk2DiYwxUjGRWEgotTY8ipVvFsIp</li> </ul>"},{"location":"CS_Electives/AI/","title":"Artificial Intelligence","text":"<p>This course offers an overview of the fundamental ideas and concepts in artificial intelligence. It sets the foundation for more advanced subjects like Machine Learning and Deep Learning.</p> <p>Taking DAA (Design &amp; Analysis of Algorithms) course really helps</p>"},{"location":"CS_Electives/AI/#references","title":"References","text":"<ul> <li> <p> Artificial Intelligence | Dr. Sujala Shetty</p> </li> <li> <p> Artificial Intelligence | John Levine</p> </li> <li> <p> AI for Everyone | Andrew Ng | Coursera</p> </li> <li> <p> MIT 6.034 Artificial Intelligence, Fall 2010</p> </li> <li> <p> Stanford CS221: Artificial Intelligence: Principles and Techniques | Autumn 2021</p> </li> <li> <p> How to build a career in AI</p> </li> <li> <p> Deep Multi-Task and Meta Learning</p> </li> </ul>"},{"location":"CS_Electives/AI/01_Intro/","title":"Artificial Intelligence","text":"<p>Branch of computer science which designs \u2018intelligent\u2019 machines capable of behaving, thinking and making decisions like a human.</p> <p></p>"},{"location":"CS_Electives/AI/01_Intro/#rapid-rise","title":"Rapid Rise","text":"<p>Due to data availability from digitalization</p> <p></p>"},{"location":"CS_Electives/AI/01_Intro/#key-areas","title":"Key Areas","text":"<ol> <li>Machine Learning</li> <li>Natural Language Processing</li> <li>Robotics</li> <li>Object Detection</li> <li>Speech Recognition</li> </ol>"},{"location":"CS_Electives/AI/01_Intro/#applications","title":"Applications","text":"<ol> <li>Fashion and Art</li> <li>Science</li> <li>Games</li> <li>Music and Sounds</li> <li>Videos and Images  </li> <li>Business and Finance</li> <li>Security and Safety and the list goes on...</li> </ol>"},{"location":"CS_Electives/AI/01_Intro/#why-ai","title":"Why AI?","text":"<ul> <li>Replicate human intelligence to provide precise decisions and outcomes</li> <li>Solve knowledge-intensive tasks to reduce human workload</li> <li>Establish an intelligent connection of perception and action</li> </ul>"},{"location":"CS_Electives/AI/01_Intro/#types","title":"Types","text":"<p>There are 2 types of artificial intelligence</p> ANI AGI Full form Artificial Narrow Intelligence Artificial General Intelligence Concept Specific Task Do anything a human can do Advancements Rapid Slow Examples Self-driving car, web search Learning to drive a car through ~20hrs of practiceCompleting a PhD thesis after ~5 yrs of work"},{"location":"CS_Electives/AI/01_Intro/#limitations","title":"Limitations","text":"<p>Don\u2019t be too optimistic or pessimistic about AI</p> <ol> <li>Performance</li> <li> <p>Explainability: AI finds it hard to justify its decisions</p> </li> <li> <p>Biases due to biased data</p> </li> <li>Attacks on AI</li> <li>Adverse use of AI</li> </ol>"},{"location":"CS_Electives/AI/01_Intro/#responsible-ai","title":"Responsible AI","text":"<ol> <li>Fairness: Ensure AI does not perpetuate/amplify biases</li> <li>Transparency: Making AI systems and their decisions understandable to stakeholders impacted</li> <li>Privacy: Protecting user data and ensure confidentiality</li> <li>Security: Safeguard AI systems from malicious attacks</li> <li>Ethical use: Ensuring AI is used for beneficial purposes</li> </ol>"},{"location":"CS_Electives/AI/01_Intro/#ai-company","title":"AI Company","text":"<p>This section is not relevant from the AI courses as such, but is important to know.</p> <p>Company + Deep learning \\(\\ne\\) AI company</p>"},{"location":"CS_Electives/AI/01_Intro/#features","title":"Features","text":"<ol> <li>Strategic data acquisition: some companies release non-monetised products just for data collection</li> <li>Unified data warehouse: makes it easier to connect and link</li> <li>Task Automation</li> <li>New roles (ML engineer) and division of labor</li> </ol>"},{"location":"CS_Electives/AI/01_Intro/#roles","title":"Roles","text":"<p>Roles aren\u2019t concrete</p> <ol> <li>Software engineer - program/task execution</li> <li>ML engineer - Trains the neural network</li> <li>ML researcher - extend state-of-the-art in ML</li> <li>Applied ML scientist - does roles of ML engineer and researcher</li> <li>Data scientist</li> <li>examine data</li> <li>provide insights</li> <li>presentations</li> <li>Data engineer</li> <li>organize data</li> <li>ensure data security, accessibility and cost-efficiency</li> <li>AI product manager</li> <li>decide what to build</li> <li>feasibility and value</li> </ol>"},{"location":"CS_Electives/AI/01_Intro/#transformation","title":"Transformation","text":"<p>Steps for a company to transform into an AI company</p> <ol> <li> <p>Execute pilot projects</p> </li> <li> <p>focus on success rather than value of pilot projects</p> </li> <li>some sort of progress in 6-12 months</li> <li>In-housed/outsourced</li> <li>Build in-house AI team</li> <li>Provide AI training to all employees</li> </ol> <p>Curate (online courses, books, etc) instead of create training 4. AI strategy    1. use AI to create an advantage specific to the required sector    2. strategic data acquisition</p> <ol> <li> <p>unified data warehouse</p> </li> <li> <p>network effects and platform advantages</p> <p>Eg: social media - more users join, more lucrative for prospective users</p> </li> <li> <p>aligned with \u2018Virtuous Cycle of AI\u2019</p> </li> </ol> <pre><code>flowchart LR\n\na[More&lt;br/&gt;Data] --&gt;\nb[Better&lt;br/&gt;Product] --&gt;\nc[More&lt;br/&gt;Users] --&gt;\na</code></pre> <ol> <li>Communications:    everyone should be aligned with how company is \u2018navigating' with AI</li> <li>internal</li> <li>external<ol> <li>investors</li> <li>governments</li> <li>customers</li> <li>talent</li> </ol> </li> </ol>"},{"location":"CS_Electives/AI/01_Intro/#projects","title":"Projects","text":"<p>Outsource whatever does not require specialisation/customisation - especially industry-standards</p>"},{"location":"CS_Electives/AI/01_Intro/#choosing-a-project","title":"Choosing a project","text":""},{"location":"CS_Electives/AI/01_Intro/#brainstorming","title":"Brainstorming","text":"<ol> <li>Automate tasks instead of automating jobs</li> <li>what are the main drivers of business value</li> <li>what are the main recurring weak points in your business</li> </ol>"},{"location":"CS_Electives/AI/01_Intro/#proofing","title":"Proofing","text":"<ul> <li>Techincal Diligence - AI experts</li> <li>AI must be able to perform</li> <li>how much data is required</li> <li>resources required</li> <li>Business diligence - domain experts</li> <li>valuable</li> <li>does it<ul> <li>lower costs</li> <li>increase revenue</li> <li>allow to launch new product/business</li> </ul> </li> <li>Ethical diligence</li> <li>does it benefit/harm society</li> </ul>"},{"location":"CS_Electives/AI/01_Intro/#implementing","title":"Implementing","text":"<ol> <li>Specify acceptance criteria</li> <li>do not expect 100% accuracy<ol> <li>limitations of ML</li> <li>Insufficient data</li> <li>mislabelled data</li> <li>ambiguous labels</li> </ol> </li> <li>provide AI team with <ol> <li>training set</li> <li>test set</li> </ol> </li> <li>to measure performance</li> <li>Eg: 95% accuracy</li> </ol>"},{"location":"CS_Electives/AI/01_Intro/#implementation","title":"Implementation","text":"<p>IDk</p> <ul> <li>Augmentation: Help humans with a task</li> <li>Automation: Perform task without human</li> </ul> <p>Potential</p> <ul> <li>Business Value: Does this significantly fasters, cheaper, more consistently create substantial value</li> <li>Technical Feasibility: Can AI do it?</li> </ul>"},{"location":"CS_Electives/AI/02_Search/","title":"Search","text":""},{"location":"CS_Electives/AI/02_Search/#steps","title":"Steps","text":"<ol> <li>Formulate goal</li> <li>Formulate problem</li> <li>Find solution</li> </ol>"},{"location":"CS_Electives/AI/02_Search/#types-of-search","title":"Types of Search","text":"Type Examples Single-Agent Traveling Salesperson8-Puzzle (Sudoku)Wiring of VLSI circuitsFinding faults in vehicle Two-Agent ChessTic-Tac-ToeCheckersGoTzaar Constraint-Satisfication Scheduling8-QueensF-Block"},{"location":"CS_Electives/AI/02_Search/#parts-of-search","title":"Parts of Search","text":""},{"location":"CS_Electives/AI/02_Search/#agents","title":"Agents","text":"Type of Agents Intelligent/Problem Solving/Rational Agents has goals and tries to perform a series of actions that yield the best outcome/achieve a goal Reflex Agents Don\u2019t think about consequences of its actions, and selects an action based on current state of the world."},{"location":"CS_Electives/AI/02_Search/#keywords","title":"Keywords","text":"Keyword Meaning State Description of possible state of the worldIncludes all features relevant to problem Initial/Start State State from where agent begins the search Goal State State where success is attained, which we want to reachMultiple goal states can exist Goal Test Function that observes current state and returns whether goal state is achieved/not Action Function that maps transitions from one state to another Problem Definition of general problem as search problem Solution Sequence of actions that help go from initial state to goal state Solution cost Cost associated to perform solution Search Process of looking for solution State Space Set of all states that are possible and can be reached in an environment/system. State space size Total number of states. Counted using fundamental counting principle. Search Tree Tree representation of search problem."},{"location":"CS_Electives/AI/02_Search/#search-type","title":"Search Type","text":""},{"location":"CS_Electives/AI/02_Search/#idk","title":"IDK","text":"Type Path is relevant? Direction Planning Sequence of actions \u2705 Backward chaining Diagonosis \u2705 Forward chaining Identification Assignment to variables \u274c"},{"location":"CS_Electives/AI/02_Search/#information","title":"Information","text":"Information Other names Comment Uninformed BlindBrute-ForceUndirected Informed Tends to be faster"},{"location":"CS_Electives/AI/02_Search/#search-property","title":"Search Property","text":"Property Meaning Completeness Algo guaranteed to find soln in a finite duration \\(\\iff \\exists\\) a soln Optimality Algo guaranteed to find least cost path to goal state"},{"location":"CS_Electives/AI/02_Search/#heuristic","title":"Heuristic","text":"<p>A heuristic function \\(h(x)\\) is an estimated cost from one node to another</p> <ul> <li>Heuristics are problem-specific</li> <li>Over-estimating heuristic is better than under-estimating</li> <li>As heuristics get closer to the true cost, you will expand fewer nodes but usually do more work per node to compute the heuristic itself</li> </ul> <p>Eg: Manhattan distance, Euclidean distance</p>"},{"location":"CS_Electives/AI/02_Search/#characteristics","title":"Characteristics","text":"<p>Consider</p> <ul> <li>\\(h(a, b)\\) is heuristic from \\(a\\) to \\(b\\)</li> <li>\\(c(a, b)\\) is true cost from \\(a\\) to \\(b\\)</li> </ul> Characteristic Definition Comment Implication Admissible \\(h(x, G) \\le c(x, G) \\quad \\forall x\\) Heuristic never overestimates true cost Admissible slows down bad plans, but never outweigh true costsInadmissible/pessimistic compromises optimality but with lower (better) search time Consistent \\(\\vert h(x_1, G) - h(x_2, G) \\vert \\le c(x_1, x_2)\\)where \\(x_2\\) is an intermediate node b/w \\(x_1\\) and \\(G\\) Every consistent heuristic is also admissible \\(f\\) value along path never decreasesYou can skip checking for shortest path when a node is encountered 2<sup>nd</sup> time. Informedness/Domination \\(h_1(x, G) \\ge h_2(x, G) \\quad \\forall x\\) where\\(h_1, h_2\\) are admissible \\(h_1\\) is more informed than \\(h_2\\)\\(h_1\\) dominates \\(h_2\\) Semi-lattice of heuristics \\(\\max(h_1, h_2)\\) is admissible Trivial heuristic Bottom of lattice is zero heuristic \\(\\implies\\) top of lattice is exact heuristic <p></p>"},{"location":"CS_Electives/AI/02_Search/#search-algorithms","title":"Search Algorithms","text":"Search Type Algo Comment Complete Optimal Time Complexity Space Complexity Disadvantage Advantage Hyperparameter Uninformed DFS Keep picking left-most node possible Explore deepest node from start node(Stack - LIFO) \u274c\u2705 (if no cycles) \u274c \\(O(b^m)\\) \\(O(bm)\\) May get \u201clost\u201d deep in graph, missing the shortest path Avoids searching \u201cshallow states\u201d for long solution BFS Traverse left to right, level by level Explore shallowest node from start node(Queue - FIFO) \u2705 \u2705 \\(O(b^{m_s} + 1)\\) \\(O(b^{m_s}+1)\\) High memory usage if states have high avg no of children Always finds shortest path Iterative-Deepening Combination of DFS and BFS) Perform DFS for every levelDFS with depth bound Not necessarily \\(O(b^d)\\) \\(O(bd)\\) Repeated work (negligible though: \\(O(1/b)\\)) Prevents DFS from getting lost in infinite path Depth threshold UCS (Uniform Cost)/Branch &amp; Bound Orders by path/backward cost \\(c(i, x)\\) Explore least cost node from start node \u2705(\\(\\iff\\) all costs are non-negative) \u2705 \\(O(b^m)\\) \\(O(b^m)\\) Explores options in \u201cevery direction\u201d Keeps cost low Bi-Directional Performed on search graph Two simultaneous search - forward search from start vertex toward goal vertex and backward search from goal vertex toward start vertex \u2705 \u2705\\(\\iff\\) we use BFS in both searches \\(O(b^{d/2})\\) \\(O(b^{d/2})\\) Can prune many options - Which goal state(s) to use- Handling search overlap- Which search to use in each direction- 2 BFS searches Informed Greedy/Best-First Explore the node with lowest heuristic value which takes closer to goalOrders by goal proximity/forward cost \\(h(x, G)\\) Similar to UCS, but with a priority queue \u274c \u274c \\(O(b^m)\\) \\(O(b^m)\\) TentativeMay directly go to wrong end stateMay behave like a poorly-guided DFS Helps find solution quickly A* Explore the node with lowest total cost value\\(f = C(i, x) + h(x, G)\\) Uses priority queueCombination of UCS &amp; Greedy \u2705 \u2705(\\(\\iff\\) heuristic is admissible) \\(O(b^m)\\) \\(O(b^m)\\) Hill-Climbing Basically gradient-descent \u274c \u274c \\(O(bm)\\) \\(O(b)\\) IrreversibleIf bad heuristic, may prune away goalsStuck at local minima/maximaSkips ridgesPlateaus FastLow memory Beam Compromise b/w hill-climbing &amp; greedy \\(n=1:\\) Hill-Climbing\\(n=\\infty:\\) Best-First\\(n \\in (1, \\infty) :\\) Beam width (no of children to search) \u274c \u274c \\(O(nm)\\) \\(O(bn)\\) IDA*(Iterative Deepening A *) Similar to Iterative Deepening, but uses A* cost threshold Increase always includes at least one new node \u2705 \u2705 \\(O(b^m)\\) \\(O(m)\\) Some redundant search, but negligibleDangerous if continuous \\(h\\) values or if \\(h\\) values very close to threshold Ensures search never looks beyond optimal cost soln - Threshold- \\(h\\)(root)- \\(f\\)(min_child)min_child is the cut off child with min \\(f\\) RBFS(Recursive Best-First/Greedy) Linear space variant of \\(A^*\\) Backtrack if current node is worse than next best alternativePerform \\(A^*\\) but discard subtrees when performing recursionKeep track of alternative (next best) subtreeExpand subtree until \\(f&gt;\\) boundUpdate \\(f\\) before (from parent) and after (from child) recursive call \\(O(2^m)\\) \\(O(bm)\\) Stores more info than IDA* More efficient than IDA* SMA* Simplified Memory-Bounded A* Perform A*, but when memory is full, discard worst leaf (highest \\(f\\))Back value of discarded node to parent Hill-Climbing Random restart helps overcome local maxima/minimaRandom sideways moves help escape from- shoulders- loop on flat maxima \u274cTrivially complete with random restart \u274c Irreversible stepsSkips ridges FastLow memory requirement Local Beam \\(k\\) hill climbs Choose \\(k\\) random successorsSimilar to natural selection \u274c \u274c InefficientAll \\(k\\) states end up on same local hill \\(k\\) Simulated Annealing Trade-off b/w hill-climbing &amp; random search Randomness at high \u201ctemperature\u201dWhen temperature cools, reduce prob of random moves Can find global optima when temperature chosen correctly Temperature Genetic Algorithm <p>where</p> <ul> <li>\\(b =\\) max branching factor (nodes at each level)</li> <li>\\(m=\\) depth</li> <li>\\(m_s =\\) depth of shallowest solution</li> <li>\\(C^*\\) is optimal path cost</li> <li>\\(\\epsilon\\) is minimal cost between 2 nodes</li> </ul> <p>Fringe is a priority queue</p>"},{"location":"CS_Electives/AI/02_Search/#iterative-improvement-search","title":"Iterative Improvement Search","text":"<p>Local search</p> <p>Hill-climbing, local beam, simulated annealing, genetic</p> <p>Appropriate when only reaching goal state is required; solution path is irrelevant</p>"},{"location":"CS_Electives/AI/02_Search/#idk_1","title":"IDK","text":""},{"location":"CS_Electives/AI/02_Search/#graph-search","title":"Graph Search","text":"<p>Helps avoid repeated states - Do not return to parent/grand-parent states - Do not create solution paths with cycles - Do not generate repeated states as options (need to store &amp; check more states)</p>"},{"location":"CS_Electives/AI/02_Search/#implementation","title":"Implementation","text":"<ul> <li>Data structures</li> <li>Tree (as usual)</li> <li>Set of expanded (visited/closed) nodes</li> <li>Traversal</li> <li>Visit node from open set</li> <li>Check if visited previously</li> <li>If visited, skip node and go to step 1</li> <li>Else<ol> <li>expand node</li> <li>add node to closed set</li> <li>add children to open set</li> </ol> </li> </ul>"},{"location":"CS_Electives/AI/02_Search/#implications","title":"Implications","text":"<ul> <li>Completeness maintained</li> <li>Optimality is not guaranteed</li> </ul>"},{"location":"CS_Electives/AI/03_Genetic_Algorithms/","title":"Genetic Algorithm","text":"<p>Nature-inspired search technique to find true/approximate solutions to optimization and search problems</p> <p>Categorized as global search heuristics</p> <p>Neither complete nor optimal</p>"},{"location":"CS_Electives/AI/03_Genetic_Algorithms/#terminology","title":"Terminology","text":"Term Meaning Individual Any possible solution Population Collection of all individuals Gene Single bit that represents an attribute Trait Possible features of an individual Chromosome String of genes that represent the trait of an individual Genome Collection of all chromosome (traits) for an individual Fitness Target function that we are optimizing(each individual has a fitness)"},{"location":"CS_Electives/AI/03_Genetic_Algorithms/#algorithm","title":"Algorithm","text":"<ol> <li>Initialize random population of solution guesses</li> <li>Repeat for each generation</li> <li>Evaluate each chromosome in population using a fitness function</li> <li>Apply GA operators to create a new population</li> <li>Repeat until desired fitness/stopping criterion is met</li> </ol>"},{"location":"CS_Electives/AI/03_Genetic_Algorithms/#ga-operators","title":"GA Operators","text":""},{"location":"CS_Electives/AI/03_Genetic_Algorithms/#representation","title":"Representation","text":"<ul> <li>Binary strings</li> <li>Arrays of integers (usually bound)</li> <li>Array of letters</li> </ul>"},{"location":"CS_Electives/AI/03_Genetic_Algorithms/#selection","title":"Selection","text":"<p>Selecting a subset of individuals \\(x\\) according to fitness function \\(f(x)\\) like beam search</p> Selection Technique Logic Roulette-wheel/Fitness-proportionate Each individual gets slice of wheel proportional to fitness\\(p_i = \\dfrac{f_i}{\\sum_j^n f_j}\\) Elitist Select only \\(n\\) most fit members of each generation Cutoff selection Select only members with fitness &gt; threshold Scaling Rank-Space Fitness ignores diversity, hence populations tend to become uniform1. Sort population by sum of fitness rank and diversity rank2. Diversity rank is the result of sorting by the function \\((1/d^2)\\)"},{"location":"CS_Electives/AI/03_Genetic_Algorithms/#crossoverrecombination","title":"Crossover/Recombination","text":"<ul> <li>Parents are randomly (with prob) recombined to form new offsprings</li> <li>Chromosome of other parents copied onto next generation as is</li> </ul>"},{"location":"CS_Electives/AI/03_Genetic_Algorithms/#simple","title":"Simple","text":"<ol> <li>For each couple, using a pre-determined prob, decide if crossover to be performed</li> <li>Select 2 parents</li> <li>Select cross site</li> <li>Cut &amp; substitute substring of one parent with another</li> </ol> \\[ \\begin{aligned} &amp; \\textcolor{hotpink}{101} 110 \\quad \\textcolor{orange}{110} 001 \\\\ \\\\ \\implies &amp; \\textcolor{orange}{110} 110 \\quad \\textcolor{hotpink}{101} 001 \\end{aligned} \\]"},{"location":"CS_Electives/AI/03_Genetic_Algorithms/#2-point","title":"2 Point","text":"<p>Helps avoid cases when genes at the beginning and end of chromosome are always split $$ \\begin{aligned} &amp; 1 \\textcolor{hotpink}{01} 110 \\quad 1 \\textcolor{orange}{10} 001 \\ \\ \\implies &amp; 1 \\textcolor{orange}{10} 110 \\quad 1 \\textcolor{hotpink}{01} 001 \\end{aligned} $$</p>"},{"location":"CS_Electives/AI/03_Genetic_Algorithms/#k-pointmulti-point","title":"k-point/Multi-point","text":"<ol> <li>Pick \\(k\\) random splice points</li> <li>Splice for \\(k-1\\) substrings</li> </ol>"},{"location":"CS_Electives/AI/03_Genetic_Algorithms/#uniform-crossover","title":"Uniform crossover","text":"<ul> <li>Random subset is chosen for both parents</li> <li>Subset of parent 1 is substituted with subset of parent 2</li> </ul>"},{"location":"CS_Electives/AI/03_Genetic_Algorithms/#mutation","title":"Mutation","text":"<p>Random alteration of mutating offsprings with small probability</p> <ul> <li>An insurance policy against lost bits</li> <li>Pushes out of local minima</li> </ul>"},{"location":"CS_Electives/AI/03_Genetic_Algorithms/#inversion","title":"Inversion","text":"<p>Reverse selected subsequence</p> <p>1011011 -&gt; 10 | 110 | 11 -&gt; 10 | 011 | 11 -&gt; 1001111</p> <ul> <li>Preserves adjacency information</li> <li>Discards order information</li> </ul>"},{"location":"CS_Electives/AI/03_Genetic_Algorithms/#elitism","title":"Elitism","text":"<p>Best chromosomes from prev generation replace few of the worst chromosomes in current generation</p>"},{"location":"CS_Electives/AI/03_Genetic_Algorithms/#idk","title":"IDK","text":"<ul> <li>Order1 crossover: inversion and recombination</li> </ul>"},{"location":"CS_Electives/AI/03_Genetic_Algorithms/#applications","title":"Applications","text":"Domain Application Control Gas PipelinesMissile evasion Design Aircraft designKeyboard configurationCommunication networks Game playing PokerCheckers Security EncryptionDecryption Robotics Trajectory Planning"},{"location":"CS_Electives/AI/03_Genetic_Algorithms/#advantages","title":"Advantages","text":"<ul> <li>Concept is easy</li> <li>Modular, separate from application</li> <li>Supports multi-objective optimization</li> <li>Easily exploits previous/alternate solutions</li> <li>Flexible building blocks for hybrid applications</li> </ul>"},{"location":"CS_Electives/AI/03_Genetic_Algorithms/#issues","title":"Issues","text":"<ul> <li>How to select original population</li> <li>How to handle non-binary solution types</li> <li>What should be size of population</li> <li>What is the optimal mutation rate</li> <li>How are mates picked for crossover</li> <li>Can any chromosome appear more than once in a population</li> <li>Stopping criteria: When should GA halt</li> <li>How to deal with local minima</li> <li>How to parallelize</li> </ul>"},{"location":"CS_Electives/AI/03_Genetic_Algorithms/#classifier-systems","title":"Classifier Systems","text":"<ul> <li>GAs &amp; load balancing</li> <li>SAMUEL</li> </ul>"},{"location":"CS_Electives/AI/04_Game_Playing/","title":"Adversarial Search","text":"<p>Also called as Game Playing</p>"},{"location":"CS_Electives/AI/04_Game_Playing/#game","title":"Game","text":"<p>A game can be defined as a type of search in AI which can be formalized of the following elements</p> <ol> <li>Initial state</li> <li>Terminal state</li> <li>Player\\((s)\\)</li> <li>Action\\((s)\\)</li> <li>Result\\((s, a)\\) - It is the transition model, which specifies the result of moves in the state space.</li> <li>Terminal-Test\\((s)\\) - Terminal test is true if the game is over, else it is false at any case. </li> <li>Utility\\((s, p)\\) - A utility function gives the final numeric value for a game that ends in terminal states \\(s\\) for player \\(p\\).</li> </ol> <p>A game tree is a tree where nodes of the tree are the game states and Edges of the tree are the moves by players. Game tree involves initial state, actions function, and result Function.</p>"},{"location":"CS_Electives/AI/04_Game_Playing/#types-of-games","title":"Types of Games","text":"Deterministic Non-Deterministic(Chance/Randomness) Fully Observable Chess Monopoly Partially Observable Battleship Card games"},{"location":"CS_Electives/AI/04_Game_Playing/#zero-sum-game","title":"Zero Sum game","text":"<ul> <li>In Zero-sum game each agent's gain or loss of utility is exactly balanced by the losses or gains of utility of another agent.</li> <li>One player of the game tries to maximize one single value, while other player tries to minimize it.</li> <li>Examples are tic tac toe and chess.</li> </ul>"},{"location":"CS_Electives/AI/04_Game_Playing/#mini-max-algorithm","title":"Mini-Max Algorithm","text":"<p>Algo to determine optimal moves for utility maximizing agent in fully-observable, deterministic games</p> <p>Min-max is complete and optimal</p>"},{"location":"CS_Electives/AI/04_Game_Playing/#assumption","title":"Assumption","text":"<p>Opponent behaves optimally, ie always perform the move that is worst for us</p>"},{"location":"CS_Electives/AI/04_Game_Playing/#logic","title":"Logic","text":"<ul> <li>Utility of each node is computed bottom up from leaves toward root. </li> <li>At each MAX node, pick move w/ max utility</li> <li>At each MIN node, pick move w/ min utility</li> </ul>"},{"location":"CS_Electives/AI/04_Game_Playing/#limitations","title":"Limitations","text":"<p>Really expensive for trees with large branding factor (complex games such as Chess, Go)</p> <p>Complexity = \\(O(b^m)\\)</p> <p>Can be overcome with \\(\\alpha \\beta\\) Pruning</p>"},{"location":"CS_Electives/AI/04_Game_Playing/#alpha-beta-pruning","title":"\\(\\alpha \\beta\\) Pruning","text":"<p>Alpha Beta Pruning</p> <ul> <li>Optimized mini-max</li> <li>Form of meta-reasoning</li> <li>Used to reduce branching factor, hence handles complex games as well </li> <li>Maintain two parameters in depth-first search</li> <li> <p>\\(\\alpha =\\) highest value found yet for MAX along any path</p> <ul> <li>\\(\\alpha_\\text{root} = - \\infty\\)</li> </ul> </li> <li> <p>\\(\\beta =\\) lowest value found yet for MIN along any path</p> <ul> <li>\\(\\beta_\\text{root} = + \\infty\\)</li> </ul> </li> <li> <p>Prune (skip) a subtree once it is known to be worse than current \\(\\alpha\\) or \\(\\beta\\)</p> </li> <li>If \\(\\alpha &gt; \\beta\\), stop evaluating children</li> </ul> <p></p>"},{"location":"CS_Electives/AI/04_Game_Playing/#implications","title":"Implications","text":"<ul> <li>Solution does not change</li> <li>Complexity = \\(O(b^{m/2})\\)</li> </ul>"},{"location":"CS_Electives/AI/04_Game_Playing/#cut-off-search","title":"Cut-off Search","text":"<ul> <li>Cutoff search tree before terminal state reached</li> <li>Use heuristic of minimax value at leaves, instead of utility</li> </ul>"},{"location":"CS_Electives/AI/04_Game_Playing/#deep-blue","title":"Deep Blue","text":"<ul> <li>Minimax</li> <li>\\(\\alpha\\beta\\) pruning</li> <li>Progressive deepening</li> <li>Parallel computing</li> <li>uneven tree development</li> </ul> <p>14-16 levels deep</p>"},{"location":"CS_Electives/AI/05_CSP/","title":"Constraint Satisfaction Problem","text":"<p>Identification problem: These are problems in which we must simply identify whether a state is a goal state or not</p> <ul> <li>State is defined by</li> <li>Variables \\(X_i\\)</li> <li> <p>with values from Domain \\(D\\)</p> </li> <li> <p>Goal test is a set of constraints</p> </li> </ul> <p>CSPs are represented as constraint graphs, where nodes represent variables and edges represent constraints between them.</p>"},{"location":"CS_Electives/AI/05_CSP/#constraints","title":"Constraints","text":"Type Example Implicit A \\(\\ne\\) B Explicit (A, B) \\(\\in\\)"},{"location":"CS_Electives/AI/05_CSP/#graph-coloring","title":"Graph Coloring","text":"Problem Contraint Graph Variables WA, NT, Q, NSW, SA, V, T Domain {Red, Green, Blue} Constraints WA \\(\\ne\\) NT\u2026"},{"location":"CS_Electives/AI/05_CSP/#n-queens","title":"N-Queens","text":"Formulation 1 Formulation 2 Variables \\(X_{ij}\\) \\(Q_k\\) Domain \\(\\{ 0, 1 \\}\\) \\(\\{ 1, 2, \\dots, N \\}\\) Constraints \\(\\sum_{i, j} X_{ij} = N\\) Implicit: \\(\\forall i, j\\): non threatening \\((Q_i, Q_j)\\)Explicit: \\((Q, _1, Q_2) \\in \\{ (1, 3), (1, 4), \\dots \\}\\)"},{"location":"CS_Electives/AI/05_CSP/#idk","title":"IDK","text":"<ul> <li>Binary CSP: Each constraint relates at most 2 variables</li> <li>Binary constraint graph: nodes are variables, arcs show constraints</li> </ul>"},{"location":"CS_Electives/AI/05_CSP/#cryptarithmetic","title":"Cryptarithmetic","text":"Variables Domain \\([0, 9]\\) Constraints alldiff(variables)"},{"location":"CS_Electives/AI/05_CSP/#sudoku","title":"Sudoku","text":"Variables \\(X_{ij}\\) Domain \\([1, 9]\\) Constraints 9-way all diff for each column9-way all diff for each row9-way all diff for each sub-grid"},{"location":"CS_Electives/AI/05_CSP/#types","title":"Types","text":"Variable Type Domain Examples Discrete Finite Size \\(d\\) means \\(O(d^n)\\) complete assignments Boolean satisfiability (np-complete) Infinite(integers, strings) Job Scheduling (Vars are start/end times for each job)Linear constraints solvableNon-linear undecidable Continuous Linear constraints solvable in polynomial time by LP methods Start/end times for Hubble telescope observations"},{"location":"CS_Electives/AI/05_CSP/#constraints_1","title":"Constraints","text":"Variety Example Unary Single variable SA \\(\\ne\\) Green Binary Pairs SA \\(\\ne\\) WA Higher-Order Cryptarithmetic column constraints Enforcement Example Soft (Preferences) Represented by cost for each var assignmentGives constrained optimization problems Red better than green Hard"},{"location":"CS_Electives/AI/05_CSP/#standard-search-formulation","title":"Standard Search Formulation","text":"<p>States are defined by the values assigned so far (partial assignments)</p> <ul> <li>Initial state: Empty assignment</li> <li>Successor function: assign value to unassigned variable</li> <li>Goal test: current assignment is complete and satisfies all constraints</li> </ul>"},{"location":"CS_Electives/AI/05_CSP/#idk_1","title":"IDK","text":""},{"location":"CS_Electives/AI/05_CSP/#backtracking-search","title":"Backtracking Search","text":"<p>Backtracking = DFS + variable-ordering + fail-on-violation</p> <p>Assumption: assignments are commutative (order of assignment doesn\u2019t matter)</p> <ol> <li>Fix an ordering for variables, and select values for variables in this order</li> <li>Consider assignments to a single var at each step</li> <li>Check constraints on the go</li> <li>When selecting values for a variable, only select values that don\u2019t conflict with any previously assigned values</li> <li>If no such values exist, backtrack and return to the previous variable, changing its value</li> </ol> <p>Can solve n-queens for \\(n \\le 25\\)</p> <p></p> <p></p>"},{"location":"CS_Electives/AI/05_CSP/#filteringpruning","title":"Filtering/Pruning","text":"<p>To improve performance, we consider filtering which checks if we can prune the domain of unassigned variables ahead of time. </p> <p>To improve performance, we can prune subtrees that will inevitably lead to failure</p>"},{"location":"CS_Electives/AI/05_CSP/#forward-checking","title":"Forward Checking","text":"<ul> <li>Whenever a new variable is assigned, we can run forward checking and prune the domains of unassigned variables adjacent to the newly assigned variable in the constraint graph.</li> <li>Basically we eliminate all the values from the domain of the adjacent variables which could cause violation of any constraint.</li> </ul> <p>This propagates info from assigned to unassigned vars, but doesn\u2019t provide early detection for all failures</p> <p>Time Complexity: \\(O(n^2 d^3)\\)</p> <p></p>"},{"location":"CS_Electives/AI/05_CSP/#arc-consistency","title":"Arc Consistency","text":"<p>An arc \\(X \\to Y\\) is consistent \\(\\iff \\forall x\\) in the tail, \\(\\exists y\\) in the head which could be assigned without violating a constraint</p> <ul> <li>Forward checking only enforces consistency of arcs pointing to each new assignment</li> <li>More advanced: If \\(X\\) loses a value, neighbors of \\(X\\) need to be rechecked</li> <li>Arc consistency detects failure earlier than forward checking</li> <li>Can be run as a pre/post-processing step for each assignment</li> </ul> <p>Note: delete from tail</p> <p>Time Complexity: \\(O(n^2 d^2)\\)</p> <p>But detecting all possible future problems is np-hard</p>"},{"location":"CS_Electives/AI/05_CSP/#limitations","title":"Limitations","text":"<ul> <li>After enforcing arc consistency</li> <li>Can have one solution left</li> <li>Can have multiple solutions left</li> <li>Can have no solutions left (and not know about it)</li> <li>Arc consistency still runs inside a backtracking search</li> </ul>"},{"location":"CS_Electives/AI/05_CSP/#ac3-algorithm","title":"AC3 Algorithm","text":"<ol> <li>Turn each binary constraint represented as undirected edge into 2 directed arcs</li> </ol> <p>Eg</p> <ul> <li>\\(A \\ne B \\implies A \\ne B, B \\ne A\\)</li> <li> <p>\\(A &lt; B \\implies A &lt; B, B &gt; A\\)</p> </li> <li> <p>Add all arcs to agenda \\(Q\\)</p> </li> <li> <p>Repeat until \\(Q\\) empty</p> </li> <li> <p>Take an arc \\((X_i, X_j)\\) off \\(Q\\) and check it</p> </li> <li>\\(\\forall X_i , \\exists X_j\\): For every element of \\(X_i\\) there should be at least one element of \\(X_j\\) that satisfies condition </li> <li>Remove any inconsistent values from \\(X_i\\)</li> <li>if \\(X_i\\) has changed, add all arcs of the form \\((X_k, X_i)\\) to agenda<ol> <li>If arc \\(X_k \\rightarrow X_i\\) is already in \\(Q\\), don't add it again</li> </ol> </li> </ul>"},{"location":"CS_Electives/AI/05_CSP/#ordering","title":"Ordering","text":"Ordering Disadvantage MRV: Minimum Remaining Values/\u201cFail-Fast\u201d Choose \u201cmost constrained var\u201d, ie the var with the fewest legal left values in domain LCV: Least Constraining Value Choose least constraining valueIe, var that rules out the fewest values in the remaining vars Extra computation for re-running filtering Degree Choose node with highest degreeChoose var involved in most no of constraints on other unassigned vars Min-Conflicts chooses randomly any conflicting variable, i.e., the variable that is involved in any unsatisfied constraint, and then picks a value which minimizes the number of violated constraints (break ties randomly)"},{"location":"CS_Electives/AI/06_Planning/","title":"Planning","text":"<p>Planning is a particular type of problem solving in which actions and goals are declaratively specified in logic and generally concerns performing actions in the real world.</p> <p>Generally languages of planning problems consist mainly of  </p> <ol> <li> <p>States - conjunction of positive literals  </p> </li> <li> <p>Goals - conjunction of positive ground literals  </p> </li> <li> <p>Actions - represented in terms of precondition and effect of the action  </p> </li> </ol> <p>STRIPS and ADL are two languages used to express planning problems</p> STRIPS Language ADL Language Only positive literals in states: \\(Poor \u2227 Unknown\\) Positive and negative literals in states:  \\(\u00acRich \u2227 \u00acFamous\\) Unmentioned literals are false Unmentioned literals are unknown Effect \\(P \u2227 \u00acQ\\) means add P and delete Q Effect \\(P \u2227 \u00acQ\\) means add P and \u00acQ and delete \u00acP and Q. Only ground literals in goals: \\(Rich \u2227 Famous\\) Quantified variables in goals:  \\(\u2203xAt(P1,x) \u2227 At(P2, x)\\) Goals are conjunctions Goals allow conjunction and disjunction Effects are conjunctions Conditional effects allowed No support for equality Equality predicate (x = y) is built in No support for types Variables can have types Example : \\(Action(Fly(p, from,to),\\)\\(PRECOND:At(p, from) \u2227 Plane(p) \u2227 Airport(from) \u2227 Airport(to)\\)\\(EFFECT:\u00acAt(p, from) \u2227 At(p,to))\\) Example : \\(Action(Fly(p : Plane, from : Airport,to : Airport),\\)\\(PRECOND:At(p, from) \u2227 (from = to)\\)\\(EFFECT:\u00acAt(p, from) \u2227 At(p, to))\\)"},{"location":"CS_Electives/AI/06_Planning/#partial-order-planning-pop","title":"Partial Order Planning (POP)","text":"<p>Any planning algorithm that can place two actions into a plan without specifying which comes first is called a partial-order planner.</p> <ul> <li>Ordering constraints - of the form \\(A \u227a B\\) (A before B) which means that action A must be executed sometime before action B</li> <li>Causal link - between two actions A and B in the plan is written as \\(A\\xrightarrow{\\text{p}}B\\)  (A achieves p for B). </li> <li>Conflict - An action C conflicts with \\(A\\xrightarrow{\\text{p}}B\\) if C has the effect \u00acp and if C could come after A and before B.</li> <li>Open precondition - A precondition is open if it is not achieved by some action in the plan.</li> </ul> <p>Consider the following description of planning problem :  </p> <p>\\(Goal(RightShoeOn \u2227 LeftShoeOn)\\) \\(Init()\\) \\(Action(RightShoe, PRECOND:RightSockOn, EFFECT:RightShoeOn)\\) \\(Action(RightSock, EFFECT:RightSockOn)\\) \\(Action(LeftShoe, PRECOND:LeftSockOn, EFFECT:LeftShoeOn)\\) \\(Action(LeftSock, EFFECT:LeftSockOn)\\)</p> <p></p> <p>Actions - \\({RightSock, RightShoe, LeftSock, LeftShoe, Start, Finish}\\) Orderings - \\({RightSock \u227a RightShoe, LeftSock \u227a LeftShoe}\\) Links - {\\({RightSock\\xrightarrow{\\text{RightSockOn}}RightShoe, LeftSock\\xrightarrow{\\text{LeftSockOn}}LeftShoe,RightShoe\\xrightarrow{\\text{RightShoeOn}}Finish, LeftShoe\\xrightarrow{\\text{RightShoeOn}}Finish}\\)} Open Preconditions - { }</p>"},{"location":"CS_Electives/AI/06_Planning/#planning-graphs","title":"Planning Graphs","text":"<p>A planning graph consists of a sequence of levels that correspond to time steps in the plan, where level 0 is the initial state. Each level contains a set of literals and a set of actions.</p> <p>NOTE : persistence actions are actions that remain true from one situation to the next if no action alters it.</p> <p>Before forming a planning graph we need to understand mutex links A mutex relation holds between two actions if  </p> <ol> <li> <p>Inconsistent effects - one action negates an effect of the other  </p> </li> <li> <p>Interference - one of the effects of one action is the negation of a precondition of the other  </p> </li> <li> <p>Competing needs - one of the preconditions of one action is mutually exclusive with a precondition of the other.  </p> </li> </ol> <p>Consider this problem : </p> <p>\\(Init(Have(Cake))\\) \\(Goal(Have(Cake) \u2227 Eaten(Cake))\\) \\(Action(Eat(Cake)\\) \\(\\quad\\) \\(PRECOND: Have(Cake)\\) \\(\\quad\\) \\(EFFECT: \u00ac Have(Cake) \u2227 Eaten(Cake))\\) \\(Action(Bake(Cake)\\) \\(\\quad\\) \\(PRECOND: \u00ac Have(Cake)\\) \\(\\quad\\) \\(EFFECT: Have(Cake))\\) </p> <p></p> <p>In the above graph rectangles indicate actions, small squares indicate persistence actions and straight lines indicate preconditions and effects. Mutex links are shown as curved gray lines.</p> <p>These notes can be refined</p>"},{"location":"CS_Electives/AI/07_Bayesian/","title":"Bayesian Network","text":"<p>Technique for describing complex joint distributions using simple, local distributions (conditional probabilities)</p> <p>Also known as probabilistic graph model</p> <p>Bayes net = Graph + Local conditional probabilities</p> <p>Local interactions chain together to give global, indirect interactions</p> <p>Note: Kindly go through Bayes' Theorem in Probability and Statistics</p>"},{"location":"CS_Electives/AI/07_Bayesian/#semantics","title":"Semantics","text":"<p>DAG that represents the dependence b/w vars</p> <ul> <li>Nodes - represents variables</li> <li>Links - X points to Y, implies X has direct influence over Y or X is a parent of Y</li> <li>Conditional Probability Table - each node has a conditional probability distribution which determines the effect of the parent on that node</li> </ul>"},{"location":"CS_Electives/AI/07_Bayesian/#why-needed","title":"Why needed?","text":"<p>Inefficient to use full joint distribution table as probabilistic model</p> <ul> <li>Joint is way too big to represent explicitly</li> <li>Hard to learn/estimate anything empirically about more than few variables at a time</li> </ul>"},{"location":"CS_Electives/AI/07_Bayesian/#aspects","title":"Aspects","text":"Definition \\(P(X=x)\\) Representation Given a BN graph, what kinds of distributions can it encode? Modelling What BN is most appropriate for a given domain Inference Given a fixed BN, what is \\(P(X \\vert e)\\) Learning Given data, what is best BN encoding Diagnosis Infer P(problem type | symptoms) Prediction Infer prob dist for values that are expensive/impossible to measure Anomaly detection Detect unlikely obs Active learning Choose most informative diagnostic test to perform given obs"},{"location":"CS_Electives/AI/07_Bayesian/#joint-probability-distribution","title":"Joint Probability Distribution","text":"\\[ \\begin{aligned} P(x_i|x_{i \\ne j}) &amp;= P(x_i |\\text{Pa}(x_i)) \\\\ P(x_1, \\dots, x_m) &amp;= \\prod_{i=1}^m P(x_i |\\text{pa}(x_i)) \\end{aligned} \\]"},{"location":"CS_Electives/AI/07_Bayesian/#independence-in-bn","title":"Independence in BN","text":"<p>\\(x {\\perp \\!\\!\\! \\perp} y | z\\) is read as \\(X\\) is conditionally independent of \\(Y\\) given \\(Z\\)</p> <p>$$ X {\\perp !!! \\perp} Y | Z \\iff P(x, y \\vert z) = P(x \\vert z) \\cdot P(y \\vert z) $$ Are 2 nodes independent given evidence?</p> <ul> <li>If yes: prove using algebra (tedious)</li> <li>If no: prove with counter example</li> </ul>"},{"location":"CS_Electives/AI/07_Bayesian/#d-separation","title":"D-Separation","text":"<p>Condition that explains the independence of subgraphs</p> <p>Query: $$ x_i {\\perp !!! \\perp} x_j \\vert { x_{k_1}, \\dots, x_{k_n} } $$ Check all (undirected) paths between \\(x_i\\) and \\(x_j\\)</p> Independence guaranteed If one/more paths active \u274c All paths inactive \u2705"},{"location":"CS_Electives/AI/07_Bayesian/#casual-chains","title":"Casual chains","text":"<pre><code>graph LR\nx((x))--&gt;y((y))\ny((y))--&gt;z((z))</code></pre> <p>\\(x {\\perp \\!\\!\\! \\perp} y | z\\) guaranteed</p>"},{"location":"CS_Electives/AI/07_Bayesian/#common-cause","title":"Common Cause","text":"<pre><code>  graph TD;\n    y((y))--&gt;x((x));\n    y((y))--&gt;z((z));</code></pre> <p>\\(x {\\perp \\!\\!\\! \\perp} y | z\\) guaranteed</p>"},{"location":"CS_Electives/AI/07_Bayesian/#common-effect","title":"Common effect","text":"<pre><code>graph TD;\nx((x))--&gt;z((z));\ny((y))--&gt;z((z));</code></pre> <ul> <li>\\(x {\\perp \\!\\!\\! \\perp} y\\) guaranteed</li> <li>\\(x \\centernot{\\perp \\!\\!\\! \\perp} y | z\\) guaranteed</li> </ul>"},{"location":"CS_Electives/AI/07_Bayesian/#active","title":"Active","text":"<p>A path is active if every triple in path is active</p> <p>NOTE : All possible configurations for active and inactive triples are below</p> <pre><code>graph TB\n\nsubgraph Inactive\n\ndirection LR\nsubgraph i1\ndirection LR;\nAi1((\"x\")) --&gt; Bi1[\"z\"] --&gt;Ci1((\"y\"));\nstyle Bi1 fill : #808080\nend\n\nsubgraph i2\ndirection TB\nBi2[\"z\"]--&gt;Ai2((\"x\")) &amp; Ci2((\"y\"));\nstyle Bi2 fill : #808080\nend\n\nsubgraph i3\n    direction TB\n    Ai3((\"x\")) &amp; Bi3((\"y\")) --&gt; Ci3((\"z\"))\nend\n\n\nsubgraph i4\n    direction TB\n    Ai4((\"x\")) &amp; Bi4((\"y\")) --&gt; Ci4((\"z\")) --&gt; Di4((\" \"))\nend\n\nend\n\nsubgraph Active\ndirection LR\nsubgraph 4\n    direction TB\n  x4((\"x\"))--&gt;C4((\"z\"));\n  y4((\"y\"))--&gt;C4((\"z\"));\n  C4((\"z\"))-.-&gt;D4[\" \"]\n  style D4 fill : #808080\nend\n\nsubgraph 3\n  direction TB\n  A3((\"x\"))--&gt;C3[\"z\"];\n  B3((\"y\"))--&gt;C3[\"z\"];\n  style C3 fill : #808080\nend\n\nsubgraph 2\n  direction TB\n  B2((\"z\"))--&gt;A2((\"x\")) &amp; C2((\"y\"));\nend\n\nsubgraph 1\ndirection LR\nA1((\"x\"))--&gt;B1((\"z\"));\nB1((\"z\"))--&gt;C1((\"y\"));\nend\nend</code></pre>"},{"location":"CS_Electives/AI/07_Bayesian/#topology-limits-distributions","title":"Topology Limits Distributions","text":"<p>Given some graph topology \\(G\\), only certain joint distributions can be encoded</p> <p>The graph structure guarantees certain (conditional) independencies</p> <p>Bayes net\u2019s joint distribution may have further (conditional) independencies that is not detectable until inspection of specific distribution</p> <p>Adding arcs increases set of distributions but has several costs</p> <p>Full conditioning can encode any distribution</p> <p></p>"},{"location":"CS_Electives/AI/07_Bayesian/#building-a-bayes-net","title":"Building a Bayes Net","text":"<ol> <li>Choose set of relevant vars</li> <li>Represent each var by a node</li> <li>Choose ordering for vars \\(x_1, \\dots, x_m\\) such that if \\(x_i\\) influences \\(x_j\\), then \\(i &lt; j\\)</li> <li>Add links: Link structure must be acyclic</li> <li>Add conditional probability table for each node</li> </ol>"},{"location":"CS_Electives/AI/07_Bayesian/#interpreting","title":"Interpreting","text":"<ol> <li>Given parents, each node is conditionally independent of all non-descendents in the tree</li> <li>2 unconnected vars may still be correlated</li> <li>Whether 2 vars are conditionally-independent can be deduced using \u201cd-separation\u201d</li> </ol>"},{"location":"CS_Electives/AI/07_Bayesian/#inference","title":"Inference","text":"<ul> <li>Doing exact inference is computationally hard</li> <li>Tractable in some cases: trees</li> <li>We can instead perform approximate inference</li> <li> <p>Likelihood-weighted sampling</p> </li> <li> <p>Marginal probability</p> </li> <li>Rewrite in terms of joint distribution<ol> <li>Fix query vars</li> <li>Sum over unknown vars</li> </ol> </li> <li>Conditional probability</li> <li>Fix query vars</li> <li>Fix evidence vars</li> <li>Sum over unknown vars</li> <li>Add normalization constant \\(\\alpha\\) such that</li> <li>Rewrite joint probability using Bayes Net factors</li> <li>Choose variable order; take summations inside</li> </ul>"},{"location":"CS_Electives/AI/07_Bayesian/#learning","title":"Learning","text":"<ul> <li>Structure learning</li> <li>Parameter learning</li> </ul>"},{"location":"CS_Electives/AI/07_Bayesian/#variable-elimination","title":"Variable Elimination","text":"<p>Eliminating all vars in turn until there is a factor (function from set of vars) with only query var</p> <p>To eliminate a var</p> <ol> <li>Join all factors containing that var</li> <li>Sum out influence of var on a new factor</li> <li>Exploits product form of joint distribution</li> </ol>"},{"location":"CS_Electives/AI/08_Generative_AI/","title":"Generative AI","text":""},{"location":"CS_Electives/AI/08_Generative_AI/#llm","title":"LLM","text":"<p>Large Language Models</p>"},{"location":"CS_Electives/AI/08_Generative_AI/#limitations","title":"Limitations","text":"<ul> <li>Bias</li> <li>Hallucinations</li> <li>Expensive to build &amp; run</li> </ul>"},{"location":"CS_Electives/AI/08_Generative_AI/#chatgpt","title":"ChatGPT","text":"<ol> <li>Train supervised policy</li> <li>Provide prompt</li> <li>Labeler demonstrates desired output behavior</li> <li>Fine-tune model</li> <li>Collect comparaison data &amp; train reward model</li> <li>Prompt and several model outputs are samples</li> <li>Labeler ranks outputs from best to worst</li> <li>Data used to train reward model</li> <li>Policy optimization</li> </ol>"},{"location":"CS_Electives/AI/08_Generative_AI/#gan","title":"GAN","text":"<p>Generative Adversarial Networks</p> <pre><code>flowchart LR\nn[/Noise/] ---&gt; g[Generator] --&gt; d\nrd[Real Data] --&gt;\nd[Discriminator] --&gt;\nrf{Real/Fake} -.-&gt;\n|Backpropagation| d &amp; g</code></pre>"},{"location":"CS_Electives/AI/08_Generative_AI/#fine-tuning","title":"Fine-Tuning","text":"<p>Process of training model using specific data, usually with a significantly smaller learning rate</p>"},{"location":"CS_Electives/AI/08_Generative_AI/#disadvantages","title":"Disadvantages","text":"<ul> <li>requires copy of the model</li> <li>associated costs of hosting it</li> <li>Risk of \u201ccatastrophic forgetting\u201d: model forgets previously learnt information</li> </ul>"},{"location":"CS_Electives/AI/08_Generative_AI/#rag","title":"RAG","text":"<p>Retrieval Augmented Generation</p> <p>Makes use of a source of knowledge, usually vector store of embeddings and associated texts</p> <p>By comparing predicted embeddings of query to embeddings in the vector store, we can form a prompt for the LLM that fits inside its context and contains the information needed to answer the question</p>"},{"location":"CS_Electives/AI/08_Generative_AI/#advantages","title":"Advantages","text":"<ul> <li>Does not require re-training</li> <li>No need to deal with internal workings of model</li> <li>Just adjust the data that the model \u201ccheats\u201d off</li> <li>Reduces the amount a model \u201challucinates\u201d</li> </ul>"},{"location":"CS_Electives/AI/08_Generative_AI/#difficulties","title":"Difficulties","text":"<ul> <li>Finding relevant data to give the model</li> </ul>"},{"location":"CS_Electives/AI/08_Generative_AI/#keywords","title":"Keywords","text":"<ul> <li>Data organization: </li> <li>Vector creation: Unique index that points right to a chunk of information</li> <li>Querying: Prompting</li> <li>Retrieval</li> <li>Prompt goes through embedding model and transforms into a vector</li> <li>Systems uses this to get the chunks most relevant to the question</li> <li>Prepending the context: Most relevant chunks are served up as context</li> <li>Answer generation</li> </ul>"},{"location":"CS_Electives/AI/08_Generative_AI/#types-of-questions","title":"Types of Questions","text":"No special skills required to answerJust need right reference material What is the capital of France? Write a poem in GermanWrite a computer program to calculate the first n natural numbers"},{"location":"CS_Electives/AI/09_Issues/","title":"Issues","text":""},{"location":"CS_Electives/AI/09_Issues/#environmental-issues","title":"Environmental Issues","text":"<p>Sources of carbon footprint</p> <ul> <li>train models</li> <li>data collection</li> <li>inference</li> <li>creating new chips</li> </ul>"},{"location":"CS_Electives/AI/09_Issues/#types","title":"Types","text":"<ul> <li>Embodied carbon</li> <li>Operational carbon</li> </ul> <p>Not just the energy cost of running computers, but also energy required to build them</p> <p></p>"},{"location":"CS_Electives/AI/09_Issues/#solutions","title":"Solutions","text":"<ul> <li>Efficiency of models</li> <li>Software infra to promote development of efficiency</li> <li>AutoML instead of grid search</li> <li>Easy-to-use APIs for efficient models</li> <li>Efficient computer chips</li> <li>Efficient datacenters</li> <li>Microsoft: underwater servers<ul> <li>8 x more reliable, using Nitrogen, etc</li> <li>No need for fresh water for cooling</li> </ul> </li> <li>Use renewable energy</li> <li>Transitioning to 100% renewables will not eliminate the carbon footprint of chips</li> <li></li> <li>Stop planned obscelence</li> </ul>"},{"location":"CS_Electives/AI/09_Issues/#4-ms-of-ai-efficiency","title":"4 M\u2019s of AI Efficiency","text":"<ol> <li>Model</li> <li>Machine</li> <li>Mechanism</li> <li>Map: Location</li> </ol>"},{"location":"CS_Electives/AI/09_Issues/#jevons-paradoxeffect","title":"Jevon\u2019s Paradox/Effect","text":"<p>Improved efficiency in resource utilization increases the total consumption of resources, due to increased rate of consumption from increased demand</p> <p>Hence, definition of \u201cefficiency\u201d is important</p>"},{"location":"CS_Electives/AI/09_Issues/#pue","title":"PUE","text":"<p>Power Usage Effectiveness</p> <p>Lower is better</p> <p>Typically \\(\\in [1.1, 1.6]\\)</p> <p>How the power input is being used for compute &amp; for other supporting consumption such as cooling, lighting, etc. $$ \\begin{aligned} \\text{PUE} &amp;= \\dfrac{\\text{Total Energy}}{\\text{Productive Energy}} \\ &amp;= 1 + \\dfrac{\\text{Overhead}}{\\text{Productive Energy}} \\end{aligned} $$</p>"},{"location":"CS_Electives/AI/09_Issues/#ethical-issues","title":"Ethical Issues","text":""},{"location":"CS_Electives/AI/09_Issues/#abstraction","title":"Abstraction","text":"<p>Researchers/Engineers often abstracted away from application they\u2019re working on, hence not aware of bad things that can be used for</p>"},{"location":"CS_Electives/AI/09_Issues/#data-ownership","title":"Data Ownership","text":"<p>Data governance</p> <p>Trade-off: Privacy vs Progress</p> <p>Model monetization trained on consumer data, eg: GitHub Copilot</p>"},{"location":"CS_Electives/AI/09_Issues/#data-bias","title":"Data Bias","text":"<p>Dataset curation</p> <p>Federated learning</p>"},{"location":"CS_Electives/AI/09_Issues/#research-inequality","title":"Research Inequality","text":"<ul> <li>Inequitable access to computing resources</li> <li>Inequitable access to datasets</li> </ul>"},{"location":"CS_Electives/AI_in_Robotics/","title":"AI in Robotics","text":"<p>Unlike AI courses by CS department which focus on the algorithms, this course looks at the application of those concepts specifically for Robotics</p>"},{"location":"CS_Electives/AI_in_Robotics/#concepts","title":"Concepts","text":"<ul> <li>Probability Distributions</li> <li>Robot Kinematics</li> <li>Tracking/Navigation</li> <li>Filters</li> <li>Localization</li> <li>Mapping</li> <li>SLAM</li> <li> <p>Programming Languages</p> </li> <li> <p>Python</p> </li> <li> <p>ROS: Robotics Operating System</p> </li> </ul>"},{"location":"CS_Electives/AI_in_Robotics/#references","title":"References","text":"<ul> <li> AI in Robotics | Dr. R. Karthikeyan | BITS Pilani Dubai Campus</li> <li> Self-Driving Cars | Andreas Geiger | T\u00fcbingen Machine Learning</li> <li> Principles of Sensing for Autonomy | Stanford</li> </ul>"},{"location":"CS_Electives/AI_in_Robotics/01_Introduction/","title":"Introduction","text":""},{"location":"CS_Electives/AI_in_Robotics/01_Introduction/#applications","title":"Applications","text":"<ul> <li>Path Planning</li> <li>Motion Control</li> <li>Computer Vision</li> <li>Machine Learning</li> <li>Optimization</li> <li>NLP</li> <li>Manipulator &amp; Mobile Robot Hardware</li> <li>Computer Vision, Pathfinding and other tasks</li> </ul>"},{"location":"CS_Electives/AI_in_Robotics/01_Introduction/#agent","title":"Agent","text":"<p>Anything that can be viewed as</p> <ul> <li>perceiving its environment through sensors</li> <li>acting upon that environment through effectors</li> </ul> Sensors Effectors Humans EyesNoseSkinTongue HandsLegs Robots CamerasInfrared Range FindersThermal Scanners Motors"},{"location":"CS_Electives/AI_in_Robotics/01_Introduction/#robot","title":"Robot","text":"<p>Software-controllable device using sensors to guide effectors through programmed motion in a workspace to manipulate physical objects</p>"},{"location":"CS_Electives/AI_in_Robotics/01_Introduction/#types-of-robots","title":"Types of Robots","text":"<ul> <li>Mobile</li> <li>Stationary</li> <li>Autonomous</li> <li>Remote-Controlled</li> <li>Virtual</li> </ul>"},{"location":"CS_Electives/AI_in_Robotics/01_Introduction/#robot-control-system","title":"Robot Control System","text":"<pre><code>flowchart LR\n\ne[/Environment/] --&gt;\nSensors --&gt;\n|Send&lt;br/&gt;Telemetery| r[Radio] --&gt;\n|Decision| c[Controllers] --&gt; Actuators\n\nOperator &lt;--&gt; cs[Control&lt;br/&gt;Station] &lt;-.-&gt;\n|Data&lt;br/&gt;Link| r\n\nsubgraph Actuators\n    direction LR\n    Motors\n    Servos\nend</code></pre>"},{"location":"CS_Electives/AI_in_Robotics/02_Probabilistic/","title":"Probabilistic Modelling","text":"<ul> <li>Every robot action is uncertain</li> <li>Every sensor measurement is uncertain</li> </ul> <p>Probabilistic approach acknowledges these uncertainties and uses models to abstract useful information from data</p> <p>Goal is an incrementally-updated probabilistic estimate of position of robot relative to map</p>"},{"location":"CS_Electives/AI_in_Robotics/02_Probabilistic/#associated-concepts","title":"Associated Concepts","text":"<ul> <li>Mean</li> <li>Variance</li> <li>Discrete &amp; Continuous Random Variables</li> <li>Total Probability</li> <li>Gaussian Distribution</li> <li>Bayes Rule</li> </ul>"},{"location":"CS_Electives/AI_in_Robotics/02_State_Space_Modelling/","title":"State Space Modelling","text":"<p>Representation of a system that replaces an \\(n\\)th order differential equation with a single first order matrix differential equation.</p> <p>The state space representation is given through 2 equations</p> State \\(\\dot q(t) = Aq(t) + Bx(t)\\) Output \\(y(t) = Cq(t) + Dx(t)\\) <p>where</p> Dimension \\(q\\) State Vector \\(n \\times 1\\) Constant \\(A\\) State Matrix \\(n \\times n\\) Constant \\(B\\) Input Matrix \\(n \\times r\\) Constant \\(x\\) Input \\(r \\times 1\\) Function of time \\(C\\) Output matrix \\(m \\times n\\) Constant \\(D\\) Direct transition matrix \\(m \\times r\\) Constant \\(y\\) Output \\(m \\times 1\\) Function of time"},{"location":"CS_Electives/AI_in_Robotics/02_State_Space_Modelling/#advantages","title":"Advantages","text":"<ul> <li>Concise notation: Even large systems can be represented using 2 simple equations</li> <li>Easy to develop general techniques to solve systems, as all systems are represented by the same notation</li> <li>Computers easily simulate first-order equations</li> </ul>"},{"location":"CS_Electives/AI_in_Robotics/02_State_Space_Modelling/#example","title":"Example","text":"\\[ 2 \\dfrac{d^3 y}{dt^3} + 4 \\dfrac{d^2 y}{dt^2} + 6 \\dfrac{dy}{dt} + 8y = 10 u(t) \\\\ \\implies 2 y''' + 4 y'' + 6 y' + 8y = 10 u(t) \\] <p>Since DE is of 3<sup>rd</sup> order, there are 3 state variables $$ x_1 = y, x_2 = \\dot y, x_3 = \\ddot y \\ \\implies 2 \\dot x_3 + 2 x_3 + 6 x_2 + 8x_1 = 10u(t) $$</p>"},{"location":"CS_Electives/AI_in_Robotics/02_State_Space_Modelling/#equation-representation","title":"Equation Representation","text":"\\[ \\begin{aligned} \\dot x_1 &amp;= x_2 \\\\ \\dot x_2 &amp;= x_3 \\\\ \\dot x_3 &amp;= -4x_1 - 3x_2 - 2x_3 + 5u(t) \\end{aligned} \\]"},{"location":"CS_Electives/AI_in_Robotics/02_State_Space_Modelling/#matrix-representation","title":"Matrix Representation","text":"\\[ \\begin{aligned} \\begin{bmatrix} \\dot x_1 \\\\ \\dot x_2 \\\\ \\dot x_3 \\end{bmatrix} &amp;= \\begin{bmatrix} 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\ -4 &amp; -3 &amp; -2 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ 0 \\\\ 5 \\end{bmatrix} u(t) \\\\ y &amp;= \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} \\end{aligned} \\]"},{"location":"CS_Electives/AI_in_Robotics/02_State_Space_Modelling/#representation-of-kalman-filter","title":"Representation of Kalman Filter","text":"\\[ x_k = A x_{k-1} + B u_{k-1} + w_{k-1} \\] \\[ z_k = H x_k + v_k \\]"},{"location":"CS_Electives/AI_in_Robotics/03_Kinematics/","title":"Kinematics","text":"<ul> <li>Displacement</li> <li>Velocity</li> <li>Acceleration</li> <li>Mobile Robots</li> <li>Robotic Manipulator</li> <li>Mobile Manipulator</li> </ul>"},{"location":"CS_Electives/AI_in_Robotics/03_Kinematics/#system-state","title":"System State","text":"<p>Future target position can be easily calculated using Newton\u2019s motion equations. $$ \\begin{aligned} x &amp;= x_0 + v_{x0} \\Delta t + \\dfrac{1}{2} a_x \\Delta t^2 \\ y &amp;= \\cdots \\ z &amp;= \\cdots \\end{aligned} $$ The target parameters \\((x, y, z, v_x, v_y, v_z, a_x, a_y, a_z)\\) are called System State.</p>"},{"location":"CS_Electives/AI_in_Robotics/04_Navigation_and_Tracking/","title":"Navigation &amp; Tracking","text":"<ul> <li>Global coordinate systems &amp; local coordinate systems</li> <li>Robot navigation means robot\u2019s ability to position itself in its frame of reference &amp; then to plan a Plath towards some goal location</li> <li>Tracking is the problem of extraction, detection, identification, and tracking of the mobile robot from its environment to obtain motion parameters such as position, trajectory, velocity, and acceleration of the mobile robot</li> </ul>"},{"location":"CS_Electives/AI_in_Robotics/06_Localization/","title":"Localization","text":"<p>Navigation is one of the most challenging competencies required of a mobile robot.</p>"},{"location":"CS_Electives/AI_in_Robotics/06_Localization/#building-blocks-of-navigation","title":"Building blocks of navigation","text":"<ol> <li>Perception: Robot must interpret its sensors to extract meaningful data</li> <li>Localization: Robot must determine its position in its environment</li> <li>Cognition: Robot must decide how to act to achieve its goals</li> <li>Motion Control: Robot must modulate its motor outputs to achieve the desired trajectory</li> </ol> <p>Devices relying on localization include IMU, encoders, LiDARs, Ultrasonic sensors, onboard cameras</p> <p>Methodologies include Markov, Monte Carlo, Kalman Filters</p>"},{"location":"CS_Electives/AI_in_Robotics/06_Localization/#particle-filter","title":"Particle Filter","text":"<p>For localization in a known map</p> <p>Estimate posterior density of state variables given the observation variables</p> <p>Generic particle filter estimates the posterior distribution of hidden states using observation measurement process</p> <p>Working principle: Bayes\u2019 Rule</p> <p></p>"},{"location":"CS_Electives/AI_in_Robotics/06_Localization/#requirements","title":"Requirements","text":"<ul> <li>Uniformly-distributed particles</li> <li>Landmarks</li> <li>Object</li> </ul>"},{"location":"CS_Electives/AI_in_Robotics/06_Localization/#concept","title":"Concept","text":"<ul> <li>Object senses landmarks and generates measurements vector for each landmark</li> <li>These measurements are bearing angles, ie angle between object and landmark location</li> <li>Since sensors are noisy, bearing noise of gaussian mean 0 is added to the measurements</li> </ul>"},{"location":"CS_Electives/AI_in_Robotics/06_Localization/#steps","title":"Steps","text":"<ol> <li> <p>Randomly generate set of particles \\(p_i =\u00a0(x_i, y_i, \\theta_i)\\)</p> </li> <li> <p>Predict next state of particles: Move particles based prediction of how the real system is behaving</p> </li> <li> <p>Update weighting of particles proportional to closeness to object</p> </li> <li> <p>Particles that closely match the measurements are weighted higher</p> </li> <li> <p>Represented with larger area</p> </li> <li> <p>Resample, by replacing highly improbable particles with copies of more probable particles</p> </li> <li> <p>Compute estimate</p> </li> </ol> <p>Optionally, compute weighted means and covariance of set of particles to get a state estimate</p> <p></p> <p></p>"},{"location":"CS_Electives/AI_in_Robotics/06_Localization/#convolution","title":"Convolution","text":"<p>Shifting &amp; flattening of probability distribution of the beliefs</p> <p></p> \\[ P(\\hat x_i \\vert x_i) = \\dfrac{P(x_i \\vert \\hat x_i) \\cdot P(x_i)}{P(\\hat x_i)} \\\\ P(\\hat x_i) = \\sum_j P(x_j \\vert \\hat x_j) \\cdot P(\\hat x_j) \\]"},{"location":"CS_Electives/AI_in_Robotics/06_Localization/#inexact-motion","title":"Inexact Motion","text":"<p>Incorporate probability of undershooting/overshooting</p>"},{"location":"CS_Electives/AI_in_Robotics/06_Localization/#multiple-measurements","title":"Multiple Measurements","text":""},{"location":"CS_Electives/AI_in_Robotics/07_Mapping_and_SLAM/","title":"Mapping &amp; SLAM","text":""},{"location":"CS_Electives/AI_in_Robotics/07_Mapping_and_SLAM/#mapping","title":"Mapping","text":"<p>Computer vision and cartography</p> <p>Goal for an autonomous robot is to be able to construct (and/or use) a map/floor plan and to localize itself and its recharging bases.</p>"},{"location":"CS_Electives/AI_in_Robotics/07_Mapping_and_SLAM/#slam","title":"SLAM","text":"<p>Simultaneous Localization and Mapping</p> <p>Method for autonomous vehicles that enables building a map and localization of vehicle in that map at the same time.</p> <p>Allow vehicle to map out unknown environments</p> <pre><code>flowchart LR\nsd[/Sensor&lt;br/&gt;Data/] --&gt; fe --&gt; be --&gt; pose[Pose Graph &lt;br/&gt;&amp; Map Information]\n\nsubgraph fe[\"Frontend&lt;br/&gt;(Sensor-Dependent)\"]\n    direction LR\n    me[\"Motion&lt;br/&gt;Estimation\"] \n    ole[\"Obstacle&lt;br/&gt;Location&lt;br/&gt;Estimation\"]\nend\n\nsubgraph be[\"Backend&lt;br/&gt;(Sensor-Independent)\"]\n    direction LR\n    rpg[\"Register&lt;br/&gt;Pose Graphs\"]\n    go[Graph&lt;br/&gt;Optimization]\nend</code></pre> Localization Mapping SLAM given Map object\u2019s trajectory (position at each time) Use sensor data to estimate current position of object map Build map and estimate trajectory"},{"location":"CS_Electives/AI_in_Robotics/07_Mapping_and_SLAM/#need-for-map","title":"Need for Map","text":"<ul> <li>Path planning</li> <li>Limiting error in state estimates, by providing opportunity to \u2018reset\u2019</li> </ul>"},{"location":"CS_Electives/AI_in_Robotics/07_Mapping_and_SLAM/#2d-graph-slam","title":"2D Graph SLAM","text":"<p>Considering uncertainty in \\(x\\) and \\(y\\), gaussian functions are applied to maximize probability of product $$ \\mu = \\Omega^{-1} \\epsilon $$ where</p> <ul> <li>\\(\\mu =\\) locations of landmarks and robot positions</li> <li>\\(\\Omega=\\)  matrix of \\(X\\) and landmarks</li> <li>\\(\\epsilon=\\)\u00a0vector of constraints</li> </ul>"},{"location":"CS_Electives/AI_in_Robotics/07_Mapping_and_SLAM/#constraints","title":"Constraints","text":"<ul> <li>Initial constraint</li> <li>Relative motion constraint</li> <li>Relative measurement constraint</li> </ul>"},{"location":"CS_Electives/AI_in_Robotics/08_Path_Planning/","title":"Path Planning","text":"<p>Goal is to plan a collision-free route from a starting point to a target point</p> <p>Find a path between 2 locations in an environment, regardless of the level of knowledge about it.</p> <p>Computational problem to find a sequence of valid configurations that move object from source to destination</p>"},{"location":"CS_Electives/AI_in_Robotics/08_Path_Planning/#algorithms","title":"Algorithms","text":"<ul> <li>A*</li> <li>Dijkstra</li> <li>GVD (Generalized Moroni Diagrams)</li> <li>RRT (Rapidly Exploring Random Tree)</li> </ul>"},{"location":"CS_Electives/AI_in_Robotics/08_Path_Planning/#local-global-frame","title":"Local &lt;-&gt; Global Frame","text":"<p>If robot position in a plane is defined by its state vector (location and orientation) is defined by $$ q(t) = \\begin{bmatrix} x(t) \\ y(t) \\ \\theta(t) \\end{bmatrix} $$ Transformation between local frame \\(m\\) and global frame \\(g\\) $$ R(\\theta) = \\begin{bmatrix} \\cos \\theta &amp; \\sin \\theta &amp; 0 \\ - \\sin \\theta &amp; \\cos \\theta &amp; 0 \\ 0 &amp; 0 &amp; 1 \\end{bmatrix} $$</p>"},{"location":"CS_Electives/AI_in_Robotics/08_Path_Planning/#directforward-kinematics","title":"Direct/Forward Kinematics","text":"<p>Determination of robot position for given control variables</p>"},{"location":"CS_Electives/AI_in_Robotics/08_Path_Planning/#odometrydead-reckoning","title":"Odometry/Dead Reckoning","text":"<p>Obtaining through integration of kinematic model $$ \\begin{aligned} x(t) &amp;= \\int_0^t v(t) \\cdot \\cos \\theta(t) \\cdot dt \\ y(t) &amp;= \\int_0^t v(t) \\cdot \\sin \\theta(t) \\cdot dt \\ \\theta(t) &amp;= \\int_0^t w(t) \\cdot dt \\end{aligned} $$</p> <p>### Euler Method</p> \\[ \\begin{aligned} x(t+1) &amp;= x(t) + v(t) T_s \\cos \\theta(t) \\\\ y(t+1) &amp;= y(t) + v(t) T_s \\sin \\theta(t) \\\\ \\theta(t+1) &amp;= \\theta(t) + w(t) T_s \\end{aligned} \\]"},{"location":"CS_Electives/AI_in_Robotics/08_Path_Planning/#control","title":"Control","text":"<p> $$ u(t) = k_p e(t) + k_i \\int_0^t e(\\tau) d \\tau + k_d \\dfrac{d}{dt} e(t) $$ where \\(e=\\) error</p>"},{"location":"CS_Electives/AI_in_Robotics/08_Path_Planning/#path-planning-algorithms","title":"Path Planning Algorithms","text":""},{"location":"CS_Electives/AI_in_Robotics/08_Path_Planning/#single-course-shortest-path-problem","title":"Single-Course Shortest Path problem","text":"<p>Finding shortest path from a source to all other vertices in the graph</p> <p>Dijkstra\u2019s algorithm</p>"},{"location":"CS_Electives/AI_in_Robotics/08_Path_Planning/#a-algorithm","title":"A* algorithm","text":""},{"location":"CS_Electives/AI_in_Robotics/09_Motion_Control/","title":"Motion Control","text":"<p>PID (Proportional Integral Derivative) controllers are used to control movement of mobile robot.</p> <p>Linear velocity loop controls the robot wheels speeds using motor speed feedback signal from the encoder</p> <p>Angular velocity control loop keeps robot always in the accepted angle boundary using a 6 degree-of-freedom gyroscope and accelerometer as a feedback signal</p> <pre><code>flowchart LR\nstart(( )) --&gt;\n|\"Desired Path&lt;br/&gt;x_d, y_d, &amp;theta;_d\"| ec[Error&lt;br/&gt;Calculation] \n\nec --&gt;|ex| cta\nec --&gt;|ey| cta\nec --&gt;|\"e&amp;theta;\"| cta\ncta[Conversion to Angular]\n\ncta --&gt;\n|ed| fpidc[1st PID Controller] --&gt;\n|u_right| mrs\n\ncta --&gt;\n|\"e&amp;theta;\"| spidc[2nd PID Controller] --&gt;\n|u_left| mrs\n\nmrs[Mobile Robot System] --&gt;\n|\"Actual Path&lt;br/&gt;x_a, y_a, &amp;theta;_a\"| ec</code></pre>"},{"location":"CS_Electives/AI_in_Robotics/10_Computer_Vision/","title":"Computer Vision","text":"<p>Extraction, analysis and comprehension of useful information from image(s)</p> <p>Used for Navigation &amp; Mapping</p> <p>An image may be defined as a 2D function \\(f(x, y)\\) where</p> <ul> <li>\\(x, y\\) are spatial coordinates</li> <li>value of \\(f\\) gives the intensity at a point</li> </ul>"},{"location":"CS_Electives/AI_in_Robotics/10_Computer_Vision/#channels","title":"Channels","text":"<ul> <li>For a gray-range image, intensity is given by just one \u2018channel\u2019</li> <li>For color images, intensity is given by 3d vector (3 channels): RGB</li> </ul> <p>Conversion from analog to a \u201cdigital image\u201d requires both coordinates and intensity to be digitized</p> <ul> <li>Sampling: Digitizing the coordinates</li> <li>Quantization: Digitizing the intensity</li> </ul>"},{"location":"CS_Electives/AI_in_Robotics/10_Computer_Vision/#types-of-images","title":"Types of images","text":"Binary Image Intensity Image Color Image Black and white Data matrix whose values have been scaled to represent intensities Intensity image with 3 channels (RGB) Values 0/1 Usually scaled to [0, 255] or [0, 1]"},{"location":"CS_Electives/AI_in_Robotics/10_Computer_Vision/#image-quantization","title":"Image Quantization","text":"<p>Transforming a real valued sampled image into one with a finite number of distinct values</p> <p>Eg: rounding the digits, setting a max/min range</p>"},{"location":"CS_Electives/AI_in_Robotics/10_Computer_Vision/#convolution-filters","title":"Convolution Filters","text":"<ul> <li>Low-pass: noise removal, blur</li> <li>High-pass: edge detection, sharpening</li> </ul>"},{"location":"CS_Electives/AI_in_Robotics/10_Computer_Vision/#averaging","title":"Averaging","text":""},{"location":"CS_Electives/AI_in_Robotics/10_Computer_Vision/#gaussian","title":"Gaussian","text":"<p>Nearest neighboring pixels have the most influence</p> <p>Center of the filter matrix will be larger than the edges and the corners</p> <p>Variance of the filter determines the smoothing</p>"},{"location":"CS_Electives/AI_in_Robotics/10_Computer_Vision/#median","title":"Median","text":"<p>Salt and pepper noise reduction</p>"},{"location":"CS_Electives/AI_in_Robotics/10_Computer_Vision/#bilateral","title":"Bilateral","text":"<p>Blur while maintaining sharp edges</p> <p>Slower</p>"},{"location":"CS_Electives/AI_in_Robotics/10_Computer_Vision/#edge-detection","title":"Edge Detection","text":"<p>Look for a neighborhood with strong signs of change</p> <ul> <li>Surface normal discontinuity</li> <li>Surface color discontinuity</li> <li>Illumination discontinuity</li> <li>Depth discontinuity</li> </ul> <p>Kernel</p> <ul> <li>Kernel should be symmetric</li> <li>Normalization required to make the sum of elements in the filter as 1</li> </ul> <p>Derivative will be non-zero at edges</p> \\[ \\begin{aligned} \\nabla f &amp;= ( f_x, f_y ) \\\\ \\theta &amp;= \\tan^{-1} \\left( \\dfrac{ f_y }{ f_x} \\right) \\\\ \\vert \\vert \\nabla f \\vert \\vert &amp;= \\sqrt{     {f_x}^2 + {f_y}^2 } \\end{aligned} \\]"},{"location":"CS_Electives/AI_in_Robotics/10_Computer_Vision/#discrete-gradient","title":"Discrete gradient","text":"\\[ \\begin{aligned} \\dfrac{\\partial f}{\\partial x} &amp;\\approx \\dfrac{f(x+1, y) - f(x, y)}{1} \\\\ &amp;\\approx f(x+1, y) - f(x, y) \\end{aligned} \\]"},{"location":"CS_Electives/AI_in_Robotics/10_Computer_Vision/#sobel-operator","title":"Sobel Operator","text":"<ul> <li>Sobel</li> <li>Prewitt</li> <li>Roberts</li> </ul>"},{"location":"CS_Electives/AI_in_Robotics/10_Computer_Vision/#difficulties","title":"Difficulties","text":"<ul> <li>Neighborhood size</li> <li>Change detection</li> </ul>"},{"location":"CS_Electives/AI_in_Robotics/10_Computer_Vision/#canny-edge-detection","title":"Canny Edge Detection","text":"<p>Robust and flexible</p> <ol> <li>Noise reduction</li> <li>Calculating intensity gradient</li> <li>Suppression of false edges</li> <li>Hysteresis thresholding</li> </ol> <p>Thresholding: \\(\\vert \\vert \\nabla f \\vert \\vert\\) are compared with 2 threshold values</p> <ul> <li>If \\(\\vert \\vert \\nabla f \\vert \\vert &gt; T_h\\), those pixels are associated with solid edges and included in the final edge map</li> <li>If \\(\\vert \\vert \\nabla f \\vert \\vert &lt; T_l\\), those pixels are suppressed and excluded from final edge map</li> <li>All other pixels are marked as \u201cweak\u201d edges, and become candidates for final edge map</li> <li>If weak pixels are connected to solid pixels, they are also included in final edge map</li> </ul> <p></p>"},{"location":"CS_Electives/AI_in_Robotics/10_Computer_Vision/#camera-calibration","title":"Camera Calibration","text":"<p>Estimates parameters of lens and image sensor, to be used for</p> <ul> <li>Correction of lens distortion</li> <li>Object measurement</li> <li>Determine location of camera in scene</li> <li>Estimate depth using stereo camera</li> <li>Estimate 3D structure from camera motion</li> </ul>"},{"location":"CS_Electives/AI_in_Robotics/10_Computer_Vision/#camera","title":"Camera","text":"<p>Converts 3D world into 2D image $$ \\begin{aligned} x &amp;= PX \\ \\begin{bmatrix} X \\ Y \\ Z \\end{bmatrix} &amp;=  \\begin{bmatrix} p_1 &amp; p_2 &amp; p_3 &amp; p_4 \\ p_5 &amp; p_6 &amp; p_7 &amp; p_8 \\ p_9 &amp; p_{10} &amp; p_{11} &amp; p_{12} \\end{bmatrix} \\begin{bmatrix} X \\ Y \\ Z \\ 1 \\end{bmatrix} \\end{aligned} $$ where</p> <ul> <li>\\(x =\\)  homogeneous image</li> <li>\\(P =\\)  camera matrix</li> <li>\\(X =\\)  homogeneous world point</li> </ul>"},{"location":"CS_Electives/AI_in_Robotics/10_Computer_Vision/#parameters","title":"Parameters","text":"Intrinsic/Internal Extrinsic/External Allows mapping between pixel coordinates and camera coordinates in the image frame Describe orientation and location of camera Example Optical centerFocal lengthRadial distortion of coefficients of lens Rotation of cameraTranslation of camerawrt world coordinate system"},{"location":"CS_Electives/AI_in_Robotics/10_Computer_Vision/#pinhole-camera-model","title":"Pinhole Camera Model","text":"<p>Basic camera model without a lens. Light passes through aperture and projects an inverted image on the opposite side of the camera.</p> <p></p> <p>Visualize the virtual image plane in front of the camera and assume that it is containing the upright image of the scene $$ \\begin{aligned} w \\begin{bmatrix} x &amp; y &amp; 1 \\end{bmatrix} &amp;= \\begin{bmatrix} X &amp; Y &amp; Z &amp; 1 \\end{bmatrix} P \\ P &amp;= \\begin{bmatrix} R \\ T \\end{bmatrix} k \\ k &amp;= \\begin{bmatrix} f_x &amp; s &amp; c_x \\ 0 &amp; f_y &amp; c_y \\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\end{aligned} $$ where</p> <ul> <li>\\(w=\\) scale factor</li> <li>\\(P=\\) camera matrix represents the camera specifications</li> <li>\\(R=\\) Rotation</li> <li>\\(T=\\) Translation</li> <li>\\(k=\\) Intrinsic matrix</li> <li>\\(F =\\) focal length in world units, usually in mm</li> <li>\\((p_x, p_y) =\\) size of pixel in world units</li> <li>\\(f_x, f_y = \\dfrac{F}{p_x}, \\dfrac{F}{p_y} =\\) focal length in pixels</li> <li>\\(\\begin{bmatrix}c_x &amp; c_y \\end{bmatrix}=\\) optical center (principal point) in pixels</li> <li>\\(s = f_x \\tan \\alpha\\) skew coefficient</li> <li>non-zero if image axes are not perpendicular</li> <li>1 pixel \\(\\approx 1/96 \\text{inch}\\)</li> </ul> <p></p>"},{"location":"CS_Electives/AI_in_Robotics/10_Computer_Vision/#distortion","title":"Distortion","text":"Barrel(Radial) Pincushion(Radial) Tangential/Decentering Meaning Straight lines of actual world appear curved outward Straight lines of actual world appear curved inward Image appears to slanted &amp; stretched -ve radial displacement +ve radial displacement Cause Unequal light bending(rays bent more at lens\u2019 border than at center)Before hitting the image sensor, the light ray is shifted radial inward/outward from its optimal point Unequal light bending(rays bent more at lens\u2019 border than at center)Before hitting the image sensor, the light ray is shifted radial inward/outward from its optimal point Picture screen/sensor is at an angle wrt lens Example"},{"location":"CS_Electives/AI_in_Robotics/10_Computer_Vision/#radial-distortion-coefficients-model","title":"Radial distortion coefficients model","text":"\\[ \\begin{aligned} x_\\text{distorted} &amp;= x (1 + k_1 r^2 + k_2 r^4 + k_3 * r^6) \\\\ y_\\text{distorted} &amp;= y (1 + k_1 r^2 + k_2 r^4 + k_3 * r^6) \\end{aligned} \\] <p>where</p> <ul> <li>\\(x_\\text{distorted}, y_\\text{distorted}\\) are distorted points</li> <li>\\(x, y\\) are undistorted and dimensionless pixel location in normalized image coordinates format, calculated from pixel coordinates by translating to optical center and dividing by focal length in pixels</li> <li>\\(k_1, k_2, k_3\\) are radial distortion coefficients of the lens</li> <li>\\(r^2 = x^2 + y^2\\)</li> </ul>"},{"location":"CS_Electives/AI_in_Robotics/10_Computer_Vision/#tangential-distortion-coefficients-model","title":"Tangential distortion coefficients model","text":"\\[ \\begin{aligned} x_\\text{distorted} &amp;= x + [2 p_1 x y + p_2(r^2 + 2x^2)] \\\\ y_\\text{distorted} &amp;= y + [2 p_1 (r^2 + 2y^2) + 2 p_2 x y] \\end{aligned} \\]"},{"location":"CS_Electives/AI_in_Robotics/10_Computer_Vision/#distortion-removal","title":"Distortion Removal","text":"<ol> <li>Calibrate camera and obtain intrinsic camera parameters</li> <li>Control percentage of undersized pixels in undistorted image by fine-tuning camera matrix</li> <li>Use revised camera matrix to remove distortion from image</li> </ol>"},{"location":"CS_Electives/API/","title":"Index","text":""},{"location":"CS_Electives/API/#apis","title":"APIs","text":"<p>Application Programming Interface</p>"},{"location":"CS_Electives/API/#architecture-style","title":"Architecture Style","text":"Best Usage Communication Protocol Data Format Analogy Security RPCRemote Procedure Calls SOAP Legacy systemsEnterprise integrations LetterSend, stamp High REST Simple PublicWeb-ServicesSimple JSON XML Postcard Low GraphQL Schema Controllable Complex dataPersonalized UXFlexible data access Own querying language gRPC PerformanceLanguage-Agnostic RealtimeAppsStreaming dataMicro-servicesHigh performance <p>Previously, SOAP was the most secure. However, now all types can be encrypted using API tools.</p>"},{"location":"CS_Electives/Blockchain/","title":"Blockchain","text":""},{"location":"CS_Electives/Blockchain/#referencs","title":"Referencs","text":"<ul> <li> Blockchain and Cryptocurrency | Stanford</li> </ul>"},{"location":"CS_Electives/CSE/","title":"Computational Science &amp; Engineering","text":""},{"location":"CS_Electives/CSE/#references","title":"References","text":"<ul> <li> Physics-Informed Neural Networks (PINNs)</li> <li> Physics Informed Machine Learning | Steve Brunton</li> <li> MIT OCW | Parallel Computing and Scientific Machine Learning</li> </ul>"},{"location":"CS_Electives/CSE/01/","title":"01","text":""},{"location":"CS_Electives/CSE/01/#typical-scientific-tasks","title":"Typical Scientific Tasks","text":""},{"location":"CS_Electives/CSE/01/#pinn","title":"PINN","text":"<p>Physics-Informed Neural Networks</p> <p></p>"},{"location":"CS_Electives/CSE/01/#digital-twin","title":"Digital Twin","text":""},{"location":"CS_Electives/CSE/01/#physics-properties-preferred-in-machine-learning","title":"Physics\u2019 Properties Preferred in Machine Learning","text":"<ul> <li> <p>Interpretability</p> </li> <li> <p>Generalizability</p> </li> <li> <p>Parsimony/Simplicity</p> </li> <li> <p>Symmetries/Conservations</p> </li> <li> <p>Invariance: \\((f \\circ g)(x) = f(x)\\)</p> </li> <li>Equivariance: \\((f \\circ g)(x) = (g \\circ f)(x)\\)</li> </ul> <p>where</p> <ul> <li>\\(f\\)\u00a0is deep learning model</li> <li>\\(g\\) is a transformation such as rotation</li> </ul>"},{"location":"CS_Electives/Causal_Inference/","title":"Causal Inference","text":""},{"location":"CS_Electives/Causal_Inference/#references","title":"References","text":"<ul> <li> Modern Data Analysis for Economics</li> <li> Statistical Rethinking | Richard McElreath</li> <li> Using Big Data to Solve Economic and Social Problems | Harvard</li> <li> Causality | Shaw Talebi</li> <li> The Effect: An Introduction to Research Design and Causality</li> <li> Program Evaluation for Public Service</li> <li> Videos</li> <li> <p> Material</p> </li> <li> <p> Prediction in Causal Research | Galit Shmueli</p> </li> <li>https://youtu.be/JQl0CeiMA5w</li> <li>https://youtu.be/8L26hmMYTRU</li> <li> Causal Inference with Applications | Harvard</li> <li> Videos</li> <li> <p> Material</p> </li> <li> <p> Causal Inference Bootcamp: Introduction to Causality | Mod\u2022U: Powerful Concepts in Social Science</p> </li> <li> Causal Inference | Intuitive MetriX \u2013 Ben Elsner</li> <li> Causal Inference | Jonas Peters</li> <li> Causal Inference Course   | Brady Neal</li> <li> Small Episodes</li> <li> <p> Large Chunks</p> </li> <li> <p> Machine Learning &amp; Causal Inference: A Short Course | Stanford Graduate School of Business</p> </li> <li> <p> Advanced Econometrics | University of Massachusetts Amherst | Matt Woerman</p> </li> </ul>"},{"location":"CS_Electives/Causal_Inference/01_Introduction/","title":"Introduction","text":""},{"location":"CS_Electives/Causal_Inference/01_Introduction/#econometric-analysis","title":"Econometric Analysis","text":"<p>Machine Learning \\(\\to\\) Statistics \\(\\to\\) Econometrics</p> <p>The focus is on using the predictions of Machine Learning to analyse future causalities. This is more insight-oriented, as identifying patterns without understanding causes and implications is useless.</p>"},{"location":"CS_Electives/Causal_Inference/01_Introduction/#statistical-vs-causal","title":"Statistical vs Causal","text":"Statistical Prediction Causal Prediction \\(\\hat y\\) \\(E \\big [y \\vert x = a \\big]\\) \\(E \\big [y \\vert \\text{do}(x = a) \\big]\\) What will be \\(y\\) if I ___\\(x=a\\)? observe set characteristic natural outcome manual action statement if \\(x\\) is correlated with \\(y\\) without any causal effect on \\(y\\), then we can only observe the correlation without the ability to change \\(y\\) by changing \\(x\\) if \\(x\\) has a causal effect on \\(y\\), then we can change \\(x\\) and expect it to cause a change in \\(y\\). True understanding enables predictions under a wide range of circumstances, including new hypothetical situations Unstable Stable Example 1 What is the expected health status of someone who has received hospitalization? What will the health status of a person if they receive hospitalization? Example 2 What is the expected sales of a company with a given amount of TV ad spending? How much will my sales increase if I increase my TV ad spending by a certain amount?"},{"location":"CS_Electives/Causal_Inference/01_Introduction/#see-vs-do","title":"see vs do","text":"<ul> <li>\\(\\text{see}(x= 1)\\) means that you observe \\(x\\) as 1</li> <li>\\(\\text{do}(x= 1)\\) means that you manually perform some action to set \\(x\\) as 1</li> </ul> <p>If there is no way to manipulate a variable (for eg, \\(\\text{do}(x= 1)\\) is not possible), then it is hard to define what its causal effect means</p> <ul> <li>A thought experiment that is often used to determine whether a variable x is manipulable in principle is to imagine a hypothetical experiment that assigns different values to x</li> <li>Research questions that cannot be answered by any experiment are FUQ\u2019d: Fundamentally Unidentified Questions</li> </ul>"},{"location":"CS_Electives/Causal_Inference/01_Introduction/#correlation-centernot-implies-causation","title":"Correlation \\(\\centernot \\implies\\) Causation","text":"<p>Correlation doesn\u2019t always imply causation. correlation is useful for prediction, but not for understanding exactly why.</p> <p>For eg, labor wage and their years of education has a strong correlation, but the reason for that could be</p> <ul> <li>education actually helps</li> <li>education just acts as the signal/proof for the employees that you possess knowledge</li> <li>or, it could just be that well-off people get better jobs, and coincidentally, they are getting more educated</li> </ul>"},{"location":"CS_Electives/Causal_Inference/01_Introduction/#rain","title":"Rain","text":"<p>We can now conclude that the barometer reading itself has no causal effect on rainfall. It is infact the atmospheric pressure that has causal effect on the rainfall.</p>"},{"location":"CS_Electives/Causal_Inference/01_Introduction/#hospital","title":"Hospital","text":"<p>Let\u2019s say there are 2 hospitals</p> A B Recovery Rate 0.6 0.4 <p>It seems like A is better than B. But A may not necessarily be the better hospital for me to go to. What if A is a regular community hospital and B is a speciality hospital for cancer patients. Obviously, A will have a better recovery rate.  So, statistical prediction (is not wrong) is accurate saying that recovery rate for a patient going to hospital A is 0.6, because we are seeing the patient going there; the patient that goes there is a patient who chose to go there. If they were a serious patient, then they would\u2019ve gone to B.</p>"},{"location":"CS_Electives/Causal_Inference/01_Introduction/#ad-sales","title":"Ad-Sales","text":"<p>Sales and Advertisement have a high correlation. But doesn\u2019t necessarily mean that Increasing advertisement will increase sales. This is because, the observed advertisment could be due to previous sales. So the observed advertisment here is a natural outcome, not an active decision; ie, last year i got high sales profits, so i increased my advertisment budget this year, or i got bad profit so i increased my ad budget to somehow increase the sales. it\u2019s like a reaction, not an active decision.</p> <p>However, when you actively decide to increase the ad budget, the change in sales depends on the causal effect of ad budget on sales. Why don\u2019t companies do this causal analysis???</p> <ol> <li>they don\u2019t know their math and stats </li> <li>causal analysis is harder</li> </ol>"},{"location":"CS_Electives/Causal_Inference/01_Introduction/#scope","title":"Scope","text":"<p>Scope is the set of populations in which a causal effect applies. A causal effect is only meaningful if we can define its scope.</p>"},{"location":"CS_Electives/Causal_Inference/01_Introduction/#populations","title":"Populations","text":"Study Population Target Population Population where experiment/observational study is conducted we\u2019re interested in learning the causal effect Sample specific diverse Population specific diverse Social, cultural and economic environment specific diverse <p>The causal effect of \\(x\\) on \\(y\\) can differ in 2 populations because:</p> <ol> <li>Causal mechanism is different in both populations</li> </ol> <p>eg: Consider a country where oil prices are determined by market, and another country where prices are determined by govt. The effect of decreasing oil supply on gas price will be different </p> <ol> <li>Distribution of effect modifier \\(P(s)\\) is different</li> </ol> <p>Eg: Consider 2 countries with the same economic structure, but different population age structures. The effect of raising retirement age will be completely different.</p>"},{"location":"CS_Electives/Causal_Inference/01_Introduction/#properties-of-results","title":"Properties of Results","text":""},{"location":"CS_Electives/Causal_Inference/01_Introduction/#validity","title":"Validity","text":"Internal External Results valid for study population Other populations Transportability of results \u274c \u2705"},{"location":"CS_Electives/Causal_Inference/01_Introduction/#transportability","title":"Transportability","text":"<p>The ability of the result can be generalized/extrapolated correctly from one population to another.</p> <p>A causal effect learnt from a study is transportable from study population to target population if both are within the scope.</p>"},{"location":"CS_Electives/Causal_Inference/02_Causal_Learning/","title":"Causal Learning","text":"<pre><code>flowchart LR\nt[Theory] --&gt; Model --&gt; Evidence --&gt; t</code></pre>"},{"location":"CS_Electives/Causal_Inference/02_Causal_Learning/#types","title":"Types","text":"Causal Effect Learning Causal Mechanism Learning Causal Inference Learning Does \\(x\\) have a causal effect on y? If yes, how large is the effect If causal effect exists, what is the mechanism behind it? Understand rational decisions that can be taken, built on causal mechanism learning and prior causal inference - What- How much - Why- How - What can we do? - discovering patterns- making predictions - understanding - decison-making \u201cEffects of causes\u201d \u201cCauses of effects\u201d"},{"location":"CS_Electives/Causal_Inference/02_Causal_Learning/#manipulation-of-x","title":"Manipulation of \\(x\\)","text":"<p>Being able to manipulate \\(x\\) to see its effect on \\(y\\) is essential to understanding causality. If there is no way to manipulate \\(x\\), then it is difficult to understand causality. </p> <p>Morever, according to the instructor, it is pointless to causal inference as if we cannot change it (even theoretically), then we can\u2019t really make better decisions, you know? So many questions we analyze when doing research is basically useless.</p> <p>For eg, analyzing \u201cwhat is the causal effect of height on your income\u201d. This is kinda pointless, because it\u2019s not like we can change our height. Atleast \u201cwhat is the causal effect of democracy on economic growth\u201d is an acceptable analysic, because theoretically we can change the democracy level.</p> <p>I have an example. Analysing the \u2018causal effect of unemployment on economic growth\u2019 is not very useful, because even though we can hypothetically manipulate unemployment indirectly, we can\u2019t exactly control it directly.</p>"},{"location":"CS_Electives/Causal_Inference/02_Causal_Learning/#type-of-manipulation","title":"Type of Manipulation","text":"<p>The mechanism with which you \u2018do\u2019 \\(x\\) will have different results. Hence, it is important to have a clear mechanism for \u2018do\u2019-ing \\(x\\) before starting your analysis.</p> <p>For eg, for the theoretical democracy example, are you going to forcefully implement a democracy? or will the citizens peacefully request?</p>"},{"location":"CS_Electives/Causal_Inference/02_Causal_Learning/#experimentational-causal-analysis","title":"Experimentational Causal Analysis","text":"<p>once the experiment is over, the correlation is mathematically equal to the causation</p>"},{"location":"CS_Electives/Causal_Inference/02_Causal_Learning/#steps","title":"Steps","text":"<ol> <li>manually set \\(x=1\\)</li> <li>observe the value of \\(y\\)</li> <li>repeat</li> <li>take average value of y</li> </ol>"},{"location":"CS_Electives/Causal_Inference/02_Causal_Learning/#disadvantages","title":"Disadvantages","text":"<ol> <li>not always feasible (especially in economics), and it is not possible to perform the experiment</li> <li>everyone is different, the experiment might not give an accurate inference</li> </ol>"},{"location":"CS_Electives/Causal_Inference/02_Causal_Learning/#example","title":"Example","text":"<p>RCT (Randomized Control Testing)</p> <ul> <li>test group is do(x=1) - taking drug</li> <li>control group is do(x=0) - not taking drug</li> </ul>"},{"location":"CS_Electives/Causal_Inference/02_Causal_Learning/#causal-inference-in-ai","title":"Causal Inference in AI","text":"<ol> <li>how should a robot acquire causual information through interaction with its environment</li> <li>how should a robot receive causal information from humans</li> </ol> <p>According to the lecturer, a lot of modern-day AI is not \u2018intelligence\u2019. Just because the algorithm can recognize images by trained data is not exactly \u2018intelligence\u2019.</p> <p>True hallmark of intelligence is the ability to make causal inference, from looking at statistical patterns.</p>"},{"location":"CS_Electives/Causal_Inference/02_Causal_Learning/#causal-inference-models","title":"Causal Inference Models","text":"<p>There are 2 types of models</p> <ol> <li>Rubin Model</li> <li>Judea Pearl Model    The instructor says that this is better, in his opinion</li> </ol>"},{"location":"CS_Electives/Causal_Inference/02_Causal_Learning/#identifiability","title":"Identifiability","text":"<p>\\(\\theta(M)\\) is if it can be uniquely determined based on observations of \\(v\\).</p> <p>I didn\u2019t really understand this.</p>"},{"location":"CS_Electives/Causal_Inference/02_Causal_Learning/#idk","title":"IDK","text":"<p>Requires prior knowledge regarding the data-generating causal mechanism.</p> <p>Such knowledge can only exist as a result of previously-observed information and conducted studies.</p> <p>Hence, causal inference builds on past causal inference</p>"},{"location":"CS_Electives/Causal_Inference/02_Causal_Learning/#source-of-associations","title":"Source of Associations","text":"<p>Reasons why \\(x\\) and \\(y\\) can be associated</p> <ul> <li>\\(x\\) causes \\(y\\) directly</li> <li>\\(x\\) causes \\(y\\) indirectly</li> <li>\\(x\\) and \\(y\\) have common cause(s)</li> <li>Analysis is conditioned on their common descendant(s)</li> </ul>"},{"location":"CS_Electives/Causal_Inference/02_Causal_Learning/#importance-of-causal-learning","title":"Importance of Causal Learning","text":""},{"location":"CS_Electives/Causal_Inference/02_Causal_Learning/#russels-chicken","title":"Russel\u2019s Chicken","text":"<p>This short story shows how pure reliance on past data is bad.</p> <p>The chicken assumes that whenever the farmer comes, it is to feed it. However, there will one day, the farmer comes to kill it.</p> <p>Hence, the lack of understanding why something happens might be very dangerous.</p>"},{"location":"CS_Electives/Causal_Inference/02_Causal_Learning/#2008-us-financial-crisis","title":"2008 US Financial Crisis","text":"<p>Default prediction was based on the historical data, in which housing prices were always rising</p> <p>However, this time, the house pricing were going down</p>"},{"location":"CS_Electives/Causal_Inference/02_Causal_Learning/#simpsons-paradox","title":"Simpson\u2019s Paradox","text":"<p>This paradox looks at the effectiveness of a drug.</p> <p>https://youtu.be/ebEkn-BiW5k</p> <p>Aggregate Reversal</p> <p>For example, in this study, the composition makes a difference, ie</p> <ul> <li>in the \u2018drug\u2019 group, there are more women than men</li> <li>in the \u2018no drug\u2019 group, there are more men than women</li> </ul> <p>This disparity will give an incorrect understanding</p> <p>Moreover, for this particular disease, women have a lower recovery rate than men. That should be taken into account as well.</p> <p>Let\u2019s take another example. Consider a simple example with 5 cats and 5 humans. Let 1 cat and 4 humans be given the drug. Now, the values in the table show the recovery rate.</p> Drug No Drug Cat \\(1/1 = 100\\%\\) \\(3/4 = 75\\%\\) Human \\(1/4 = 25\\%\\) \\(0/1 = 0\\%\\) Overall \\(2/5 = 40\\%\\) \\(3/5 = 60\\%\\) <p>If we look at individual groups, cats are better off with drugs, and so are the humans.</p> <p>However, when we look at overall we can see that the population as a whole is better without the drugs.</p>"},{"location":"CS_Electives/Causal_Inference/02_Causal_Learning/#us-political-support","title":"US Political Support","text":"<p>Similar to Simpson\u2019s Paradox</p> <p>Aggregate Reversal</p> Level Richer you are, more likely to be a __ Reason Individual Republican Republican individuals are richer and want lower taxes State Democrat Richer societies are usually morally \u2018modern\u2019; Poorer one are usually conservative and religiousDemocrats have more \u2018modern\u2019 policies"},{"location":"CS_Electives/Causal_Inference/02_Causal_Learning/#sampling-bias","title":"Sampling Bias","text":""},{"location":"CS_Electives/Causal_Inference/02_Causal_Learning/#sample-selection-bias","title":"Sample-selection bias","text":"<p>Type of sampling bias that arises when we make inference about a larger population from a sample that is drawn from a distinct subpopulation</p> <p>Sample-selection bias can be thought of as a missing data problem, where data are NMAR (not missing at random)</p>"},{"location":"CS_Electives/Causal_Inference/02_Causal_Learning/#survivorshipsurvival-bias","title":"Survivorship/Survival Bias","text":"<p>Special type of sample-selection bias</p>"},{"location":"CS_Electives/Causal_Inference/02_Causal_Learning/#mutual-fund-performance","title":"Mutual Fund Performance","text":"<p>Suppose we are interested in how the size of assets under management affects a fund\u2019s performance. If we simply look at the relationship between fund size and returns among existing funds, however, there will be what is referred to as a survival bias: we do not observe funds that have closed due to bad performance.</p> <p>So if fund size negatively affects performance, we may end up under-estimating the magnitude of the effect.</p>"},{"location":"CS_Electives/Causal_Inference/02_Causal_Learning/#planes-in-war","title":"Planes in war","text":"<p>The planes that returned from war had lot of spots with bullet shots.</p> <p>Some person suggested strengthening only those spots. Initially, that makes sense - these are the areas that got shot so we need to strengthen. But, that is wrong.</p> <p>Another person said that these are the planes that returned despite getting shot at these spots. That means that we have to focus on other places, because the planes that got shot there never returned.</p> <p>Clearly data can be misleading, without understanding the underlying cause</p>"},{"location":"CS_Electives/Causal_Inference/02_Causal_Learning/#wages","title":"Wages","text":""},{"location":"CS_Electives/Causal_Inference/02_Causal_Learning/#credit-card-default","title":"Credit card default","text":"<p>We cannot use the relationship between Income, balance, and default status for credit card holders to predict default rate for a random credit card applicant, since these people part of the available data have been filtered already as potentially good credit card users</p> <p>Hence, we can only use it predict default for a random person already having a credit card</p> <p>This is a case of censoring</p>"},{"location":"CS_Electives/Causal_Inference/02_Causal_Learning/#success-stories","title":"Success Stories","text":"<p>Advice by someone successful</p>"},{"location":"CS_Electives/Causal_Inference/02_Causal_Learning/#_1","title":"Causal Learning","text":""},{"location":"CS_Electives/Causal_Inference/02_Causal_Learning/#aggregate-reversal","title":"Aggregate Reversal","text":"<p>Any statistical relationship between two variables may be reversed by including additional factors in the analysis</p> <p>If you just look at statistical data, it might be misleading.</p> <p>Once we devide the population into sub-population based on categories such as sex, then it becomes clearer. This is because why try understanding the underlying mechanism. This phenomenon is called as aggregate reversal.</p>"},{"location":"CS_Electives/Causal_Inference/03_Treatment_Effects/","title":"Treatment Effect","text":"\\[ \\begin{aligned} E(y \\vert \\text{do}(x), s) &amp;= E(y \\vert x, s) \\\\ \\implies \\widehat{\\text{ATE}}(x, s) &amp;=\\dfrac{d}{dx} \\hat E[y \\vert \\text{do}(x)] \\\\ &amp;=\\dfrac{d}{dx} \\hat E[y \\vert \\text{do}(x), s] \\\\ &amp;= \\dfrac{d}{dx} E_s \\Big[ \\hat E[y \\vert \\text{do}(x), s] \\Big] \\\\ &amp;= \\dfrac{d}{dx} E_s \\Big[ \\hat E[y \\vert x, s] \\Big] \\\\ &amp;= E_s \\left[ \\dfrac{\\partial}{\\partial x} \\hat E[y \\vert x, s] \\right] \\end{aligned} \\] <p>HTE = Heterogeneous Treatment Effect = ATE with high dimensional \\(s\\) </p> <p>If \\(x\\) is binary $$ \\begin{aligned} x &amp;\\in { 0, 1 } \\ \\implies \\text{ATE}(x, s) &amp;= E[ y \\vert \\text{do}(x=1) ]  -  E[ y \\vert \\text{do}(x=0) ] \\ &amp;= E_s \\Bigg[ E[y \\vert x=1, s ]   -  E[ y \\vert x=0, s] \\Bigg] \\ \\end{aligned} $$</p> Treatment Model \\(\\widehat{\\text{ATE}}(x)\\) Binary Linear \\(\\hat \\beta_0 + \\hat \\beta_1 x + \\hat \\beta_2 s\\) Constant \\(\\hat \\beta_1\\) Multi-Level/Continuous Non-linear Functional"},{"location":"CS_Electives/Causal_Inference/03_Treatment_Effects/#heterogeneous-treatment-effects","title":"Heterogeneous Treatment Effects","text":"<p>If we consider effect modifier \\(s\\) $$ \\begin{aligned} \\text{ATE} &amp;= \\dfrac{\\partial}{\\partial x} E[ y \\vert \\text{do}(x), s] \\ &amp;= \\int \\limits_s E[y^1 - y^0  |  s] \\cdot P(s) \\cdot ds \\ \\end{aligned} $$</p> <p>where \\(P(s)\\) is the distribution of effect modifier.</p> <p>Then, the result of the randomized test actually gives us \\(E[\\tilde \\tau]\\), which may not be equal to \\(E[\\tau]\\)</p>"},{"location":"CS_Electives/Causal_Inference/03_Treatment_Effects/#example","title":"Example","text":"<p>For example, the yield depends on the season and the crop for which was grown previous year. Let\u2019s take the example of a a randomized test of a fertilizer used  in the summer.</p> <p>If no crop was grown the previous year, then we </p> Crop Grown in Field Last Year Result of Randomized Test \\(E[\\tilde \\tau]\\) No crop $E[\\tau Rice only $E[\\tau 50% Barley, 50% Rice $0.5 \\cdot E[\\tau"},{"location":"CS_Electives/Causal_Inference/03_Treatment_Effects/#regression-discontinuity-design","title":"Regression Discontinuity Design","text":"<p>Causal effect cannot be obtained directly due to lack of overlap for different \\(s\\), hence we take the neighborhood</p> <p>When a quasi-experiment partially determines the treatment status, the \u201cas if\u201d randomness can be used as an instrument for identifying the causal effect of interest</p> \\[ \\begin{aligned} \\widehat {\\text{LATE}} &amp;= \\lim_{z \\to {z_0}^+} E[ y \\vert x=1, z ] \\ - \\lim_{z \\to {z_0}^-} E[ y \\vert x=0, z ] \\\\ &amp;= \\lim_{z \\to {z_0}^+} E[ y \\vert z ] \\ - \\lim_{z \\to {z_0}^-} E[ y \\vert z ] \\end{aligned} \\] <p>Eg: Evaluating the treatment effect of college on students in the neighborhood of college acceptance cutoff, where the selection of students is random</p> <p>Limitations</p> <ul> <li>Poor generalizability: The validity of the results is usually restricted to this region</li> <li>Throws away the lot of information in the non-random parts</li> <li>Doesn\u2019t allow building structural causal model</li> </ul>"},{"location":"CS_Electives/Causal_Inference/03_Treatment_Effects/#differences-in-differences","title":"Differences-in-Differences","text":"<p>Let</p> <ul> <li>Control: \\(y_0\\) be the time series with \\(x=0\\)</li> <li>Treated: \\(y_1\\) be the time series with \\(x=1\\)</li> <li>\\(D_t\\) be the difference of the 2 series</li> </ul> \\[ \\begin{aligned} y_{0t} &amp;= f(w, t) + \\beta_1 (x=0) \\\\ &amp;= f(w, t) \\\\ y_{1t} &amp;= f(w, t) + \\beta_1 (x=1) \\\\ D_t &amp;= (y_1 - y_0)_t \\end{aligned} \\] <p>Assumptions</p> <ul> <li>parallel trends: confirmed by evaluating regions without the treatment</li> <li>absence treatment: no other variables</li> <li>Hence, the difference between the   treatment &amp; the control group is time-invariant, any difference in their difference   must be due to the treatment effect.</li> </ul> <p>$$ \\begin{aligned} E[\\Delta D] &amp;=  \\Big( E[y_{1  t}] - E[y_{0  t}] \\Big) - \\Big( E[y_{1  t_0}] - E[y_{0  t_0}] \\Big) \\ &amp;= \\hat \\beta_1(x=1) \\ &amp;= E(y \\vert x=1) - E(y \\vert x=1) \\ \\implies \\text{ATE}(x) &amp;\\approx \\text{ATT}(x) \\ &amp;= E[\\Delta D] \\end{aligned} $$ Technically, \\(\\hat \\beta_1\\) is an ATT because the parallel trend assumption assumes what the treated series would be like in the absence of the treatment, not what the control series would be like given the treatment</p> <p>Didn\u2019t understand</p> <p>Technically, \u03b1 is an ATT because the parallel trend assumption assumes what the treated cities would be like in the absence of the program, not what the control cities would be like given the program</p> <p></p> <p></p> <p></p>"},{"location":"CS_Electives/Causal_Inference/04_Discrete_Choice_Models/","title":"Discrete Choice Models","text":"<p>Discrete choice models are a class of econometric models of how individuals make choices, where \u201cindividuals\u201d is any unit of decision making, such as people, firms, governments</p> <p>These models are similar to a classification problem, but they are structural models of decision making based on utility maximization. Hence, they do not make the assumption of IIA and can handle it effectively.</p> <p>The ultimate goal of the researcher is to represent utility so well that the assumption of error independence is appropriate and then use Logistic regression incorporating all important features. In the absence of that, a discrete choice model that allows for correlated errors, such as the multinomial probit, can be used</p>"},{"location":"CS_Electives/Causal_Inference/04_Discrete_Choice_Models/#rum-framework","title":"RUM Framework","text":"<p>Random Utility Maximization</p>"},{"location":"CS_Electives/Causal_Inference/04_Discrete_Choice_Models/#problem-formalization","title":"Problem Formalization","text":"<p>Consider</p> <ul> <li>Individual \\(i\\) chooses \\(y\\) among \\(J\\) alternatives</li> <li>\\(x_{ij}\\) is the observed characteristics associated with individual \\(i\\) and alternative \\(k\\)</li> <li>\\(s_i:\\) Individual-specific factors (eg: income)<ul> <li>\\(r_j s_i\\)</li> </ul> </li> <li>\\(z_{ij}:\\) Alternative-specific factors with generic coefficients (eg: price)<ul> <li>\\(\\beta z_{ij}\\)</li> </ul> </li> <li>\\(w_{ij}:\\) Alternative-specific factors with alternative-specific coefficients (eg: price)<ul> <li>\\(\\alpha_j w_{ij}\\)</li> </ul> </li> <li>\\(u_{ij}\\) is the unobserved utility associated with alternative \\(j\\) for individual \\(i\\), that even the individual is not aware about</li> </ul> \\[ \\begin{aligned} u_{ij} &amp;= f_j(x_{ij}) + \\epsilon_{ij} \\\\ f_j &amp;= \\beta_j x_{ij} &amp;&amp; \\text{(Simple Model)} \\end{aligned} \\] <p>where \\(\\epsilon_{ij}\\) is the effect of unobserved factors, such that \\(\\epsilon \\sim^\\text{iid} F_e\\); different specifications of \\(f_j\\) and \\(F_e\\) lead to different discrete choice models</p> <p>Example: Temperature and Rainfall \\(x_{ij}\\) affects which crop \\(u_{ij}\\) is grown in each place \\(j\\)</p>"},{"location":"CS_Electives/Causal_Inference/04_Discrete_Choice_Models/#assumption","title":"Assumption","text":"<ul> <li>Individual knows their \\(u_{ij}\\)</li> <li>Individual\u2019s decision is deterministic</li> </ul>"},{"location":"CS_Electives/Causal_Inference/04_Discrete_Choice_Models/#choice","title":"Choice","text":"<p>A rational individual chooses the alternative that maximizes the utility $$ \\begin{aligned} u_i &amp;= \\max( { u_{ij} } ) \\ \\implies y_i &amp;= \\arg \\max_{j} { u_{ij} } \\end{aligned} $$</p> \\[ \\begin{aligned} P(y_i = k \\vert x) &amp;= P( u_{ik} &gt; u_{ij} ) &amp;&amp; \\forall j \\ne k \\in J \\\\ &amp;= P \\Big(     f_j - f_k &gt; \\epsilon_{ij} - \\epsilon_{ik} \\Big) \\\\ &amp;= F \\Big( f_j - f_k \\Big) &amp;&amp; (\\epsilon_{ij} - \\epsilon_{ik} \\sim F) \\end{aligned} \\] <p>where \\(P=\\) CCP (Conditional Choice Probability)</p>"},{"location":"CS_Electives/Causal_Inference/04_Discrete_Choice_Models/#features","title":"Features","text":"<ul> <li>Only differences in utility matter; the absolute level of utility is irrelevant</li> <li>Hence, if a constant is added to the utility of all alternatives, then the alternative with the highest utility does not change</li> <li>The overall scale of utility is irrelevant</li> <li>Hence, if a positive scale is multiplied to the utility of all alternatives, then the alternative with the highest utility does not change</li> </ul>"},{"location":"CS_Electives/Causal_Inference/04_Discrete_Choice_Models/#advantages","title":"Advantages","text":"<ul> <li>Better interpretability</li> <li>Structural model</li> </ul>"},{"location":"CS_Electives/Causal_Inference/04_Discrete_Choice_Models/#limitations","title":"Limitations","text":"<ul> <li>Since \\(u_{ij}\\) is unobserved, we can only calculate the probability of individual choosing each alternative conditional on the variables we observe</li> <li>Due to the features, we cannot learn the level of utility associated with different alternatives, only the scaled differences among them</li> <li>We cannot estimate the intercept and scale associated with \\(s_i\\) for each utility</li> <li>We can only estimate the difference of the above between 2 utilities </li> </ul>"},{"location":"CS_Electives/Causal_Inference/04_Discrete_Choice_Models/#estimation","title":"Estimation","text":"<ol> <li> <p>We need to normalize and scale such that \\(u_{ia} = 0\\)</p> </li> <li> <p>Subtract all terms by \\(u_{ia}\\)</p> </li> <li>Divide all terms to make \\(\\epsilon_{i} \\sim N(0, 1)\\)</li> </ol> <p>Reason: The parameters \\(\\mu_a, \\mu_b, \\sigma_a, \\sigma_b\\) are not separately identifiable, because an infinite number of models (corresponding to different values of \\(\\alpha\\) and \\(\\gamma\\)) are consistent with the same choice behavior</p> <ol> <li>As long as there is an intercept term \\(\\alpha_j\\), alternative-specific variables \\(z_{ij}\\) must vary with i in order to be identified</li> </ol> <p>Reason: Else, both will be constants and hence cannot be separately identified</p> <ol> <li>The scale coefficients \\(\\alpha\\) of individual-specific variables must be alternative-specific in order to be identified.</li> </ol> <p>Reason: Since only difference in utility matters, \\(\\alpha\\) cannot be identified</p> <ol> <li>Alternative-specific variables can have either alternative-specific coefficients or generic coefficients that do not change with alternatives</li> </ol> <p>Consider a binary choice problem \\(y \\in \\{ a, b \\}\\) $$ \\begin{aligned} u_{ia} &amp;= \\mu_a + \\epsilon_{ia} \\ u_{ib} &amp;= \\mu_a + \\epsilon_{ib} \\ \\end{aligned} $$ Estimate \\(\\Delta \\tilde \\mu_b = \\alpha(\\mu_b - \\mu_a)\\), which is the scaled difference between \\(\\mu_a\\) and \\(\\mu_b\\), by normalize the level and scale of utility.</p> <p></p>"},{"location":"CS_Electives/Causal_Inference/04_Discrete_Choice_Models/#probit-regression","title":"Probit Regression","text":"<p>Assumes that \\(\\epsilon\\) is a joint-normal distribution $$ \\epsilon_i \\sim N(0, \\Sigma) $$ where the covariance matrix \\(\\Sigma\\) uses the \u201cbase class\u201d as reference</p> <p>A model with \\(J\\) alternatives has \\(\\le \\dfrac{1}{2} J(J-1) - 1\\) covariance parameters after normalization, which can be evaluated using the below methods</p>"},{"location":"CS_Electives/Causal_Inference/04_Discrete_Choice_Models/#binary","title":"Binary","text":"\\[ \\begin{aligned} \\epsilon_i &amp;= \\begin{bmatrix} \\epsilon_{ia} \\\\ \\epsilon_{ib} \\end{bmatrix} \\\\ &amp; \\sim N \\left( 0, \\begin{bmatrix} \\sigma_a^2 &amp; \\sigma_{ab} \\\\ \\cdot &amp; \\sigma_b^2 \\end{bmatrix} \\right) \\end{aligned} \\] \\[ \\epsilon_{ia} - \\epsilon_{ib} \\sim N(0, \\sigma_a^2 + \\sigma_b^2 - 2 \\sigma_{ab}) \\\\ \\alpha = u_{ia} \\\\ \\lambda = \\dfrac{1}{ \\sqrt{\\sigma_a^2 + \\sigma_b^2 - 2 \\sigma_{ab}} } \\]"},{"location":"CS_Electives/Causal_Inference/04_Discrete_Choice_Models/#multinomial","title":"Multinomial","text":"\\[ \\begin{aligned} \\epsilon_i &amp;= \\begin{bmatrix} \\epsilon_{i1} \\\\ \\epsilon_{i2} \\\\ \\dots \\\\ \\epsilon_{ij} \\end{bmatrix} \\\\ &amp; \\sim N \\left( 0, \\begin{bmatrix} \\sigma^2_1 &amp; \\sigma_{12} &amp; \\dots &amp;  \\sigma_{1j} \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; \\dots  &amp; \\sigma^2_j \\end{bmatrix} \\right) \\end{aligned} \\] <p>Multinomial probit models do not have the IIA property as they allow correlated errors</p>"},{"location":"CS_Electives/Causal_Inference/04_Discrete_Choice_Models/#logistic-regression","title":"Logistic Regression","text":"<p>Assumes that \\(\\epsilon\\) is iid $$ \\epsilon_{ij} \\sim^\\text{iid} \\text{Gumbel}(0, \\sigma) $$ The difference between two extreme values is distributed as a logistic distribution $$ F(\\Delta) = \\dfrac{\\exp(\\Delta e)}{1 + \\exp(\\Delta e)} $$ The CDF of the logistic distribution is the sigmoid function</p> <p>We need to normalize the scale of \\(\\epsilon_i\\) such that \\(\\sigma=1\\) $$ \\begin{aligned} \\implies \\epsilon_{ij}' &amp; \\sim \\text{Gumbel}(0, 1) \\ &amp; \\sim N(0, 1) \\end{aligned} $$ As Gumbel and normal very similar $$ \\begin{aligned} P(y_i = k \\vert x_i) &amp;= P(u_{ik} &gt; u_{ij}) &amp;&amp; \\forall j \\ne k \\in J \\ &amp;= P(\\Delta e_i &lt; \\Delta f_i) \\ &amp;= \\dfrac{\\exp(f_{ik})}{\\sum_j^J \\exp(f_{ij})} &amp;&amp; (\\Delta e_i \\sim \\text{Logistic}) \\end{aligned} $$ Expected utility of individual \\(i\\) conditional on \\(x_i\\) $$ \\begin{aligned} E[u_i \\vert x_i] &amp;= E \\Big[ \\max_j { u_{ij} }  \\vert  x_i \\Big] \\ &amp;= \\log \\Big[ \\sum_j^J \\exp(f_{ij}) \\Big] + c &amp;&amp; (c = \\text{const}) \\end{aligned} $$ This is because we can add any \\(c\\) to the utilities and the model would be the same</p> <p>Proportional substitution is a manifestation of the IIA property of the logistic model</p>"},{"location":"CS_Electives/Causal_Inference/04_Discrete_Choice_Models/#logistic-vs-probit","title":"Logistic vs Probit","text":"<p>Binary Logistic regression \\(\\approx\\) Binary probit regression</p> <p></p> Logistic Regression Probit Speed Faster(has closed form solution) Slower(no closed form solution) Assumption \\(\\exists\\) correlation between alternatives every alternative is independent Advantage Probit seems more realistic as it incorporates similarity of alternatives Disadvantage Might struggle for large number of alternatives due to difficult optimization <p>Example</p> <p></p>"},{"location":"CS_Electives/Causal_Inference/04_Discrete_Choice_Models/#marginal-effects","title":"Marginal Effects","text":"\\[ \\begin{aligned} \\dfrac{\\partial P(y_i = j \\vert x_i)}{\\partial s_i} &amp;= P(y_i = j \\vert x_i) \\left( \\gamma_j - \\sum_j \\gamma_l P(y_i = l \\vert x_i) \\right) \\\\ \\dfrac{\\partial P(y_i = j \\vert x_i)}{\\partial z_{ij}} &amp;= \\delta_j \\cdot P(y_i = j \\vert x_i) \\Big[ 1 - P(y_i = j \\vert x_i) \\Big] \\end{aligned} \\] Variable Sign of marginal effect Alternative-Specific Sign of coefficient Individual-Specific N/A"},{"location":"CS_Electives/Causal_Inference/04_Discrete_Choice_Models/#choice-probability-elasticity","title":"Choice Probability Elasticity","text":"Denotation Formula Meaning Own \\(e_{i}^{jj}\\) \\(\\delta z_{ij}[1- P(y_i = j \\vert x_i)]\\) \\(\\dfrac{\\partial P(y_i=j \\vert x_i)}{\\partial z_{ij}} \\times \\dfrac{z_{ij}}{P(y_i = j \\vert x_i)}\\) Cross \\(e_{i}^{jk}\\) \\(-\\delta z_{ij} \\cdot P(y_i = k \\vert x_i)\\) \\(\\dfrac{\\partial P(y_i=j \\vert x_i)}{\\partial z_{ik}} \\times \\dfrac{z_{ik}}{P(y_i = j \\vert x_i)}\\)"},{"location":"CS_Electives/Causal_Inference/05_Discrete_Game/","title":"Discrete Game","text":"<p>Individual choices involve strategic interactions, which we can model using game theory.</p> <p>Similar to discrete choice models, but here individuals\u2019 choices affect each other\u2019s choices</p> <p>Each individual\u2019s choice can be considered the result of an equilibrium strategy. The resulting model is called a discrete game model. $$ u_{ij} = f(x_{ij}) + g(x_{i' j}) + \\epsilon_{ij} \\ i = \\text{other individuals} $$ Assume that players simultaneously make their decisions without knowing what the other players will do. Such a game is called a game of incomplete information. Because players do not know what others will do, their choices will depend on their beliefs (expectations) about other players\u2019 choice probabilities $$ \\begin{aligned} u_{i0} &amp;= \\epsilon_i^0 \\ u_{i1} &amp;= \\pi_i(y_j) + \\epsilon_i^1 \\ \\ y_i &amp;= \\arg \\max { u_{i0}, E_i[u_{i1}] } \\ &amp;= \\arg \\max { \\ &amp; \\quad  \\epsilon_i^0, \\ &amp; \\quad  \\pi_{i0} p_i (a_j=0) + \\pi_{i1} p_i (a_j = 1) + \\epsilon_i^1 \\ &amp; \\quad  } \\end{aligned} $$ where</p> <ul> <li>\\(\\pi_i =\\) profit function of \\(i\\)</li> <li>\\(y \\in \\{ 0, 1 \\}\\) is the choice</li> <li>\\(j\\) is another player</li> <li>\\(p_i(y_j)\\) is \\(i\\)\u2019s belief of \\(j\\)\u2019s probability of choosing \\(y_j\\)</li> <li>\\(E_i(y_j)\\) is \\(i\\)\u2019s belief of \\(j\\)\u2019s expected utility from choosing \\(y_j\\)</li> </ul> <p>Assumption:</p> <ul> <li>\\(\\pi_{i0} &gt; \\pi_{i1}\\) if players are competitors</li> <li>\\(\\pi_{i0} &lt; \\pi_{i1}\\) if players are complementers/symbiotic</li> </ul>"},{"location":"CS_Electives/Causal_Inference/05_Discrete_Game/#nash-bayesian-equilibrium","title":"Nash Bayesian Equilibrium","text":"<p>In equilibrium, each player has the correct belief about the choice probabilities of other players, ie \\(p_i(y_j) = p(y_j) , \\forall i, j\\)</p> <p>Assuming</p>"},{"location":"CS_Electives/Causal_Inference/05_Discrete_Game/#-epsilon_i0-epsilon_i1-sim-textgumbel0-1","title":"- \\(\\epsilon_i^0, \\epsilon_i^1 \\sim \\text{Gumbel}(0, 1)\\)","text":"\\[ \\begin{aligned} &amp;p(y_i = 1) \\\\ &amp;= \\dfrac{ \\exp[ \\pi_{i0} \\cdot p(y_j = 0) + \\pi_{i1} \\cdot p(y_j = 1) ] }{ 1 + \\exp[ \\pi_{i0} \\cdot p(y_j = 0) + \\pi_{i1} \\cdot p(y_j = 1) ] } \\\\ &amp; \\forall i, j \\end{aligned} \\]"},{"location":"CS_Electives/Causal_Inference/06_Rubin_Model/","title":"Rubin Model","text":"<p>Also called as Potential Outcomes Framework</p> <p>We find the \u2018treatment effect\u2019 of \\(x\\). This is just a fancy way of saying causal effect</p> <p>Uses statistical analysis of experiments to model causality</p> <p>Framework for causal inference that conceptualizes observed data as if they were outcomes of experiments, conducted through 1. actual experiments by researcher(s) 2. observational studies by subjects of the research</p>"},{"location":"CS_Electives/Causal_Inference/06_Rubin_Model/#terms","title":"Terms","text":"Keyword Meaning \\(x\\) Treatment/Intervention/Mediation input \\(y\\) Outcome output \\(x \\perp (y^0, y^1)\\) Exchangeability/Exogenous input is independent \\(x \\perp \\!\\!\\! \\perp (y^0, y^1)\\) Conditional exchangeability input is independent only for a certain sub-population Endogeneous input is dependent (self-chosen)"},{"location":"CS_Electives/Causal_Inference/06_Rubin_Model/#ill-defined-intervention","title":"ill-Defined Intervention","text":"<p>When the treatment is not defined specifically, there exists multiple variations of the treatment. Hence, derived effect will not be meaningful, and may be misleading.</p>"},{"location":"CS_Electives/Causal_Inference/06_Rubin_Model/#effect-of-democracy-on-economic-growth","title":"Effect of democracy on economic growth","text":"<p>You need to keep in mind that there are multiple variations of</p> <ul> <li>democracy - parliamentary, presidential, \u2026</li> <li>country becoming democratic - peaceful transition, civil uprising, revolt, \u2026</li> </ul> <p>In this case, the \u2018effect of democracy on economic growth\u2019 will not be meaningful, as each of these various treatments will have different outcomes, and cannot be generalized.</p>"},{"location":"CS_Electives/Causal_Inference/06_Rubin_Model/#effect-of-obesity-on-health","title":"Effect of obesity on health","text":"<ul> <li>What is obesity as a treatment?</li> <li>How do we intervene on obesity?</li> <li>Multiple channels to becoming obese or un-obese: (lack of) exercise, (un)healthy diet, surgery, ...</li> <li>The apparently straightforward comparison of the health outcomes of obese and non-obese individuals masks the true complexity of the interventions \u201cmake someone obese\u201d and \u201cmake someone non-obese.\u201d</li> </ul>"},{"location":"CS_Electives/Causal_Inference/06_Rubin_Model/#potential-outcomes","title":"Potential Outcomes","text":"<p>Consider an input \\(x_i\\) which takes binary values \\(0/1\\). Then, there will be</p> <ul> <li>4 potential outcomes</li> <li>2 potential outcomes for each treatment</li> </ul> <p>Suppose the treatment is \\(x_1\\), then</p> \\(x = a\\) actual treatment \\(x \\ne a\\) counterfactual treatment \\(y^a\\) realized outcome \\(y^{\\ne a}\\) counterfactual outcome \\(\\{ y_i^a , y_i^{\\ne a} \\}\\) potential outcomes \\[ \\begin{aligned} y &amp;= \\sum_{a=1}^A y^a I(x=a) \\\\ \\text{Binary } x \\implies y &amp;= x y_i^1 + (1-x) y_i^0 \\end{aligned} \\] <p>\\(x\\) has causal effect on \\(y\\) \\(\\iff P(y^0) \\ne P(y^1)\\), where P is the probability. This is because</p> <ul> <li>if \\(x\\) has no effect, changing it won\u2019t have any effect on the probability of either outcome, so the probabilities will be equal.</li> <li>but if it has effect, then obviously the outcome probabilities will be different</li> </ul>"},{"location":"CS_Electives/Causal_Inference/06_Rubin_Model/#effects","title":"Effects","text":"<p>Let \\(\\tau = y^1 - y^0\\).</p> <p>\\(\\tau\\) will have a distribution because it is a random variable (\\(\\tau_1, \\tau_2, \\dots, \\tau_i\\))</p> Term Meaning \\(\\tau_i\\) ITE Individual Treament Effectit is never observed, because we only observe \\(y_i^0\\) or \\(y_i^1\\) \\(E[\\tau]\\) ATE Average Treatment Effect $E[\\tau x = 1]$ ATT $E[\\tau x = 0]$ ATU \\(E(y^1)\\) Expectation of outcome 1 of the entire population (hypothetical, counterfactual) $E(y^1 x = 1)$ <p>Why are there 3 different average variables? The people in each group is different. So, \\(\\tau\\) for the entire group, treated and untreated groups are different, due to \u2018selection effect\u2019. This is like people who go to uni vs don\u2019t. $$ \\begin{aligned} y &amp;= y^0 \\cdot I(x=0) \\times y^1 \\cdot I(x=1)\\ &amp;\\text{where \\(I\\) means if}\\</p> <p>\\implies y &amp;= f(x, y^0, y^1) \\end{aligned}</p> <p>\\label{y} $$</p>"},{"location":"CS_Electives/Causal_Inference/06_Rubin_Model/#ate","title":"ATE","text":"<p>ATE = difference of the mean = mean of the difference $$ \\begin{aligned} \\text{ATE} =&amp; E(y^1 - y^0) \\ =&amp; E(y^1 - y^0 | x = 1) \\cdot P(x=1) \\ &amp;+ E(y^1 - y^0 | x = 0) \\cdot P(x=0) \\ =&amp; \\text{ATT}  \\cdot P(x=1) + \\text{ATU} \\cdot P(x=0) \\ =&amp; \\int E[y^1 - y^0 \\vert s] \\cdot p(s) \\cdot ds \\end{aligned} \\label{ATE} $$ This \\(\\eqref{ATE}\\) reminds me of the total probability like in Bayes\u2019 conditional probability. But here, we are taking expectation \\(E\\) (mean), because it\u2019ll more accurate than taking one value from the PDF, as \\(\\tau\\) is a random variable.</p>"},{"location":"CS_Electives/Causal_Inference/06_Rubin_Model/#idk","title":"IDK","text":"\\(E[y^1 - y^0]\\) \\(E[y \\vert x=1] - E[y \\vert x=0]\\) Compares what would happen if the __ sample receives treatment \\(x=1\\) vs \\(x=0\\) same 2 different Provides Average causal effect Average difference in outcome b/w sub populations defined by treatment group"},{"location":"CS_Electives/Causal_Inference/06_Rubin_Model/#idk_1","title":"IDK","text":"<p>$$ \\begin{aligned}</p> <p>\\text{ATT} &amp;= E(y^1 - y^0 | x = 1) \\ &amp;= \\underbrace{E(y^1 | x = 1)}{E(y | x = 1)} - \\underbrace{E(y^0 | x = 1)} \\ \\text{ATU} &amp;= E(y^1 - y^0 | x = 0) \\ &amp;= \\underbrace{E(y^1 | x = 0)}}{\\text{Cannot be estimated}} - \\underbrace{E(y^0 | x = 0)} \\ %{ %%\\text{Similarly,} &amp;\\ %%\\text{ATE} %%&amp;= \\underbrace{E(y^1 | x = 0)}{\\text{Cannot be estimated}} - %%\\underbrace{E(y^0 | x = 0)} %} \\end{aligned} $$</p> <p>Solution: Randomized Treatment</p>"},{"location":"CS_Electives/Causal_Inference/06_Rubin_Model/#pdf-graph","title":"PDF Graph","text":"<p>$$ \\begin{aligned} \\text{ATE}(x) &amp;= \\int \\tau(x, y) P(y)  dy \\ &amp;= \\frac{     dE[y| \\text{ do}(x)] }{dx} \\</p> <p>T(x, y) &amp;= \\frac{     \\partial P(y | \\text{do}(x)) }{     \\partial x } \\ \\end{aligned} $$</p> <p>We could also interpret this entire distribution as a 3 variable joint PDF of the form \\(P(x, y^0, y^1)\\)</p>"},{"location":"CS_Electives/Causal_Inference/06_Rubin_Model/#idk_2","title":"IDK","text":"Treatment\\(x\\) Observed Outcome\\(y\\) Potential Outcome\\(y^0\\) Potential Outcome\\(y^1\\) ITE 0 -0.34 -0.34 3.46 3.8 0 1.67 1.67 4.03 2.36 0 -0.77 -0.77 3.08 3.85 0 2.64 2.64 0.90 -1.74 0 -0.02 -0.02 0.96 0.98 1 2.31 -1.52 2.31 3.83 1 2.79 1.05 2.79 1.74 1 1.53 -0.13 1.53 1.65 1 3.61 -1.41 3.61 5.02 1 3.36 0.60 3.36 2.76 <p>Here</p> <ul> <li>Modelling the ITE is correct</li> <li>Modelling \\(y\\) vs \\(x\\) is incorrect</li> </ul>"},{"location":"CS_Electives/Causal_Inference/06_Rubin_Model/#shortcomings","title":"Shortcomings","text":"<ol> <li>Since it is more experiment-oriented, it is hard to analyze continuous treatment. It is only feasible to do binary \\(0/1\\) treatment.</li> <li>Cannot learn individual treatment effects, since counterfactual outcomes are not observed. This is the fundamental problem of causal inference</li> <li>We can only learn causal effects at population sample level</li> <li>Therefore, when learning a causal effect, we should always be clear       about the population sample on which it is defined</li> <li>According to Rubin, causal inference is a \u2018missing data\u2019 problem, but that\u2019s just like every other statistical predictive model</li> <li>It does not\u00a0model choice as assignment of unit\u2019s ability and eligibility for treatment; it models model choice as assignment to a treatment</li> </ol>"},{"location":"CS_Electives/Causal_Inference/07_Random_Testing/","title":"Randomized Tests","text":""},{"location":"CS_Electives/Causal_Inference/07_Random_Testing/#randomization","title":"Randomization","text":"<p>ensures that correlation = causation</p> <p>helps estimate counterfactual outcome by ensuring</p> <ul> <li>\\(y^0, y^1 \\perp \\!\\!\\! \\perp x\\): independence/exchangeability of \\(y^0, y_1\\) wrt \\(x\\)</li> <li> <p>Treatment \\(x\\) is exogenous</p> </li> <li> <p>Similarity of population and treated sample</p> </li> </ul> \\[ \\text{ATE = ATT = ATU} \\notag \\]"},{"location":"CS_Electives/Causal_Inference/07_Random_Testing/#independence","title":"Independence","text":"<p>If you randomly assign \\(x\\) to people, then the \\(x\\) will be independent from everything. \\(x\\) will therefore be independent from \\(y_0\\) and \\(y_1\\).</p> <p>If \\(P(y|x=0) = P(y|x=1) = P(y)\\), then \\(x\\) and \\(y\\) are independent. This is because, whatever the value of \\(x\\) we put, the probability of \\(y\\) did not get changed.</p> <p>So conversely, if we are able to make \\(x\\) and \\(y^0\\) independent, then we can take $$ E(y^0 | x = 1) = E(y^0 | x = 0) = E(y | x = 0) $$</p> \\[ \\begin{aligned} E(y^1 - y^0) &amp;= E(y^1 | x = 1) - E(y^0 | x = 0) \\\\ &amp;= E(y | x = 1) - E(y | x = 0) \\end{aligned} \\]"},{"location":"CS_Electives/Causal_Inference/07_Random_Testing/#similarity","title":"Similarity","text":"<p>To ensure that \\(E(y|x=1) = E(y^1)\\), we need to ensure that the population and treated-sample are the similar in their features.</p>"},{"location":"CS_Electives/Causal_Inference/07_Random_Testing/#derivations-from-randomized-tests","title":"Derivations from Randomized Tests","text":"Property Meaning Association (Correlation) $P(y Causation \\(P(y^a) \\ne P(y^b)\\) Correlation \\(\\implies\\) Causation Random assignment of $x \\implies (y^a) = P(y"},{"location":"CS_Electives/Causal_Inference/07_Random_Testing/#effect-of-randomization-on-deriving-ate","title":"Effect of Randomization on deriving ATE","text":""},{"location":"CS_Electives/Causal_Inference/07_Random_Testing/#without-randomization","title":"Without Randomization","text":"<p>There will be statistical correlation without causal correlation (to be avoided). This is due to Self-Selection Effect. So the selection will be biased.</p> <p>$$ \\begin{aligned} &amp; E(y | x = 1) - E(y | x = 0)  \\ =&amp;  \\underbrace{     E( y|x = 1) \\textcolor{orange} {- E(y^0|x=1)} }\\text{ATT} \\ &amp;+ \\underbrace{ \\textcolor{orange}{ E(y^0|x=1) } - E(y | x = 0) } \\} \\ne \\text{ATU} </p> <p>\\ne &amp; E(y' - y^0) \\ \\ne &amp; \\text{ATE} \\end{aligned} $$</p>"},{"location":"CS_Electives/Causal_Inference/07_Random_Testing/#with-randomization","title":"With Randomization","text":"\\[ \\begin{aligned} A \\perp \\!\\!\\! \\perp B &amp;\\iff P(A) = P(A \\vert B=0) = P(A \\vert B=1) \\\\ \\\\ \\implies y^0 \\perp \\!\\!\\! \\perp x &amp;\\iff P(y^0) = P(y^0 \\vert x=0) = P(y^0 \\vert x=1) = P(y \\vert x=0) \\\\ \\implies y^1 \\perp \\!\\!\\! \\perp x &amp;\\iff P(y^1) = P(y^1 \\vert x=0) = P(y^1 \\vert x=1) = P(y \\vert x=1) \\end{aligned} \\] \\[ E[y^1 - y^0] = E[y \\vert x=1] - E[y \\vert x=0] \\] \\[ \\begin{aligned} &amp;\\text{ATE} = \\text{ATT} = \\text{ATE} \\\\ &amp;= E[y \\vert x=1] - E[y \\vert x=0] \\end{aligned} \\] <p>This only applies since main is a linear operator. This does not apply for non-linear operators: median, variance, percentiles</p>"},{"location":"CS_Electives/Causal_Inference/07_Random_Testing/#conditional-randomized-experiments","title":"Conditional Randomized Experiments","text":"<p>If one or more external parameters affect the causal effect of the treatment on the outcomes, then we have to do different randomized conditions.</p> <p>By independent testing at different conditions, we can keep the effect of the external parameter as a constant, and we will get the true causal effect of the treatment.</p> <p>It leads to conditional exchangeability, for the particular sub-population $$ x \\perp !!! \\perp (y^1, \\dots, y^A) | s $$ where \\(s\\) are the fixed Effect Modifiers $$ \\begin{aligned} E[y^a] &amp;= \\sum_{j=1}^S E[y^a \\vert s=j] \\cdot p(s=j) \\ &amp;= \\sum_{j=1}^S E[y \\vert x=a, s=j] \\cdot p(s=j) \\end{aligned} $$ In experimental design, effect modifiers \\(s\\) are called the nuisance factors that experimenter controls when performing the RCT. Nuisance factors are vars that can affect \\(y\\) either directly/indirectly, but is not of primary interest to the experimenter.</p>"},{"location":"CS_Electives/Causal_Inference/07_Random_Testing/#self-selection-effectbias","title":"Self-Selection Effect/Bias","text":"<p>In economics, we assume that everyone</p> <ul> <li>is rational</li> <li>makes decisions/selections to maximize self-interests</li> </ul> <p>When individuals choose their own treatments, those who choose to receive a treatment may be systematically different than those who choose not to, leading to a correlation between treatment and outcome that is not due to direct causation</p>"},{"location":"CS_Electives/Causal_Inference/07_Random_Testing/#limitations","title":"Limitations","text":"<p>Without understanding the various Effect Modifiers, we will get wrong inferences, because you will mistake a local effect for a global effect that applies for all scenarios. Hence, there are limits for Random testing without understanding the causal mechanism. This clearly disproves the thinking that \u201cRCTs are the golden standard for causal inference\u201d</p> <p>Only Conditional Randomized Experiments give correct readings, because it helps obtain the true causality without effect of any other factors. For eg, a lot of Psychology studies are performed on Psychology students, hence it doesn\u2019t really give true research findings.</p> <p>Also, it\u2019s nearly impossible to perform random tests in economics, due to the following</p> <ul> <li>infeasible (govt/monetary policies)</li> <li>ethical reasons (smoking - lung cancer)</li> <li>cost</li> <li>duration  (childhood intervention &amp; adult outcomes)</li> <li> <p>Long duration studies often suffer from significant (non-random) attrition</p> </li> <li> <p>Hinderances</p> </li> <li>RCTs require special conditions if they are to be conducted successfully</li> <li>local agreements</li> <li>compliant subjects</li> <li>affordable administrators</li> <li>multiple blinding</li> <li>people competent to measure and record outcomes reliably</li> <li>High dimensional treatment/nuisance factors</li> <li>possibility of too many known/unknown effect modifiers</li> <li> <p>if we do not control for the effect modifiers, the causal effect estimate obtained will be very local. This limits the usefulness of study</p> </li> <li> <p>Scaling-up of effects to the population may give opposite results of the RCT sample</p> </li> <li>Predicting the same results at scale as in the trial can be problematic, as the larger target population can be very different from the study population, so the causal effects may not be transportable</li> <li> <p>General equilibrium effects</p> <ul> <li>even if the trial sample is a random sample of the target population, so that the target population \\(\\sim\\) the study population, applying the same intervention to everyone in the population could generate very different effects than in the trial</li> <li>Hence, the result we obtain is a local result conditional to the current equilibrium</li> </ul> </li> <li> <p>Violation of SUTVA (Stable Unit Treatment Value Assumption)</p> <ul> <li>SUTVA = Assumption that individual\u2019s potential outcome under a treatment does not depend on the treatments received by other individuals, as there is an assumption that there is no interaction b/w individuals.</li> <li>SUTVA can be thought of as an i.i.d. assumption on causal effects</li> <li>If the causal effect depends on how many individuals receive the treatment, then SUTVA is violated. Treatment dilution: treatment is less effective as more people get it</li> <li>In the scaling up effects explanation, we can see that market equilibrium is affecting the outcomes</li> </ul> </li> </ul>"},{"location":"CS_Electives/Causal_Inference/07_Random_Testing/#scaling-up-of-rt","title":"Scaling-Up of RT","text":"<p>Govt policy to increase farmers\u2019 incomes through subsidized fertilizers, based on effectiveness in RCT. Increased production for the sample farmers would increase their revenue, but if all farmers used this fertilizer, then the overall supply would increase. Assuming that the demand for the produce is inelastic, then the price would reduce. Hence, the income of the farmers would actually reduce. Therefore, the policy of encouraging all farmers to use fertilizers would be bad.</p> <p>The same thing goes for effect of education on earnings. If everyone is now educated, the supply for high-skilled labor increases but the demand is still the same, hence its value decreases.</p>"},{"location":"CS_Electives/Causal_Inference/07_Random_Testing/#sutva-violation","title":"SUTVA Violation","text":"<ul> <li>Violation of SUTVA can also be viewed as a problem of ill-defined interventions</li> <li>When SUTVA is violated</li> <li>Only slight violation can be tolerated</li> <li>the sample treatment and the population treatment are essentially different interventions</li> <li>If violated, then we need to take the interaction into account</li> </ul>"},{"location":"CS_Electives/Causal_Inference/07_Random_Testing/#handling","title":"Handling","text":"<p>Others receiving the treatment must be considered as an effect modifier of the Randomized Test.</p> <ul> <li>Learn \\(p( \\ y_i \\vert \\text{do}(x_i) \\ )\\) or \\(p( \\ y_i \\vert \\text{do}(x_i), x_j \\ )\\) treating \\(x_j\\) as an effect modifier</li> <li>When estimating the treatment effect on a individual, if SUTVA is violated then we need to consider the treatments received by other individuals as effect modifiers</li> <li>Learn \\(p( \\ y_i \\vert \\text{do}(x_i, x_j) \\ )\\)</li> <li>This requires changing the unit of analysis from the original individual to a population of those units where interaction occurs and is confined in<ul> <li>We can define each individual \\(i\\) to be a \u201clocal population\u201d where such interference occurs and is confined in, and let the underlying population be a population of such local populations</li> </ul> </li> </ul>"},{"location":"CS_Electives/Causal_Inference/07_Random_Testing/#why-does-violation-happen-a-lot","title":"Why does violation happen a lot?","text":"<p>Unlike medical sciences, socio-economic outcomes are often results of individual interaction. If the market is not perfectly-competitive, individual choices are rarely independent and each person\u2019s choice affects other people.</p> Scale Micro Social/Strategic Interaction(Firm competition in oligopolistic markets) Macro General Equilibrium effects <p>However, it is negligible in certain cases: buyers and sellers in competitive markets</p>"},{"location":"CS_Electives/Causal_Inference/07_Random_Testing/#examples","title":"Examples","text":""},{"location":"CS_Electives/Causal_Inference/07_Random_Testing/#exams","title":"Exams","text":"<p>For eg, if there are 2 exam sets. The treatments are</p> <ul> <li>\\(x = 0 \\to\\) easy set</li> <li>\\(x = 1 \\to\\) hard set</li> </ul> Scenario Variable Comment Asking students to volunteer for hard test $E(y x = 1)$ Forcing everyone to take the hard test \\(E(y^1)\\) Everyone has received the input (taking hard test) Randomly assigning sets $E(y x=1) = E(y^1)$"},{"location":"CS_Electives/Causal_Inference/07_Random_Testing/#demand-estimation","title":"Demand Estimation","text":""},{"location":"CS_Electives/Causal_Inference/07_Random_Testing/#goal","title":"Goal","text":"<p>to know consumer demand for a product wrt price</p>"},{"location":"CS_Electives/Causal_Inference/07_Random_Testing/#given-data","title":"Given Data","text":"\\[ D = \\{ (p_1, q_1), \\dots, (p_n, q_n) \\} \\] <ul> <li>\\(q_i = Q_i^L \\cdot (p_i==L) \\ + \\ Q_i^H \\cdot (p_i==H)\\)</li> <li>\\(\\{ (p_1, Q_1^L, Q_1^H), \\dots, (p_n, Q_n^L, Q_n^H) \\} \\overset{\\text{iid}}{\\sim} P(p, Q^L, Q^H)\\)</li> </ul>"},{"location":"CS_Electives/Causal_Inference/07_Random_Testing/#method","title":"Method","text":"<ul> <li>treatment - price \\(p\\)</li> <li>outcome - purchases \\(q\\)</li> </ul> <p>Let\u2019s assume there are only 2 inputs (price level) - \\(p \\in \\{ L, H \\}\\). Then \\(\\exists\\)</p> <ul> <li>2 potential outcomes (demand level) - \\(q \\in \\{ Q^L, Q^H \\}\\)</li> <li>Desired causal effect</li> </ul> \\[ \\text{ATE} = E[Q^L - Q^H] \\] <p>From the data, we can learn \\(P(q \\vert p = a), a \\in L, H\\)</p>"},{"location":"CS_Electives/Causal_Inference/07_Random_Testing/#problem-with-observational-data","title":"Problem with observational data","text":"<p>We cannot directly use P and Q to estimate the demand/supply function. This is because every data point is an equilibrium point and cannot be taken as the demand/supply curve. So, self-selection effect comes into play.</p> <p>Without exchangeability, \\(P(Q^a) \\ne P(Q^a \\vert p=a) = P(q \\vert p=a)\\)</p> <p>The group that \u201creceived\u201d the treatment \\(p=L\\) could be systematically different than the group that \u201creceived\u201d \\(p=H\\)</p> <ul> <li>People that buy when price is high can be richer than those who buy when price is low</li> <li>If we observe the person over time, then their income may be different when the price is low vs price is high</li> </ul> <p>Hence, there will be effect of income elasticity which will alter our understanding the price elasticity</p> <p>Changing price will not help in determination of the causal effect of price change. $$ \\text{ATE} \\ne E(q|p = l) - E(q|p = h) $$ This is because, in real life, \\(p\\) is not randomly-assigned. So, the people who buy at high price and low price are completely-different; the populations are different in both the cases. So, the effect of income comes into picture. Therefore, the true and direct causal effect of price will not be understood.</p>"},{"location":"CS_Electives/Causal_Inference/07_Random_Testing/#solution","title":"Solution","text":"<p>Companies could run experiments by randomly assigning prices to customers in different markets and over time</p> <p>Companies could perform A/B testing by running experiments by randomly assigning prices in different markets and over time. This change in price will target the individual, so the true treatment effect will be learnt, as the income of people is quite constant.</p> <p>This is mainly used by online companies, as it is inexpensive to do so.</p>"},{"location":"CS_Electives/Causal_Inference/07_Random_Testing/#giffen-behavior","title":"Giffen Behavior","text":"<p>cannot be trusted through observational data. This is because</p> <p>Higher prices are often associated with more purchases, but is it</p> <ul> <li>higher demand causing both higher prices and more purchases, or</li> <li>higher prices causing people to buy more (Giffen behavior)</li> </ul> <p>The prices are \u201cchosen\u201d, so the analysis using observed increased prices is not necessarily a treatment and does not help us obtain the true causal effect</p> <p>We need to keep in mind</p> <ol> <li>inflation</li> <li>increase in wages</li> </ol> <p>However, with randomized treatment (such as subsidies for the commodity), we can derive the true giffen behavior and hence make correct analysis.</p>"},{"location":"CS_Electives/Causal_Inference/07_Random_Testing/#classroom-size","title":"Classroom Size","text":""},{"location":"CS_Electives/Causal_Inference/07_Random_Testing/#goal_1","title":"Goal","text":"<p>to know the effect of classroom size on student performance</p>"},{"location":"CS_Electives/Causal_Inference/07_Random_Testing/#given-data_1","title":"Given Data","text":"\\[ D = \\{ (p_1, q_1), \\dots, (p_n, q_n) \\} \\]"},{"location":"CS_Electives/Causal_Inference/07_Random_Testing/#method_1","title":"Method","text":"<ul> <li>treatment - classroom size</li> <li>outcome - student performance</li> </ul> <p>Let\u2019s assume there are only</p> <ul> <li>2 inputs (room size) - \\(s \\in \\{ S, L \\}\\)</li> <li>2 potential outcomes (performance) - \\(p \\in \\{ p^S, p^L \\}\\)</li> </ul> \\[ \\text{ATE} = E[p^L - p^S] \\]"},{"location":"CS_Electives/Causal_Inference/07_Random_Testing/#problem","title":"Problem","text":"<p>Weaker students often deliberately grouped into smaller classes</p> <p>Hence, Many studies of education production using non-experimental data suggest there is little or no link between class size and student learning</p>"},{"location":"CS_Electives/Causal_Inference/07_Random_Testing/#solution_1","title":"Solution","text":"<p>Randomized assignment of classroom size is necessary, as usually the more well-off students will be in private schools , so clearly there is self-selection effect here. So we will take the same group of kids, and randomly assign a small and large room.</p>"},{"location":"CS_Electives/Causal_Inference/07_Random_Testing/#study-in-tennesee-usa","title":"Study in Tennesee, USA","text":"<p>Random assignment of classroom sizes to students showed that students in smaller classrooms performed better.</p> <p>However, this may conclusion may not be accurate for the state of Tennesee itself, because there could be some other factors in play here. Maybe the students are not accustomed to the new large classrooms, which affects the performance. So over time, difference in performance might be nothing the more the students get used to it.</p> <p>This result definitely can**not** be used directly elsewhere. This is because the composition of tested sample and the other populations will be different:</p> <ul> <li>income</li> <li>race</li> <li>culture</li> </ul>"},{"location":"CS_Electives/Causal_Inference/07_Random_Testing/#solution_2","title":"Solution","text":"<p>Causal Mechanism Learning</p> <p>In this case, let\u2019s analyze the following: </p> <p>How would a small class help in performance?</p> <p>hmm\u2026 because, students sit closer to the board. This mechanism is applicable everywhere. If this is the only mechanism, then we can confidently say that the results of the Tennessee experiment can be applied everywhere.</p> <p>For example, maybe smaller classrooms enable easier interactions between students and teachers. Hence, smaller classrooms helpful in the US/Europe where they have a lot of group work and interactions; but not in India/China as the teacher mostly just teaches without much interactions.</p>"},{"location":"CS_Electives/Causal_Inference/07_Random_Testing/#fertilizer","title":"Fertilizer","text":""},{"location":"CS_Electives/Causal_Inference/07_Random_Testing/#goal_2","title":"Goal","text":"<p>to know the effect of fertilizer on crop output</p>"},{"location":"CS_Electives/Causal_Inference/07_Random_Testing/#method_2","title":"Method","text":"<ul> <li>treatment - using fertilizer</li> <li>outcome - crop output</li> </ul>"},{"location":"CS_Electives/Causal_Inference/07_Random_Testing/#problem_1","title":"Problem","text":"<p>The result might not be accurate with just randomized experiment. This is because, the effectiveness of fertilizer depends on the temperature (effect modifier)</p>"},{"location":"CS_Electives/Causal_Inference/07_Random_Testing/#solution_3","title":"Solution","text":"<p>Conditional Randomized Experiment</p> <p>Randomized experiments at different temperatures</p> <ul> <li>a randomized experiment at low temperature</li> <li>a randomized experiment at high temperature</li> </ul> <p>This is because, the temperature affects the causal effect of fertilizer on the crop output. By independent testing at different temperatures, we can keep the effect of temperature as a constant, and we will get the true causal effect of fertilizer.</p>"},{"location":"CS_Electives/Causal_Inference/07_Random_Testing/#fumigation-and-yield","title":"Fumigation and Yield","text":"<p>Fumigation is the use of fumigants to control eelworms which affects crop yield</p> <p>Suppose, in a place A, we conduct an RCT to study the effect of fumigation on yield by randomly selecting \\(N\\) barley fields and randomly applying fumigation to \\(M\\) of them. The result shows that fumigation increases barley yield by 20%.</p> <p>The understanding of the result depends on our understanding of the causal mechanism and its implied effect modifiers.</p> <p>We need to investigate the effect of</p> <ul> <li>season when the study was performed</li> <li>previous year\u2019s crop on that same field</li> <li>other prior known/hypothesised effect modifiers</li> </ul> <p>For eg: $$ \\begin{aligned} &amp;E(\\text{Causal effect} \\vert \\text{Summer}, \\text{Same Crop}) \\ \\ne &amp; E(\\text{Causal effect} \\vert \\text{Winter}, \\text{Same Crop}) \\ \\ne &amp; E(\\text{Causal effect} \\vert \\text{Summer}, \\text{Alternated Crop}) \\ \\ne &amp; E(\\text{Causal effect} \\vert \\text{Winter}, \\text{Alternated Crop}) \\end{aligned} $$ Hence, this causal effect is local</p>"},{"location":"CS_Electives/Causal_Inference/07_Random_Testing/#psychology-studies","title":"Psychology Studies","text":"<p>Studies of most Psychology studies are WEIRD: Western, Educated, Industrialized, Rich, Democratic, particularly American undergrads</p>"},{"location":"CS_Electives/Causal_Inference/07_Random_Testing/#idk","title":"IDK","text":"<p>We always need to have \\(x=0\\), to effectively quantify the causal effect.</p> <p>Let\u2019s say \\(x\\) is non-binary, for eg: \\(x=\\) Sunlight</p> <ul> <li>0: Night</li> <li>0.5: Cloudy</li> <li>1: Sunny</li> </ul> \\(E(y \\vert \\text{do}(x=0))\\) \\(E(y \\vert \\text{do}(x=0.5))\\) \\(E(y \\vert \\text{do}(x=1))\\) Conclusion:Changing from 0.5 to 1 is significant 0 100 101 \u274c 0 0 101 \u2705 100 100 101 \u2705"},{"location":"CS_Electives/Causal_Inference/08_Causal_Graphical_Model/","title":"Causal Model","text":"<p>A causal/scientific model \\(M\\) for a set of RVs \\(\\{ x_1, \\dots, x_n \\}\\) is a model of the joint distribution \\(p(x_1, \\dots, x_n)\\) as well as the causal structure governing \\(\\{ x_1, \\dots, x_n \\}\\) which describes the causal relationships among the vars $$ M: y \\leftarrow x $$</p>"},{"location":"CS_Electives/Causal_Inference/08_Causal_Graphical_Model/#causal-graph-model","title":"Causal Graph Model","text":"<p>Causal diagram/graph is a graph that can be used to represent a causal structure and hence, describe our quantitative knowledge about a causal mechanism</p> <p>Causal model that uses a causal graph to represent the causal structure is called as Causal Graph Model/Causal Bayesian/Belief network.</p> <p>RCM was developed in statistics, causal graphical model is derived from CS/AI.</p>"},{"location":"CS_Electives/Causal_Inference/08_Causal_Graphical_Model/#parts-of-causal-model","title":"Parts of Causal Model","text":"<ol> <li>Causal Structure \\(G\\): Direction of causality (What causes what)</li> <li>Statistical Model \\(H\\): Joint distribution function</li> </ol> \\[ M = (H, G) \\]"},{"location":"CS_Electives/Causal_Inference/08_Causal_Graphical_Model/#example","title":"Example","text":"<p>Consider a model \\(y = 2x\\). This is a statistical model.</p> <p>If \\(y \\leftarrow 2x\\), then it means that \\(x\\) causes \\(y\\). This is a causal model.</p> <p>Let\u2019s analyze the \\(x,y\\) pairs for the following sequential changes.</p> Model 1. do\\((x=2)\\) 2. do\\((x=3)\\) 3. do\\((y=2)\\) Statistical 2, 4 3, 6 1, 2 Causal 2, 4 3, 6 3, 2 <p>This is because, \\(x\\) causes \\(y\\); not the other way around.</p>"},{"location":"CS_Electives/Causal_Inference/08_Causal_Graphical_Model/#causal-diagramgraph","title":"Causal Diagram/Graph","text":"<p>Directed graph that represents the causal structure of a model. It is represented using a bayesian network: DAG (Directed Acyclic Graph) in which each node has associated conditional probability of the node given in its parents. </p> <p>The joint distribution is obtained by taking the product of the conditional probabilities. $$ \\begin{aligned} p(x_1, \\dots, x_n) &amp;= \\Pi_{i=1}^n  p(  x_i \\vert \\text{pa}(x_i)  ) \\ x_i  &amp; {\\tiny \\coprod}  \\text{nd}(x_i) \\vert \\text{pa}(x_i) \\end{aligned} $$ where</p> <ul> <li>pa = parent</li> <li>nd = not descendant</li> </ul> <p>Useful when you are only interested in the causal structure, and not so much in the statistical model</p> <pre><code>flowchart TD\nB([B]) &amp; C([z]) --&gt; A([A])</code></pre> \\[ P(A, B, C) = P(A | B, C) \\cdot P(B) \\cdot P(C) \\]"},{"location":"CS_Electives/Causal_Inference/08_Causal_Graphical_Model/#why-dag","title":"Why DAG?","text":"<ul> <li>Directed, because we want causal effects directions</li> <li>Acyclic, because a variable cannot cause itself.</li> </ul> <p>my question is: What about recursive loops, such as our body\u2019s feedback loops</p>"},{"location":"CS_Electives/Causal_Inference/08_Causal_Graphical_Model/#parts","title":"Parts","text":"Nodes - vars - Rounded - Unconditioned- Square - Conditioned Directed Edges - Causal directions 1. The presence of an arrow from \\(x_i\\) to \\(x_j\\) indicates either that \\(x_i\\) has a direct causal effect on \\(x_j\\) \u2013 an effect not mediated through any other vars on the graph, or that we are unwilling to assume such a effect does not exist2. The absence of an arrow from \\(x_i\\) to \\(x_j\\) indicates the absence of a direct effect; the absence of an arrow hence represents a more substantive assumption <pre><code>flowchart TB\na([a])\nw([w])\nx([x])\ny([y])\nz([z])\n\nx --&gt; y &amp; z\nz --&gt; a\n\nw --&gt; y</code></pre> Term Condition Above Example Parent Node from which arrow(s) originate \\(x\\) is parent of \\(y\\) and \\(z\\) Child Node to which arrow(s) end \\(y\\) and \\(z\\) are children of \\(x\\) Descendants Ancestors Exogenous vars w/o any parent \\(w, x\\) Endogenous vars w/ \\(\\ge 1\\) parent \\(y, z, a\\) Causal Path Uni-directional path \\(x z a\\)\\(z a\\) \\(x y \\(&lt;br /&gt;\\)w y\\) Non-Causal Path Bi-directed path \\(x y w\\) Collider Node having multiple parents, where path \u2018collides\u2019 \\(y\\) in the path \\(x y w\\) Blocked Path Path with a- conditioned non-collideror- unconditioned collider, w/o conditioned descendants Back-Door Paths Non-causal paths b/w \\(x\\) and \\(y\\), which if left open, induce correlation b/w \\(x\\) and \\(y\\) without causation d-separated vars all paths b/w vars are blocked d-connected vars \\(\\exists\\) path between the vars which isn\u2019t blocked conditionally-independent vars If 2 vars are d-separated after conditioning on a set of vars conditionally-dependent/associated vars If 2 vars are d-connected after conditioning on a set of varsw/o faithfulness condition, this may not be true"},{"location":"CS_Electives/Causal_Inference/08_Causal_Graphical_Model/#properties","title":"Properties","text":"<p>Causal diagrams compatible with a joint distribution \\(p(x_1, \\dots, x_n)\\) must satisfy</p> Property Meaning Causal Markov Condition Every var is independent of any other vars (except its own effects) conditional on its direct causes Completeness All common causes (even if measured) of any pair of vars on the graph are on the graphThis property is as important as the requirement that all relevant factors are accounted for. Our ability to extract causal information from data is predicated on this untestable assumption Faithfulness The joint distribution \\(p(x_1, \\dots, x_n)\\) has all the conditional independence relations implied by the causal diagram, and only those conditional independence relations <ul> <li>\\(\\text{nd}:\\) non-descendent</li> <li>\\(\\text{pa}:\\) parents</li> </ul>"},{"location":"CS_Electives/Causal_Inference/08_Causal_Graphical_Model/#causal-relations","title":"Causal Relations","text":""},{"location":"CS_Electives/Causal_Inference/08_Causal_Graphical_Model/#general-framework","title":"General Framework","text":"<pre><code>flowchart TB\noc([Observed&lt;br/&gt;Confounders])\nuc([Unobserved&lt;br/&gt;Confounders])\noce([Observed&lt;br/&gt;Common&lt;br/&gt;Effects])\nuce([Unobserved&lt;br/&gt;Common&lt;br/&gt;Effects])\nx([x])\ny([y])\n\nx --&gt; y\noc ---&gt; x &amp; y\nuc -..-&gt; x &amp; y\n\nx &amp; y --&gt; oce\nx &amp; y -.-&gt; uce\n\nclassDef dotted stroke-dasharray:5\nclass uc,uce dotted\n\nlinkStyle 0 stroke:green</code></pre>"},{"location":"CS_Electives/Causal_Inference/08_Causal_Graphical_Model/#types","title":"Types","text":"Type \\(x, y\\) are statistically-independentCorrelation \\(=\\) Causation\\(E[y \\vert x] = E[y \\vert \\text{do}(x)]\\) \\(E[y \\vert \\text{do}(x)]\\) Comment Path \u2018__\u2019 by collider \\(c\\) Example\\(z\\) Example\\(x\\) Example\\(y\\) Mutual Dependence/Confounding/Common Cause \u274c Info on \\(x\\) helps predict \\(y\\), even if \\(x\\) has no causal effect on \\(y\\) Smoker Carrying a lighter Cancer Conditioned Mutual Dependence \u2705 \\(\\sum_i E[y \\vert \\text{do}(x), c_i] \\cdot P(c_i)\\)\\(=\\sum_i E[y \\vert x, c_i] \\cdot P(c_i)\\) blocked Smoker=FALSE Carrying a lighter Cancer Mutual Causation/Common Effect \u2705 blocked Revenue of company Size of company Survival of company Conditioned Mutual Causation \u274c opened Revenue of company Size of company Survival of company=TRUE(we usually only have data for companies that survive)(Survivorship bias) Mediation \u274c Conditioned Mediation \u2705 blocked 1. Cancer2. Tax 1. Tar deposits in lung2. Economic consequences 1. Smoking2. Economic conditions <pre><code>flowchart TB\n\nsubgraph Mediation\n  subgraph ucm[Unconditioned]\n    direction TB\n    x1([x]) --&gt; c1([z]) --&gt; y1([y])\n    %% x1 --&gt; y1\n  end\n\n  subgraph cm[Conditioned]\n    direction TB\n    x2([x]) --&gt; c2[z] --- y2([y])\n    %% x2 --&gt; y2\n  end\nend\n\nsubgraph mc2[Mutual Causation 2]\n    subgraph ucmc2[Unconditioned]\n        direction TB\n        x8([x]) &amp; y8([y]) --&gt; w8([w]) --&gt; c8([z])\n        %% x8 --&gt; y8\n    end\n    subgraph cmc2[Conditioned]\n        direction TB\n        x7([x]) &amp; y7([y]) --&gt; w7([w]) --&gt; c7[z]\n        %% x7 --&gt; y7\n    end\nend\n\nsubgraph mc[Mutual Causation 1]\n    subgraph ucmc[Unconditioned]\n        direction TB\n        x4([x]) &amp; y4([y]) --&gt; c4([z])\n        %% x4 --&gt; y4\n    end\n    subgraph cmc[Conditioned]\n        direction TB\n        x3([x]) --&gt; c3\n        y3([y]) --&gt; c3[z]\n        %% x3 --&gt; y3\n    end\nend\n\nsubgraph md[Mutual Dependence]\n    subgraph ucmd[Unconditioned]\n        direction TB\n        c6([z]) --&gt; x6([x]) &amp; y6([y])\n        %% x6 --&gt; y6\n    end\n    subgraph cmd[Conditioned]\n        direction TB\n        c5[z] --- x5([x]) &amp; y5([y])\n        %% x5 --&gt; y5\n    end\nend\n\n%%linkStyle 4,24,23 stroke:none;</code></pre>"},{"location":"CS_Electives/Causal_Inference/08_Causal_Graphical_Model/#confounding","title":"Confounding","text":""},{"location":"CS_Electives/Causal_Inference/08_Causal_Graphical_Model/#self-selection-bias","title":"Self Selection Bias","text":"<p>Special case of confounding, when \\(z\\) affects the selection of \\(x\\) and also has a causal effect on \\(y\\), then \\(z\\) is confounder.</p> <pre><code>flowchart TB\nc([Ability/Talent])\na([Education])\nb([Earnings])\n\nc --&gt;|Affects&lt;br/&gt;selection| a\nc --&gt;|Directly&lt;br/&gt;Affects| b\n\na --&gt;|Associated but&lt;br/&gt;not necessarily causes| b</code></pre> <p>Here, education may not necessarily causally affect earnings.</p>"},{"location":"CS_Electives/Causal_Inference/08_Causal_Graphical_Model/#unmeasured-confounding","title":"Unmeasured Confounding","text":"<p>We need to find new ways to identify causal effect</p> \\(z\\) fully observed \\(\\not \\exists\\) unmeasured confounding Selection on \u2705 \u2705 observables \u274c \u274c unobservables <p>However, in some cases, there can exist a set of observed vars conditioned such that it satisfies the back-door criterion, by blocking all non-causal paths b/w \\(x\\) and \\(y\\)</p> <pre><code>flowchart TB\n\nsubgraph Conditioned\n    direction TB\n  wc([w]) -.- zc &amp; yc\n  zc[z] --- xc([x]) --&gt; yc([y])\nend\n\nsubgraph Unconditioned\n    direction TB\n  wu([w]) -.-&gt; zu &amp; yu\n  zu([z]) --&gt; xu([x]) --&gt; yu([y])\nend\n\nlinkStyle 3,7 stroke:green\nlinkStyle 0,1,2 stroke:none\n\nclassDef dotted stroke-dasharray:5\nclass wu,wc dotted</code></pre> <p>\\(w\\) is a confounder to \\(x\\) and \\(y\\), but we do not need to observe it, as causal effect of \\(x\\) on \\(y\\) is identifiable by conditioning on \\(z\\)</p>"},{"location":"CS_Electives/Causal_Inference/08_Causal_Graphical_Model/#conditioning","title":"Conditioning","text":"<ul> <li>Condition common causes</li> <li>Do not condition common effects</li> </ul> <p>If we condition on a set of vars \\(z\\) that block all open non-causal paths between treatment \\(x\\) and outcome \\(y\\), then</p> <ul> <li>the causal effect of \\(x\\) on \\(y\\) is identified (estimated from observed data)</li> <li>\\(z\\) is said to satisfy the \u2018back-door criterion\u2019</li> <li>Conditioning on z makes \\(x\\) exogenous to \\(y\\)</li> </ul>"},{"location":"CS_Electives/Causal_Inference/08_Causal_Graphical_Model/#covariate-balance","title":"Covariate Balance","text":"<p>In a sample where is \\(z\\) is a conditioned confounder $$ P(z \\vert x = x_i) = P(z \\vert x = x_j) \\quad \\forall i, j $$</p> <p>If \\(x\\) is binary $$ \\begin{aligned} &amp; x \\in { 0, 1 } \\ \\implies &amp; P(z \\vert x=1) = P(z \\vert x=0) \\end{aligned} $$</p>"},{"location":"CS_Electives/Causal_Inference/08_Causal_Graphical_Model/#matching","title":"Matching","text":"<p>Converting non-RCT observed sample into a sample that satisfies Covariate Balance, ie behaves similar to a RCT sample through control of observed confounding.</p>"},{"location":"CS_Electives/Causal_Inference/08_Causal_Graphical_Model/#effect-modifiers","title":"Effect Modifiers","text":"<p>Confounders \\(s\\) that change the causal effect of a treatment \\(x\\), since their causal effect on the outcome \\(y\\) interacts with treatment\u2019s causal effect on \\(y\\)</p> <p>Controlling them help control conditionally randomized experiments.</p> <p>Consider the following causal diagram.</p> <pre><code>flowchart TD\n\nsubgraph After Simplification\n    others([s&lt;sup&gt;y&lt;/sup&gt;]) &amp; x2([X]) --&gt; y2([Y])\nend\n\nsubgraph Before Simplification\n    a([A]) &amp; b([B]) &amp; c([C]) &amp; w([W]) &amp; x1([X]) --&gt; y([Y])\nend</code></pre> <p>where \\(s^y\\) is anything other than \\(x\\) that can causally affect \\(y\\); ie, the set of potential effect modifiers</p> <p>An example could be gender, temperature, etc.</p> <p>Mathematically, \\(s\\) is an effect modifier if $$ \\begin{aligned} P(y) &amp;\\ne P(y \\vert s) \\ E[y \\vert \\text{do}(x)] &amp;\\ne E(y \\vert \\text{do}(x), s) \\end{aligned} $$</p>"},{"location":"CS_Electives/Causal_Inference/08_Causal_Graphical_Model/#instrumental-variable","title":"Instrumental Variable","text":"<p>Variable \\(iv\\) that satisfies</p> <ol> <li>\\(iv\\) correlated with \\(x\\)</li> <li>Every open path connecting \\(iv\\) w/ \\(y\\) has an arrow to \\(x\\)</li> <li>\\(iv\\) exogenous to \\(y\\) (desired but not strictly necessary)</li> <li>\\(iv\\) affects \\(y\\) only though correlation with \\(x\\)</li> </ol> <p>If we use a model $$ \\begin{aligned} \\hat y &amp;= \\beta_0 + \\beta_1 x + u\\ \\implies \\beta_1 &amp;= \\dfrac{\\text{Cov}(y, iv)}{\\text{Cov}(x, iv)} \\end{aligned} $$</p>"},{"location":"CS_Electives/Causal_Inference/08_Causal_Graphical_Model/#fixed-effects","title":"Fixed Effects","text":"<p>Variable that captures effect of unobserved confounders $$ \\hat y = \\tau + \\beta_0 + \\beta_1 x + u \\ \\implies \\widehat{\\text{ATE}} = \\beta_1 $$</p>"},{"location":"CS_Electives/Causal_Inference/08_Causal_Graphical_Model/#example_1","title":"Example","text":"<pre><code>flowchart TB\npe([\"Parents&lt;br/&gt;Education&lt;br/&gt;(IV)\"]) --&gt; ce([\"Child&lt;br/&gt;Education&lt;br/&gt;(x)\"]) --&gt; ci([\"Child&lt;br/&gt;Income&lt;br/&gt;(y)\"])\n\npe -.- ag([\"Child&lt;br/&gt;Age, Gender&lt;br/&gt;(s)\"]) --&gt; ce &amp; ci\ncity([\"City&lt;br/&gt;(FE)\"]) --&gt; pe &amp; ce &amp; a &amp; ci\n\na([\"Ability&lt;br/&gt;(s)\"]) -..-&gt; ce &amp; ci\n\nclassDef dotted stroke-dasharray:5\nclass a dotted\n\nclassDef highlight fill:darkred,color:white\nclass pe highlight\n\nlinkStyle 1 stroke:green</code></pre>"},{"location":"CS_Electives/Causal_Inference/08_Causal_Graphical_Model/#parents-education-is-an-instrument-variable-for-childs-wage-through-child-education","title":"Parent\u2019s education is an instrument variable for child\u2019s wage through child education.","text":"<p>College-educated parents have +ve impact on children\u2019s college attainment, either through better home education or because they are more capable of affording college education.</p> <p>If we assume that more educated parents</p> <ul> <li>don\u2019t produce children with higher unobserved abilities/preferences that affect education and wages</li> <li>don\u2019t directly help children obtain higher wage jobs</li> </ul> <p>The statement holds true as the only way parent\u2019s education affects individual\u2019s earnings is through effect on child\u2019s education.</p>"},{"location":"CS_Electives/Causal_Inference/08_Causal_Graphical_Model/#city-is-a-fixed-effect","title":"City is a Fixed Effect","text":"<p>City captures ability, school quality, productivity and other unobserved factors</p>"},{"location":"CS_Electives/Causal_Inference/08_Causal_Graphical_Model/#types-of-intermediary-vars","title":"Types of Intermediary vars","text":"<pre><code>flowchart TB\n\nsubgraph Mediator\ndirection TB\nz3((z)) --&gt; y3\nx3((x)) --&gt; y3((y))\nx3((x)) --&gt; z3\nend\n\nsubgraph Collider\ndirection TB\nx2((x)) --&gt; y2((y))\nx2((x)) &amp; y2((y)) --&gt; z2((z)) \nend\n\nsubgraph Confounder\ndirection TB\nx1((x)) --&gt; y1((y))\nz1((z)) --&gt; x1((x)) &amp; y1((y))\nend</code></pre> Type Should Condition to remove block Confounder \u2705 Collider \u274c Mediator \u274c"},{"location":"CS_Electives/Causal_Inference/08_Causal_Graphical_Model/#intervention","title":"Intervention","text":"<p>\\(\\text{do}(x_i = a)\\) implies removing all arrows entering \\(x_i\\) while setting \\(x_i = a\\), hence removing the effect of all parents of \\(x_i\\)</p> <p>Hence \\(x_i\\) becomes endogenous after intervention</p> <pre><code>flowchart TB\n\nsubgraph Post-Intervention\ndirection TB\na2((a)) &amp; b2((b)) --- c2((c)) --&gt; d2((d))\nend\n\nsubgraph Pre-Intervention\ndirection TB\na1((a)) &amp; b1((b)) --&gt; c1((c)) --&gt; d1((d))\nend\n\nlinkStyle 0,1 stroke:none</code></pre> Pre-Intervention \\(p(A, B, C, D) = p(D \\vert C) \\cdot p(C \\vert A, B) \\cdot P(A) \\cdot p(B)\\) Post-Intervention \\(p( \\ A, B, C, D \\vert \\text{do}(C=c) \\ ) = p(D \\vert C=c) \\cdot P(A) \\cdot p(B)\\) \\[ p(A \\vert \\text{do}(C)) = p(A) \\\\ p(A \\vert C) \\ne p(A) \\]"},{"location":"CS_Electives/Causal_Inference/08_Causal_Graphical_Model/#idk","title":"IDK","text":"\\[ \\begin{aligned} p(x_a, \\dots, x_b) &amp;= \\Pi_i \\  P(x_i \\vert \\text{pa}(x_i)) &amp;&amp; i = [a, b] \\\\ p(x_a, \\dots, x_b \\vert \\text{do}(x_c=1))  &amp;= \\Pi_i \\  P(x_i \\vert \\text{pa}(x_i)) &amp;&amp; i = [a, b], i \\ne c \\end{aligned} \\]"},{"location":"CS_Electives/Causal_Inference/08_Causal_Graphical_Model/#limitation-of-causal-graph","title":"Limitation of Causal Graph","text":"<p>There is no way for causal graph to represent the causal effect of input \\(x\\) on output \\(y\\) due to some effect modifier. For example</p> <pre><code>flowchart TD\nf((\"Fertilizer&lt;br/&gt; (x)\")) &amp; t((\"Temperature&lt;br/&gt; (s&lt;sup&gt;y&lt;/sup&gt;)\")) --&gt; y((\"Yield&lt;br/&gt; (y)\"))</code></pre> <p>Temperature does not affect how much usage of fertilizer, but we know that the temperature affects the effectiveness of the fertilizer, but we have no way of representing that here. However, the Causal Effect Formula will take care of that.</p>"},{"location":"CS_Electives/Causal_Inference/09_Causal_Effect_Learning/","title":"Causal Effect Learning","text":"<p>Causal effect learning = statistical learning with</p> <ul> <li>Target function: \\(E[y \\vert \\text{do}(x)]\\), instead of \\(E[y \\vert x]\\)</li> </ul> <p>or - Target distribution: \\(p(y|\\text{do}(x))\\), instead of \\(p(y \\vert x)\\)</p>"},{"location":"CS_Electives/Causal_Inference/09_Causal_Effect_Learning/#probabilistic","title":"Probabilistic","text":"<p>Causal effect of a treatment is a probability distribution: it is not the same for every individual.</p> <ul> <li>Learning the individual-level is nearly impossible</li> <li>Learning the pdf of the effect is hard</li> </ul> <p>Hence, we use the Average Treatment Effect</p>"},{"location":"CS_Electives/Causal_Inference/09_Causal_Effect_Learning/#stages-of-causal-effect-learning","title":"Stages of Causal Effect Learning","text":"<ol> <li>Identification of causes: Causal Reasoning    Just reasoning and understanding; no math, statistics</li> <li>Reformulate into statistical form</li> <li>Estimation</li> </ol>"},{"location":"CS_Electives/Causal_Inference/09_Causal_Effect_Learning/#identifiability-of-causal-effects","title":"Identifiability of Causal Effects","text":"<p>Causal effect \\(\\theta(M)\\) is identifiable if it can be uniquely determined from \\(D\\)</p> <ul> <li>\\(M=\\) Model with vars \\(x=(x_1, \\dots, x_n)\\)</li> <li>\\(v = (v_1, \\dots, v_m) =\\) set of observed RVs</li> <li>\\(D \\sim p(v) =\\) Observed data</li> <li>\\(\\theta(M) = g_M(x)\\) be a function of \\(x\\) according to \\(M\\), which is the causal effect of \\(x_i\\) on \\(x_j\\)</li> </ul>"},{"location":"CS_Electives/Causal_Inference/09_Causal_Effect_Learning/#identification","title":"Identification","text":"<p>Express \\(P(y \\vert \\text{do}(x) \\ )\\) in terms of \\(P(v)\\), where \\(v\\) is the set of observed variables \\(\\{ x, y, \\dots \\}\\)</p> <p>Questions</p> <ol> <li>Is it possible to learn the causal effect of our interest from the available observable variables.</li> <li>What causal assumptions do we need to make to do so?</li> </ol>"},{"location":"CS_Electives/Causal_Inference/09_Causal_Effect_Learning/#types","title":"Types","text":"Type Identification Example Non-Parametric w/o any parametric assumptions on the relationships among the variables RCT Parametric Statistical and functional form assumptions are involved Observational studies"},{"location":"CS_Electives/Causal_Inference/09_Causal_Effect_Learning/#estimation","title":"Estimation","text":"<p>Learn the identified causal effect from a finite sample</p> <p>Once we have established identification using causal reasoning based on causal models, we are left with a pure statistical learning problem</p>"},{"location":"CS_Electives/Causal_Inference/09_Causal_Effect_Learning/#types_1","title":"Types","text":"Type Estimation Non-Parametric w/o any parametric assumptions on the relationships among the variables Parametric Statistical and functional form assumptions are involved"},{"location":"CS_Electives/Causal_Inference/09_Causal_Effect_Learning/#idk","title":"IDK","text":"<p>Hence we have a matrix</p> Identification | Estimation Parametric Non-Parametric Parametric Non-Parametric"},{"location":"CS_Electives/Causal_Inference/09_Causal_Effect_Learning/#limitations-of-causal-effect-learning","title":"Limitations of Causal Effect learning","text":"<ul> <li>Causal effects do not exist in a vacuum. They are usually the effects of complex and scientific/economic processes</li> <li>Causal effects are limited to the scope of the study, ie, population-specific. When we talk about \u201cthe causal effect of \\(x\\) on \\(y\\)\u201d, it is always wrt to a specific population within a specific social, cultural, and economic environment.</li> </ul>"},{"location":"CS_Electives/Causal_Inference/09_Causal_Effect_Learning/#intervention","title":"Intervention","text":"<p>The effect of an intervention \\(\\text{do}(x_i=a)\\) is to transform the pre-intervention distribution into the post-intervention distribution $$ \\begin{aligned} P(  x_{-i} \\vert \\text{do}(x_i=a)  ) &amp;= \\Pi_{j \\ne i}   p(  x_j \\vert \\text{pa}(x_j)  ) \\ &amp;= \\dfrac{p(x_{-i}, x_i = a)}{p(  x_i = a \\vert \\text{pa}(x_i)  )} \\ &amp;= p(  x_{-i} \\vert x_i = a, \\text{pa}(x_i)  ) \\times p(  \\text{pa}(x_i)  ) \\end{aligned} $$</p>"},{"location":"CS_Electives/Causal_Inference/09_Causal_Effect_Learning/#causal-effect-formula","title":"Causal Effect Formula","text":""},{"location":"CS_Electives/Causal_Inference/09_Causal_Effect_Learning/#general-formula","title":"General Formula","text":"\\[ \\begin{aligned} &amp;P(y| \\ \\text{do}(x) \\ ) \\\\ &amp;= \\int P(y| \\ \\text{do}(x), s^y \\ ) \\cdot P(s^y) \\cdot ds^y \\end{aligned} \\]"},{"location":"CS_Electives/Causal_Inference/09_Causal_Effect_Learning/#rct","title":"RCT","text":"\\[ \\begin{aligned} &amp;P(y| \\ \\text{do}(x) \\ ) \\\\ &amp;= P(y \\vert x) \\end{aligned} \\]"},{"location":"CS_Electives/Causal_Inference/09_Causal_Effect_Learning/#observational-studies","title":"Observational Studies","text":"<p>Let \\(z=\\text{pa}(x)\\), ie all the causes of \\(X\\)  $$ \\begin{aligned} p(  y \\vert \\text{do}(x)  ) &amp;= \\int p(y \\vert x, z) \\cdot p(z) \\cdot dz \\ &amp; \\quad \\text{(Causation)} \\ p(  y \\vert x  ) &amp;= \\int p(y \\vert x, z) \\cdot p(z \\vert x) \\cdot dz \\ &amp; \\quad \\text{(Correlation)} \\end{aligned} $$</p> <p>Assuming we observe a random sample of \\(\\{ x, y \\}\\) $$ \\begin{aligned} \\ x {\\small \\coprod} z \\implies p(z \\vert x) &amp;= p(z) \\ \\implies p(y \\vert \\text{do}(x), z) &amp;= p(y \\vert x, z) \\ E[y \\vert \\text{do}(x), z] &amp;= E[y \\vert x, z] \\end{aligned} $$</p> <p>$$ \\begin{aligned} &amp; p(  y \\vert \\text{do}(x=a)  ) \\ &amp;= \\int p( y \\vert \\text{do}(x=a), z ) \\cdot p(  z  ) \\cdot dz \\ &amp;= \\int p(y \\vert x=a, z) \\cdot p(  z  ) \\cdot dz \\ &amp;E(  y \\vert \\text{do}(x=a)  ) \\ &amp;= \\int E[y \\vert \\text{do}(x=a), z] \\cdot p(  z  ) \\cdot dz \\ &amp;= \\int E[y \\vert x=a, z] \\cdot p(  z  ) \\cdot dz \\end{aligned} $$ where</p> <ul> <li>\\(E[y \\vert x=a, z]\\) can be estimated from data using any appropriate statistical model</li> <li>\\(p(z)\\) can be estimated using empirical distribution</li> </ul> <p>What if we don't observe \\(z\\)?</p> \\(z\\) Fully Observed Identifiability of \\(p( \\ y \\vert \\text{do}(x) \\ )\\) \u2705 \u2705 Non-parametrically \u274c \u274c\u2705 Non-parametrically only if identification criteria satisfied"},{"location":"CS_Electives/Causal_Inference/09_Causal_Effect_Learning/#non-parametric-identification","title":"Non-Parametric Identification","text":"<p>Provide sufficient conditions</p>"},{"location":"CS_Electives/Causal_Inference/09_Causal_Effect_Learning/#back-door-criterion","title":"Back-Door Criterion","text":"<p>Set of variables \\(s\\) satisfy this criterion \\(\\iff\\)</p> <ol> <li>conditioning on \\(s\\) blocks every back-door path from \\(x\\) to \\(y\\)</li> <li>no var in \\(s\\) is a descendant of \\(x\\): \\(s\\) only includes pre-treatment vars</li> </ol> <p>IDK</p> <ul> <li>\\(x\\) is exogenous to \\(y \\iff \\not \\exists\\) open back-door path from \\(x\\) to \\(y\\): \\(p(y \\vert \\text{do}(x)) = p(y \\vert x)\\)</li> <li>\\(x\\) is conditionally exogenous to \\(y \\iff \\not \\exists\\) open back-door path from \\(x\\) to \\(y\\) after conditioning on \\(s\\): \\(p(y \\vert \\text{do}(x), s) = p(y \\vert x, s)\\)</li> </ul> <p>When \\(x\\) is (conditionally) exogenous to y, individual units with different values of \\(x\\) are (conditionally) exchangeable with respect to \\(y\\), in which case an observational study resembles a (conditionally) randomized experiment: treatment assignment mechanism is ignorable $$ s^y \\coprod x \\vert s^B $$ where \\(s^B =\\) set of variables that meet back-door condition</p> <p>idk</p> <ul> <li>\\(\\exists\\) an open back-door path from \\(x\\) to a variable \\(s \\in s^y\\) such that \\(s \\centernot {\\small \\coprod} x \\iff \\exists\\) open back-door path from \\(x \\to y\\)</li> <li>There, if conditioning on \\(s^B\\) blocks all back-door paths from \\(x \\to y \\implies s {\\small \\coprod} x \\vert s^B, \\quad \\forall s \\in s^y\\)</li> <li>Therefore, back-door criterion can be equivalently states as requiring that no direct causes of \\(y\\) are correlated with \\(x\\) conditional on \\(s^B\\)</li> <li>\\(x\\) is exogenous to \\(y\\) conditioning on \\(s^B\\) if \\(s^y {\\small \\coprod} x \\vert s^B\\), else it is endogenous</li> </ul> \\[ \\begin{aligned} p( \\ y \\vert \\text{do}(x), s^B \\ ) &amp;= p( \\ y \\vert x, s^B \\ ) \\\\ p( \\ y \\vert \\text{do}(x) \\ ) &amp;= \\int p( \\ y \\vert x, s^B \\ ) \\cdot p(s^B) \\cdot d s^B \\end{aligned} \\]"},{"location":"CS_Electives/Causal_Inference/09_Causal_Effect_Learning/#front-door-criterion","title":"Front-Door Criterion","text":"<p>If we can establish an isolated and exhaustive mechanism that relates \\(x \\to y\\), then the causal effect of \\(x\\) on \\(y\\) can be calculated as it propagates through the mechanism</p> <p>A set of variables \\(s^F\\) satisfies the front-door criterion when</p> <ol> <li>conditioning on \\(s^F\\) blocks all causal paths from x to \\(y\\)</li> </ol> <p>and</p> <ol> <li>\\(x\\) is exogenous to \\(s^F\\), ie no open back-door paths exist from x to \\(s^F\\)</li> </ol> <p>and</p> <ol> <li>\\(s^F\\) is exogenous to \\(y\\) conditional on \\(x\\), ie conditioning on \\(x\\) blocks all back-door paths from \\(s^F\\) to \\(y\\)</li> </ol> <pre><code>flowchart LR\nU --&gt; x &amp; y\nx --&gt; M --&gt; y</code></pre> <p>\\(x \\to M \\to y\\) represents an mechanism that is</p> <ul> <li>isolated: not affected by U</li> <li>exhaustive: only causal path from \\(x \\to y\\)</li> <li>all of the effects of \\(x\\) on \\(y\\) is mediated through \\(x\\)\u2019s effect on \\(M\\)</li> <li>\\(M\\)\u2019s effect on \\(y\\) is confounded by the back-door path \\(M \\leftarrow x \\leftarrow U \\to y\\) but \\(x\\) blocks this path</li> <li>We can find \\(p(y \\vert \\text{do}(x))\\)<ul> <li>directly find \\(p(M \\vert \\text{do}(x)) = p(M \\vert x)\\)</li> <li>use perform back-door adjustment to find \\(p(y \\vert \\text{do(M)})\\)</li> </ul> </li> </ul> \\[ \\begin{aligned} &amp;p( \\ y \\vert \\text{do}(x) \\ ) \\\\ &amp;= \\int p \\left( \\ y \\vert \\text{do}(s^F) \\ \\right) \\cdot p \\left( \\ s^F \\vert \\text{do}(x) \\ \\right) \\cdot ds^F \\\\ &amp;= \\int \\left[{\\small \\int} p \\left( \\ y \\vert (s^F, x) \\cdot p(x) \\cdot dx \\ \\right) \\right] \\cdot p \\left( \\ s^F \\vert \\text{do}(x) \\ \\right) \\cdot ds^F \\end{aligned} \\]"},{"location":"CS_Electives/Causal_Inference/09_Causal_Effect_Learning/#parametric-identification","title":"Parametric Identification","text":"<p>If we use observational data, but neither the back-door nor the front-door criterion is satisfied by the variables that we observe, then we may need additional parametric assumptions in order to identify the causal effect of interest</p>"},{"location":"CS_Electives/Causal_Inference/09_Causal_Effect_Learning/#instrumental-vars-strategy","title":"Instrumental Vars Strategy","text":"<pre><code>flowchart TB\n\nsubgraph Scenario B\ndirection TB\nu2((u)) --&gt; x2((x)) &amp; y2((y))\nx2 --&gt; y2\nv((v)) --&gt; I2((I)) &amp; x2\nend\n\nsubgraph Scenario A\ndirection TB\nu((u)) --&gt; x((x)) &amp; y((y))\nI((I)) --&gt; x --&gt; y\nend</code></pre> <p>The causal effect of \\(x\\) on \\(y\\) is not identified if \\(U\\) is not observed and we assume only the causal relationships encoded in this diagram. If in addition we assume \\(Y = \u03b2X + \u03b1U\\), then \\(\u03b2\\) can be identified using \\(I\\) as an instrument</p> <p>A variable \\(s^I\\) can serve as an instrumental variable (IV) for identifying the causal effect of \\(x\\) on \\(y\\) if</p> <ol> <li>\\(I\\) is associated with x</li> <li>\\(I\\) is associated with \\(e\\), where \\(y=\\beta_1 x + e\\)</li> </ol> <p>Every open path connecting \\(I\\) and \\(y\\) has an arrow pointing into \\(x\\): \\(I\\) is d-separated and independent from any common causes of \\(x\\) and \\(y\\), since \\(x\\) is a collider on their paths</p> <p>Both conditions imply that any association between \\(I\\) and \\(y\\) could only be a result of the association between \\(I\\) and \\(x\\) and the causal effect of \\(x\\) on \\(y\\)</p> <p>One of the simplest parametric assumptions under which an IV strategy works is that of a linear model: \\(y = \\beta x + \\alpha u\\), where \\(u\\) are the causes of \\(y\\) other than \\(x\\), including those that are \\(x\\) and \\(y\\)\u2019s common causes $$ \\begin{aligned} &amp; \\text{Causal Effect } \\ &amp; \\implies \\beta = \\dfrac{\\text{Cov}(y, I)}{\\text{Cov}(x, I)} \\end{aligned} $$</p>"},{"location":"CS_Electives/Causal_Inference/09_Causal_Effect_Learning/#sample-selection-bias","title":"Sample Selection Bias","text":"<p>Let \\(c \\in \\{0, 1\\}\\) indicate whether an individual unit belongs to the subpopulation.</p> <p>The subpopulation with \\(c = 0\\) and the subpopulation with \\(c = 1\\) are not exchangeable. Hence, analyses based on samples drawn from the subpopulation can be thought of as analyses using the whole population, but conditional on \\(c = 1\\)</p> <p>Suppose we want to learn \\(p(y)\\) in the whole population, then \\(c\\) can be considered as a treatment itself: \\(p(y ) = p (y | \\text{do} (c = 1))\\), i.e. the distribution of \\(y\\), where everyone is \u2018assigned\u2019 to the \u2018observed group\u2019</p> <p>Hence nonparametric identification of \\(p (y)\\) relies on whether we can make \\(c\\) exogenous to \\(y\\), which can be done by blocking all backdoor paths between \\(c\\) and \\(y\\)</p>"},{"location":"CS_Electives/Causal_Inference/09_Causal_Effect_Learning/#credit-card-default","title":"Credit Card Default","text":"<p>Suppose credit card companies determine whether to accept \\(x\\) credit card applications based solely on income \\(I\\). Once a person is issued a credit card, income determines the probability of her default (\\(y\\))</p> <p>Assume we observe I for entire population, but only observe \\(y\\) for \\(x = 1\\)</p> <ul> <li>Observed: \\(p(y \\vert x=1)\\)</li> <li>Interested: \\(p(y \\vert \\text{do}(x=1))\\)</li> <li>Interested: \\(p(y)\\)</li> </ul> <pre><code>flowchart TB\nI((I)) --&gt; Y((y)) &amp; C[C=1]</code></pre> <p>Since there is sample selection bias to estimate effect of \\(c\\) on \\(y\\):</p> \\[ p (y |\\text{do}(c = 1)) \\ne p (y |c = 1) \\] <p>Since \\(I\\) satisfies backdoor condition for \\(c \\to y\\) $$ \\begin{aligned} p (y |\\text{do}(c = 1)) &amp;= p (y) \\ &amp;=\\int p( y \\vert c=1, I) \\cdot p(I) \\cdot dI \\end{aligned} $$</p> <p>There is no sample selection bias here in learning the causal effect of \\(I\\) on \\(y\\): $$ \\begin{aligned} &amp; p(y \\vert \\text{do}(I)) \\ &amp;= p (  y \\vert \\text{do}(I, c=1)  ) \\ &amp;= p (  y \\vert \\text{do}(I), c=1  ) \\end{aligned} $$</p>"},{"location":"CS_Electives/Causal_Inference/09_Causal_Effect_Learning/#idk_1","title":"IDK","text":"<pre><code>flowchart TB\nx((x)) --&gt; C[C=1]\nL --&gt; C\nU((U)) --&gt; L((L))\nU ---&gt; y((y))\n\nx -.-&gt;|Cauality?| y</code></pre> <p>\\(C\\) is a collider on the path \\(x \\to C \\leftarrow L \\leftarrow U \\to y\\). Hence, the path is blocked without conditioning on \\(C\\) $$ p( y \\vert \\text{do}(x) ) \\ne  p( y \\vert x) $$ Conditioning \\(c=1\\) opens the path, making \\(A\\) and \\(y\\) associated without causal, leading sample selection bias in learning \\(p(y \\vert \\text{do}(A))\\) $$ p( y \\vert \\text{do}(x) ) \\ne  p( y \\vert  x) $$ Assume we observe</p> <ul> <li>\\(\\{ x, L \\}\\) for both \\(C = 0\\) and \\(C = 1\\)</li> <li>\\(y\\) only for \\(C = 1\\)</li> </ul> <p>Since conditioning on L blocks all back-door paths from \\(x \\to Y\\) and from \\(C \\to Y\\), we have $$ \\begin{aligned} &amp;p( y \\vert \\text{do}(x) ) \\ &amp;= p( y \\vert \\text{do}(x, c=1) ) \\ &amp;= \\int p( y \\vert x, L, c=1 ) \\cdot p(L) \\cdot dL \\end{aligned} $$ Hence, \\(p(y \\vert \\text{do}(x))\\) is non-parametrically identifiable and estimated</p>"},{"location":"CS_Electives/Causal_Inference/09_Causal_Effect_Learning/#observational-studies-vs-rct","title":"Observational Studies vs RCT","text":"<p>RCT helps ensure that the links from \\(\\text{pa}(x)\\) to \\(x\\) are broken, so that \\(x \\coprod pa (x) \\implies p (\\text{pa} (x) |x = a) = p (\\text{pa} (x) |x = b)\\). The distribution of \\(\\text{pa}(x)\\) is balanced across the different values of treatment \\(x \\implies x\\) becomes exogenous, so there is no confounding to worry about</p> <p>Compared with observational studies, RCTs have less knowledge requirement: we do not need to know the treatment assignment mechanisms that generate the observational data, since we now determine the mechanism ourselves. In graph terms, only knowledge of the post-intervention causal diagram is needed</p> <p>A major limitation of RCTs, as already discussed, is that it is hard to generate data from a \\(p(y \\vert \\text{do}(x))\\) that is the result of a desired distribution of effect modifiers</p>"},{"location":"CS_Electives/Causal_Inference/09_Causal_Effect_Learning/#idk_2","title":"IDK","text":""},{"location":"CS_Electives/Causal_Inference/09_Causal_Effect_Learning/#fumigation-and-yield","title":"Fumigation and Yield","text":"<p>Suppose the mechanism that generates our observed data works as follows: fumigation (\\(X\\)) helps control eelworms (E), which affects crop yield (Y). Farmers\u2019 use of fumigation is affected by weather (W) and the price of fumigants (C). Weather also affects yield independently. Finally, we observe the equilibrium crop price (P), which is affected by both the price of fumigants and the realized crop yield</p> <pre><code>flowchart TB\nx((x)) --&gt; f((f)) &amp; y((y))\ny((y)) --&gt; p((p))\nc((c)) --&gt; f((f)) &amp; p((p))\nf((f)) --&gt; e((e))\ne((e)) --&gt; y((y))</code></pre> <p>Assuming \\(X\\) is binary and other variables are discrete $$ \\begin{aligned} &amp; P( y \\vert \\text{do}(X=1) ) \\ &amp;= \\sum_{w, c} p(y \\vert X=1, W=w, C=c) \\times p(W=w) \\times p(C=c) \\end{aligned} $$</p>"},{"location":"CS_Electives/Causal_Inference/10_Econometrics/","title":"Econometrics","text":"<p>Seeks to link economic theory to data</p> <p>Intermediate between mathematics, statistics, and economics, we find a new discipline which for lack of a better name, may be called econometrics.</p> <p>Econometrics has as its aim to subject abstract laws of theoretical political economy or \u2019pure\u2019 economics to experimental and numerical verification, and thus to turn pure economics, as far as possible, into a science in the strict sense of the word.</p> <p>\u2026</p> <p>So far, we have been unable to find any better word than \"econometrics\". We are aware of the fact that in the beginning somebody might misinterpret this word to mean economic statistics only. But ... we believe that it will soon become clear to everybody that the society is interested in economic theory just as much as in anything else.</p> <p>~ Ragnar Frisch</p>"},{"location":"CS_Electives/Causal_Inference/10_Econometrics/#causal-reasoning","title":"Causal Reasoning","text":"<p>The econometric approach to causal reasoning starts with assuming a \u201ctrue model\u201d $$ y = \\alpha_0 + \\alpha_1 x + u $$ where</p> <ul> <li>\\(x\\) is observed</li> <li>\\(u\\) is unobserved</li> <li>\\(\\alpha_1\\) represents the average   causal effect of \\(x\\) on \\(y\\)</li> </ul> <p>To estimate \\(\\beta\\), we run a linear regression $$ y = \\beta_0 + \\beta_1 x + e $$ \\(u\\) and \\(x\\) are uncorrelated \\(\\implies\\) back-door criterion is satisfied: \\(x\\) is exogenous and OLS estimation produces that \\(\\hat \\beta\\) that is a unbiased estimate of \\(\\beta\\). Else, \\(x\\) is endogenous and \\(\\beta\\) is biased estimate of \\(\\beta\\)</p> <p>\\(\\beta\\) is a statistical parameter; \\(\\alpha\\) is a causal parameter</p>"},{"location":"CS_Electives/Causal_Inference/10_Econometrics/#limitations","title":"Limitations","text":""},{"location":"CS_Electives/Causal_Inference/10_Econometrics/#statistical-vs-structural","title":"Statistical vs structural","text":"<p>The econometric approach does not clearly distinguish between causal reasoning and statistical modeling, ie what assumptions are causal and what are statistical - Just because does \\(x\\) and \\(y\\) are not statistically related, does not mean that \\(x\\) does not cause \\(y\\) - Most econometrics textbooks use one equation to represent both models, confusing what is causal and what is statistical.</p> <p>Econometric literature states: \u201cWhen the error term is correlated with the regressor, the estimation result is biased\u201d - By the \u201cerror term\u201d, it is referring to \\(u\\), not \\(e\\) - By \u201cestimation result is biased\u201d, it is saying that \\(E[\\hat \\beta] \\ne \\alpha\\)   - ie, \\(\\hat \\beta\\) is an unbiased estimate of statistical parameter \\(\\beta\\), but a biased estimate of the causal parameter \\(\\alpha\\)</p> <p>The requirement that \\(u\\) be (linearly) uncorrelated with \\(x\\) is often stated as an essential assumption on the linear regression model itself, under which the OLS estimator is unbiased \u2013 again confusing what is causal and what is statistical</p> <p>Failure to difference b/w statistical &amp; structural equations can lead to confusion.</p> <p>For eg, the following causal model with the causal effect of \\(x\\) on \\(y\\) is 0, ie no effect $$ \\begin{aligned} x &amp; \u223c U (0, 1) \\ y &amp; \u223c U (0, 1) \\ u &amp; \\leftarrow y\u2212x \\end{aligned} $$ However, the causal model implies the following statistical equation: \\(y = \\alpha x + u\\) , where \\(\\alpha = 1\\) and \\(u\\) is correlated with both \\(x\\) and \\(y\\)</p> <p>Failure to understand this as a statistical rather than structural equation will lead us to wrongly conclude that a regression of y on x will produce biased causal estimates \u2013 it won\u2019t. It will only produce a biased estimate of \\(\\alpha\\), but \\(\\alpha\\) is not the average causal effect of \\(x\\) on \\(y\\)</p>"},{"location":"CS_Electives/Causal_Inference/10_Econometrics/#identification-estimation","title":"Identification &amp; Estimation","text":"<p>The econometric approach does not clearly distinguish between nonparametric and parametric identification, and between identification and estimation.</p> <p>Identification in the econometric approach refers to whether the parameters in a \u201ctrue model\u201d can be uniquely determined given infinite data on the observed variables. A \u201ctrue model\u201d, however, already makes parametric assumptions on the underlying causal structure.</p> <p>Therefore, while causal inference based on causal graphical models recognizes causal effect learning as a two-stage process \u2013 identification and estimation \u2013 and uses the back-door and front-door criteria to establish clear rules for nonparametric identification, the econometric approach fails to do so. This leads to confusion over how to apply statistical and machine learning models for causal inference.</p>"},{"location":"CS_Electives/Causal_Inference/10_Econometrics/#control-variable-selection","title":"Control Variable Selection","text":"<p>The econometric approach does not provide an easily operational way of choosing control variables for identifying a desired causal effect.</p> <p>If \\(x\\) is endogenous, we can try to find control variables \\(z\\) such that: conditional on \\(z\\), \\(u\\) is no longer correlated with \\(x\\) in the following model $$ y = \\alpha_0 + \\alpha_1 x + \\alpha_2 z + u $$ Then, running a regression $$ y = \\beta_0 + \\beta_1 x + \\alpha_2 z + u $$ \\(\\hat \\beta\\) will be an unbiased estimate of \\(\\alpha\\)</p> <p>The requirement that \\(u\\) be (linearly) uncorrelated with \\(x\\) conditional on \\(z\\) can be thought of as implied by the back-door criterion. However, unlike the back-door criterion, the econometric approach to causal reasoning does not offer a clear guidance on the choice of \\(z\\), because it is not based on a clear thinking of the underlying causal mechanism (such as a causal diagram representation); clear thinking on causal mechanism forms part of the appeal of structural estimation over the reduced-form approach</p> <p>The statistics and econometrics literature often state that in order for causal effect to be identifiable by conditioning \u2013 there must be no unmeasured confounding (statistics) or selection on observables (econometrics). However, the back-door criterion shows that we do not need to observe and condition on all confounders, only a sufficient set of variables that renders all back-door paths blocked.</p> <p>There are now more than one \u201ctrue models\u201d, because for most problems, multiple sets of variables exist that can render all back-door paths blocked.</p> <ul> <li>The econometrics literature defines omitted variable bias as the bias that arises when a variable is omitted from the \u201ctrue model.\u201d But with more than one true model \u2013 with multiple sets of \\(z\\) that can sufficiently control for confounding \u2013 what is an omitted variable?</li> </ul> <p>Meaning of \u201ctrue model\u201d is now no longer clear. Once we include control variables \\(z\\), \u201ctrue model\u201d can no longer be interpreted as a structural equation in a causal model</p> <ul> <li>The back-door criterion makes it clear that z can include not only direct causes of \\(y\\), but also variables that are not causes of \\(y\\). Therefore, \u201ctrue model\u201d is no longer a structural equation specifying the relation between y and its determinants.</li> <li>The meaning of the equation itself has become unclear</li> <li>Unclear of its ability to offer meaningful guidance on the choice of \\(z\\)</li> <li>The back-door criterion also makes it clear that \\(z\\) should not include descendants of \\(x\\) and should not include colliders that may open a back-door path. The econometric approach offers none of these guidances</li> </ul>"},{"location":"CS_Electives/Causal_Inference/11_Causal_Mechanism_Learning/","title":"Causal Mechanism Learning","text":"<p>To understand the true meaning and scope of a causal effect, we need to understand the underlying causal mechanism, based on prior knowledge - information and analyses.</p> <p>This is important to understand</p> <ul> <li>causal effect - what it means, where it applies</li> <li>transportability of results</li> </ul> <p>Blindly following the causal effect, without understanding the underlying mechanism, is incorrect.</p> <p>The learning problem is to choose the causal model from a hypothesis set that best fits our observed data</p> <p>A causal structure is a set of conditional independence relations. Its identifiability depends on whether these relations can be uniquely determined by the set of conditional independence relations observed in the population.</p> <p>Causal relationships are more stable than causal effects and statistical relationships. That\u2019s why our knowledge about the physical world is largely encoded and transmitted in the qualitative language of causal relationships (\u201cpushing the glass off the table will cause it to break\u201d), rather than the quantitative language of causal effects and statistical relationships (\u201cpushing the glass off the table will result in a 95% probability of breakage.\u201d)</p>"},{"location":"CS_Electives/Causal_Inference/11_Causal_Mechanism_Learning/#observationally-equivalent","title":"Observationally-equivalent","text":"<p>Causal structures that imply the same set of conditional independence relations: 2 DAGs are compatible with the same probability distribution $$ \\begin{aligned} &amp; A \\to B \\quad B \\to A \\ \\implies &amp; P(A, B)  \\ne P(A) \\cdot P(B) \\end{aligned} $$ In this case, they cannot be distinguished without resorting to manipulative experimentation or temporal information. Hence, experiments are required to identify observationally-equivalent causal structures</p>"},{"location":"CS_Electives/Causal_Inference/11_Causal_Mechanism_Learning/#markov-equivalence","title":"Markov Equivalence","text":"<p>Two DAGs are observationally equivalent \\(\\iff\\) they have the same skeleton and the same set of immoralities, where</p> <ul> <li>skeleton: nodes of a directed graph</li> <li>immoralities: configuration of 3 nodes: A,B,C, such that</li> <li>C is a child of both A and B</li> <li>A and B not directly connected</li> <li>It\u2019s called immorality because C is a child of A and B, but they\u2019re not \u2018married\u2019</li> </ul>"},{"location":"CS_Electives/Causal_Inference/11_Causal_Mechanism_Learning/#scientific-progress","title":"Scientific Progress","text":"<p>We (human beings) have been learning causal mechanisms by formulating models, then conducting experiments or observational studies, and based on the results of which, updating our belief about each model\u2019s probability of being true. This, in essence, is the process of scientific progress</p> <p>This view of scientific progress as continuous Bayesian updating based on evidence has been challenged by historians like Thomas Kuhn, who pointed out that the sociological nature of the scientific community leads to periodic paradigm shifts rather than continuous progress</p>"},{"location":"CS_Electives/Causal_Inference/11_Causal_Mechanism_Learning/#structural-estimation","title":"Structural Estimation","text":"<p>Causal models based on theory are referred to as structural models. Their estimation is called structural estimation/\u201cidentification by functional form\u201d</p> <p>Structural estimation \\(\\ne\\) causal mechanism learning. Structural estimation is learning based on an assumed causal mechanism</p> <p>A complete structural model may specify preferences, technology, the information available to agents, the constraints under which they operate, and the rules of interaction among agents in market and social settings</p> <p>Structural models, by explicitly modeling the data-generating causal mechanisms, make clear what prior knowledge (assumptions) are relied upon to draw causal inference.</p> <p>By using theory to specify the functional forms of causal relationships, structural models can be used to</p> <ul> <li>identify causal effects or the values of unobserved variables that cannot be non-parametrically identified</li> <li>serve as a model selection mechanism for causal effects that can be identified non-parametrically</li> </ul> <p>Using structural models, what we learn from one set of data  \\(D \\sim p(x,y)\\) can be potentially used to explain and predict data drawn from another distribution, say \\(p(u,v)\\), if \\(\\{ x,y \\}\\) and \\(\\{ u, v \\}\\) are generated from a similar causal mechanism.</p> <ul> <li>In other words, what we learn from one observed phenomenon can be used to explain and predict other related phenomena.</li> <li>For example, we can learn individuals\u2019 risk aversion from their investment behavior, which can help explain &amp; predict their career choices.</li> </ul> <p>Structural models make it possible to predict effects of existing treatments in a new population/environment, or the effects of completely new treatments.</p> <ul> <li>To do so, a structural model must be \u201cdeep\u201d enough so that its parameters remain invariant in the new population/environment, or when new treatments are applied.</li> <li>The concept of invariance is closely related to the concept of stability for causal relationships. The need for invariant parameters is key to causal analysis and policy evaluation.</li> </ul> <p>Once we have learned a structural model, we can use it to generate synthetic data and perform counterfactual simulations.</p> <p>Structural models allow the economist to make welfare calculations and normative statements; Individual choices reveal information about their preferences and the potential outcomes they face</p> <p>Structural models can potentially deliver better predictive performance than statistical models trained on single data sets, because their parameters can be learned from a combination of data from various sources that share the same underlying causal mechanism</p>"},{"location":"CS_Electives/Causal_Inference/11_Causal_Mechanism_Learning/#program-evaluation","title":"Program Evaluation","text":"Validity Evaluating the impacts of historical programs on outcomes Internal Policy in same country Forecasting the impacts of programs implemented in one population/environment in other populations/environments External Requires structural model Policy in another country Forecasting the impacts of programs never historically experienced. Requires structural model Effect of tax <p>For all three types of problems, if we want to evaluate welfare impact, we need a structural model.</p>"},{"location":"CS_Electives/Causal_Inference/11_Causal_Mechanism_Learning/#counterfactual-simulation","title":"Counterfactual Simulation","text":"<p>One of the main benefits of learning a structural model is that it allows us to predict the effect of a completely new treatment \u2013 a treatment that has never been observed before</p> <p>For eg: If in the observed data, \\(x_j =0\\) always, what would be the effect of \\(\\text{do} (x_j = 1)\\)?</p> <p>Because structural models are generative models, once we have learned a model, we can use it to generate synthetic data</p> <p>\\(D= \\{(x_{i, 1}, \\dots, x_{i,j} = a, x_{i,j+1}, x_{i,n}) \\}\\) from \\(p(x_1, \\dots, x_n|\\text{do}(x_j= a))\\)</p>"},{"location":"CS_Electives/Causal_Inference/11_Causal_Mechanism_Learning/#dynamic-structural-model","title":"Dynamic Structural Model","text":"<p>In a changing environment, with new information arriving each period, individual are forward-looking when making decisions: choices are made partly based on expectations of the future.</p> <p>Decisions are also often influenced by the past. Since it can be costly to transition from one state to another, payoffs to different choices are often history-dependent: our past partly shapes our future.</p> <p>In dynamic models, treatment effects can be time-varying and it\u2019s often useful to distinguish between short-run and long-run effects</p>"},{"location":"CS_Electives/Causal_Inference/11_Causal_Mechanism_Learning/#negative-political-advertising","title":"Negative Political Advertising","text":"<ul> <li>Candidate decides whether to go negative based on polling</li> <li>Going negative affects future polling, which in turn, affects future   negative advertising decisions.</li> <li>Outcome: final vote share.</li> </ul> <p>Static (single-shot) causal inference</p> <p></p> <p>Dynamic</p> <p></p>"},{"location":"CS_Electives/Causal_Inference/11_Causal_Mechanism_Learning/#idk","title":"IDK","text":"\\[ L_t = \\beta_0 + \\beta_1 i_t \\] <p>where</p> <ul> <li>\\(L_t=\\) Leverage taken up by financial firms</li> <li>\\(i_t=\\) Interest rate set by central bank</li> </ul> <p>Problem</p> <ul> <li>\\(L_t\\) is forward-looking: Banks take up loans based on future expectations; nobody makes a decision-based on last year\u2019s interest rate only</li> <li>The effect is smooth &amp; gradual over time, not sudden</li> </ul>"},{"location":"CS_Electives/Causal_Inference/11_Causal_Mechanism_Learning/#crop-supply","title":"Crop Supply","text":"<ul> <li>At the beginning of each period t, each field owner decides whether or not to plant the crop in the current period</li> <li>The decision is based on observed period\u2212t price as well as expectations of future prices.</li> <li>If a field has not been cultivated for k periods, then in order to (re)-cultivate it, the farmer needs to pay a one time cost c (k).</li> <li>Farmers have rational expectation: their expectations of future prices are unbiased conditional on the information they have.</li> <li>Here we assume that crop prices follow an AR(1) process, which is known to the farmers.</li> </ul> <p>Counter-factual simulation</p> <ul> <li>How would crop supply change in response to changes in crop prices   if farmers are myoptic: if they are not forward-looking?</li> <li>How would crop supply change in response to changes in crop prices if farmers are static: if they are neither forward-looking, nor subject to any re-cultivation costs, so that planting decisions are made entirely based on current prices?</li> </ul> <p></p> <p>In general, if we are interested in the effect of x on y, but x is self-selected based on expectations of y, then without any measures of such expectations, the causal effect cannot be non-parametrically identified and we need to rely on theory to specify how expectations are formed.</p> <ul> <li>Sub-population: little/no equilibrium at play</li> <li>Here, farmers take crop prices as given and decide whether or not to plant. Models like this are called dynamic discrete choice models</li> <li>Population: equilibrium at play</li> <li>If prices are endogenous \u2013 if farmers\u2019 planting decisions affect equilibrium prices \u2013 then we need to model both crop supply and crop demand. Such models are called dynamic general equilibrium models.</li> </ul>"},{"location":"CS_Electives/Causal_Inference/12_Game_Theory/","title":"Game Theory","text":""},{"location":"CS_Electives/Causal_Inference/12_Game_Theory/#auction","title":"Auction","text":"<p>Consider \\(n\\) risk-neutral bidders, with independent private value \\(v_i ~ F\\)</p> <p>Each bidder knows their own \\(v_i\\) and distribution \\(F\\), but not the \\(v_i\\) of others</p> <p>Observed winning bids are the Bayesian Nash equilibrium outcome of each game $$ \\begin{aligned} b_i &amp;= v_i - \\dfrac{1}{F(v_i)^{n-1}} \\int \\limits_0^{v_i} F(x)^{n-1} \\cdot dx \\ &amp;= v_i - \\dfrac{1}{n-1} \\dfrac{G_n(b_i)}{g_n(b_i)} \\end{aligned} $$ where</p> <ul> <li>\\(b_i\\) is the bid amount</li> <li>\\(g_n\\) is pdf of bid distribution</li> <li>\\(G_n\\) is cdf of bid distribution</li> </ul> <p>There is no confounding between \\(N\\) (the number of bidders) and \\(b_\\max\\) (the winning bid). Hence \\(f (N) \u2261 E [b_\\max|N]\\)non-parametrically identifies the effect of N on \\(b_\\max\\)</p> <p>The estimation problem is to learn \\(f (N)\\) from data. Here, theory helps specify the functional form of \\(f (N)\\) and therefore serves as a model selection mechanism</p> <p>Theory also helps us to learn the values of the bidders \u2013 which cannot be identified nonparametrically \u2013 by specifying the functional form of the mapping from \\(\\{ v_i \\}\\) to \\(\\{ b_i \\}\\)</p> <p>Furthermore, structural modelling will allows us to obtain other things as well such as: 2<sup>nd</sup>-highest bid, etc</p>"},{"location":"CS_Electives/Causal_Inference/12_Game_Theory/#structural-estimation","title":"Structural Estimation","text":"<ol> <li>For each auction, non-parametrically estimate \\(g_n\\) and \\(G_n\\) from observed bids \\(b_1, \\dots, b_n\\)</li> <li>For each bidder, calculate</li> </ol> \\[ \\hat v_i = b_i + \\dfrac{1}{n-1} \\dfrac{\\hat G_n(b_i)}{\\hat g_n(b_i)} \\] <ol> <li>Estimate \\(\\hat F\\) using \\(\\hat v_i\\)</li> <li>Predict winning bid</li> </ol> \\[ \\begin{aligned} &amp;E[\\max \\{ b_i \\}] \\\\ &amp;= E \\Bigg[ \\max \\Big\\{ v_i - \\dfrac{1}{\\hat F(v_i)^{n-1}} \\int\\limits_0^{v_i} \\hat F(x^{n-1}) \\cdot dx \\Big\\} \\Bigg] \\end{aligned} \\]"},{"location":"CS_Electives/Causal_Inference/12_Game_Theory/#monopoly","title":"Monopoly","text":"<p>In each market \\(m\\) with population \\(N_m\\) and mean income \\(I_m\\), consumers choose between monopoly product and an outside good. Individual utilities are given by:</p> <p>For each market \\(m\\), given demand \\(q_m(p)\\), the monopoly firm chooses \\(p\\) that maximizes its revenue $$ \\begin{aligned} u_{i0}^m &amp;= \\epsilon_{i0}^m \\ u_{i1}^m &amp;= \\beta_0 + \\beta_1 I_m - \\beta_2 p_m + \\epsilon_{i1}^m \\ \\pi_m &amp;= \\sigma(\\beta_0 + \\beta_1 I_m - \\beta_2 p_m) \\ &amp;=\\dfrac{1}{1 + \\dfrac{1}{\\exp(\\beta_0 + \\beta_1 I_m - \\beta_2 p_m)}} \\ p &amp;= \\max_p { p \\times q_m(p) - c(q_m(p)) } \\ c' (q_m) &amp;= p_m + [q'_m (p_m)]^{-1} q_m \\end{aligned} $$ where</p> <ul> <li>\\((u_{i0}^m, u_{i1}^m)\\) are indirect utilities of outside good and monopoly product resp</li> <li>\\(\\epsilon_{ij}^m \\sim \\text{Gumbel}(0, 1)\\)</li> <li>\\(q_m \\sim \\text{Binomial}(N_m, \\pi_m)\\)</li> <li>\\(c(q)\\) is the monopoly firm\u2019s cost function</li> </ul> <p>Estimated marginal cost and demand curves for a market with median income and population</p> <p></p> <p>Here, theory helps us to learn the marginal cost function of the monopoly firm as well as the consumer utility function \u2013 neither of which is observed and neither can be nonparametrically identified.</p> <p>Using the estimation results, we can conduct welfare analysis and make normative statements: For example, calculating the total deadweight loss due to monopoly</p>"},{"location":"CS_Electives/Causal_Inference/13_Causal_Effect_Estimation_under_Unconfoundness/","title":"Causal Effect Estimation under Unconfoundedness","text":"<p>Causal effect estimation under sufficient control for confounding is called causal effect estimation under unconfoundedness.</p> <p>When there are open back-door paths from w to y, according to the back-door criterion, if we observe a set of pre-treatment variables \\(x\\) such that conditioning on \\(z\\) blocks these paths, then \\(E[y| \\text{do}(z)]\\) is non-parametrically identifiable</p>"},{"location":"CS_Electives/Causal_Inference/13_Causal_Effect_Estimation_under_Unconfoundness/#disjunctive-cause-criterion","title":"Disjunctive Cause Criterion","text":"<p>Method to select what variables to control for confounding among the observed variables</p> <p>Control for all (observed) direct causes of \\(x\\) and \\(y\\)</p> <p>If there is possible elimination of back-door path, then DCC will guaranteed to enforce it: Let \\(V\\) be the set of variables selected based on the disjunctive cause criterion. If \\(\\exists\\) set of observed vars \\(z\\) that satisfy the back-door criterion, then \\(z \\subset V\\)</p> <p>Advantage: Analyst only needs to know the causes \\(x\\) and \\(y\\), without requiring understanding of interactions of other variables</p> <p>Disadvantage: Conditioning on a var may open up unobserved back-door path, but there is nothing else can be done</p>"},{"location":"CS_Electives/Causal_Inference/13_Causal_Effect_Estimation_under_Unconfoundness/#monte-carlo-integration","title":"Monte-Carlo Integration","text":"<p>Assume we observe a set of vars that satisfy the back-door criterion $$ \\begin{aligned} &amp;E [y \\vert \\text{do}(x=a)] \\ &amp;= \\int E[y \\vert x = a, z] \\cdot p(z) \\cdot dz \\ &amp;= \\dfrac{1}{n} \\sum_i^n E[y_i \\vert x_i = a, z_i] \\ &amp;\\text{ATE}(x) \\ &amp;= \\dfrac{d E [ y \\vert \\text{do}(x) ] }{ dx } \\ &amp;= E [ y \\vert \\text{do}(x=1) ] - E [ y \\vert \\text{do}(x=0) ] \\ &amp; \\qquad (x \\text{ is binary}) \\ &amp;= E [ y \\vert x=1, z ] - E [ y \\vert x=0, z ] \\end{aligned} $$ \\(E[ y \\vert x, z]\\) can be estimated using machine learning model</p> <p>In linear regression $$ \\hat y = \\beta_0 + \\beta_1 x + \\beta_2 z \\ \\text{ATE} = \\beta_1 $$ Treatment effects can be</p> <ul> <li>Homogenous: same for all units</li> <li>Heterogenous: different for all units</li> </ul> <p></p> <p>Alternatively, you can estimate in the following manner $$ \\begin{aligned} \\hat y &amp;= \\begin{cases} \\hat y_1 = f_1(z), &amp; x=1 \\ \\hat y_0 = f_0(z), &amp; x=0 \\end{cases} \\ \\text{ATE} &amp;= \\hat y_1 - \\hat y_0 \\end{aligned} $$ Where \\(f_1\\) and \\(f_0\\) are completely different models</p> <p></p>"},{"location":"CS_Electives/Causal_Inference/13_Causal_Effect_Estimation_under_Unconfoundness/#matching","title":"Matching","text":"<p>Matching is another method for controlling confounders. The goal of matching is to construct a new sample in which the confounding variables have the same distribution conditional on each value of the treatment variable.</p> <p>In randomized trials, covariate balance \u2013 the balance of \\(w\\) across values of \\(x\\) \u2013 is achieved at the design phase.</p> <p>Matching is a method that attempts to achieve covariate balance in observational studies, thereby making them resemble randomized trials.</p>"},{"location":"CS_Electives/Computer_Vision/","title":"Computer Vision","text":""},{"location":"CS_Electives/Computer_Vision/#references","title":"References","text":"<ul> <li> Deep Learning for Computer Vision | Stanford</li> <li> 2016 | Andrej Karparthy</li> <li> 2017</li> <li> Deep Learning for Computer Vision | IITM</li> <li> Computer Vision | UC Berkeley</li> </ul>"},{"location":"CS_Electives/CyberSecurity/","title":"Computer Systems Security","text":"<p>What are issues with Computer Systems security and How to improve them</p> <p>There isn\u2019t exactly a textbook on this topic</p>"},{"location":"CS_Electives/CyberSecurity/#references","title":"References","text":"<ul> <li> Introduction to Cybersecurity | Harvard</li> <li> Computer Systems Security | MIT 6.858</li> <li> Spring 2022</li> <li> Fall 2014</li> <li> Google Cybersecurity Certificate</li> <li> Machine Learning for Cyber Security | University of Purdue</li> <li>Lectures</li> <li>Labs</li> <li> Machine Learning for Cyber Security | Cathal Smyth</li> <li> Cyber Security | IIT Bombay</li> <li> Digital Forensics</li> <li> Cybersecurity and Privacy | IIT Madras</li> <li> Ethical Hacking | FreeCodeCamp</li> </ul>"},{"location":"CS_Electives/CyberSecurity/01_Introduction/","title":"Security","text":"<p>Achieving a goal against an adversary</p>"},{"location":"CS_Electives/CyberSecurity/01_Introduction/#idk","title":"IDK","text":"<p>Whenever you\u2019re designing a system, everyone should have the least amount of privileges as possible.</p>"},{"location":"CS_Electives/CyberSecurity/01_Introduction/#aspects","title":"Aspects","text":""},{"location":"CS_Electives/CyberSecurity/01_Introduction/#policy","title":"Policy","text":"<p>CIA</p> <ul> <li>Confidentiality</li> <li>Protection of data</li> <li>Integrity</li> <li>Maintaining the data as it is</li> <li>Availabiltiy (Capacity Planning)</li> <li>Making information available when required</li> </ul>"},{"location":"CS_Electives/CyberSecurity/01_Introduction/#thread-model","title":"Thread Model","text":"<p>Assumptions about adversary</p> <p>Better to be over-cautious about adversary than casual</p>"},{"location":"CS_Electives/CyberSecurity/01_Introduction/#mechanism","title":"Mechanism","text":"<p>Hardware, Software, Algorithms to enforce policy against the threat model</p> <p>This process is iterative, as you are trying to achieve a negative goal, so you need to keep updating the system.</p> <p>A good mechanism is one that enforces the policy using as few components as possible. The more endpoints you have, the more vulnerabilities</p>"},{"location":"CS_Electives/CyberSecurity/01_Introduction/#cyber-security-awareness","title":"Cyber Security Awareness","text":"<p>Not a science</p> <p>It is an evolving list of best practices</p> <p>Instilling knowledge to people about these practices</p>"},{"location":"CS_Electives/CyberSecurity/01_Introduction/#cyber-security-threats","title":"Cyber Security Threats","text":"<p>These are very easy for hackers</p> <ul> <li>Web Application</li> <li>Software Vulnerability</li> <li>Credentials Theft</li> <li>\u2018Strategic Web Compromise\u2019</li> <li>DDOS (Distributed Denial of Service)</li> <li>Malware/Ransomware</li> <li>Phishing</li> <li>Social Engineering</li> </ul>"},{"location":"CS_Electives/CyberSecurity/01_Introduction/#threat-creators","title":"Threat Creators","text":"Skill Goal Example Script Kddies Low Hacktivism Varied Public show against unethical organizations \u2018Anonymous\u2019 Crime ORganizations Medium \\(\\iff\\) High Nation State Actors Very High Insider Threat Low Edward Snowden"},{"location":"CS_Electives/CyberSecurity/01_Introduction/#cyber-security","title":"Cyber-Security","text":"<p>Protection of important information and related systems. All fields require cyber-security, especially financial and govt institutions.</p> <p>For eg, we can generate a large digital footprint of a person\u2019s social media tracks.</p> <p>You should know your roles and responsibilities, which varies based on the strategy of the organization.</p> <p>Highly influenced by military.</p> <p>Never hack into a system you\u2019re not authorized to do so, not even a scan.</p>"},{"location":"CS_Electives/CyberSecurity/01_Introduction/#career-scope","title":"Career Scope","text":"<p>Skilled cyber-security professionals are in shortage. The issue is not number of people; the issue is skilled.</p>"},{"location":"CS_Electives/CyberSecurity/01_Introduction/#basic-requirements","title":"Basic Requirements","text":"<ul> <li>CLI (Linux)</li> <li>Powershell (Windows)</li> </ul>"},{"location":"CS_Electives/CyberSecurity/01_Introduction/#fields-of-cyber-security","title":"Fields of Cyber-Security","text":""},{"location":"CS_Electives/CyberSecurity/01_Introduction/#risk-management","title":"Risk-Management","text":"<ul> <li>Security Strategies</li> <li>Risk Assessments</li> <li>List out the possible outcomes</li> <li>Security Architecture</li> <li>Compliance Reviews</li> <li>Risk &amp; Governance</li> <li>Policy</li> <li>Awareness</li> </ul>"},{"location":"CS_Electives/CyberSecurity/01_Introduction/#offensive","title":"Offensive","text":"<ul> <li>Vulerability Assessment</li> <li>Penetration Testing</li> <li>Bug Bounty</li> <li>Netowrk Instrusion</li> <li>Hacking</li> </ul>"},{"location":"CS_Electives/CyberSecurity/01_Introduction/#defensive","title":"Defensive","text":"<p>Mostly white hat hackers do this</p> <ul> <li>Data Protection</li> <li>Log Monitoring</li> <li>Incident Response</li> <li>Threat Intelligence &amp; Detection</li> <li>Using indicators, like fraud detection in data mining</li> <li>Cyber Forensics</li> </ul>"},{"location":"CS_Electives/CyberSecurity/01_Introduction/#types-of-hackers","title":"Types of Hackers","text":"<ul> <li>White Hat = Ethical Hacking</li> <li>Black Hat =  Cracking/Unethical Hacking</li> <li>Grey Hat = Mix of both</li> </ul>"},{"location":"CS_Electives/CyberSecurity/01_Introduction/#parts-of-cyber-security","title":"Parts of Cyber-Security","text":""},{"location":"CS_Electives/CyberSecurity/01_Introduction/#vulnerability","title":"Vulnerability","text":"<p>Weakness in the system</p> <ul> <li>Bugs</li> <li>Misconfiguration</li> <li>Poor process or control</li> </ul>"},{"location":"CS_Electives/CyberSecurity/01_Introduction/#threat","title":"Threat","text":"<p>Circumstance/event with potential harm</p> <ul> <li>Cyber criminals</li> <li>Nation State Actors</li> <li>Internal Threats</li> </ul>"},{"location":"CS_Electives/CyberSecurity/01_Introduction/#cyber-controls","title":"Cyber Controls","text":""},{"location":"CS_Electives/CyberSecurity/01_Introduction/#types","title":"Types","text":"<p>Defence-in-depth is followed, using a collection of these types</p> <ul> <li>Administrative   You cannot proceed to the other types without first addressing this</li> <li>Compliance to regulators</li> <li>Standarsds</li> <li>Policies</li> <li>Precedures</li> <li>Legal contracts</li> <li>Service level agreements</li> <li>Preventative</li> <li>Firewalls</li> <li>Web Proxy<ul> <li>Internet Content Inspection</li> </ul> </li> <li>Email Gateways</li> <li>Anti-Virus</li> <li>Patch Management</li> <li>Detective</li> <li>Vulnerability Scanning</li> <li>Log Monitoring</li> <li>Security Reviews</li> <li>Honey Bot<ul> <li>Enticement of hackers, to detect how they would attack your main system</li> <li>Not lure/entrap (that is illegal)</li> </ul> </li> <li>Corrective</li> <li>Incident Response</li> <li>Cyber Forensics</li> <li>Business Continuity &amp; Disaster Recovery</li> <li>Deterrent Control</li> <li>Just to deter any possible hackers; Doesn\u2019t provide security</li> <li>Such as a \u2018Beware of Dog\u2019, without actually having a dog</li> </ul>"},{"location":"CS_Electives/CyberSecurity/01_Introduction/#examples","title":"Examples","text":"<ul> <li>Firewall is like Locks</li> <li>Log Monitoring is like CCTV</li> </ul>"},{"location":"CS_Electives/CyberSecurity/01_Introduction/#risk","title":"Risk","text":"<p>Materialized threat exploiting a vulnerability $$ \\text{Inherent Risk} + \\underset{\\approx \\text{ Brakes}}{\\text{Cyber Controls}} = \\text{Residual Risk} $$ The organization should decide the residual risk it can tolerate.</p>"},{"location":"CS_Electives/CyberSecurity/01_Introduction/#4-ways-to-handle-risk","title":"4 ways to handle risk","text":"<ul> <li>Risk assessement</li> <li>Risk transfer</li> <li>Risk mitigation</li> <li>(one more thing)</li> </ul>"},{"location":"CS_Electives/CyberSecurity/02_Control_Hijacking_Attacks/","title":"Control Hijacking Attacks","text":""},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/","title":"999 cybersecurity workshop","text":"<p>Take these points and plug them into other pages</p>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#why-does-cyber-security-awareness-fails","title":"Why does Cyber Security Awareness Fails","text":"<ul> <li>Complicated/Boring training content</li> <li>Poor Enforcement</li> <li>Lack of monitoring</li> <li>Unfocused awareness</li> <li>Point-in-Time Compliance</li> <li>Culture Misalignment</li> <li>Shared responsibility</li> </ul>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#solution","title":"Solution","text":"<ul> <li>Conventional vs UNconventional Thinking</li> <li>Policy vs Story telling</li> <li>Point-in-Time Assessment vs Continuous monitoring</li> <li>Compliance vs Culture</li> <li>Single-Unit vs Cross-Functional Team</li> </ul> <p>Awareness has to fun and exciting, not like work.</p>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#using-virtual-machine","title":"Using Virtual Machine","text":"<p>If you isolate your virtual machine from your OS, you may get away even in the case of an attack.</p>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#zero-day-vulnerability","title":"Zero Day Vulnerability","text":"<p>Zero day vulnerability is something that was not known before.</p>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#phases-of-cyber-kill-chain","title":"Phases of Cyber Kill Chain","text":"<p>The earlier you detect, the better</p> <ul> <li> <p>Reconnaisance</p> </li> <li> <p>Observation</p> </li> <li> <p>Attacker identifies a target and explores vulnerabilities</p> </li> <li> <p>The one who knows their enemy more than themselves wins the war.</p> <pre><code>&gt; If you don\u2019t know how someone will attack you, then you can\u2019t protect yourself.\n</code></pre> </li> <li> <p>Learn to attack so you can defend yourself</p> </li> <li> <p>Weaponization</p> </li> <li> <p>Develop \u2018ammunition\u2019</p> </li> <li> <p>Create a payload/malware</p> </li> <li> <p>This is the only thing the defender has no control over, as it is in the attacker\u2019s hands</p> </li> <li> <p>Delivery</p> </li> <li> <p>Send the payload to the intended target</p> </li> <li> <p>Social Engineering</p> </li> <li> <p>Web Proxy helps protect users</p> </li> <li> <p>Exploitation</p> </li> <li> <p>Malicious explotaition is executed within the victim\u2019s system</p> </li> <li> <p>Reverse shell is a response to a shell</p> </li> <li> <p>Installation</p> </li> <li> <p>C&amp;C</p> </li> <li> <p>Command and Control of system resources</p> </li> <li> <p>Action &amp; Objectives</p> </li> </ul> <p>1 attacker and many victims leads to a bot(something)</p>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#cyber-threat-intelligence","title":"Cyber Threat Intelligence","text":"<p>The first phrase people say when something happens is \u2018failure of intelligence\u2019</p> <ul> <li>Strategy is long-term</li> <li>Tactics is short-term</li> </ul>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#fundamentals","title":"Fundamentals","text":"<ol> <li>Intel Planning/Strategy</li> <li>Data Collection &amp; Aggegration</li> <li>Threat Analytics</li> <li>Intel Uage &amp; Dissemination</li> </ol>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#dark-web","title":"Dark Web","text":"<ul> <li>Gives illusion of anonymity</li> <li>Gives you access to information</li> </ul>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#onion-network","title":"Onion Network","text":"<p>Be careful when using Tor</p> <p>Most Tor networks are actually the host of the network, so they are infact monitored.</p>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#types-of-malware","title":"Types of Malware","text":"Type Meaning Backdoor Botnet Downloader Information-Stealing Scareware"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#dark-web-economy","title":"Dark Web Economy","text":""},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#incident-response","title":"Incident Response","text":"<pre><code>flowchart LR\np[Preparation] --&gt;\nIdentification --&gt;\nContainment --&gt;\nEradication --&gt;\nRecovery --&gt; \nl[Lessons Learnt] --&gt; p</code></pre>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#compliance-management","title":"Compliance Management","text":""},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#compliance-montoring","title":"Compliance Montoring","text":"<p>You need to know what to comply to.</p>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#unified-compliance-framework","title":"Unified Compliance Framework","text":""},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#service-level-agreements","title":"Service Level Agreements","text":""},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#shell","title":"Shell","text":"<p>Communication tunnel between 2 computers?</p>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#types-of-ip","title":"Types of IP","text":"<p>The class of IP depends on the size of the organization</p> <ul> <li>Class A</li> <li>Class B</li> <li>Class C</li> </ul>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#skilled","title":"Skilled","text":"<p>Really skilled hackers won\u2019t even be known.</p>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#miscelaneous","title":"Miscelaneous","text":"<p>Any field you are in, you should learn the terminologies.</p> <p>Never believe you know everything; keep learning</p> <p>Keep an open-mind always; don\u2019t go with a bank mindset to another field.</p> <p>Always see if you get importance in your organizations. But then again, if you do get importance, never let it get to your head.</p> <p>You may not certificates/acknowledgment, but don\u2019t let it get to your head.</p> <p>Success will won\u2019t teach much; only failure will teach</p> <p>The only time you can stop learning is \u201cwhen you\u2019re 6ft under\u201d</p> <p>You should be able to adapt to any situation.</p>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#book-on-kingpin","title":"Book on Kingpin","text":""},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#impact","title":"Impact","text":"<p>The extent to which a risk event might affect the enterprise.</p> Asset Vulnerability Threat Security Controls Risk Possibilty that a threat will exploit a vulnerability, affecting asset"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#nist-cybersecurity-framework","title":"NIST Cybersecurity Framework","text":"<ul> <li>Identify</li> <li>Protect</li> <li>Detect</li> <li>Respond</li> <li>Recover</li> </ul>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#gdpr","title":"GDPR","text":"<p>General Data Protection Regulation</p>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#control","title":"Control","text":"<p>All risks should be evaluated, and a corresponding control measure should be used.</p> <p>We need effectiveness</p> <p>We cannot use 7 factor authentication </p>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#control-designmeasure","title":"Control Design/Measure","text":""},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#control-operation","title":"Control Operation","text":""},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#risk-management","title":"Risk Management","text":"<ul> <li>Risk Tolerance</li> <li></li> <li>Risk Transfer</li> <li>Getting insurance after doing whatever you can do yourself</li> <li>Risk Mitigation</li> <li>Something</li> </ul>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#business-continuity","title":"Business Continuity","text":"<p>Ensures bare minimum services are provided in case of a business</p> <pre><code>flowchart LR\n\nsubgraph BCM Lifecycle\n    direction LR\n  pp[Policy &amp; Program] --&gt;\n  Analysis --&gt;\n  Design --&gt;\n  Implementation --&gt;\n  Validation --&gt;\n  e[Embedding Business Continuity] --&gt; pp\nend</code></pre>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#bia","title":"BIA","text":"<p>Business Impact Analysis</p> <p>Analyze what are your critical processes</p>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#idk","title":"IDK","text":"<ul> <li>It is not safe to swipe your credit card, as the magnetic stripe is not encrypted</li> <li>It is safer to either</li> <li>insert your card, and use the encrypted Electro-Magnetic chip</li> <li>or, tap it and use NFC</li> </ul> <p>Be careful about scams. A bank would never ask you for your credit card details.</p> <p>Keep updated with the latest news related to cyber-security</p> <p>There is high demand for cyber-security, but not enough supply to meet the demand.</p>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#supply-chain-attack","title":"Supply-Chain Attack","text":"<p>Attack on backdoor (3<sup>rd</sup> Party Software dependency compromise)</p> <p>Solar Winds</p>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#log4j-jndi-attack","title":"log4j JNDI Attack","text":""},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#security-assessment","title":"Security Assessment","text":"<p>We need to test our tools before attackers do.</p>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#ddos","title":"DDOS","text":"<p>A Denial-of-Service (DoS) attack is an attack meant to shut down a machine or network, making it inaccessible to its intended users. DoS attacks accomplish this by flooding the target with traffic, or sending it information that triggers a crash.</p>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#half-flood-attack","title":"Half-Flood Attack","text":"<p>Uses TCP</p> <p>Evolution and </p>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#evolution-app-infrastructrue","title":"Evolution App Infrastructrue","text":"<p>Mainframe</p> <p>Shared Responsibility</p>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#monilithic-vs-microservice-architecture","title":"Monilithic vs Microservice Architecture","text":"<p>Microservice is basically modular distributed computing, for each service/page of the product</p>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#watch-list","title":"Watch List","text":"<p>History repeats itself.</p> <p>Learn about history, and learn about what thought processes they used</p> <ul> <li> Pirates of the Silicon Valley</li> <li> Steve Jobs Autobiography</li> </ul>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#types-of-assessment","title":"Types of Assessment","text":"<ul> <li>Vulernaribility Assessment</li> <li>Penetration Testing</li> <li>Red Teaming</li> </ul>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#tools","title":"Tools","text":"<ul> <li>SATAN (Security Admin Tool for Analyzing Networks)</li> <li>NMAP</li> <li>Nessus Project</li> <li>Appscan</li> <li>Burp Suite</li> <li>Used for bug bounties</li> </ul>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#cve","title":"CVE","text":"<p>Maintained in the National Vulnerability Database</p>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#vulnerabilities-scanner-operation-models","title":"Vulnerabilities Scanner - Operation Models","text":"Scan Direction"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#types-of-vulnerability-scanner","title":"Types of Vulnerability Scanner","text":"<ul> <li>Signtature-Based</li> <li>Behavioural</li> </ul> <p>Something</p> <ul> <li>Static</li> <li>Dynamic</li> </ul>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#security-assessment-lifecycle","title":"Security Assessment Lifecycle","text":"<pre><code>flowchart LR\ni[Identify Scope] --&gt;\nf[Finalize&lt;br/&gt;Assesment Methology&lt;br/&gt;Tools Required] --&gt;\nr[Reporting]</code></pre>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#risk-rating-and-prioritization","title":"Risk Rating and Prioritization","text":"<p>NVD rates vulnerabilities</p>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#where-to-start","title":"Where to Start","text":"<p>NICE (National Initiative for Cybersecurity Education) Framework</p> <ul> <li>Employers</li> <li>Learners</li> <li>Something</li> </ul> <p>There are lists of required skills and knowledge</p>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#something","title":"Something","text":"<p>Data protection is needed to protect only for outgoing confidential data.</p> <ul> <li>It is not feasible to protect all data</li> </ul> <p>Importance/Confidentiality of data varies over time</p> <ul> <li>Exam questions are important for uni before exam</li> <li>After exam, it is not important</li> </ul>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#biggest-risks","title":"Biggest Risks","text":"<ul> <li>Human Errors</li> <li>Insider threats</li> <li>BYOD (Bring Your Own Device)</li> <li>Public Networks</li> <li>Especially in airports, where they harvest user data</li> <li>Even your user-id and password will go through the public access point</li> <li>Even https is not free from this risk</li> <li>Charging Ports</li> <li>Using a \u2018dumb intermediary\u2019 (such as power bank) prevents this</li> </ul> <p>Data Residency Law</p>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#data-protection-model","title":"Data Protection Model","text":"<pre><code>flowchart LR\ndd[Data&lt;br/&gt;Discovery] --&gt;\ndc[Data&lt;br/&gt;Classification] --&gt;\ndlp[Data&lt;br/&gt;Loss&lt;br/&gt;Prevention] --&gt;\ndp[Data&lt;br/&gt;Protection]</code></pre>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#dlp-architecture","title":"DLP Architecture","text":""},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#training-techniques","title":"Training Techniques","text":"<ul> <li>Provide data</li> <li>Exact data Matching</li> <li>Exact match data identifier</li> <li>Indexed document matching</li> <li>OCR</li> </ul>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#cloud-data-protection","title":"Cloud Data Protection","text":"<ul> <li>Intune</li> <li>O365 DLP</li> <li>Cloud Access Security Broker</li> </ul>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#aip-azure-information-protection","title":"AIP (Azure Information Protection)","text":"<p>Encryption travels with the file</p> <p>Used in Netflix Offline Saved</p>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#digital-rights-management","title":"Digital Rights Management","text":""},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#credit-card-pattern","title":"Credit Card Pattern","text":"<ul> <li>1: Credit Service Provider</li> <li>4 = Visa</li> <li>5 = Mastercard</li> <li>2-5: Bin Number</li> </ul> <p>IBAN number?</p>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#idk_1","title":"IDK","text":"<p>Never share your secrets with anyone; it will destroy you.</p> <p>Trust anyone; never trust the devil inside.</p> <p>\u201cIf you can't measure it, you can't manage it.\u201d If you don't measure, then how do you know how you are doing?</p>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#siem","title":"SIEM","text":"<p>Used for log monitoring</p> <p>Cannot simulate attacks</p>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#log","title":"Log","text":"<p>Historical record of an event</p>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#log-management","title":"Log Management","text":"<p>Approach to deal with large volumnes of computer-generated log messages.</p> <p>Open-source something Wazoo</p>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#why","title":"Why?","text":""},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#challenges","title":"Challenges","text":"<ul> <li>Variety of formats</li> <li>Large volume of data</li> </ul>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#load-balancing","title":"Load Balancing","text":"<p>Load Balancer manages the load that each collector receives</p>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#steps","title":"Steps","text":"<ol> <li>Log </li> <li>Incident</li> </ol>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#incident-response_1","title":"Incident Response","text":"<pre><code>flowchart LR\np[Preparation] --&gt;\nda[Detection &amp;&lt;br/&gt;Analysis] --&gt;\ncer[Containment,&lt;br/&gt;Eradication &amp;&lt;br/&gt;Recovery] --&gt;\npia[Post-Incident&lt;br/&gt;Activity] --&gt;\np\n\ncer --&gt; da</code></pre>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#database-activity-monitoring","title":"Database Activity Monitoring","text":"<p>Monitor the activities of database admins.</p> <p>For eg, if someone suddenly performs a <code>select</code> query of all tables</p>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#iam","title":"IAM","text":"<p>Identity Access &amp; Management</p>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#credential-security","title":"Credential Security","text":"<ul> <li>Something you have (password)</li> <li>Something you are </li> <li>Something you know (childhood bestfriend)</li> </ul>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#aaa-principles","title":"AAA Principles","text":"<ul> <li>Authentication</li> <li>Authorization</li> <li>Accountability/Auditing</li> <li>Authorized person has access to authorized resources only at authorized times</li> </ul>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#access-control-models","title":"Access Control Models","text":"<ul> <li>Discretionary Access Control</li> <li>Mandatory Access Control</li> <li>Rule-Based Access Control</li> <li>Attribute-Based Access Control</li> <li>Role-Based Control</li> <li>Most common in companies</li> </ul>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#identity-life-cycle-management","title":"Identity Life Cycle Management","text":""},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#segregation-of-duties","title":"Segregation of Duties","text":"<p>If both maker and checker are the same person,</p> <ul> <li>Possibility of frauds</li> <li>Possibility of human error</li> </ul>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#ui-path-rpa","title":"UI Path RPA","text":"<p>You\u2019ll get job tomorrow at First Abu Dhabi bank if you are RPA-certified</p> <p>CASSP, CSSP, CEH</p> <p>~ Senthilkumar</p> <p>Most important thing is surveillance and find out weaknesses.</p>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#bridge-mode","title":"Bridge Mode","text":"<p>You can run a </p> <p>VM is paired to machine</p>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#nat-mode","title":"NAT Mode","text":"<p>VM is subset of machine</p>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#idk_2","title":"IDK","text":"<p>Go watch Chinese movies</p> <p>Snake in the Monkey\u2019s Shadow</p> <p>The world throws questions for which there are no answers. No point in memorizing stuffe</p>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#vuca","title":"VUCA","text":"<p>Volatile Uncertain Complex Ambiguous</p> <p>US Military Strategy</p> <ol> <li>nsetn</li> <li>Preachers preach change, but they never do</li> <li>Most people do not realize their prejudices when taking decisions</li> </ol> <p>When making a decision, first try to see if any problem is similar to past experience, or is it a new problem</p> <p>God, give me the courage to do what i can change, the something to , and the wisdom know the difference between both</p> <p>Insult is the better way to force someone to learn. Getting stones helps you build a castle.</p> <p>Fear is the greatest thing in life.</p> <p>Fear and Confidence, Pain and Pleasure are the 4 driving forces of life.</p> <p>You may treat people nicely, but doesn\u2019t mean they will treat you the same. Life isn\u2019t fair. You don\u2019t eat the lion doesn\u2019t mean that it won\u2019t eat you.</p> <p>Nowadays, students coming out of university are Unskilled Unemployable Something Resource. Industry wants Deployable Something Resource.</p>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#supply-chain-risk","title":"Supply Chain Risk","text":"<p><code>node.js</code>, <code>faker.js</code></p>"},{"location":"CS_Electives/CyberSecurity/999_cybersecurity_workshop/#macos-is-not-safe","title":"MacOS is not safe","text":"<p>It is based on Linux FreeBSD/BSD</p>"},{"location":"CS_Electives/CyberSecurity/Lab/","title":"Kali","text":"<pre><code>ifconfig\n</code></pre> <pre><code>cat /etc/hosts\n</code></pre> <pre><code>ping kali\n</code></pre> <pre><code>ping google\nping www.google.ae\n</code></pre> <pre><code>64 bytes from any.in\n</code></pre> <pre><code>netstat -ano\n</code></pre> <pre><code>nmap 172.16.22.5\n</code></pre> <p>We can see what ports are open</p>"},{"location":"CS_Electives/CyberSecurity/Lab/#telnet-vs-sshsecure-shell","title":"Telnet vs SSH(Secure SHell)","text":"<p>Telnet(Port 23) is unencrypted and insecure, as all traffic occurs in cleartext.</p> <p>Block any service on Port 23 of the server.</p> <p>If you see that port 23 is being used</p> <pre><code>telnet 172.16.22.5\n</code></pre> <p>All traffic will be in plain text.</p> <pre><code>ssh 2020A7PS0198U@172.16.22.5\n</code></pre> <p>All traffic will be encrypted.</p>"},{"location":"CS_Electives/CyberSecurity/Lab/#wire-shark","title":"Wire Shark","text":"<p>You can analyze network. Click ethernet</p> <p>Understand what all nmap is doing</p> <pre><code>ip.addr==172.16.22.5\n</code></pre>"},{"location":"CS_Electives/CyberSecurity/Lab/#ip-address","title":"IP Address","text":"<p>Address for 2 computers to communicate with each other</p> <p>Public can </p> <p>Private can access another Private without </p> 10. Internal 172.16. Internal 172.168. Internal"},{"location":"CS_Electives/CyberSecurity/Lab/#dns","title":"DNS","text":"<p>Domain Naming Server</p> <p>Lookup of domain with the corresponding IP address $$ X. X. X \\ X \\in [0, 255] $$</p>"},{"location":"CS_Electives/CyberSecurity/Lab/#icmp","title":"ICMP","text":"<p>Internet Control Message Protocol</p>"},{"location":"CS_Electives/CyberSecurity/Lab/#automation-softwares","title":"Automation Softwares","text":"<pre><code>flowchart LR\n\n1 --&gt; 2 --&gt; 3\n\nsubgraph 1[Recon]\n    NMap\nend\n\nsubgraph 2[Vulnerability Identification]\n    Nessus\nend\n\nsubgraph 3[Exploitation]\n    Metasploit\n    CobaltStrike\n    BurpSuite\nend</code></pre>"},{"location":"CS_Electives/CyberSecurity/Lab/#advantages","title":"Advantages","text":"<ul> <li>Scalability</li> <li>Standardization</li> <li>Accuracy</li> <li>Reduced manual effort</li> </ul>"},{"location":"CS_Electives/CyberSecurity/Lab/#nessus","title":"Nessus","text":"<pre><code>curl --request G\n</code></pre> <pre><code>sudo apt install ./Nessus.deb\n</code></pre>"},{"location":"CS_Electives/CyberSecurity/Lab/#ngrok","title":"ngrok","text":"<p>This is like alias for wordpress localhosting</p>"},{"location":"CS_Electives/CyberSecurity/Lab/#idk","title":"IDK","text":"<pre><code>showmount -e 192.168.100.25\n\nmkdir /tmp/infosec # not necessary\nmount -t nfs 192.168.100.25:/home /tmp/infosec\n</code></pre>"},{"location":"CS_Electives/Data_Mining/","title":"Data Mining","text":"<p>This course explores the concepts and techniques of data mining, a promising and flourishing frontier in database systems. Data Mining is the automated extraction of patterns representing knowledge implicitly stored in large databases, data warehouses, and other massive information repositories. It serves as a decision support tool that addresses unique problems that cannot be solved by other data analysis tools such as Online Analytical Processing (OLAP).</p>"},{"location":"CS_Electives/Data_Mining/#key-learning-objectives-include","title":"Key learning objectives include:","text":"<ul> <li>Understanding data mining tasks such as constructing decision trees, finding association rules, classification, and clustering.</li> <li>Gaining broad insights into the design and use of data mining algorithms.</li> <li>Exploring a holistic view of data mining from database, statistical, algorithmic, and application perspectives.</li> <li>Applying data mining concepts using Python.</li> </ul> <p>This course equips students with the foundational knowledge and practical skills needed to leverage data mining techniques for effective decision-making and data analysis.</p>"},{"location":"CS_Electives/Data_Mining/#why-data-mining","title":"Why Data Mining","text":"<ol> <li>Scalability</li> <li>Handling high-dimensional data</li> <li>Complex &amp; Heterogeneous Data</li> <li>Handling Poor Quality Data</li> <li>Data ownership and Distribution</li> </ol>"},{"location":"CS_Electives/Data_Mining/#applications-of-this-course","title":"Applications of this course","text":"<p>To fill in areas where traditional data analysis methods cannot be applied</p> <ul> <li>Optimize business operations</li> <li>Understand customers</li> <li>Computer-Aided Diagnosis</li> <li>Image<ul> <li>Segmentation</li> <li>Captioning</li> </ul> </li> <li>Object Detection</li> </ul>"},{"location":"CS_Electives/Data_Mining/#sources","title":"Sources","text":"<p>Kaggle, UCI ML repository</p>"},{"location":"CS_Electives/Data_Mining/#references","title":"References","text":"<ul> <li> Data Mining | Dr. Angel Arul Jothi</li> <li> From Data to Decisions: Measurement, Uncertainty, Analysis and Modeling | Chris Mack | University of Texas</li> <li> How to write a good scientific paper | Chris Mack | University of Texas</li> <li> Statistics literacy for non-statisticians | Mike x Cohen</li> <li> Big Data Analytics | Caltech</li> <li> MIT 14.310x Data Analysis for Social Scientists, Spring 2023</li> <li> IIT Roorkee July 2018 | Data Analytics with Python</li> <li> ORIE 5355 -- People, Data, Systems -- Fall 2021 -- Cornell Tech</li> <li> Mining Massive Datasets | Stanford University</li> <li> Quantitative Social Science Methods | Harvard</li> <li> Data Mining | University of Utah</li> <li> Visualization for Data Science | University of Utah</li> <li> Winter 2017, STAT 442 / 842 Data Visualization</li> <li> Data Visualization | IIT Madras</li> <li> Introduction to Bigdata | IIT Madras</li> <li> Empirical Methods CMU</li> <li> Fall 2022</li> <li> Spring 2021</li> <li> Introduction to Data Analysis, Design of Experiment, and Machine Learning | Ashraf Alam | Purdue University</li> <li> Data Analytics | Gary Holness | Clark University</li> <li> Intro to Data Science | Gary Holness | Clark University</li> <li> Statistical Research Methods | Mikko R\u00f6nkk\u00f6</li> </ul>"},{"location":"CS_Electives/Data_Mining/01_Intro/","title":"Introduction","text":"<p>Many times very high-quality professionals are not able to produce well, as they are usually incentivized to use complex methodologies. But data science is best when you actually solve the problem at hand, and help make decisions.</p>"},{"location":"CS_Electives/Data_Mining/01_Intro/#data-professionals","title":"Data Professionals","text":""},{"location":"CS_Electives/Data_Mining/01_Intro/#decision-making","title":"Decision-Making","text":"<p>Iterative process with feedback loops; not linear</p>"},{"location":"CS_Electives/Data_Mining/01_Intro/#knowledge-hierarchy","title":"Knowledge Hierarchy","text":"<p>Chain of increasing value</p> <pre><code>flowchart LR\nData --&gt;\nInformation --&gt;\nKnowledge --&gt;\nDecision</code></pre> Data Collection of numbers with known context and uncertainty estimates Information Right data at right time in right context, organized for access Knowledge Interpretation of information based on model (understanding) of cause and effect Decision Acting on knowledge for benefit"},{"location":"CS_Electives/Data_Mining/01_Intro/#process","title":"Process","text":"<ul> <li>Preparation: Plan to turn data into information, with a specific model and decision in mind</li> <li>Testing/Experimenting/Measurement</li> <li>Analysis, with uncertainty: Use model to turn information to knowledge</li> <li>Decision</li> <li>Using uncertainty</li> <li>Risk/benefit analysis</li> <li>Post-mortem: Learnings to improve things</li> </ul>"},{"location":"CS_Electives/Data_Mining/01_Intro/#notes","title":"Notes","text":"<ul> <li>All models are wrong, some are useful</li> <li>Applying data mining algorithms on data that you don\u2019t understand may result in false conclusions</li> <li>Always keep track of performed tests &amp; analyses, to factor in data snooping</li> </ul>"},{"location":"CS_Electives/Data_Mining/01_Intro/#questions","title":"Questions","text":"Question Why Precise (not vague)Bad:- Planning- Decision-makingGood- What plans/decisions- How are these plans/decisions made- How would data mining help What Goal, Level of aggregation, Forecast horizonBad- Sales- Market shareGood- Demand When FrequencyTime of day/year Who Human judgementComputer-generated with human judgementComputer-generatedConsiderations- Number &amp; frequency of predictions- Availability of historical data- Relative accuracy of options Where Predictions originate in different departments How"},{"location":"CS_Electives/Data_Mining/01_Intro/#fields-overview","title":"Fields Overview","text":"Analytics AI/ML Statistical Inference Goal Descriptive Predictive Prescriptive Decisions Large scale repetitive(with uncertainty) Small scale(with uncertainty)"},{"location":"CS_Electives/Data_Mining/01_Intro/#types-of-analysis","title":"Types of Analysis","text":"Type Topic Nature Time Comment Examples Descriptive/Positive What is happening? Objective Past No emotions/explanations if good or bad Increasing taxes will lower consumer spendingIncreasing interest rate will lower demand for loansRaising minimum wage will increase unemployment Diagnostic Why is it happening? Objective/Subjective Past Helps in understanding root cause Predictive What will happen if condition happens Subjective Future Understanding future, using history Prescriptive/Normative What to do Subjective Future what actions to be taken Taxes must be increased <p>The complexity increases as we go down the above list, but the value obtained increases as well</p>"},{"location":"CS_Electives/Data_Mining/01_Intro/#project-lifecycle","title":"Project Lifecycle","text":"<pre><code>flowchart TB\n\nsubgraph Scoping\n    dp[Define&lt;br/&gt;Project] --&gt;\n    me[\"Define Metrics&lt;br/&gt;(Accuracy, Recall)\"] --&gt;\n    re[Resources&lt;br/&gt;Budget] --&gt;\n    ba[\"Establish&lt;br /&gt;Baseline\"]\nend\n\nsubgraph Data\n    d[(Data Source)] --&gt;\n    l[Label &amp;&lt;br /&gt;Organize Data]\nend\n\nsubgraph Modelling\n  pre[Preprocessing] --&gt;\n    s[Modelling] --&gt;\n    train[Training] --&gt;\n  pp[Post&lt;br /&gt;Processing] --&gt;\n    vt[Validation &amp;&lt;br /&gt;Testing] --&gt;\n    e[Error Analysis] --&gt;\n    pre\nend\n\nsubgraph Deploy\n    dep[Deploy in&lt;br /&gt;Production] --&gt;\n    m[Monitor &amp;&lt;br /&gt;Maintain] &amp; dss[Decision&lt;br /&gt;Support System]\nend\n\nScoping --&gt; Data --&gt; Modelling --&gt; Deploy</code></pre> <p>https://www.youtube.com/watch?v=UyEtTyeahus&amp;list=PLkDaE6sCZn6GMoA0wbpJLi3t34Gd8l0aK&amp;index=5</p>"},{"location":"CS_Electives/Data_Mining/01_Intro/#data-mining","title":"Data Mining","text":"<p>Generate Decision Support Systems</p> <p>Non-trivial extraction of implicit, previously-unknown and potentially useful information from data</p> <p>Automatic/Semi-automatic means of discovering meaningful patterns from large quantities of data</p>"},{"location":"CS_Electives/Data_Mining/01_Intro/#predictive-tasks","title":"Predictive Tasks","text":"<p>Predict value of target/independent variable using values of independent variables</p> <ul> <li>Regression - Continuous</li> <li>Classification - Discrete</li> </ul>"},{"location":"CS_Electives/Data_Mining/01_Intro/#descriptive-tasks","title":"Descriptive Tasks","text":"<p>Goal is to find</p> <ul> <li>Patterns</li> <li>Associations/Relationships</li> </ul>"},{"location":"CS_Electives/Data_Mining/01_Intro/#association-analysis","title":"Association Analysis","text":"<p>Find hidden assocations and patterns, using association rules</p>"},{"location":"CS_Electives/Data_Mining/01_Intro/#applications","title":"Applications","text":"<ul> <li>Gene Discovery</li> <li>Market Baset Data Analysis   Find items that are bought together</li> </ul>"},{"location":"CS_Electives/Data_Mining/01_Intro/#clusteringcluster-analysis","title":"Clustering/Cluster Analysis","text":"<p>Grouping similar customers</p>"},{"location":"CS_Electives/Data_Mining/01_Intro/#metrics","title":"Metrics","text":"<ul> <li>Similarity</li> <li>Dissimilarity/Distance Metrics</li> </ul>"},{"location":"CS_Electives/Data_Mining/01_Intro/#applications_1","title":"Applications","text":"<ul> <li> <p>Grouping similar documents</p> </li> <li> <p>Clustering documents</p> </li> <li> <p>Vocabulary - All terms(key words) from all docs</p> </li> <li> <p>Generate document-term frequency matrix</p> Document \\vert  Term T1 T2 \u2026 Tn D1 D2 \u2026 Dm </li> </ul>"},{"location":"CS_Electives/Data_Mining/01_Intro/#deviationoutlieranomaly-detection","title":"Deviation/Outlier/Anomaly Detection","text":"<p>Outlier is a data point that does not follow the norms.</p> <p>Don\u2019t mistake outlier for noise.</p>"},{"location":"CS_Electives/Data_Mining/01_Intro/#application","title":"Application","text":"<ul> <li> <p>Credit Card Fraud Detection</p> <ul> <li>Collect user profile such as Name, Age, Location</li> <li>Collect user behavior data</li> </ul> </li> <li> <p>Network Intrusion Detection</p> </li> <li>Identify anomalous behavior from surveillance camera videos</li> </ul>"},{"location":"CS_Electives/Data_Mining/01_Intro/#misconceptions","title":"Misconceptions","text":"<ul> <li>All forecasts will be inaccurate, so no point</li> <li>If we had the latest forecasting technology, all problems would be solved</li> </ul>"},{"location":"CS_Electives/Data_Mining/01_Intro/#idk","title":"IDK","text":"<p>Ensure you are looking at the correct scale</p> <p>Model \\(y_t/y_0\\) instead of \\(y_t\\) to standardize all time series</p>"},{"location":"CS_Electives/Data_Mining/01_Intro/#common-problems-with-analysis","title":"Common problems with analysis","text":"<ul> <li>Poorly-defined goals</li> <li>Data doesn\u2019t meet needs of analysis objectives</li> <li>Analysis makes unwarranted assumptions</li> <li>Model is wong</li> <li>Data doesn\u2019t support conclusion</li> </ul> <p>Learning Process</p> <ol> <li>Model building: Functional form</li> <li>Identify parameter weights</li> <li>Distribution of random errors</li> </ol> <p>Each of them can have different levels of generalizability</p> <p>For eg: Ohm\u2019s Law</p> <ol> <li>\\(V=IR\\) remains constant for all materials (under certain conditions)</li> <li>\\(R\\) Changes for different materials</li> <li>Errors are dependent on measurement and experimental methods, and are independent of materials</li> </ol>"},{"location":"CS_Electives/Data_Mining/02_Data/","title":"Data","text":"<p>Data can be anything. It depends on the data engineer on what the input and output data is</p> <p>Data = results of measurement</p> <ul> <li>Definition of measurand (quantity being measured)</li> <li>Measurement value</li> <li>number</li> <li> <p>unit</p> </li> <li> <p>Experimental context</p> </li> <li>Test method</li> <li>sampling technique</li> <li> <p>environment</p> </li> <li> <p>Estimate of uncertainty</p> </li> <li>Measurement uncertainty: estimate of dispersion of measurement values around true value</li> <li>Context uncertainty: uncertainty of controlled and uncontrolled input parameters</li> <li>Metrology/Measurement model: science of measurement; theory, assumptions and definitions used in making measurement</li> </ul>"},{"location":"CS_Electives/Data_Mining/02_Data/#types","title":"Types","text":"<ul> <li>Structured</li> <li>Numbers</li> <li>Tables </li> <li>Unstructured</li> <li>Audio</li> <li>Image</li> <li>Video</li> </ul>"},{"location":"CS_Electives/Data_Mining/02_Data/#means-of-data-collection","title":"Means of data collection","text":"<p>Garbage-in, Garbage-out</p> <ul> <li>Manual Labelling</li> <li>Manually marking as cat/not cat, etc.</li> <li>Observing Behaviour</li> <li>taking data from user activity and seeing whether they purchased or not</li> <li>machine temperatures and observing for faults or not</li> <li>Download from the web</li> </ul>"},{"location":"CS_Electives/Data_Mining/02_Data/#mistakes","title":"Mistakes","text":"<ol> <li>Waiting too long for implementing a data set</li> <li>implement it early so that AI team can give feedback to the IT team</li> <li>Not all data is valuable</li> <li>Messy</li> <li>Garbage in, garbage out</li> <li>incorrect data</li> <li>multiple types of data</li> </ol>"},{"location":"CS_Electives/Data_Mining/02_Data/#datasets","title":"Datasets","text":"<p>Collection of data in rows and columns</p> <ul> <li>Rows = Objects, Records, Samples, Instances</li> <li>Columns = Attributes, Variables, Dimensions, Features</li> </ul>"},{"location":"CS_Electives/Data_Mining/02_Data/#types_1","title":"Types","text":"<ul> <li>Labelled has Target variable</li> <li>Unlabelled does not have target variable</li> </ul>"},{"location":"CS_Electives/Data_Mining/02_Data/#types-of-attributes","title":"Types of Attributes","text":"Nominal Ordinal Interval Ratio Order \u2705 \u2705 \u2705 Magnitude \u2705 \u2705 Absolute Zero \u2705 Mode \u2705 \u2705 \u2705 \u2705 \\(=\\) \u2705 \u2705 \u2705 \u2705 \\(&gt;, \\ge, &lt;, \\le\\) \u2705 \u2705 \u2705 \\(-, +\\) \u2705 \u2705 \\(/, \\times\\) \u2705 Type D D N N Median \u2705 \u2705 \u2705 Mean \u2705 \u2705 Min/Max \u2705 \u2705 t-Test \u2705 Example - Colors - Player Jersey #- Gender- Eye color- Employee ID - Ratings- Course Grades - Finishing positions in a race; 4star is not necessarily twice as good as 2 star - Temperature units - 100C &gt; 50C &gt; 0C; 0C, 0F doesn't mean no temperature; 50C isn't \\(\\frac{1}{2}\\) of 100C - pH scale - Age- Kelvin - 0K is absolute absence of heat; 50K = half of 100K - Number of children <ul> <li>D = Discrete/Qualitative/Categorical</li> <li>N = Numerical/Quantitative/Continuous</li> </ul>"},{"location":"CS_Electives/Data_Mining/02_Data/#asymmetric-attributes","title":"Asymmetric Attributes","text":"<p>Attributes where only non-zero values are important. It can be</p> <ul> <li>Binary (0 or 1)</li> <li>Discrete (0, 1, 2, 3, \u2026)</li> <li>Continuous (0, 33.35, 52.99, \u2026)</li> </ul>"},{"location":"CS_Electives/Data_Mining/02_Data/#characteristics-of-dataset","title":"Characteristics of Dataset","text":""},{"location":"CS_Electives/Data_Mining/02_Data/#minimum-sample-size","title":"Minimum Sample Size","text":"<p>To learn effectively</p> \\(n_\\text{min}\\) Structured: Tabular \\(k+1\\) Unstructured: Image \\(1000 \\times C\\) <p>where</p> <ul> <li>\\(n =\\) no of sample points</li> <li>\\(k =\\) no of input variables</li> <li>\\(C =\\) no of classes</li> </ul>"},{"location":"CS_Electives/Data_Mining/02_Data/#dimensionality","title":"Dimensionality","text":"<p>No of features</p>"},{"location":"CS_Electives/Data_Mining/02_Data/#sparseness","title":"Sparseness","text":"<p>If majority of attributes have 0 as value, depending on the context</p>"},{"location":"CS_Electives/Data_Mining/02_Data/#resolution","title":"Resolution","text":"<p>Detail/Frequency of the data (hourly, daily, monthly, etc)</p>"},{"location":"CS_Electives/Data_Mining/02_Data/#types-of-datasets","title":"Types of Datasets","text":""},{"location":"CS_Electives/Data_Mining/02_Data/#records","title":"Records","text":"<p>Collection of records having fixed attributes, without any relationship with other records</p> Type Characteristic Example Data Matrix All attributes are numerical Usually what we have Sparse Data Matrix Majority of values are 0 - Frequency distribution kinda thingy for market basket data- Document term matrix Market Basket Data Every record of transactions, with collection of items - Association analysis market data"},{"location":"CS_Electives/Data_Mining/02_Data/#graph","title":"Graph","text":"Type Example Data objects with relationships Nodes(data objects) with edges (relationships) between them Google Search indexing Data objects that are graphs Chemical structures"},{"location":"CS_Electives/Data_Mining/02_Data/#ordered","title":"Ordered","text":"<p>Relationships between attributes</p>"},{"location":"CS_Electives/Data_Mining/02_Data/#sequentialtemporal","title":"Sequential/Temporal","text":"<p>Extension of record, where each record has a time associated with it.</p> <p>Even this can be time series data, if recorded periodically.</p> Time Customer Items Purchased t1 c1 A, B t2 c2 A, C"},{"location":"CS_Electives/Data_Mining/02_Data/#time-associated","title":"Time-Associated","text":"Customer Time and Items Purchased C1 \\(\\{t1, (A, B) \\}, \\{t2, (A, C) \\}\\) C2 \\(\\{t1, (B, C) \\}, \\{t2, (A, C) \\}\\)"},{"location":"CS_Electives/Data_Mining/02_Data/#sequence-data","title":"Sequence Data","text":"<p>Sequence of entities</p> <p>Eg: Genomic sequence data</p>"},{"location":"CS_Electives/Data_Mining/02_Data/#time-series-data","title":"Time Series Data","text":"<p>Series of observations over time recorded periodically</p> <p>Each record is a time series as well.</p> 12AM 6AM 12PM 6PM June 11 2020 June 12 2020 June 13 2020 June 14 2020"},{"location":"CS_Electives/Data_Mining/02_Data/#spatial-data","title":"Spatial Data","text":"<p>Data has spatial attributes, such as positions/areas</p> <p>Weather data collected for various locations</p>"},{"location":"CS_Electives/Data_Mining/02_Data/#spatio-temporal-data","title":"Spatio-Temporal Data","text":"<p>Data has both spatial and temporal attributes</p> Abu Dhabi Dubai Sharjah Ajman UAQ RAK Fujeirah June 11 2020 June 12 2020 June 13 2020 June 14 2020"},{"location":"CS_Electives/Data_Mining/02_Data/#issues-with-data-quality","title":"Issues with Data Quality","text":"Issue Solution is to ___ data object/attributes Example Improper sampling Unknown context Noise - Random component of measurement- Distorts the data Drop Anomaly/Rare events Obs that occur very rarely but it is possible Height of Person is 7\u20195 Artifacts/Spurious Obs Known Distortion that can be removed Height of Person is -10 Outliers/Flyers/Wild obs/Maverick Actual data, but very different from othersExtreme value of \\(y\\) Depends Height of Person is 8\u20195 Leveraged points Extreme value of \\(x\\) Influential points Outliers with high leverageRemoving the data point \u2018substantially\u2019 changes the regression results Missing Values Null values - Eliminate- Estimate/Interpolate- Ignore Inconsistent Data illogical data 50yr old with 5kg weight Duplicate Data De-Duplication - Same customer goes to multiple showrooms"},{"location":"CS_Electives/Data_Mining/02_Data/#estimation","title":"Estimation","text":"Attribute Type Interpolation Value Example Discrete Mode Grade Continuous Mean/Median(depending on the situation) Marks"},{"location":"CS_Electives/Data_Mining/02_Data/#data_1","title":"Data","text":"<p>Data can be structured/unstructured</p> <ul> <li>Each column = feature</li> <li>Each row = instance</li> </ul>"},{"location":"CS_Electives/Data_Mining/02_Data/#data-split","title":"Data Split","text":"<ul> <li>Train-Inner Validation-Outer Validation-Test is usually 60:10:10:20</li> <li>Split should be mutually-exclusive, to ensure good out-of-sample accuracy</li> </ul> <p>The size of test set is important; small test set implies statistical uncertainty around the estimated average test error, and hence cannot claim algo A is better than algo B for given task.</p> <p>Random split is the best. However, random split will not work well all the time, where there is auto-correlation, for eg: time-series data</p> <pre><code>flowchart LR\n\ntd[(Training Data)] --&gt;\n|Training| m[Model] --&gt;\n|Validation| vd[(Validation)] --&gt;\n|Tuning| m ---&gt;\n|Testing| testing[(Testing Data)]</code></pre>"},{"location":"CS_Electives/Data_Mining/02_Data/#multi-dimensional-data","title":"Multi-Dimensional Data","text":"<p>can be hard to work with as</p> <ul> <li>requires more computing power</li> <li>harder to interpret</li> <li>harder to visualize</li> </ul>"},{"location":"CS_Electives/Data_Mining/02_Data/#feature-selection","title":"Feature Selection","text":""},{"location":"CS_Electives/Data_Mining/02_Data/#dimension-reduction","title":"Dimension Reduction","text":"<p>Using Principal Component Analysis</p> <p>Deriving simplified features from existing features</p> <p>Easy example: using area instead of length and breadth.</p>"},{"location":"CS_Electives/Data_Mining/02_Data/#categories-of-data","title":"Categories of Data","text":"Mediocristan Extremistan Each observation has low effect on summary statistics \u2705 \u274c Example IQ, Weight, Height, Calories, Test Scores Wealth, Sales, Populations, Pandemics Law of Large Numbers Requires more samples for approaching the true mean Mean is meaningless Regression does not work\\(R^2\\) reduces with larger sample sizes Payoffs diverge from probabilitiesIt\u2019s not just about how often you are right, but also what happens when you\u2019re wrong: Being wrong 1 time can erase the gain of being right 99 times"},{"location":"CS_Electives/Data_Mining/02_Data/#fat-tailedness","title":"\u201cFat-Tailedness\u201d","text":"<p>Degree to which rare events drive the aggregate statistics of a distribution</p> <ul> <li>Lower \\(\\alpha \\implies\\) Fatter tails</li> <li></li> <li>Kurtosis (breaks down for \\(\\alpha \\le 4\\))</li> <li>Variance of Log-Normal distribution</li> <li></li> <li>Taleb\u2019s \\(\\kappa\\) metric</li> <li></li> </ul>"},{"location":"CS_Electives/Data_Mining/02_Data/#leverage","title":"Leverage","text":"<p>Leverage points = data points with extreme value of input variable(s)</p> <p>Like outliers, high leverage data points can have outsize influence on learning $$ h_{ii} = \\dfrac{\\text{cov}(\\hat y_i, y_i)}{\\text{var}(y_i)} \\ h_{ii} \\in [0, 1] \\ \\sum h_{ii} = k \\implies \\bar h = p/n $$ For univariate regression $$ h_{ii} = \\dfrac{1}{n} + \\dfrac{1}{n-1} \\left( \\dfrac{x_i - \\bar x}{s_x} \\right)^2 $$ High leverage points have lower variance $$ \\text{var}(u_i) = \\sigma^2_u (1-h_{ii}) \\ \\text{SE}(u_i) = \\text{RMSE} \\sqrt{1-h_{ii}} $$ </p> <p>Hence, when doing statistical tests on residuals (Grubbs\u2019 test, skewness, etc.) you should only use externally-studentized residuals </p> Internally Externally Data all data are included in the calculation \\(i\\)th data point is excluded from calculation of \\(\\text{RMSE}\\) Formula \\(\\text{isr}_i = \\dfrac{u_i}{\\text{SE}(u_i)} \\\\ = \\dfrac{u_i}{\\text{RMSE} \\sqrt{1-h_{ii}}}\\) \\(\\text{esr}_i = \\text{isr}_i \\sqrt{\\dfrac{n-p-1}{n-p- (\\text{isr}_i)^2}}\\) Distribution Complicated \\(t\\) distributed with DOF=\\(n-p-1\\) for \\(u \\in N(0, \\sigma_u)\\)"},{"location":"CS_Electives/Data_Mining/02_Data/#normalized-leverage","title":"Normalized Leverage","text":"\\[ \\begin{aligned} h_\\text{norm} &amp;= \\dfrac{h_{ii}}{\\bar h} \\\\ &amp;= h_{ii} \\times \\dfrac{n}{p} \\\\ \\end{aligned} \\]"},{"location":"CS_Electives/Data_Mining/02_Data/#williams-graph","title":"William\u2019s Graph","text":"<p>To inspect for both outliers and high-leverage data, plot the ESR vs Normalized Leverage</p> <p></p>"},{"location":"CS_Electives/Data_Mining/02_Data/#influence","title":"Influence","text":"<p>They are of concern, due to fragility of conclusions: our conclusions may depend only on a few influential data points</p> <p>We just identify influential points: We don\u2019t remove/adjust highly influential points</p> <p>\\(\\hat y_{j(i)}\\) is \\(\\hat y_j\\) without \\(i\\) in the training set</p> Formula Criterion\\(n \\le 20\\)\\(n &gt; 20\\) Cook\u2019s Distance \\(\\begin{aligned} &amp; D_i \\\\ &amp; = \\dfrac{\\sum\\limits_{j=1}^n (\\hat y_{j (i)} - \\hat y_j)}{k \\times \\text{MSE}} \\\\ &amp;= \\dfrac{u_i^2}{k \\times \\text{MSE}} \\times \\dfrac{h_{ii}}{(1-h_{ii})^2} \\\\ &amp;= \\dfrac{\\text{isr}_i^2}{k} \\times \\dfrac{h_{ii}}{(1-h_{ii})} \\end{aligned}\\) \\(1\\)\\(4/n \\quad \\approx F(k, n-k)\\).inv(0.5) Difference in Beta \\(\\begin{aligned} &amp; \\text{DFBETA}_{i, j} \\\\ &amp;= \\dfrac{\\beta_j - \\beta_{j(i)}}{\\text{SE}(\\beta_{k(i)})} \\end{aligned}\\) \\(1\\)\\(\\sqrt{4/n}\\) Difference in Fit \\(\\begin{aligned} &amp;\\text{DFFITS}_{i} \\\\ &amp;= \\dfrac{ \\hat y - \\hat y_{i(i)} }{ s_{u(i)} \\sqrt{h_{ii}} } \\\\ &amp;= \\text{esr}_i \\sqrt{ \\dfrac{h_{ii}}{1-h_{ii}} } \\end{aligned}\\) \\(1\\)\\(\\sqrt{4k/n}\\) Mahalanobis Distance"},{"location":"CS_Electives/Data_Mining/03_Measurement/","title":"Measurement","text":""},{"location":"CS_Electives/Data_Mining/03_Measurement/#notes","title":"Notes","text":"<ul> <li>Most measurements are indirect: What we actually measure is different what we want to study</li> <li>For eg: measuring temperature with mercury thermometer: we look at the difference in mercury height</li> <li>Measurement can change the thing that you are measuring</li> </ul>"},{"location":"CS_Electives/Data_Mining/03_Measurement/#measurement-stability","title":"Measurement Stability","text":"<p>Temporal &amp; Spatial</p> <p>Repeated measurements are taken at different times, locations, conditions</p> <ul> <li>How constant is the sample</li> <li>How constant is the measurement process</li> <li>How constant is the measurement context</li> </ul>"},{"location":"CS_Electives/Data_Mining/03_Measurement/#observation-decomposition","title":"Observation Decomposition","text":"<p>Process observation</p> <ul> <li>Process True Value</li> <li>Process Error</li> <li>Measurement Error</li> <li>Procedure Error</li> <li>Sensor Error</li> </ul>"},{"location":"CS_Electives/Data_Mining/03_Measurement/#error-components","title":"Error Components","text":"<ul> <li>Systematic errors</li> <li>Produces bias</li> <li>We try to correct systematic error, but can never be totally free from systematic error</li> <li>We can put an upper limit on the expected systematic errors</li> <li>Random errors: Can be evaluated statistically, through repeated measurements</li> </ul>"},{"location":"CS_Electives/Data_Mining/03_Measurement/#measurement-metrics","title":"Measurement Metrics","text":"<ul> <li>Accuracy: 1 - systematic error</li> <li>Precision: standard deviation of repeated measurements (random error component)</li> <li>Repeatability: standard deviation of repeated measurements under conditions as nearly identical as possible</li> <li>Reproducibility: standard deviation of repeated measurements under conditions that vary (different operators, instruments, days, time)</li> </ul>"},{"location":"CS_Electives/Data_Mining/03_Measurement/#uncertainty-types","title":"Uncertainty Types","text":"<ul> <li>Type A: Process Noise</li> <li>Caused by fluctuations in nature that propagate through measurement model</li> <li>obtained by statistical analysis of repeated measurements</li> <li>Type B: Measurement Noise</li> <li>Types<ul> <li>Measurement Procedure Noise</li> <li>Incomplete definition of measurement</li> <li>Imperfect realization of procedure</li> <li>Sample not representative</li> <li>Environmental conditions</li> <li>Biases in reading analog scales</li> <li>Instrument resolution</li> <li>Values of constants used in calculations</li> <li>Changes in measuring instrument performance since last calibration</li> <li>Approximations/assumptions in measurement model</li> <li>Sensor Noise</li> </ul> </li> <li>Evaluated by scientific judgement (Prior experience or data, manufacture\u2019s specs)</li> </ul>"},{"location":"CS_Electives/Data_Mining/03_Measurement/#effective-degrees-of-freedom","title":"Effective Degrees of Freedom","text":"<p>When using combined uncertainty , we assume that the measurement is t-distributed</p> <p>Welch-Satterthwaite approximation $$ \\text{DOF}_\\text{eff} = \\dfrac{(\\sum u_i<sup>2)</sup>2}{\\sum (u_i^4/\\text{DOF}_i)} $$</p>"},{"location":"CS_Electives/Data_Mining/03_Measurement/#replication","title":"Replication","text":""},{"location":"CS_Electives/Data_Mining/03_Outliers/","title":"Outliers","text":""},{"location":"CS_Electives/Data_Mining/03_Outliers/#causes","title":"Causes","text":"<ul> <li>True distribution has heavy tails</li> <li>Data \u201ccontaminated\u201d by another distribution withe either</li> <li>significantly different mean</li> <li>significantly larger variance</li> </ul>"},{"location":"CS_Electives/Data_Mining/03_Outliers/#studentized-data","title":"Studentized Data","text":"<p>How many SD away from the mean is this data point $$ t_i =\\dfrac{x_i - \\bar x}{s_x} $$</p>"},{"location":"CS_Electives/Data_Mining/03_Outliers/#robustness-outliers","title":"Robustness &amp; Outliers","text":"\\[ E[\\bar x] = \\mu + \\dfrac{x_o - \\mu}{n}  \\\\ E[s^2_x] = \\sigma^2_x + \\dfrac{(x_o - \\mu)^2}{n} \\] <p>For large outliers \\(x_o \\gg n\\mu, n\\sigma\\) $$ \\bar x \\approx \\dfrac{x_o}{n} \\ s \\approx \\dfrac{\\vert x_o \\vert}{\\sqrt{n}} \\ \\implies T = \\dfrac{\\vert x_o - \\bar x \\vert}{s} \\approx \\dfrac{n-1}{\\sqrt{n}} $$</p>"},{"location":"CS_Electives/Data_Mining/03_Outliers/#testing-for-outliers","title":"Testing for Outliers","text":"<p>Outlier: Observation so different from others that it is suspected to be generated by a different mechanism with a one-time, large systematic error</p> <p>Useful only when \\(n&gt;20\\)</p>"},{"location":"CS_Electives/Data_Mining/03_Outliers/#causes_1","title":"Causes","text":"<ul> <li>Error in measurement recording</li> <li>Failure of measurement process/tool</li> <li>One sample was fundamentally different from other samples being measured</li> <li>Failure of experimental process (eg: sample didn't receive proper treatment)</li> </ul>"},{"location":"CS_Electives/Data_Mining/03_Outliers/#idk","title":"IDK","text":"<p>Detecting outliers is the first step to discover the mechanism that caused the outlier</p> <p>Sometimes the causes of outliers is more insightful that the analysis of the \u201cgood\u201d data</p>"},{"location":"CS_Electives/Data_Mining/03_Outliers/#rareness-detection","title":"Rareness Detection","text":"\\[ x \\text{ is outlier} \\iff P(x) &lt; 1/n \\] <p>\\(p\\)-value \\(\\times n\\) = probability of getting one data point (out on \\(n\\)) this unusual or more due to random chance</p> <p>Assumes that we know the underlying distribution</p> <p>Chauvenet\u2019s criterion: reject if \\(p\\)-value \\(&lt;1/(2n)\\)</p> <p>Simple, assumes normal distribution, arbitrary cut-off, not rigorous</p> <p>Not recommended</p>"},{"location":"CS_Electives/Data_Mining/03_Outliers/#multiple-of-iqr","title":"Multiple of IQR","text":"<ul> <li>IQR = Q3-Q1</li> <li>Outliers</li> <li>Upper cutoff = Q3 + 1.5 IQR</li> <li>Lower cutoff = Q1 - 1.5 IQR</li> <li>Using this technique, usually about 1% of data points could be expected to be labelled as outliers</li> <li>Far outliers</li> <li>Upper cutoff = Q3 + 3 IQR</li> <li>Lower cutoff = Q1 - 3 IQR</li> </ul> <p>Robust test</p>"},{"location":"CS_Electives/Data_Mining/03_Outliers/#dixon-q-test","title":"Dixon \\(Q\\)-test","text":"<p>Identify one extreme data point $$ Q = \\dfrac{\\vert x_\\text{suspect} - x_\\text{closest} \\vert}{x_\\max - x_\\min} $$ Classify as outlier if \\(Q &gt; Q_\\alpha\\), where \\(\\alpha=\\) risk of rejecting good data</p> <ul> <li>Usually use \\(\\alpha&lt;\\) what you use for other tests</li> <li>For eg: use \\(\\alpha=0.01\\) instead of \\(0.05\\)</li> </ul> <p>Mostly used when \\(n&lt;20\\) and where calculating SD is difficult</p>"},{"location":"CS_Electives/Data_Mining/03_Outliers/#problem","title":"Problem","text":"<p>Masking (what if there are multiple outliers)</p> <p></p>"},{"location":"CS_Electives/Data_Mining/03_Outliers/#grubbs-test","title":"Grubbs\u2019 Test","text":"<p>The test assumes that underlying distribution is normal; given this assumption holds true, Grubb\u2019s test has more power than Q-test</p> <p>Steps</p> <ol> <li>Identify number of outliers that you want to test</li> <li>For 2 outliers, we use a different critical value depending on whether the outliers are in the same/different tails</li> <li>Calculate Grubb\u2019s statistic</li> </ol> \\[ \\begin{aligned} \\text{G} &amp;= \\dfrac{\\text{SSE}_{\\cancel o}}{\\text{SSE}_{o}} \\\\ \\text{SSE} &amp;= \\sum (x_i - \\bar x)^2 \\end{aligned} \\] <p>where</p> <ul> <li>\\(\\text{SSE}_{\\cancel o} =\\) SSE for dataset without outlier (after removal)</li> <li>\\(\\text{SSE}_{o} =\\) SSE for dataset with outlier (before removal)</li> </ul> \\[ t_c = \\dfrac{n-1}{\\sqrt{n}} \\sqrt{ \\dfrac{ (t_{(\\alpha/2n), n-2})^2 }{ n-2 + (t_{(\\alpha/2n), n-2})^2 } } \\] <ul> <li>Usually, we take \\(\\alpha/2\\), but here we are looking at a collection of \\(n\\) numbers</li> <li>\\((n-2) =\\) degree of freedom</li> <li>\\((n-1)/\\sqrt{n} =\\) max value of \\(t\\)</li> </ul>"},{"location":"CS_Electives/Data_Mining/03_Outliers/#multiple-outliers","title":"Multiple outliers","text":"<p>Iterative Grubb\u2019s Test Extreme Studentized Deviate</p> <p>To find unknown number of outliers \\(k\\), apply Grubb\u2019s test iteratively</p> <ol> <li>Search for outliers</li> <li>If outlier detected</li> <li>remove it</li> <li>else, stop</li> <li>Repeat steps 1-2</li> </ol> <p>Note: the \\(t_c\\) depends on \\(k\\)</p>"},{"location":"CS_Electives/Data_Mining/03_Outliers/#peirces-criterion","title":"Peirce\u2019s Criterion","text":"<p>Compare probability of the data with outliers to probability of the data without the outliers</p> <p>Assumes a normal distribution</p> <p>Can remove multiple outliers in a single iteration</p> <p>Not as common as Grubb\u2019s test</p>"},{"location":"CS_Electives/Data_Mining/03_Outliers/#handling-outliers","title":"Handling outliers","text":"<pre><code>flowchart LR\n\ni{Identify&lt;br /&gt;outlier cause?}\n\ndc{Data&lt;br /&gt;correction&lt;br /&gt;possible?}\n\nc[\"1. Correct data&lt;br /&gt;2. Include in analysis\"]\nr[\"1. Address outlier&lt;br /&gt;2. Document removal\"]\n\np[\"Perform analysis&lt;br /&gt;1. w/ outlier&lt;br /&gt;2. w/o outlier\"]\nda{\"Influential outlier?&lt;br/&gt;(Affects conclusions?)\"}\ndw[\"1. No issues&lt;br /&gt;2. Document everything\"]\nouch[\"1. Take more data (Repeat exp)&lt;br /&gt;2. Abandon normality assumption\"]\n\n\nstart([ ]) --&gt;\ni --&gt; |Y| dc --&gt;\n|Y| c\n\ndc --&gt; |N| r\n\ni --&gt; |N| p --&gt; da\n\nda --&gt; |N| dw\nda --&gt; |Y| ouch</code></pre> <p>Techniques to address outlier</p> <ul> <li>Delete outlier</li> <li>Always delete spurious data</li> <li>Truncate (delete both min and max data points)</li> <li>Winterize outlier (set value equal to closest neighbor)</li> <li>Replace outlier with expected value from Q-Q plot</li> <li>Use robust methods instead</li> </ul>"},{"location":"CS_Electives/Data_Mining/03_Outliers/#identifying-causes","title":"Identifying causes","text":"<p>Always identify cause outlier as there could be lessons to be learnt</p> Time of identifying cause Comment Report? Before outlier detection Measurement instrument breaks and must be repaired; you suspect calibration will be off \u26a0\ufe0fDepends; not very useful, but good practice After outlier detection Beware of just-so stories \u2705 Never \u2705"},{"location":"CS_Electives/Data_Mining/03_Outliers/#idk_1","title":"IDK","text":"<p>Importance of outlier depends on decision you are trying to make</p> <p>Spurious vs outlier depends on what is important to you</p>"},{"location":"CS_Electives/Data_Mining/03_Outliers/#recommend-testing-sequence","title":"Recommend Testing Sequence","text":"<ol> <li>Graph the data: histogram, box plot, Q-Q plot</li> <li>Perform moment tests (mean, standard deviation, skewness, kurtosis)</li> <li>If non-normality detected, check for outliers</li> <li>If outliers removed, recheck for outliers</li> <li>If non-normal distribution suspected, use empirical CDF to identify candidate distributions</li> </ol>"},{"location":"CS_Electives/Data_Mining/04_Data_Preprocessing/","title":"Data Preprocessing","text":"<p>You don\u2019t have to apply all these; it depends. You have to first understand the dataset.</p> Technique Meaning Advantage Disadvantage Aggregation Combining/Merge data objects/attributesContinuous: Sum, mean, max, max, min, etcDiscrete: Mode, Summarization, Ignoring - Low processing cost, space, time- Higher view- More stable Losing details Sampling Creating representative subset of a dataset, whose characteristics are similar to the original dataset Dimensionality Reduction Mathematical algorithm resulting in a set of new combination of old attributes Eliminate noise and unnecessary featuresBetter understandabilityReduce time, memory and other processing costEasier visualization Getting the original data is not possible after transformation Feature Subset Selection Removing irrelevant and redundant attributes Same as ^^ Extra resources required Feature Creation Create new attributes that can capture multiple important features more efficiently Discretization Convert continuous attribute into categorial/discrete (for classification) Binarization(Encoding) Convert continuous/categorical attribute into binary (association mining) Attribute Transformation Mathematical transformations <p>Feature selection and Dimensionality reduction are used for biomarkers analysis</p>"},{"location":"CS_Electives/Data_Mining/04_Data_Preprocessing/#types-of-sampling","title":"Types of Sampling","text":""},{"location":"CS_Electives/Data_Mining/04_Data_Preprocessing/#random-sampling","title":"Random Sampling","text":"Random Sampling Data object put back into original population? Duplicates? With replacement \u2705 \u2705 Without replacement \u274c \u274c"},{"location":"CS_Electives/Data_Mining/04_Data_Preprocessing/#problem","title":"Problem","text":"<p>It may lead to misclassification, as not all classes are represented proportionally in the sample.</p>"},{"location":"CS_Electives/Data_Mining/04_Data_Preprocessing/#stratified-sampling","title":"Stratified Sampling","text":"<p>Different types of objects/classes with different frequency are used in the sample.</p> <p>Useful especially in imbalanced dataset, where all the classes have large variation in their counts.</p> <p>Ensures all classes of the population are well-represented in the sample.</p>"},{"location":"CS_Electives/Data_Mining/04_Data_Preprocessing/#steps","title":"Steps","text":"<ul> <li>Draw samples from each class<ul> <li>equal samples, or</li> <li>proportional samples, using % of the total of all classes</li> <li>Gives us imbalanced dataset</li> </ul> </li> <li>Combine these samples into a larger sample</li> </ul>"},{"location":"CS_Electives/Data_Mining/04_Data_Preprocessing/#progressiveadaptive-sampling","title":"Progressive/Adaptive Sampling","text":"<p>Useful when not sure about good sample size</p> <p>Computationally-expensive</p>"},{"location":"CS_Electives/Data_Mining/04_Data_Preprocessing/#steps_1","title":"Steps","text":"<pre><code>flowchart LR\ns[\"Start with small sample (100-1000)\"] --&gt;\na[Apply data mining algorithm] --&gt;\ne[Evaluate results] --&gt;|Increase Sample Size| s\n\ne --&gt;|Best Result Obtained| st[/Stop/]</code></pre>"},{"location":"CS_Electives/Data_Mining/04_Data_Preprocessing/#data-augmentation","title":"Data Augmentation","text":"<ul> <li>Images</li> <li>Flip</li> <li>Rotation</li> <li>Adding noise</li> <li>Warping</li> <li>Mixup</li> <li>Convert labels</li> <li>\\(x' = \\lambda x_i + (1-\\lambda) x_j; y' = \\lambda y_i + (1-\\lambda) y_j\\)</li> <li> </li> <li> <p>Fit it to a distribution</p> </li> </ul>"},{"location":"CS_Electives/Data_Mining/04_Data_Preprocessing/#dimensionality-reduction","title":"Dimensionality Reduction","text":"<p>Unsupervised learning problem where we find a low-dimensional representation \\(\\mathcal Z\\) of \\(\\mathcal X\\) $$ \\begin{aligned} \\mathcal{Z} &amp;= f_\\theta(\\mathcal{X}) \\ f_\\theta &amp;: R^k \\to R^d \\ d &amp;&lt; k \\end{aligned} $$ Note: It is important to standardize the data</p>"},{"location":"CS_Electives/Data_Mining/04_Data_Preprocessing/#dimensionality-reduction-algorithms","title":"Dimensionality Reduction Algorithms","text":"Technique Working Reduce dimensionality while Learning Type Comment No Hyperparameter Tuning Required Fast Deterministic Linearity LDA Maximize distance between classes Separating pre-known classes in the data Supervised \u2705 \u2705 \u2705 Linear PCA/SVD using PCA Maximize variance in dataFind linear combinations of predictor vars that are orthogonal to each other1. Calculate correlation matrix of predictors2. Find eigenvalues and corresponding eigenvectors of correlation matrix3. Orthogonalize design matrix by multiplying by a rotation matrix made up of eigenvectors\\(\\mu(\\text{PC}_i)=0 \\quad \\forall i\\)\\(\\sigma^2(\\text{PC}_i)=\\text{Eigenvalue} \\quad \\forall i\\)Learning more: Correlation matrix b/w PCs and original predictors Generating clusters previously not known Unsupervised \\(2k\\) contaminated points can destroy top \\(k\\) components \u2705 \u2705 \u2705 Linear Kernel PCA MDS ^^ Unsupervised \u274c \u274c \u274c Non-Linear t-SNE ^^ Unsupervised \u274c \u274c \u274c Non-Linear UMAP ^^ Unsupervised \u274c \u2705 \u274c Non-Linear Variational auto-encoder Independent Component Analysis Maximize signal independence"},{"location":"CS_Electives/Data_Mining/04_Data_Preprocessing/#pca","title":"PCA","text":""},{"location":"CS_Electives/Data_Mining/04_Data_Preprocessing/#feature-selection","title":"Feature Selection","text":"<pre><code>flowchart LR\n\nAttributes --&gt;\nss[Search Strategy] --&gt;\nsa[/Subset of Attributes/] --&gt;\nEvaluation --&gt;\nsc{Stopping Criterion Reached?} --&gt;\n|No| ss\n\nsc --&gt;\n|Yes| sel[/Select Attributes/] --&gt;\nSomething</code></pre>"},{"location":"CS_Electives/Data_Mining/04_Data_Preprocessing/#mutual-information","title":"Mutual Information","text":"<p>Mutual information (MI) between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency. The function relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances.</p>"},{"location":"CS_Electives/Data_Mining/04_Data_Preprocessing/#brute-force-approach","title":"Brute Force Approach","text":"<p>Consider a set with \\(n\\) attributes. Its power set contains \\(2^n\\) sets. Ignoring \\(\\phi\\), we get \\(2^{n-1}\\) sets.</p> <p>Steps</p> <ul> <li>Evaluate the performance of all possible combinations of subsets</li> <li>Choose the subset of attributes which gives the best results</li> </ul>"},{"location":"CS_Electives/Data_Mining/04_Data_Preprocessing/#embedded-approach","title":"Embedded Approach","text":"<p>The data mining algorithm itself performs the selection, without human intervention</p> <p>Eg: A decision tree automatically chooses the best attributes at every level</p> <p>Builds a model in the form of a tree</p> <ul> <li>Internal nodes = labelled with attributes</li> <li>Leaf nodes = class label</li> </ul>"},{"location":"CS_Electives/Data_Mining/04_Data_Preprocessing/#filter-approach","title":"Filter Approach","text":"<p>Independent of data mining algorithm</p> <pre><code>flowchart LR\no[(Original&lt;br /&gt; Feature Set)] --&gt;\n|Select&lt;br /&gt; Subset| r[(Reduced&lt;br /&gt; Feature Set)] --&gt;\n|Mining&lt;br /&gt; Algorithm| Result</code></pre> <p>eg: Select attributes whose evaluation criteria(pairwise correlation/Chi<sup>2</sup>, entory) is as high/low as possible</p>"},{"location":"CS_Electives/Data_Mining/04_Data_Preprocessing/#wrapper-approach","title":"Wrapper Approach","text":"<p>Use the data mining algorithm (capable of ranking importance of attributes) as a black box to find best subset of attributes</p> <pre><code>flowchart LR\no[(Original&lt;br /&gt; Feature Set)] --&gt;\ns[Select&lt;br /&gt; Subset] --&gt;\ndm[Mining&lt;br /&gt; Algorithm] --&gt;\nr[(Reduced&lt;br /&gt; Feature Set)] &amp; s\n\nsubgraph Black Box[\"Ran n times\"]\n    s\n    dm\nend</code></pre>"},{"location":"CS_Electives/Data_Mining/04_Data_Preprocessing/#feature-engineering","title":"Feature Engineering","text":""},{"location":"CS_Electives/Data_Mining/04_Data_Preprocessing/#feature-extraction","title":"Feature extraction","text":""},{"location":"CS_Electives/Data_Mining/04_Data_Preprocessing/#mapping-data-to-new-space","title":"Mapping data to new space","text":"<ul> <li>Time series data \\(\\to\\) frequency domain</li> <li>For eg, fourier transformation</li> </ul>"},{"location":"CS_Electives/Data_Mining/04_Data_Preprocessing/#feature-construction","title":"Feature Construction","text":"<p>Construct new features from existing features</p> <p>Eg - Area = length * breadth - Density = mass/volume</p> <p>Operators</p> <ul> <li>Univariate</li> <li>Normalize: Divide</li> <li>Scale: Power, Power Root, Exponent, Log, Factorial</li> <li>Multivariate (interaction terms)</li> <li>Combine: Add, Multiply</li> <li>Contrast: Subtract, Divide, Absolute value</li> </ul>"},{"location":"CS_Electives/Data_Mining/04_Data_Preprocessing/#discretization","title":"Discretization","text":"<ol> <li> <p>Sort the data in ascending order</p> </li> <li> <p>Generate</p> <ul> <li> <p>\\(n-1\\) split points</p> </li> <li> <p>\\(n\\) bins \\(\\to\\) inclusive intervals (specified by the analyst)</p> </li> </ul> </li> </ol> <p>Then convert using binarization. But, why?</p>"},{"location":"CS_Electives/Data_Mining/04_Data_Preprocessing/#types","title":"Types","text":"Equal-Width Binning Equal-Frequency Binning Analyst specifies No of bins Frequency of data objects in each bin Width \\(\\frac{\\text{Max-Min}}{\\text{No of bins}}\\) Make sure atleast \\(n-1\\) bins have the correct frequency"},{"location":"CS_Electives/Data_Mining/04_Data_Preprocessing/#binarization","title":"Binarization","text":"Method 1 One-Hot Encoding For \\(m\\) categories, we need ___ digits \\(\\lceil \\log_2 m \\rceil\\) \\(m\\) No unusual relationship \u274c \u2705 Fewer variables? \u2705 \u274c"},{"location":"CS_Electives/Data_Mining/04_Data_Preprocessing/#attribute-transform","title":"Attribute Transform","text":"\\(x'\\) Property Simple \\(x^2, \\log x, \\vert  x  \\vert\\) Min-Max Normalization \\(\\frac{x - x_{\\text{min}}}{x_{\\text{max}} - x_{\\text{min}}}\\) \\(0 \\le x \\le 1\\) Standard Normalization \\(\\frac{x-\\mu}{\\sigma}\\) \\(\\mu' = 0, \\sigma' = 1\\) <p>Standardization</p> <ul> <li>Reduces collinearity: even if \\(r(x, x^2) \\ne 0\\) standardizing will ensure \\(r(\\tilde x, {\\tilde x}^2)=0\\)</li> <li>However, it does NOT reduce multicollinearity</li> </ul>"},{"location":"CS_Electives/Data_Mining/04_Data_Preprocessing/#naming","title":"Naming","text":"Feature Transform Input variables Target Transform Output variables"},{"location":"CS_Electives/Data_Mining/04_Data_Preprocessing/#target-transform","title":"Target Transform","text":"<p>Make sure the target range is standardized to ensure model can generalize, especially when using non-linear models such as Decision Trees/ANN</p> <p>Non-linear transforms not recommended, as you will face all the disadvantages of MSLE </p>"},{"location":"CS_Electives/Data_Mining/04_Data_Preprocessing/#box-coxbickel-doksum-transform","title":"Box-Cox/Bickel-Doksum Transform","text":"\\[ y'_t = \\begin{cases} \\log \\vert y_t \\vert, &amp; \\lambda = 0 \\\\ \\dfrac{\\text{sign}(y_t) \\vert y_t \\vert ^\\lambda - 1}{\\lambda}, &amp; \\lambda \\ne 0 \\end{cases} \\] \\(\\lambda\\) Transformation 1 None \\(\\dfrac{1}{2}\\) Square root plus linear transformation 0 Natural log -1 Inverse plus 1"},{"location":"CS_Electives/Data_Mining/04_Data_Preprocessing/#back-transform","title":"Back Transform","text":"\\[ \\hat y_t = \\text{Med (y|t)} = \\begin{cases} \\exp(\\hat y'_t), &amp; \\lambda = 0 \\\\ \\text{sign}(\\lambda \\hat y'_t + 1) \\cdot {\\vert \\lambda \\hat y'_t + 1 \\vert}^{1/\\lambda}, &amp; \\lambda \\ne 0 \\end{cases} \\] <p>Back-transformed Prediction Intervals have correct coverage, but point forecasts are medians</p> <p>Hence, if we need the mean, we need to perform correction. (Didn\u2019t really understand the correction.)</p> \\[ E[y_t] = \\begin{cases} \\exp(\\hat y'_t) \\left[1 + \\dfrac{\\sigma^2}{2} \\right], &amp; \\lambda = 0 \\\\ (\\lambda \\hat y'_t + 1)^{1/\\lambda} \\left[1 + \\dfrac{\\sigma^2 (1-\\lambda)}{2(\\lambda \\hat y'_t + 1)^2} \\right], &amp; \\lambda \\ne 0 \\end{cases} \\]"},{"location":"CS_Electives/Data_Mining/04_Data_Preprocessing/#linear-basis-function","title":"Linear Basis Function","text":"\\[ \\begin{aligned} \\phi_i &amp;= \\text{exp} \\left\\{ \\frac{-(x- \\mu_i)^2}{2 \\sigma^2} \\right\\} \\\\ &amp;= \\begin{cases} 0, &amp; |x_i - x| \\to \\infty \\\\ 1, &amp; |x_i - x| \\approx 0 \\end{cases} \\end{aligned} \\] <ul> <li>\\(\\mu\\) = pivot</li> <li>\\(\\sigma^2\\) = bandwidth</li> <li>Higher, smoother</li> <li>Lower, sharper</li> </ul>"},{"location":"CS_Electives/Data_Mining/04_Data_Preprocessing/#lda","title":"LDA","text":"<p>Linear Discriminant Analysis, using Fisher Linear Discriminant</p> <p>Maximizes separation using multiple classes, by seeking a projection that best discriminates the data</p> <p>It is also used a pre-processing step for ML application</p>"},{"location":"CS_Electives/Data_Mining/04_Data_Preprocessing/#goals","title":"Goals","text":"<ul> <li>Find directions along which the classes are best-separated (ie, increase discriminatory information)</li> <li>Maximize inter-class distance</li> <li>Minimize intra-class distance</li> <li>It takes into consideration the scatter(variance) within-classes and between-classes</li> </ul>"},{"location":"CS_Electives/Data_Mining/04_Data_Preprocessing/#steps_2","title":"Steps","text":"<ol> <li>Find within-class Scatter/Covariance matrix</li> </ol> <p>\\(S_w = S_1 + S_2\\)</p> <ul> <li>\\(S_1 \\to\\) Covariance matrix for class 1</li> <li>\\(S_2 \\to\\) Covariance matrix for class 2</li> </ul> \\[ S_1 = \\begin{bmatrix} \\text{cov}(x_1, x_1) &amp; \\text{cov}(x_1, x_2) \\\\    \\text{cov}(x_2, x_1) &amp; \\text{cov}(x_2, x_2) \\end{bmatrix} \\] \\[ \\begin{aligned} \\text{Cov}(x_j, x_k) &amp;= \\frac{1}{n_j - 1} \\sum_{i=1, x \\in C_j}^{n_1} (x_i - \\mu_1)(x_i - \\mu_1) \\\\ \\text{Cov}(x_1, x_1) &amp;= \\frac{1}{n_1 - 1} \\sum_{i=1, x \\in C_1}^{n_1} (x_i - \\mu_1)^2 \\end{aligned} \\] <ol> <li>Find between-class scatter matrix</li> </ol> \\[ S_B = (\\mu_1 - \\mu_2) (\\mu_1 - \\mu_2)^T \\] <ol> <li> <p>Find Eigen Value</p> </li> <li> <p>Find Eigen Vector</p> </li> <li> <p>Generate LDA Projection Normalized Eigen Vector</p> </li> <li> <p>Generate LDA score (projected value) in reduced dimensions</p> </li> </ol> \\[ \\text{LDA Score} = x_1 v_1 + x_2 v_2 \\]"},{"location":"CS_Electives/Data_Mining/04_Data_Preprocessing/#eigen-value","title":"Eigen Value","text":"\\[ | A - \\lambda I | = 0 \\\\ |S_w^{-1} S_B - \\lambda I| = 0 \\] <ul> <li>\\(\\lambda =\\) Eigen Value(s)</li> <li>If we get multiple eigen values, we only take the highest eigen value</li> <li>It helps preserve more information. How??</li> <li>\\(I =\\) Identity Matrix</li> </ul> <p>We are taking \\(A=S_w^{-1} S_B\\) because taking \\(S_w^{-1}\\) helps us maximize \\(\\frac{1}{x}, x \\in S_w\\)</p> <ul> <li>Hence \\(x\\) is minimized</li> <li>Thereby, within-class distance is minimized</li> </ul>"},{"location":"CS_Electives/Data_Mining/04_Data_Preprocessing/#eigen-vector","title":"Eigen Vector","text":"\\[ (S_w^{-1} S_B - \\lambda I)  \\textcolor{hotpink}{V} = 0 \\] <ul> <li>\\(\\lambda =\\) Highest eigen value</li> <li>\\(V =\\) Eigen Vector</li> </ul>"},{"location":"CS_Electives/Data_Mining/04_Data_Preprocessing/#normalized-eigen-vector","title":"Normalized Eigen Vector","text":"\\[ V_\\text{norm} = \\begin{bmatrix} \\frac{v_1}{\\sqrt{v_1^2 + v_2^2}} \\\\ \\frac{v_2}{\\sqrt{v_1^2 + v_2^2}} \\end{bmatrix} \\]"},{"location":"CS_Electives/Data_Mining/04_Data_Preprocessing/#missing-value-imputation","title":"Missing Value Imputation","text":"<p>Just replacing value with mean, without understanding the underlying process is not good</p> <ul> <li>The fact that the value is missing is an important observation itself</li> <li>Replace missing \\(x\\) with \\(\\arg \\max \\limits_x P(x_j \\vert x_{\\centernot j}, y)\\)</li> <li>Replace missing \\(y\\) with \\(\\arg \\max \\limits_x P(y \\vert x)\\)</li> </ul> <p>This may be</p> <ul> <li>median</li> <li>mode</li> <li>min</li> <li>max: for eg, sensors won\u2019t take reading if temperature is too high</li> </ul>"},{"location":"CS_Electives/Data_Mining/05_Data_Exploration/","title":"Exploratory Data Analysis","text":"<p>Preliminary investigation of data, to understand its characteristics</p> <p>Helps identify appropriate pre-processing technique and data mining algorithm</p> <p>Involves</p> <ul> <li>Summary Statistics</li> <li>Visualization</li> </ul>"},{"location":"CS_Electives/Data_Mining/05_Data_Exploration/#summary-statistics","title":"Summary Statistics","text":"<p>Note: Statistics about the data \\(\\ne\\) data itself</p>"},{"location":"CS_Electives/Data_Mining/05_Data_Exploration/#robustness","title":"Robustness","text":"<p>Ability of a statistical procedure to handle a variety of distributions (non-normal) and contamination (outliers, etc)</p> <p>There is a trade-off between efficiency and robustness</p>"},{"location":"CS_Electives/Data_Mining/05_Data_Exploration/#breakdown-point","title":"Breakdown Point","text":"<p>Fraction of contaminated data in a dataset that can be tolerated by the statistical procedure</p> <p>Max logical BP is 0.5, because after that, you can\u2019t tell what is correct data and what is contaminated</p>"},{"location":"CS_Electives/Data_Mining/05_Data_Exploration/#contamination","title":"Contamination","text":"<p>Fraction of data comes from a different distribution</p> <p>There are 2 models for contamination</p> <ul> <li>Mean shift</li> <li>Variance shift</li> </ul>"},{"location":"CS_Electives/Data_Mining/05_Data_Exploration/#univariate-summary-statistics","title":"Univariate Summary Statistics","text":"<p>Minimal set of value(s) that captures the characteristics of large amounts of data, and show the properties of a distribution</p> Meaning Formula Moment Breakdown Point(Higher is better) SEStandard Error\\(\\sigma(\\text{Estimate})\\)(Lower is better) SNRSignal Noise Ratio\\(\\dfrac{E [\\text{Estimate}]}{\\sigma(\\text{Estimate})}\\)(Higher is better) Comment Mean/Arithmetic Mean\\(\\mu\\) Central tendency of distribution \\(\\dfrac{\\sum x_i}{n}\\) 1<sup>st</sup> \\(\\dfrac{1}{n}\\) \\(1 \\times \\dfrac{s}{\\sqrt{n}}\\)(assumes Normal dist) Trimmed Mean \\(k \\%\\) obs from top of dist are removed\\(k \\%\\) obs from bottom of dist are removed\\(\\implies 2k \\%\\) obs are removed in total \\(\\dfrac{k}{n}\\) \\(\\left( 1+\\dfrac{2k}{n} \\right)\\dfrac{s}{\\sqrt{n}}\\) For \\(k&gt;12.5\\), better to use median Winsorized Mean \\(k \\%\\) obs from top of dist are replaced with \\((1-k)\\)th percentile\\(k \\%\\) obs from bottom of dist are replaced with \\(k\\)th percentile\\(\\implies 2k \\%\\) obs are replaced in total \\(\\dfrac{k}{n}\\) \\(\\left( 1+\\dfrac{2k}{n} \\right)\\dfrac{s}{\\sqrt{n}}\\) For \\(k&gt;12.5\\), better to use median Weighted Mean \\(\\dfrac{\\sum w_i x_i}{n}\\) \\(\\dfrac{1}{n}\\) Geometric Mean \\(\\sqrt[{\\Large n}]{\\Pi x}\\) \\(\\dfrac{1}{n}\\) Root Mean Squared \\(\\sqrt{\\dfrac{\\sum_{i=1}^n (x_i)^2}{n}}\\) Gives more weightage to larger values Root Mean N \\(\\sqrt[p]{\\dfrac{\\sum_{i=1}^n (x_i)^p}{n}}\\) Gives more weightage based on power Harmonic Mean \\(\\dfrac{n}{\\sum \\frac{1}{x}}\\) \\(\\dfrac{1}{n}\\) Gives more weightage to smaller values Median Middle most observation50<sup>th</sup> quantile \\(\\begin{cases} x_{{n+1}/2}, &amp; n = \\text{odd} \\\\ \\dfrac{x_{n} + x_{n+1}}{2}, &amp; n = \\text{even}\\end{cases}\\) \\(\\dfrac{1}{2}\\) \\(1.253 \\dfrac{s}{\\sqrt{n}}\\) Robust to outliers Mode Most frequent observation Unstable for small samples Variance\\(\\sigma^2\\)\\(\\mu_2\\) Squared average deviation of observations from mean \\(\\dfrac{\\sum (x_i - \\mu)^2}{n}\\)\\(\\dfrac{\\sum (x_i - \\bar x)^2}{n} \\times \\dfrac{n}{n-1}\\) 2<sup>nd</sup> Centralised \\(\\dfrac{1}{n}\\) \\(2 s \\times \\dfrac{s}{\\sqrt{2 (n-1)}}\\)(Assumes Normal dist) \\(\\dfrac{n-1}{2}\\) Standard Deviation Average deviation of observations from mean \\(\\sqrt{\\text{Variance}}\\) \\(\\dfrac{1}{n}\\) \\(1 \\times \\dfrac{s}{\\sqrt{2(n-1)}}\\)(Assumes Normal dist) \\(\\sqrt{\\text{SNR}(\\sigma^2)}\\) Mean Absolute Deviation Mean deviation of observations from mean \\(\\dfrac{\\sum \\vert x_i - \\mu \\vert}{n}\\)\\(\\dfrac{\\sum \\vert x_i - \\bar x \\vert}{n} \\times \\dfrac{n}{n-1}\\) MADMedian Absolute Deviation Median deviation of observations from median \\(\\text{med} (\\vert x_i - \\text{med}_x \\vert)\\)\\(\\text{med} (\\vert x_i - \\hat {\\text{med}_x} \\vert ) \\times \\dfrac{n}{n-1}\\)\\(1.4826 \\times \\text{MAD}\\) corrects it to be comparable to standard deviation \\(\\dfrac{1}{2}\\) \\(1.67 \\times \\dfrac{s}{\\sqrt{2 (n-1)}}\\) Skewness\\(\\mu_3\\) Direction of tail \\(\\dfrac{\\sum (x_i - \\mu)^3}{n \\sigma^3}\\)\\(\\dfrac{3(\\mu - \\text{Md})}{\\sigma}\\)\\(\\dfrac{\\mu - \\text{Mo}}{\\sigma}\\)\\(\\dfrac{\\sum (x_i - \\bar x)^3}{n s^3} \\times \\dfrac{n}{(n-2)}\\) 3<sup>rd</sup> Standardized 0: Symmetric\\([-0.5, 0.5]\\): Approximately-Symmetric\\([-1, 1]\\): Moderately-skewedelse: Higly-skewed Kurtosis\\(\\mu_4\\) Peakedness of distribution \\(\\dfrac{\\sum (x_i - \\mu)^4}{n \\sigma^4}\\)\\(\\dfrac{\\sum (x_i - \\bar x)^4}{n s^4} \\times \\dfrac{n}{(n-3)}\\) 4<sup>th</sup> standardized Excess Kurtosis\\(\\mu_4'\\) Kurtosis compared to Normal distribution \\(\\mu_4-3\\) Max Min Quantile Divides distributions into 100 parts Unstable for small datasets Quartile Divides distributions into 4 parts Decile Divides distributions into 10 parts Range Range of values Max-Min Susceptible to outliers IQRInterquartile Range Q3 - Q1\\(1.349 \\sigma\\) (Normal dist)\\(0.7413 \\times \\text{IQR}\\) corrects it to be comparable to standard deviation \\(\\dfrac{1}{4}\\) \\(2.23 \\times \\dfrac{s}{\\sqrt{2(n-1)}}\\) Robust to outliers CVCoefficient of Variation \\(\\dfrac{\\sigma}{\\mu}\\) <p></p>"},{"location":"CS_Electives/Data_Mining/05_Data_Exploration/#standard-error-of-statistic","title":"Standard Error of Statistic","text":"<ul> <li>Standard deviation of statistic in sampling distribution</li> <li>Measure of uncertainty in the sample statistic wrt true population mean</li> </ul>"},{"location":"CS_Electives/Data_Mining/05_Data_Exploration/#relationship-between-mean-median-mode","title":"Relationship between Mean, Median, Mode","text":"\\[ \\text{Mo} = 3 \\text{Md} - 2 \\mu \\]"},{"location":"CS_Electives/Data_Mining/05_Data_Exploration/#skewness","title":"Skewness","text":"Skewness Property \\(&gt; 0\\) Mode &lt; Median &lt; Mean Positively Skewed \\(0\\) Mode = Median = Mean \\(&lt;0\\) Mean &lt; Median &lt; Mode Negatively Skewed"},{"location":"CS_Electives/Data_Mining/05_Data_Exploration/#moment","title":"Moment","text":"\\[ \\begin{aligned} M_k &amp;= E(x^k) \\\\ &amp;= \\dfrac{(x-M_{k-1})^k}{n} \\\\ &amp;= \\dfrac{(x-m_{k-1})^k}{n} \\times \\dfrac{n}{n-k+1} \\end{aligned} \\]"},{"location":"CS_Electives/Data_Mining/05_Data_Exploration/#multivariate-summary-statistics","title":"Multivariate Summary Statistics","text":"How 2 variables vary together Covariance \\(-\\infty &lt; C &lt; +\\infty\\) Correlation \\(-1 \\le r \\le +1\\)"},{"location":"CS_Electives/Data_Mining/05_Data_Exploration/#covariance-matrix","title":"Covariance Matrix","text":"<p>It is always \\(n \\times n\\), where \\(n =\\) no of attributes</p> \\(A_1\\) \\(A_2\\) \\(A_3\\) \\(A_1\\) \\(\\sigma^2_{A_1}\\) \\(\\text{Cov}(A_1, A_2)\\) \\(\\text{Cov}(A_1, A_3)\\) \\(A_2\\) \\(\\text{Cov}(A_2, A_1)\\) \\(\\sigma^2_{A_2}\\) \\(\\text{Cov}(A_2, A_3)\\) \\(A_3\\) \\(\\text{Cov}(A_3, A_1)\\) \\(\\text{Cov}(A_3, A_2)\\) \\(\\sigma^2_{A_3}\\) <p>The diagonal elements will be variance of the corresponding attribute</p> \\[ \\begin{aligned} \\text{Cov}(x, y) &amp;= \\frac{1}{n} \\sum_{k=1}^n (x_k - \\bar x) (y_k - \\bar y) \\\\ \\implies \\text{Cov}(x, x) &amp;= \\frac{1}{n} \\sum_{k=1}^n (x_k - \\bar x) (y_k - \\bar y) \\\\ &amp;= \\frac{1}{n} \\sum_{k=1}^n (x_k - \\bar x) (x_k - \\bar x) \\\\ &amp;= \\frac{1}{n} \\sum_{k=1}^n (x_k - \\bar x)^2 \\\\ &amp;= \\sigma^2_x \\end{aligned} \\]"},{"location":"CS_Electives/Data_Mining/05_Data_Exploration/#correlation-matrix","title":"Correlation Matrix","text":"\\(A_1\\) \\(A_2\\) \\(A_3\\) \\(A_1\\) \\(1\\) \\(r(A_1, A_2)\\) \\(r(A_1, A_3)\\) \\(A_2\\) \\(r(A_2, A_1)\\) \\(1\\) \\(r(A_2, A_3)\\) \\(A_3\\) \\(r(A_3, A_1)\\) \\(r(A_3, A_2)\\) \\(1\\) <p>The diagonal elements will be 1</p> \\[ \\begin{aligned} r(x, y) &amp;= \\frac{ \\text{Cov}(x, y) }{ \\sigma_x \\sigma_y } \\\\ \\implies r(x, x) &amp;= \\frac{ \\text{Cov}(x, x) }{ \\sigma_x \\sigma_x } \\\\ &amp;= \\frac{ \\frac{1}{n} \\sum_{k = 1}^n (x_k - \\bar x) (x_k - \\bar x) }{ \\left( \\sqrt{ \\frac{1}{n} (x_k - \\bar x)^2 } \\right)^2 } \\\\ &amp;= 1 \\end{aligned} \\]"},{"location":"CS_Electives/Data_Mining/05_Data_Exploration/#why-n-k-for-sample-statistics","title":"Why \\((n-k)\\) for sample statistics?","text":"<p>where \\(k=\\) No of estimators</p> <ol> <li>High probability that variance of sample is low, so we correct for that</li> <li>Lost degree of freedom</li> </ol>"},{"location":"CS_Electives/Data_Mining/06_Data_Visualization/","title":"06 Data Visualization","text":""},{"location":"CS_Electives/Data_Mining/06_Data_Visualization/#visualization","title":"Visualization","text":"<p>Display of data in a graphical/tabular format</p> <p>Helps us understand the data</p>"},{"location":"CS_Electives/Data_Mining/06_Data_Visualization/#why-is-visualization-important","title":"Why is visualization important?","text":"<p>Widely different distributions can have the same statistical properties</p> <p>Below is the Anscombe\u2019s quartet</p> <p></p>"},{"location":"CS_Electives/Data_Mining/06_Data_Visualization/#note","title":"Note","text":"<p>Use the correct minimum &amp; max range, such that only possible values are included</p> <ul> <li>Do not skew the axis</li> <li>For eg: for human body temperature, you should show 98-105 F; you shouldn\u2019t start at 0</li> </ul>"},{"location":"CS_Electives/Data_Mining/06_Data_Visualization/#uni-variate","title":"Uni-Variate","text":""},{"location":"CS_Electives/Data_Mining/06_Data_Visualization/#boxbox-whiskers-plot","title":"Box/Box-Whiskers Plot","text":"<p>Helps understand the range and central tendancy of a variable</p> <p></p>"},{"location":"CS_Electives/Data_Mining/06_Data_Visualization/#1d-histogram","title":"1D Histogram","text":"<p>Visualizes the frequency distribution of attribute</p> <p>Relative uncertainty of each bin frequency \\(\\propto \\dfrac{1}{\\sqrt{\\text{count}}}\\) </p>"},{"location":"CS_Electives/Data_Mining/06_Data_Visualization/#categorical-data","title":"Categorical Data","text":"<p>Each category will have a line denoting the frequency associated with that category</p>"},{"location":"CS_Electives/Data_Mining/06_Data_Visualization/#continuous-data","title":"Continuous Data","text":"<ul> <li>Apply binning<ul> <li>Usually equal-width binning</li> </ul> </li> <li>Each bin will be treated as a different category</li> <li>Now each bin will have a line denoting the frequency associated with that category</li> </ul> <p>The convention of analyzing these bins</p> <ul> <li>Values are left-inclusive and right-exclusive</li> <li>Last bin is right-inclusive</li> </ul> <p>~ Oracle Docs</p>"},{"location":"CS_Electives/Data_Mining/06_Data_Visualization/#limitations","title":"Limitations","text":"<ul> <li>Shape may change dramatically depending on bin settings</li> <li>Bins with few counts have high statistical uncertainty</li> <li>Interpretation can be difficult without huge amounts of data</li> </ul>"},{"location":"CS_Electives/Data_Mining/06_Data_Visualization/#q-q-plot","title":"Q-Q Plot","text":"<p>Quantile-Quantile plot comparing a distribution\u2019s quantiles with quantiles of a known distribution (such as Normal distribution)</p>"},{"location":"CS_Electives/Data_Mining/06_Data_Visualization/#bi-variate","title":"Bi-Variate","text":""},{"location":"CS_Electives/Data_Mining/06_Data_Visualization/#scatter-plot","title":"Scatter Plot","text":""},{"location":"CS_Electives/Data_Mining/06_Data_Visualization/#line-plot","title":"Line Plot","text":""},{"location":"CS_Electives/Data_Mining/06_Data_Visualization/#2d-histogram","title":"2D Histogram","text":"<p>Helps understand frequency of co-occurance of 2 attributes</p> <p></p>"},{"location":"CS_Electives/Data_Mining/06_Data_Visualization/#pair-plot","title":"Pair Plot","text":"<p>Basically a matrix of scatter plots</p>"},{"location":"CS_Electives/Data_Mining/06_Data_Visualization/#stem-leaf-plots","title":"Stem &amp; Leaf Plots","text":"<p>Understand the distribution of values of an attribute</p> <p>Useful when there aren\u2019t many values</p>"},{"location":"CS_Electives/Data_Mining/06_Data_Visualization/#steps","title":"Steps","text":"<ul> <li>Split values into groups, where each group contains those values that are the same except for the last digit</li> <li>Each group becomes a stem, while the last digit of a group are the leaves<ul> <li>Stems will be the higher-order digits</li> <li>Leaves will be the lower-order digits</li> </ul> </li> <li>Plot stems vertically and leaves horizontally</li> </ul>"},{"location":"CS_Electives/Data_Mining/06_Data_Visualization/#tri-variate","title":"Tri-Variate","text":""},{"location":"CS_Electives/Data_Mining/06_Data_Visualization/#contour-plots","title":"Contour Plots","text":"<p>Used for spacial data</p>"},{"location":"CS_Electives/Data_Mining/06_Data_Visualization/#multi-variate","title":"Multi-Variate","text":""},{"location":"CS_Electives/Data_Mining/06_Data_Visualization/#parallel-coordinates","title":"Parallel Coordinates","text":""},{"location":"CS_Electives/Data_Mining/06_Data_Visualization/#conditional-quantitative-plots","title":"Conditional Quantitative Plots","text":"<ul> <li>Bin quantitative data</li> <li>Make different plots</li> </ul> <p>This will be useful for error distribution inspection</p>"},{"location":"CS_Electives/Data_Mining/06_Data_Visualization/#cheat-sheet","title":"Cheat Sheet","text":""},{"location":"CS_Electives/Data_Mining/06_Data_Visualization/#data-visualization-guidelines","title":"Data Visualization Guidelines","text":"Company Guideline Last Updated Description Adobe https://spectrum.adobe.com/page/data-visualization-fundamentals/ December 2019 Data visualization guidelines for Spectrum\u2014Adobe's design system. Airbnb https://airbnb.io/visx Amazon Web Services https://cloudscape.design/patterns/general/data-vis/ Apple https://developer.apple.com/design/human-interface-guidelines/charts September 2022 General data visualization guidelines for application developers Aviva https://standards.aviva.com/framework/element-library/components/pie-chart/ 2021 Some guidelines on the graph / charts styles Baltimore City Data Fellows Project https://storymaps.arcgis.com/stories/d19f7d4d2a9b49c7b8f68730e3cda1e6 April 1, 2022 BBC https://www.bbc.co.uk/gel/guidelines/how-to-design-infographics 2016 How we design infographics for BBC editorial content and how you can make your own. BBC https://bbc.github.io/rcookbook/ 2019 An R package and an R cookbook to make the process of creating publication-ready graphics in our in-house style using R\u2019s ggplot2 library a more reproducible process, as well as making it easier for people new to R to create graphics. BBC Audiences https://public.tableau.com/profile/bbc.audiences#!/vizhome/BBCAudiencesTableauStyleGuide/Hello 2022 Guide for Tableau Desktop users. This style guide is here to help you achieve consistent user experiences, help you make effective visualisations and make your visualisation a part of the high quality output of the BBC's Audiences team. Cato Institute https://github.com/glosophy/CatoDataVizGuidelines/blob/master/PocketStyleBook.pdf 2019 It is meant to improve the quality of data visualization in order to create a more appealing experience for the reader and to more effectively communicate Cato\u2019s mission and message to the public.  You will find graphic guidelines for each figure and table as well as the justification for each one\u2019s design. Consumer Financial Protection Bureau https://cfpb.github.io/design-system/guidelines/data-visualization-guidelines 2018 The data visualization design manual provides guidance to designers, business intelligence analysts, researchers and developers to consider who their audience is, and the impacts of what they include in a title, some basics for accessibility, as well as keeping everything feeling like CFPB. Data Design System Collection https://airtable.com/appmJoE6s8PAWWHaU/shrSvvfZySawZHvQa/tblLVYunaKUqILW3k/viwZ8ay3WHvzmCC6Z?blocks=hide Hey there - first of all big thanks and kudos to you for all the ressources and articles here!  Just wanted to add a collection which i started also last year - maybe there is soemthing new for your google list or if you want to exchange on the metrics and way of representation. Best regards, Emanuel Dallas Morning News https://drive.google.com/file/d/16qdtjdnMPQt_rJDfSCEwr8sUUO555W4I/view 2005 In the following pages you will find more than just specifications of our graphics style; you\u2019ll find guidelines that should be considered for every graphic you make as well as clarity of graphic choice. DHL https://www.dpdhl-brands.com/en/dhl/tables-and-charts January 2024 Basic guidelines for shipping company DHL. Elastic https://eui.elastic.co/#/elastic-charts/creating-charts Finastra https://design.fusionfabric.cloud/data-visualization Gitlab https://design.gitlab.com/data-visualization/overview GitLab https://handbook.gitlab.com/handbook/business-technology/data-team/platform/tableau-style-guide/ November 3, 2023 GitLab's style guide specifically for Tableau Goldman Sachs https://design.gs.com/components/charts Google- Material Design https://material.io/design/communication/data-visualization.html 2019 Sections include: Principles, Types, Selecting charts, Style, Behavior, Dashboards Government of Canada https://design.gccollab.ca/data/data-overview/ Human Managed https://www.figma.com/file/jixsiIT7pCeiPMk8oiM6Qb/Data-viz-system?node-id=547%3A12066 2021 A style guide for Human Managed dashboards and data viz in general, with explanation on design choices and rationale. It's a work in progress. Humanitarian Data Exchange HDX https://data.humdata.org/dataviz-guide/dataviz-elements/ International Business Communication Standards (IBCS\u00ae) https://www.ibcs.com/standards 2018 The International Business Communication Standards (IBCS\u00ae) are practical proposals for the design of reports, presentations, dashboards and the diagrams and tables contained therein. This involves the conception of the content, the visual perception and the application of a semantic notation. The further development of the IBCS\u00ae Standards is an ongoing process, which is managed by the not-for-profit IBCS Association. IBM https://www.ibm.com/design/v1/language/experience/data-visualization/ 2016 See how IBM uses data visualization to provide meaningful context and precision. IBM https://www.ibm.com/design/language/data-visualization/overview 2020 (new URL) - 2020 Brand guideline Liferay https://liferay.design/lexicon/core-components/charts/ Mayo Clinic Genomic and Bioinformatic Services https://docs.google.com/document/d/1G1RluXmPC5xhiq8kddKYOOSl4Sqc3UtAHywva9FPh6g/edit 2023 London City Intelligence https://data.london.gov.uk/blog/city-intelligence-data-design-guidelines/ 2019 A first step toward improvement was creating a set of guidelines to help us all understand the basic principles of data visualisation, provide some examples of good practice, working processes and links to tools we can all use.  The guidelines are a \u201cwork in progress\u201d, that have evolved through documenting what processes and tools have worked well so far in our work, and we hope will expand further in the future.  As we begin to create more interactive data visualisation applications, built on the expanding Datastore platform, these guidelines will be important to underpin the design of those applications. MailChimp https://ux.mailchimp.com/patterns/data Data should be clear; it should not be overly decorative and it should be presented in a meaningful way. Below are a few standards that we\u2019ve set for telling visual data stories. Microsoft https://docs.microsoft.com/en-us/office/dev/add-ins/design/data-visualization-guidelines 2017 Guidelines to help you design effective data visualizations in your add-ins for Excel and other Office apps. MinnPost http://code.minnpost.com/minnpost-styles/ 2016 Welcome to MinnPost Styles, a super-fly, style guide focused on interactive and news applications made by the MinnData team. A work in progress.   MinnPost Styles is a CSS and JS framework. The CSS source is written in SASS and is extendable if you want to include the framework via SASS. The source code can be found on Github. Monash Climate Change  Communication Research Hub  (MCCCRH) https://apo.org.au/node/314650 Morning Star https://designsystem.morningstar.com/charts/chart-elements-status/ NZZ https://nzzdev.github.io/Storytelling-Styleguide/#/ 2019-07-24 This documentation is designed to collected and maintain the styles and visual language of the Neue Z\u00fcrcher Zeitung\u2019s Visuals team. Office for National Statistics https://style.ons.gov.uk/category/data-visualisation/ 2019 Guidance for creating charts and tables and best practice for using colour in your work. Opower https://ux.opower.com/opattern/how-to-charts.html 2015? How to create and use charts Pearson https://accessibility.pearson.com/resources/dataviz/ Unknown It seems to be a work in progress Pinterest https://gestalt.pinterest.systems/foundations/data_visualization/overview 2023 Royal Statistical Society https://rss.org.uk/datavisguide 2024 Best practices and style guides for data visualisations which is written primarily for contributors to Royal Statistical Society publications \u2013 chiefly, Significance magazine, the Journal of the Royal Statistical Society Series A, and Real World Data Science \u2013 but the information and advice within is of broad relevance and use to any data visualisation task. Salesforce https://lightningdesignsystem.com/guidelines/charts/ 2019 Chart guidance in Lightning Design System semrush https://developer.semrush.com/intergalactic/data-display/chart-showcase/chart-showcase Shopify https://polaris.shopify.com/design/data-visualizationst Outlines data visualization practices at Shopify and how to leverage them. By maintaining consistent styles and formats for our data visualizations, we ensure that data is presented in a truthful and accurate manner to maintain integrity with merchants. Sonos https://www.agencysr.co.uk/works/data-visualisation-guidelines Following an initial audit of the Sonos brand collateral and guidelines, Surgery red cow agency was asked to create a new set of guidelines for their data visualisation. Sunlight Foundation https://sunlightfoundation.com/2014/03/12/datavizguide 2014 The pdf guide covers the common applications and provides guidance on basic design and branding principles that all company charts should meet. Templates are created to get as close as possible to meeting the specs in this guide. Trafford Data Lab https://www.trafforddatalab.io/graphics_companion/ 5 June 2018 The Graphics Companion provides the R code for different data visualisations that can be created using the ggplot2 package. The Economist https://design-system.economist.com/documents/CHARTstyleguide_20170505.pdf 2017 The Urban Institute https://urbaninstitute.github.io/graphics-styleguide/ Use this data visualization style guide to create a uniform look and feel to all of Urban\u2019s charts and graphs. This site contains guidelines that are in line with data visualization best practices and proven design principles. It also eliminates the burden of design and color decisions when creating charts. Trafford Data Lab https://www.trafforddatalab.io/interactive_graphics_companion/ January 2019 The Interactive Graphics Companion provides the raw JSON for creating different data visualisations in Vega-Lite. U.S. Design System https://designsystem.digital.gov/components/data-visualizations/ 2021 Best practices for basic visualization design, including some accessibility guidance. US Agency for International Development | Office of HIV/AIDS https://issuu.com/achafetz/docs/oha_styleguide Feb 2021 The purpose of our style guide is to provide the USAID Office of HIV/AIDS (OHA) with a set of guide rails on how to present data in-line with an overall vision so that all OHA data visualizations have a common look and feel - through color choice, graphic elements, and principles. Visa https://developer.visa.com/pages/chart-components Basic set of charts in a design system with a focus on accessibility. Support for React, Angular, and R. VTEX https://styleguide.vtex.com/#/Components/%F0%9F%91%BB%20Experimental/Charts World Health Organization https://apps.who.int/gho/data/design-language/ 2023"},{"location":"CS_Electives/Data_Mining/09_Rule-Based_Classifier/","title":"Rule-Based Classifier","text":"<p>Knowledge about dataset is stored in the form of if-then rules, in a rule database \\(R\\)</p>"},{"location":"CS_Electives/Data_Mining/09_Rule-Based_Classifier/#rule","title":"Rule","text":"\\[ \\text{LHS} \\to \\text{RHS} \\] LHS RHS Contains Condition/Conjunct with attributes Class label Alternate Names AntecedantPre-Condition Consequent <p>If precondition of rule \\(r\\) matches attributes of record \\(x\\)</p> <ul> <li>\\(r\\) covers \\(x\\)</li> <li>\\(x\\) fires/triggers \\(r\\)</li> </ul>"},{"location":"CS_Electives/Data_Mining/09_Rule-Based_Classifier/#quality-of-classification-rule","title":"Quality of Classification Rule","text":"<p>Consider \\(r: A \\to y\\), where</p> <ul> <li>\\(r\\) is the rule</li> <li>\\(A\\) is the antecedent</li> <li>\\(D\\) is the dataset</li> <li>\\(|A|\\) is the number of records covered by rule</li> <li>\\(|D|\\) is the total number of records</li> </ul> Quality Measure Formula Coverage\\((r)\\) Fraction of records covered by rule \\(\\dfrac{\\vert A\\vert}{\\vert  D  \\vert}\\) Accuracy\\((r)\\)Confidence Factor Fraction of records for which the rule correctly predicted \\(\\dfrac{\\vert  A \\cap y \\vert}{\\vert  A  \\vert}\\)"},{"location":"CS_Electives/Data_Mining/09_Rule-Based_Classifier/#steps","title":"Steps","text":"<ol> <li>Find rule(s) that match(es) antecedent of record</li> <li>Next steps:</li> </ol> Number of rules triggered Rules have same class label Steps 0 N/A Add default ruleFallback to default class 1 N/A Assign consequent of rule as class label of test record Multiple \u2705 Assign consequent of rules as class label of test record Multiple \u274c - Use the highest-priority ordered rule (computationally-expensive for training)or- Use majority voting scheme using unordered rules(computationally-expensive for testing)"},{"location":"CS_Electives/Data_Mining/09_Rule-Based_Classifier/#default-rule","title":"Default Rule","text":"\\[ \\underbrace{}_\\text{Empty Antecedant} \\to \\underbrace{y_d}_\\text{Default class} \\]"},{"location":"CS_Electives/Data_Mining/09_Rule-Based_Classifier/#default-class","title":"Default Class","text":"<p>Majority class represented by records not covered by rules in rulebase</p>"},{"location":"CS_Electives/Data_Mining/09_Rule-Based_Classifier/#desired-propertes-of-rule-based-classifier","title":"Desired Propertes of Rule-Based Classifier","text":"Desired Property Meaning Rules are Mutually-Exclusive Only 1 rule is triggered for any record Rules are Exhaustive \\(\\exists \\ge 1\\) rule(s) that covers every record"},{"location":"CS_Electives/Data_Mining/09_Rule-Based_Classifier/#types-of-rules","title":"Types of Rules","text":"Ordered Unordered Priority assigned based on- coverage- accuracy No priority"},{"location":"CS_Electives/Data_Mining/09_Rule-Based_Classifier/#i-missed-15min","title":"I missed 15min","text":""},{"location":"CS_Electives/Data_Mining/09_Rule-Based_Classifier/#extraction-from-decision-tree","title":"Extraction from Decision Tree","text":"<p>One rule is created for each path from root leaf</p> <p>Keep taking the edges and use \u2018and\u2019 as the conjuction</p>"},{"location":"CS_Electives/Data_Mining/09_Rule-Based_Classifier/#why","title":"Why?","text":"<p>Rules are easier to understand than larger trees</p>"},{"location":"CS_Electives/Data_Mining/09_Rule-Based_Classifier/#sequential-covering-algorithm","title":"Sequential Covering Algorithm","text":"<ol> <li>Start with empy decision list \\(R\\), training records \\(E\\), class \\(y\\)</li> <li>Learn-One-Rule function is used to extract the best rule for class \\(y\\) that covers the current set of training records</li> <li>Remove training records covered by the rule</li> <li>New rule is added to the bottom of the decision list \\(R\\)</li> <li>Repeat Steps 2, 3, 4 until stopping criterion is met</li> <li>Algorithm proceeds to generate rules for the next class</li> </ol> <p>During rule extraction, all training records for class \\(y\\) are considered to be +ve examples, while those that belong to other classes are considered to be -ve examples</p> <p>Rule is desirable such that it covers most of the +ve examples and none/very few -ve examples</p>"},{"location":"CS_Electives/Data_Mining/09_Rule-Based_Classifier/#learn-one-rule-function","title":"Learn-One-Rule Function","text":""},{"location":"CS_Electives/Data_Mining/09_Rule-Based_Classifier/#general-to-specific","title":"General-to-Specific","text":""},{"location":"CS_Electives/Data_Mining/09_Rule-Based_Classifier/#initial-seed-rule","title":"Initial Seed Rule","text":"\\[ \\underbrace{\\phantom{\\text{Empty Antecedent}}}_\\text{Empty Antecedent} \\to y_0 \\] <p>Keep refining this initial seed rule, by adding more conjucts</p>"},{"location":"CS_Electives/Data_Mining/09_Rule-Based_Classifier/#specific-to-general","title":"Specific-to-General","text":"\\[ \\text{1 example of } y_0 \\to y_0 \\] <p>Keep refining this initial seed rule, by removing conjucts</p>"},{"location":"CS_Electives/Data_Mining/09_Rule-Based_Classifier/#metrics","title":"Metrics","text":""},{"location":"CS_Electives/Data_Mining/09_Rule-Based_Classifier/#foil-information-gain","title":"Foil\u2019 Information Gain","text":"<p>First order inductive learner</p> <p>Higher value \\(\\implies\\) better rule</p>"},{"location":"CS_Electives/Data_Mining/09_Rule-Based_Classifier/#likelihood-ratio-statistic","title":"Likelihood Ratio Statistic","text":"<p>Let</p> <ul> <li>\\(k\\) be number of classes</li> <li>\\(f_i\\) be observed frequency of class \\(i\\) examples, that are covered by rule</li> <li>\\(e_i\\) be expected frequency of rule that makes random predictions<ul> <li>Probability of \\(i\\) x Number of records covered by rule</li> </ul> </li> </ul> \\[ \\begin{aligned} R &amp;= 2 \\sum_{i=1}^k \\  f_i \\ \\log_2 \\left(\\frac{f_i}{e_i} \\right) \\\\ e_i &amp;= \\text{Frac}_i \\times n_i \\end{aligned} \\] <p>Higher value \\(\\implies\\) better rule</p>"},{"location":"CS_Electives/Data_Mining/09_Rule-Based_Classifier/#types-of-rule-based-classifier-algorithms","title":"Types of Rule-Based Classifier Algorithms","text":"Direct Indirect Extract rules from data directly other classification models Example - Ripper- CN2- 1R - C4.5 Rules"},{"location":"CS_Electives/Deep_Learning/","title":"Deep Learning","text":""},{"location":"CS_Electives/Deep_Learning/#references","title":"References","text":"<ul> <li> <p> Deep Learning | Dr. Tamizharasan</p> </li> <li> <p> Andrej Karpathy</p> </li> <li> <p> The spelled-out intro to neural networks and backpropagation: building micrograd | Andrej Karpathy</p> </li> <li> <p> Deep Learning - Deep Understanding | Mike x Cohen</p> </li> <li> <p> MIT 6.S191: Introduction to Deep Learning</p> </li> <li> <p> Deep Learning in Life Sciences | MIT</p> </li> <li> <p> Deep Learning for Physicists | Florian Marquardt</p> </li> <li> <p> Deep Learning Crash Course | Leo Isikdogan</p> </li> <li> <p> Intro to Deep Learning | CMU</p> </li> <li> <p> Deep Learning Systems: Algorithms and Implementation | CMU</p> </li> <li> <p> Stanford</p> </li> <li> Deep Learning Coursera | Andrew Ng</li> <li> <p> Deep Learning | Stanford CS230</p> </li> <li> <p> Deep Learning | VU University Amsterdam</p> </li> <li> <p> Maziar Raissi | Applied Deep Learning</p> </li> <li> <p> Applied Deep Learning 2023 | TU Wien</p> </li> <li> <p> Deep Learning | T\u00fcbingen Machine Learning | Andreas Geiger, 2023</p> </li> <li> <p> Deep Learning | IIT Madras</p> </li> <li> New</li> <li> <p> Old</p> <ul> <li> Part 1</li> <li> Part 2</li> </ul> </li> <li> <p> Deep Learning Concepts (Simply Explained) | Pedram Jahangiry</p> </li> <li> <p> Deep Learning for Economics (Spring 2023) | Harvard</p> </li> <li> <p> Unleashing Novel Data at Scale | Harvard</p> </li> <li> <p> Neural Networks for Machine Learning \u2014 Geoffrey Hinton, UofT</p> </li> <li> <p> CS 152: Neural Networks/Deep Learning\u2014Spring, 2021 | Neil Rhodes</p> </li> <li> <p> Deep Learning for Autonomous Vehicles | MIT</p> </li> <li> <p> Local Explanations for Deep Learning Models</p> </li> <li> <p> Deep Learning (STAT 940) , Fall 2023 | University of Waterloo</p> </li> <li> <p> Deep Learning - F2023  University of Guelph</p> </li> <li> <p> Foundations of Deep Learning | Soheil Feizi | University of Maryland</p> </li> <li> <p> Deep Unsupervised Learning | UC Berkeley</p> </li> <li> <p> Deep Learning | Se Young Yun</p> </li> <li> <p> Geometric Deep Learning | Michael Bronstein | Oxford</p> </li> <li> <p> Transformers United | Stanford</p> </li> </ul>"},{"location":"CS_Electives/Deep_Learning/01_Introduction/","title":"Deep Learning","text":"<p>Deep Learning is subset of machine learning, which involves a deep neural network. Large availability of data in present-day has led to the rise in demand for deep learning applications.</p> <p>Refer Machine Learning concepts, to understand this course well.</p>"},{"location":"CS_Electives/Deep_Learning/01_Introduction/#types","title":"Types","text":"<pre><code>flowchart TB\nDL --&gt; gm[Generative&lt;br/&gt;Models] &amp; ha[Hybrid&lt;br/&gt;Architecture] &amp; dm[Discriminative&lt;br/&gt;Models]\n\ngm --&gt; dbn[Deep&lt;br/&gt;Belief&lt;br/&gt;Networks] &amp; da[Deep&lt;br/&gt;Autoencoder] &amp; dbm[Deep&lt;br/&gt;Boltzmann&lt;br/&gt;Machine]\n\nha --&gt; dnn[Deep&lt;br/&gt;Neural&lt;br/&gt;Networks]\n\ndm --&gt; cnn[Convolutional&lt;br/&gt;Neural&lt;br/&gt;Network] &amp; dsn[Deep&lt;br/&gt;Stacking&lt;br/&gt;Networks]</code></pre>"},{"location":"CS_Electives/Deep_Learning/01_Introduction/#applications-of-dl","title":"Applications of DL","text":"<ul> <li>Object detection/counting</li> <li>Image/Video</li> <li>classification</li> <li>segmentation</li> <li>captioning</li> <li>sentence matching</li> <li>face recognition</li> <li>Natural language processing</li> <li>At the time of writing this sentence, ChatGPT\u2019s successor GPT4 has come out, and it looks pretty insane</li> </ul>"},{"location":"CS_Electives/Deep_Learning/01_Introduction/#advantages","title":"Advantages","text":"<ol> <li>Flexible</li> <li>Automatic</li> <li>Robust</li> <li>Generalizable</li> <li>Parallelizable \\(\\implies\\) Scalable</li> </ol>"},{"location":"CS_Electives/Deep_Learning/01_Introduction/#disadvantages","title":"Disadvantages","text":"<ol> <li>Low interpretability (Black box)</li> <li>Too many hyperparameters</li> <li>Tend to overfit; poor generalizability</li> <li>Require lot of data</li> <li>Computationally-expensive wrt to Resource Constraints</li> </ol>"},{"location":"CS_Electives/Deep_Learning/01_Introduction/#resource-constraints","title":"Resource Constraints","text":"<ol> <li>Processor Speed</li> <li>Memory Size</li> <li>Power Consumption</li> </ol>"},{"location":"CS_Electives/Deep_Learning/01_Introduction/#challenges","title":"Challenges","text":"<ul> <li>Difficult for generalization</li> <li>Difficult for efficient optimization</li> <li>Lack of adequate data (addressed through Transfer Learning, Shallow learning, Incremental learning)</li> <li>Data inconsistencies</li> <li>Low battery life of edge devices (h/w controlling data flow at boundary b/w 2 networks)</li> <li>Resource-constrained algorithm development issues</li> <li>Diversity in computing units</li> <li>Privacy &amp; security concerns (addressed through Encryption)</li> </ul>"},{"location":"CS_Electives/Deep_Learning/01_Introduction/#why-deep-learning","title":"Why Deep Learning?","text":"<ul> <li>Deep networks can represent complex functions with fewer parameters</li> <li>Each layer of the network learns a \u201crepresentation\u201d</li> </ul>"},{"location":"CS_Electives/Deep_Learning/01_Introduction/#image-representation","title":"Image Representation","text":"<p>Every images is a matrix of pixels, where each pixel is represented as a combination of red, green, blue; usually as a 8-bit value (0-255)</p> <p>So if the width and height of image are \\(w, h\\)</p>"},{"location":"CS_Electives/Deep_Learning/01_Introduction/#key-metrics","title":"Key Metrics","text":"<ul> <li>Accuracy</li> <li>Throughput</li> <li>Latency</li> <li>Energy efficiency</li> <li>Hardware costs</li> <li>Flexibility</li> </ul>"},{"location":"CS_Electives/Deep_Learning/01_Introduction/#popular-models-datasets","title":"Popular Models &amp; Datasets","text":"Dataset Sample Size Content Classes MNIST 50,000 Images of handwritten digits (0-9) 10 CIFAR 60,000 Airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks 10"},{"location":"CS_Electives/Deep_Learning/01_Introduction/#transfer-learning","title":"Transfer Learning","text":""},{"location":"CS_Electives/Deep_Learning/01_Introduction/#idk","title":"IDK","text":""},{"location":"CS_Electives/Deep_Learning/01_Introduction/#why-deep-learning_1","title":"Why Deep Learning?","text":"<p>Deep networks</p> <ol> <li>empirically work better for a given parameter count</li> <li>provably more efficient at representing functions that neural networks cannot actually learn (such as odd/even parity)</li> </ol>"},{"location":"CS_Electives/Deep_Learning/02_ANN/","title":"Artificial Neural Networks","text":"<p>A neural network refers to a type of hypothesis class containing multiple, parameterized differentiable functions (layers) composed together in a manner to map the input to the output</p> <p>It is made of layers of neurons, connected in a way that the input of one layer of neuron is the output of the previous layer of neurons (after activation)</p> <p>They are loosely based on how our human brain works: Biological structure -&gt; Biological function</p> <p></p> <p>You can think of a neural network as combining multiple non-linear decision surfaces into a single decision surface.</p> \\[ \\hat y = w \\times \\phi(x) \\] <p>where \\(\\phi\\) is a non-linear function</p> <p>Neural networks can be thought of \u2018learning\u2019 (and hence optimizing loss by tweaking)</p> <ul> <li>features (instead of manual feature specification)</li> <li>parameters</li> </ul>"},{"location":"CS_Electives/Deep_Learning/02_ANN/#universal-function-approximation","title":"Universal Function Approximation","text":"<p>A 2 layer ANN is capable of approximate any function over a finite subset of the input space</p> <p>Catch: The size of NN should be equal to number of datapoints</p> <p>Over-exaggerated property; same property is shared by Nearest Neighbors and splines, but no one cares</p>"},{"location":"CS_Electives/Deep_Learning/02_ANN/#hyperparameters","title":"Hyperparameters","text":"<ul> <li>Batch size</li> <li>Input size</li> <li>Output size</li> <li>No of hidden layers</li> <li>No of neurons in hidden layers</li> <li>Regularization</li> <li>Loss function</li> <li>Weight initialization technique</li> <li>Optimization</li> <li>Algorithm</li> <li>Learning rate</li> <li>No of epochs</li> </ul>"},{"location":"CS_Electives/Deep_Learning/02_ANN/#artificial-neuron","title":"Artificial Neuron","text":"<p>Most basic unit of an artificial neural network</p>"},{"location":"CS_Electives/Deep_Learning/02_ANN/#tasks","title":"Tasks","text":"<ol> <li>Receive input from other neurons and combine them together</li> <li>Perform some kind of transformation to give an output. This transformation is usually a mathematical combination of inputs and application of an activation function.</li> </ol>"},{"location":"CS_Electives/Deep_Learning/02_ANN/#visual-representation","title":"Visual representation","text":""},{"location":"CS_Electives/Deep_Learning/02_ANN/#mp-neuron","title":"MP Neuron","text":"<p>McCulloch Pitts Neuron</p> <p>Highly simplified compulational model of neuron</p> <p>\\(g\\) aggregates inputs and the function \\(f\\) and gives \\(y \\in \\{ 0, 1 \\}\\)</p> \\[ \\begin{aligned} y &amp;= f \\circ g \\ (x) \\\\ &amp;= f \\Big( g (x) \\Big) \\end{aligned} \\] \\[ y = \\begin{cases} 1, &amp; \\sum x_i \\ge \\theta \\\\ 0, &amp; \\text{otherwise} \\end{cases} \\] <ul> <li>\\(\\sum x_i\\) is the summation of boolean inputs</li> <li>\\(\\theta\\) is threshold for the neuron</li> </ul>"},{"location":"CS_Electives/Deep_Learning/02_ANN/#limitation","title":"\u274c Limitation","text":"<p>MP neuron can be used to represent linearly-separable functions</p>"},{"location":"CS_Electives/Deep_Learning/02_ANN/#perceptron","title":"Perceptron","text":"<p>MP neuron with a mechanism to learn numerical weights for inputs</p> <p>\u2705 Input is no longer limited to boolean values</p> \\[ \\begin{aligned} y &amp;= \\begin{cases} 1, &amp; \\sum w_i x_i \\ge \\theta \\\\ 0, &amp; \\text{otherwise} \\end{cases} \\\\ \\Big( x_0 &amp;= 1, w_0 = -\\theta \\Big) \\end{aligned} \\] <ul> <li>\\(w_i\\) is weights for the inputs</li> </ul>"},{"location":"CS_Electives/Deep_Learning/02_ANN/#key-terms-for-logic","title":"Key Terms for Logic","text":"<ul> <li>Pre-Activation (Aggregation)</li> <li>Activation (Decision)</li> </ul>"},{"location":"CS_Electives/Deep_Learning/02_ANN/#perceptron-learning-algorithm","title":"Perceptron Learning Algorithm","text":""},{"location":"CS_Electives/Deep_Learning/02_ANN/#perceptron-vs-sigmoidal-neuron","title":"Perceptron vs Sigmoidal Neuron","text":"Perceptron Sigmoid/Logistic Type of line Step Graph Gradual Curve Smooth Curve? \u274c \u2705 Continuous Curve? \u274c \u2705 Differentiable Curve? \u274c \u2705"},{"location":"CS_Electives/Deep_Learning/02_ANN/#general-form","title":"General Form","text":"\\[ \\begin{aligned} w_{ij}^{(l)} &amp;= \\begin{cases} l \\in [1, L] &amp; \\text{layers} \\\\ i \\in [0, d^{(l-1)}] &amp; \\text{inputs} \\\\ j \\in [1, d^{(l)}] &amp; \\text{outputs} \\end{cases} \\\\ x_{j}^{(l)} &amp;= \\sigma(s_j^{(L)}) \\\\ &amp;= \\sigma \\left( \\sum_{i=0}^{d^{(l-1)}} w_{ij}^{(l)} x_i^{(l-1)} \\right) \\end{aligned} \\]"},{"location":"CS_Electives/Deep_Learning/02_Architectures/","title":"Architectures","text":"Meaning Efficientat MajorApplication ComputationComplexity Limitation Advantage FCFully-Connected Poor scalability for large input sizesDo not capture \u201cintuitive\u201d invariances CNN(Convolutional) - Require that activations between layers occur only in \u201clocal\u201d manner- Treat hidden layers themselves as spatial images- Share weights across all spatial locations Detecting spatial pattens Images, Videos High Reduce parameter countCapture [some] \u201cnatural\u201d invariances RNN(Recurrent) Forward-feed, backward-feed, and self-loop is allowed Detecting dependent/sequential pattens Time Series ResNet(Residual Network) Time Series U-Net Basis of diffusion modelsSegmentationSuper-ResolutionDiffusion Models PINN(Physics-Informed) Lagrangian Deep Operator Fourier Neural Operator Graph Neural Networks"},{"location":"CS_Electives/Deep_Learning/03_Activation_Functions/","title":"Activation Functions","text":"Name Output \\(f(x)\\) Output Type Range Identity \\(x\\) Continuous \\([-1, 1]\\) BinaryStep \\(\\begin{cases} 0, &amp;x &lt; 0 \\\\ 1, &amp; x \\ge 0 \\end{cases}\\) Binary \\({0, 1}\\) Tariff/Tanh \\(\\tanh(x)\\) Discrete \\([-1, 1]\\) ArcTan \\(\\tan^{-1} (x)\\) Continuous \\((-\\pi/2, \\pi/2)\\) ReLU (RectifiedLinear Unit) \\(\\begin{cases} 0, &amp;x &lt; 0 \\\\ x, &amp; x \\ge 0 \\end{cases}\\) Continuous \\([0, \\infty]\\) SoftPlus(smooth alt to ReLU) \\(\\log(1+e^x)\\) Continuous \\([0, \\infty]\\) Parametric/Leaky ReLU \\(\\begin{cases} \\alpha x, &amp;x &lt; 0 \\\\ x, &amp; x \\ge 0 \\end{cases}\\) Continuous \\([-\\infty, \\infty]\\) ExponentialLinear Unit \\(\\begin{cases} \\alpha (e^x-1), &amp;x &lt; 0 \\\\ x,&amp;  x \\ge 0 \\end{cases}\\) Continuous \\([-\\infty, \\infty]\\) Logistic \\(\\dfrac{L}{1+e^{-k(x-x_0)}}\\) Binary Sigmoid/Standard Logistic/Soft Step \\(\\dfrac{1}{1+e^{-x}}\\) Binary \\([0, -1]\\) Softmax \\(\\dfrac{e^{x_i}}{\\sum_{j=1}^k e^{x_j}}\\)where \\(k=\\) no of classessuch that \\(\\dfrac{\\sum p_i}{k} = 1\\) Discrete \\([0, 1]\\) Softmax with Temperature \\(\\dfrac{e^{x_i/{\\small T}}}{\\sum_{j=1}^k e^{x_j/{\\small T}}}\\) Discrete Exposes more \u201cdark knowledge\u201d"},{"location":"CS_Electives/Deep_Learning/03_Activation_Functions/#softmax-with-temperature","title":"Softmax with temperature","text":""},{"location":"CS_Electives/Deep_Learning/03_Activation_Functions/#why-use-activation-function-for-hidden-layers","title":"Why use activation function for hidden layers?","text":"<p>Else, it would just be regular linear regression/logistic regression, so no point of hidden layers</p> <p>Not using activation function \\(\\implies\\) using identity activation function</p> <p>The only place identity activation function is acceptable is for the final output activation function in regression.</p>"},{"location":"CS_Electives/Deep_Learning/03_Activation_Functions/#linear-regression","title":"Linear Regression","text":"<pre><code>flowchart LR\na((x1)) &amp; b((x2)) --&gt;\nd((h1)) &amp; e((h2)) --&gt;\ny((\"&amp;ycirc;\"))</code></pre> \\[ \\begin{aligned} \\hat y &amp;= w_{h_1 \\hat y} h_1 + w_{h_2 \\hat y} h_2 \\\\ &amp;= w_{h_1 \\hat y} (w_{x_1 h_1} x_1 + w_{x_2 h_1} x_2) + w_{h_2 \\hat y} (w_{x_1 h_2} x_1 + w_{x_2 h_2} x_2) \\\\ &amp;= \\cdots \\\\ &amp;= w_1 x_1 + w_2 x_2 \\end{aligned} \\]"},{"location":"CS_Electives/Deep_Learning/03_Activation_Functions/#logistic-regression","title":"Logistic Regression","text":"<pre><code>flowchart LR\na((x1)) &amp; b((x2)) --&gt;\nd((h1)) &amp; e((h2)) --&gt;\ns((\"&amp;sigma;\")) --&gt;\ny((\"&amp;ycirc;\"))</code></pre> \\[ \\begin{aligned} \\hat y &amp;= \\sigma(w_{h_1 \\hat y} h_1 + w_{h_2 \\hat y} h_2) \\\\ &amp;= \\sigma(w_{h_1 \\hat y} (w_{x_1 h_1} x_1 + w_{x_2 h_1} x_2) + w_{h_2 \\hat y} (w_{x_1 h_2} x_1 + w_{x_2 h_2} x_2)) \\\\ &amp;= \\cdots \\\\ &amp;= \\sigma(w_1 x_1 + w_2 x_2) \\end{aligned} \\]"},{"location":"CS_Electives/Deep_Learning/03_Fully_Connected/","title":"Fully Connected Networks","text":""},{"location":"CS_Electives/Deep_Learning/03_Fully_Connected/#mlp","title":"MLP","text":"<p>Multi-Layer Perceptron</p> <p>Simple neural network with 3 Layers</p> <pre><code>flowchart LR\n\nx1 &amp; x2 --&gt;\nh1 &amp; h2 &amp; h3 &amp; h4 --&gt;\ny\n\nsubgraph il[Input&lt;br /&gt;Layer]\n    x1 &amp; x2\nend\n\nsubgraph hl[Hidden&lt;br /&gt;Layer]\n    h1 &amp; h2 &amp; h3 &amp; h4\nend\n\nsubgraph ol[Output&lt;br /&gt;Layer]\n    y\nend</code></pre> <p>For an input layer with \\(n\\) nodes, we will have</p> <ul> <li>1 output</li> <li>\\(2^n\\) nodes in hidden layer</li> </ul>"},{"location":"CS_Electives/Deep_Learning/03_Fully_Connected/#feed-forward","title":"Feed-Forward","text":"<p>NN (with \\(&gt; 3\\) layers) where every layer feeds forward to the next layer; backward/self-loop is not allowed</p> <p>For an input layer with \\(n\\) nodes, we will have</p> <ul> <li> \\[   hidden layers =    \\] </li> <li> <p>\\(W_i\\) is the weights to layer \\(i\\)</p> </li> </ul> \\[ \\begin{aligned} \\textcolor{hotpink}{\\text{PreActivation}_{H_1}} &amp;= b_1 + w_1 x_1 + w_2 x_2 + \\dots \\\\ \\text{Activation}_{H_1} &amp;= \\frac{1}{1 + e^{- \\textcolor{hotpink}{\\text{PreActivation}_{H_1}}}} \\end{aligned} \\]"},{"location":"CS_Electives/Deep_Learning/03_Fully_Connected/#decision-boundary","title":"Decision Boundary","text":"Hidden Layers Shape of Region 0 Open 1 Closed/Open \\(\\ge 2\\) Closed <p>As you increase the number of hidden layers, the possibility of open decision boundary decreases (which is good).</p>"},{"location":"CS_Electives/Deep_Learning/03_Sequence_Models/","title":"Sequence Models","text":"<p>Consider a phenomenon where an observation is determined by its past values, eg: any time series variable, such as weather, precipitation, etc.</p> \\[ p(x) = p(x_1) \\cdot p(x_2 | x_1) \\ldots p(x_t | x_1, \\dots, x_{t\u22121}) \\]"},{"location":"CS_Electives/Deep_Learning/03_Sequence_Models/#why-not-regular-ann","title":"Why not regular ANN?","text":"<ol> <li>ANN requires fixed number of input &amp; output neurons. </li> <li>However, with sequential data, we do not know the length of the input</li> <li>theoretically, we could convert all inputs to fixed size by padding shorter sentences, but this is infeasible</li> <li>ANN does not care about order of input</li> </ol>"},{"location":"CS_Electives/Deep_Learning/03_Sequence_Models/#memoryless-models","title":"Memoryless Models","text":""},{"location":"CS_Electives/Deep_Learning/03_Sequence_Models/#auto-regressive-model","title":"Auto-Regressive Model","text":"<p>Predict next term in a sequence from a fixed number of previous terms using \u2018delay taps\u2019</p> <p></p> \\[ p(x_t | x_1, ..., x_{t\u22121}) = p\\Big(x_t | f (x_1, ...x_{t\u22121}) \\Big) \\]"},{"location":"CS_Electives/Deep_Learning/03_Sequence_Models/#markov-assumption-based-auto-regressive-model","title":"Markov Assumption-based Auto-Regressive Model","text":"<p>Assume that only a limited past upto time period \\(\\tau\\) affects the present value</p> <p></p> \\[ \\begin{aligned} p(x_t | x_1, ...x_{t\u22121}) &amp;= p\\Big(x_t | f (x_{t\u2212\u03c4}, \\dots, x_{t\u22121}) \\Big) \\\\ \\implies \\hat x &amp;= f(x_{t-\\tau}, \\dots, x_{t-1} ) \\end{aligned} \\] <p>We predict the next step and iterate.</p>"},{"location":"CS_Electives/Deep_Learning/03_Sequence_Models/#feed-forward-network","title":"Feed-Forward Network","text":"<p>Generalized auto-regressive models with one/more layers of non-linear hidden units</p> <p></p>"},{"location":"CS_Electives/Deep_Learning/03_Sequence_Models/#memory-models","title":"Memory Models","text":"<p>Generative models</p>"},{"location":"CS_Electives/Deep_Learning/03_Sequence_Models/#linear-dynamic-systems-stochastic","title":"Linear Dynamic Systems (Stochastic)","text":"<p>Real-valued hidden state that cannot be observed directly</p> <p>Hidden state has linear dynamics with Gaussian noise and produces the observations using a linear model with Gaussian noise. However, there may also be driving inputs.</p> <p>A linearly transformed Gaussian is a Gaussian. So the distribution over the hidden state given the data so far is Gaussian. It can be computed using \u201cKalman filtering\u201d.</p> <p></p>"},{"location":"CS_Electives/Deep_Learning/03_Sequence_Models/#hidden-markov-models-stochastic","title":"Hidden Markov Models (Stochastic)","text":"<p>have a discrete one-of-N hidden state</p> <p>Transitions between states are stochastic and controlled by a transition matrix. The outputs produced by a state are stochastic. We cannot be sure which state produced a given output. So the state is \u201chidden\u201d. It is easy to represent a probability distribution across N states with N numbers.</p> <p>To predict the next output we need to infer the probability distribution over hidden states. HMMs have efficient algorithms for inference and learning.</p> <p></p>"},{"location":"CS_Electives/Deep_Learning/03_Sequence_Models/#limitations","title":"Limitations","text":"<p>At each time step it must select one of its hidden states. So with N hidden states it can only remember \\(\\log(N)\\) bits about what it generated so far.</p>"},{"location":"CS_Electives/Deep_Learning/03_Sequence_Models/#latent-variable-modelrnn-deterministic","title":"Latent Variable Model/RNN (Deterministic)","text":"<p>layered, feed-forward net with shared weights</p> <p>Next page</p>"},{"location":"CS_Electives/Deep_Learning/03_Sequence_Models/#language-modelling","title":"Language Modelling","text":"<p>Inputs are tokens, not necessarily real numbers</p> \\[ p(w_1, w_2, \\dots, w_T) = \\Pi_{t=1}^T \\ p(w_t|w_1,...,w_{t\u22121}) \\] <p>For eg: Consider the sentence <code>Statistics is fun.</code> We can model it as:</p> \\[ \\begin{aligned} p(\\text{Statistics, is, fun, . }) &amp;= p(\\text{Statistics}) \\\\ &amp; \\times p(\\text{is | Statistics})\\\\ &amp; \\times p(\\text{fun | Statistics, is}) \\\\ &amp; \\times p( . | \\text{Statistics, is, fun}) \\end{aligned} \\] \\[ \\hat p(\\text{is|Statistics}) = \\frac{n \\text{(Statistics is)}}{n \\text{(Statistics)}} \\]"},{"location":"CS_Electives/Deep_Learning/03_Sequence_Models/#applications","title":"Applications","text":"<p>Named Entity Tagging is when we identify the entities in a input sequence. For eg: Kelly worked at Google; Kelly is a person and Google is an organization.</p>"},{"location":"CS_Electives/Deep_Learning/03_Training/","title":"IDK","text":""},{"location":"CS_Electives/Deep_Learning/03_Training/#optimization","title":"Optimization","text":"<p>Refer to Optimization Algorithms</p>"},{"location":"CS_Electives/Deep_Learning/03_Training/#regularization","title":"Regularization","text":""},{"location":"CS_Electives/Deep_Learning/03_Training/#dropout","title":"Dropout","text":""},{"location":"CS_Electives/Deep_Learning/03_Training/#regular-dropout","title":"Regular Dropout","text":"<p>May lead to missing relevant information, since sequential part may involve variable-length inputs</p>"},{"location":"CS_Electives/Deep_Learning/03_Training/#variational-dropout","title":"Variational Dropout","text":"<p>IDK</p>"},{"location":"CS_Electives/Deep_Learning/03_Training/#zoneout","title":"Zoneout","text":"<p>Skip hidden state update and keep the same as previously during training</p> \\[ h_t = h_{t\u22121} \\] <p></p> <ul> <li>Robustness against skipping observations in sequence</li> <li>Robustness of state representation relative to hidden state updates</li> </ul>"},{"location":"CS_Electives/Deep_Learning/03_Training/#parameter-averaging","title":"Parameter Averaging","text":"<p>Train RNN and average weights over run</p>"},{"location":"CS_Electives/Deep_Learning/03_Training/#stochastic-weight-averaging","title":"Stochastic Weight Averaging","text":"<p>Parameter averaging + Continuously varying learning rate</p>"},{"location":"CS_Electives/Deep_Learning/03_Training/#fraternal-dropout","title":"Fraternal Dropout","text":"<p>Dropout while minimizing variation between outputs to increase robustness to parameterization</p>"},{"location":"CS_Electives/Deep_Learning/04_CNN/","title":"Convolutional Neural Networks","text":"<p>Convolutional Neural Networks is a type of architecture that exploits special properties of image data and are used in computer vision applications.</p> <p>Images are a 3-dimensional array of features: each pixel in the 2-D space contains three numbers from 0\u2013255 (inclusive) corresponding to the Red, Green and Blue.</p> <p>The first important type of layer that a CNN has is called the Convolution (Conv) layer. It uses parameter sharing and applies the same smaller set of parameters spatially across the image. </p> <p>Essentially the parameters (i.e. weights) associated to the input remain the same but the input itself is different as the layer computes the output of the neurons at different regions of the image.</p> <p>Hyperparameters of conv layers are  - Filter size - corresponds to how many input features in the width and height dimensions one neuron takes in - Stride - how many pixels we want to move (towards the right/down direction) when we apply the neuron again</p> <p>Then we have the pooling layer. The purpose of the pooling layer is to reduce the spatial size (width and height) of the layers. This reduces the number of parameters (and thus computation) required in future layers.</p> <p>We use a fully connected layers at the end of our CNNs. When we reach this stage, we can flatten the neurons into a one-dimensional array of features.</p> <p></p> <p></p> <p>We control output shape via padding, strides and channels</p> <p>Very nice Youtube explanations. Watch this!</p>"},{"location":"CS_Electives/Deep_Learning/04_CNN/#advantages","title":"Advantages","text":"<ul> <li>Universal visual feature extractor</li> <li>Can be used as building blocks in conjunction with architectures such as RNN/FCNN for complex tasks</li> </ul>"},{"location":"CS_Electives/Deep_Learning/04_CNN/#disadvantages","title":"Disadvantages","text":"<ul> <li>High computational cost</li> <li>Large data requirements</li> </ul>"},{"location":"CS_Electives/Deep_Learning/04_CNN/#variables-in-this-page","title":"Variables in this page","text":"Variable Meaning \\(I\\) Input matrix \\(i\\) Size of input matrix \\(f\\) Size of filter matrix \\(p\\) Padding applied to input matrix (default=0) \\(s\\) Stride length \\(n\\) no of filters \\(c\\) no of channels- Grayscale: 1- Color: 3 \\(b\\) Bias"},{"location":"CS_Electives/Deep_Learning/04_CNN/#principles","title":"Principles","text":"<ul> <li>Translation invariance</li> <li>Locality</li> </ul>"},{"location":"CS_Electives/Deep_Learning/04_CNN/#types-of-layers","title":"Types of Layers","text":"Convolutional Layer Pooling Layer Purpose Extracting spatially-invariant featuresControl output shape via padding, strides and channelsEdge DetectionImage Sharpening Gradually reduce spatial resolution of hidden representationsSome degree of invariance to translationImage size reduction, without much data lossImage Sharpening Operation Cross-Correlation Pooling Representation$O = $ \\(I \\star F + b\\)\\(\\sum (I \\star F + b)\\) (multiple channels) \\(O_{i, j} =\\) \\(\\sum_{k=1}^f \\sum_{l=1}^f F_{k, l} \\odot  I_{i+k, j+l} + b\\) \\(\\sum_{k=1}^f \\sum_{l=1}^f \\text{func}(F_{k, l} \\odot  I_{i+k, j+l})\\) Steps tocalculate 1. Perform padding2. Start from the left3. Place kernel filter over input matrix(if there are multiple channels, place each filters over corr matrix)4. Output value of one element = sum of products + Bias(if there are multiple channels, then sum of product of all the channels result in one single value)5. Perform stride rightward6. Repeat steps 3-5, until there are no remaining columns on the right7. Repeat steps 2-6, until there are no remaining rows on the left 1. Start from the left2. Place filter over input matrix3. Output value of one element = func(product of elements), where func = max, min, avg4. Perform stride rightward5. Repeat steps 3-5, until there are no remaining columns on the right6. Repeat steps 2-5, until there are no remaining rows on the left Defaultstride length 1 \\(f\\) Size ofoutput \\(\\dfrac{i-f \\textcolor{hotpink}{+2p}}{s} + 1\\) \\(\\dfrac{i-f}{s} + 1\\) When Applied First Only after convolutional layer CommonPaddingValue \\(f-1\\) 0 CommonStrideValue 1 1 No ofinput channels \\(c\\) 1 No of outputimages \\(n\\) 1 No ofoutput channels per output image 1 1 \\[ \\text{Total output dimension } o' = o \\times n \\times c \\]"},{"location":"CS_Electives/Deep_Learning/04_CNN/#notes","title":"Notes","text":"<ul> <li>Convolution and cross-correlation operations are slightly different, but it doesn\u2019t matter if kernel is symmetric</li> <li>Since images are of different sizes, instead of using weight matrix of fixed size, convolution is applied various times depending on size of input</li> <li>1 x 1 Convolutional Layer doesn\u2019t recognize spatial patterns, but fuses channels</li> <li>\\(\\times\\) is not multiplication; it is depth/no of activation maps</li> </ul>"},{"location":"CS_Electives/Deep_Learning/04_CNN/#example","title":"Example","text":"<p>The following shows convolution on with 0 padding and stride 1.</p> <p></p>"},{"location":"CS_Electives/Deep_Learning/04_CNN/#convolutional-layer","title":"Convolutional Layer","text":"<p>Each filter in the conversation layer produces an \u201cactivation map\u201d</p> <p></p>"},{"location":"CS_Electives/Deep_Learning/04_CNN/#padding-striding","title":"Padding &amp; Striding","text":"Padding Striding Meaning Number of extra row(s) and columns added around matrixIf \\(p =\\) odd, then pad \\(\\lceil p/2 \\rceil\\) on one side and \\(\\lfloor p/2 \\rfloor\\) on the other Step length in movement of kernel filter on input image Purpose Overcome loss of pixels, by increasing effective image size Zero padding means padding using 0s"},{"location":"CS_Electives/Deep_Learning/04_CNN/#common-filters","title":"Common Filters","text":"Application Filter Used Vertical edges detection \\(\\begin{bmatrix}1 &amp; 0 &amp; -1 \\\\ 1 &amp; 0 &amp; -1 \\\\ 1 &amp; 0 &amp; -1\\end{bmatrix}\\) Horizontal edge detection \\(\\begin{bmatrix}1 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\\\ -1 &amp; -1 &amp; -1\\end{bmatrix}\\) Blur <p>Edge-Detection example</p> <p></p> <p>Smoothing</p> <p></p>"},{"location":"CS_Electives/Deep_Learning/04_CNN/#object-detection","title":"Object Detection","text":"<p>mAP = mean Average Precision</p> <p></p> <p></p>"},{"location":"CS_Electives/Deep_Learning/04_CNN/#advanced-cnn","title":"Advanced CNN","text":"RCNN Fast-RCNN Faster RCNN Major idea Region-Based Do not recompute features for every box independently Integrate bounding box proposals in CNN predictions Steps 1. Generate category-independent region proposals (~2k)2. Compute 4096-dimensional CNN feature vector from each region proposal3. Classify regions w/ class- specific linear SVMs 1. Produce single convolutional feature map with several convolutional &amp; max-pooling layers 2. Region of interest (RoI) pooling layer extracts fixed-length feature vector from region feature map 1. Compute proposals with a deep convolutional Region Proposal Network (RPN)2. Merge RPN and Fast-RCNN into a single network Advantages SimpleImproved mAP compared to RNN Higher mAPSingle end-to-end training stageNo disk storage required Cost-free region proposals Disadvantages SlowMultistage pipelineDisk storage required for feature cachingTraining is expensive Proposals generation is computationally expensive Flowchart"},{"location":"CS_Electives/Deep_Learning/04_CNN/#yolo","title":"YOLO","text":"<p>You Only Look Once</p> <p>Single CNN</p> <p>No proposal for bounding box</p> <p>Treat this as a single regression (not classification), straight from images pixels to bounding box coordinates and class probabilities</p>"},{"location":"CS_Electives/Deep_Learning/04_CNN/#steps","title":"Steps","text":"<ol> <li>Residual block</li> <li>Input split into 7x7 grids</li> <li>Each cell trains a detector<ol> <li>Detector needs to predict object\u2019s class distributions</li> <li>detector has 2 bounding box predictor to predict bounding box and confidence scores</li> </ol> </li> <li>Generate probability for each grid having an object</li> <li>Confidence Score = probability * IOU</li> <li>Bounding box regression</li> <li>IoU (Intersection over Union)</li> <li>Non-max supression</li> </ol>"},{"location":"CS_Electives/Deep_Learning/04_CNN/#non-max-supression","title":"Non-Max Supression","text":"<pre><code>Algorithm Non-Max Supression\n    Input: A list of proposal boxes B, corresponding confidence scores S and overlap threshold N\n    Output:\n        List of filtered proposals D\n\n    select proposal with highest confidence score, remove it from B and add it to the fnal proposal list D\n    Compare IOU of this proposal with all the proposal. If IOU &gt; N, remove proposal from B\n\n    more steps are there\n</code></pre>"},{"location":"CS_Electives/Deep_Learning/04_CNN/#iou","title":"IOU","text":"<p>Intersection Over Union</p> <p>Let</p> <ul> <li>True bounding box be \\(T\\)</li> <li>Proposed bounding box be \\(P\\)</li> </ul> \\[ \\text{IOU} = \\frac{\\text{Area}(T \\cap P)}{\\text{Area}(T \\cup P)} \\]"},{"location":"CS_Electives/Deep_Learning/04_CNN/#interpretation","title":"Interpretation","text":"IOU &gt; 0.9 Excellent &gt; 0.7 Good &lt; 0.7 Poor"},{"location":"CS_Electives/Deep_Learning/04_CNN/#popular-architectures","title":"Popular Architectures","text":"Architecture Name Description LeNet-5 Recognizing handwritten digits AlexNet VGGNet DenseCap Image captioning SqueezeNet GoogLeNet Inception Modules (Network inside a network) DCNN ResNet CUImage SENet &amp; SE-ResNet"},{"location":"CS_Electives/Deep_Learning/04_CNN/#operations","title":"Operations","text":"Padding Pooling Aggregate information Striding Depth-wise/Grouped Group together channels so that groups of channels in output only depend on corresponding groups of channels in inputEnforce filter weight matrices to be block-diagonal Dilations Dilate convolution filter so that it covers more of the imageRequires padding to ensure same size as input"},{"location":"CS_Electives/Deep_Learning/04_CNN/#derivative","title":"Derivative","text":"<p>Adjoint $$ \\begin{aligned} \\bar v \\dfrac{\\partial \\text{conv}(x, W)}{\\partial x} &amp;= \\text{conv}(\\bar v, \\text{flip}(w)) \\ \\bar v \\dfrac{\\partial \\text{conv}(x, W)}{\\partial W} &amp;=  \\end{aligned} $$</p>"},{"location":"CS_Electives/Deep_Learning/04_RNN/","title":"Recurrent Neural Networks","text":"<p>A recurrent neural network (RNN) is a kind of artificial neural network mainly used in speech recognition and natural language processing (NLP).  </p> <p>A recurrent neural network looks similar to a traditional neural network except that a memory-state is added to the neurons.</p> <p></p> <p></p> <p>A RNN cell is a neural network that is used by the RNN.</p> <p>As you can see, it\u2019s the same cell repeats over time. The weights are updated as time progresses.</p>"},{"location":"CS_Electives/Deep_Learning/04_RNN/#idk","title":"IDK","text":"<p>We introduce a latent variable, that summarizes all the relevant information about the past</p> <p></p> <p></p> \\[ h_t = f(x_1, \\dots, x_{t\u22121}) = f(h_{t\u22121},x_{t\u22121}) \\]"},{"location":"CS_Electives/Deep_Learning/04_RNN/#hidden-state-update","title":"Hidden State Update","text":"\\[ h_t = \\phi \\Big(W_{hh} h_{t\u22121} + W_{hx} x_{t\u22121} + b_h \\Big) \\]"},{"location":"CS_Electives/Deep_Learning/04_RNN/#observation-update","title":"Observation Update","text":"\\[ o_t = \\phi(W_{ho} h_t + b_o) \\]"},{"location":"CS_Electives/Deep_Learning/04_RNN/#advantages","title":"Advantages","text":"<ol> <li>Require much less training data to reach the same level of performance as other models</li> <li>Improve faster than other methods with larger datasets</li> <li>Distributed hidden state allows storage of information about pass efficiently</li> <li>Non-linear dynamics allows them to update their hidden state in complicated ways</li> <li>With enough neurons &amp; time, RNNs can compute anything that can be done by a computer</li> <li>Good behaviors</li> <li>Can oscillate (good for motor control)</li> <li>Can settle to point attractors (good for retrieving memories)</li> <li>Can behave chaotically (bad for info processing)</li> </ol>"},{"location":"CS_Electives/Deep_Learning/04_RNN/#disadvantages","title":"Disadvantages","text":"<ol> <li>High training cost</li> <li>Difficulty dealing with long-range dependencies</li> </ol>"},{"location":"CS_Electives/Deep_Learning/04_RNN/#an-example-rnn-computational-graph","title":"An Example RNN Computational Graph","text":""},{"location":"CS_Electives/Deep_Learning/04_RNN/#implementing-rnn-cell","title":"Implementing RNN Cell","text":""},{"location":"CS_Electives/Deep_Learning/04_RNN/#tokenizationinput-encoding","title":"Tokenization/Input Encoding","text":"<p>Map text into sequence of IDs</p> <p></p>"},{"location":"CS_Electives/Deep_Learning/04_RNN/#granularity","title":"Granularity","text":"Granularity ID for each Limitation Character character Spellings not incorporated Word word Costly for large vocabularies Byte Pair Frequent subsequence (like syllables)"},{"location":"CS_Electives/Deep_Learning/04_RNN/#minibatch-generation","title":"Minibatch Generation","text":"Partitioning Independent samples? No need to reset hidden state? Random Pick random offestDistribute sequences @ random over mini batches \u2705 \u274c Sequential Pick random offesetDistribute sequences in sequence over mini batches \u274c \u2705(we can keep hidden state across mini batches) <p>Sequential sampling is much more accurate than random, since state is carried through</p>"},{"location":"CS_Electives/Deep_Learning/04_RNN/#hidden-state-mechanics","title":"Hidden State Mechanics","text":"<ul> <li>Input vector sequence \\(x_1, \\dots, x_t\\)</li> <li>Hidden states \\(h_1, \\dots, x_t\\), where \\(h_t = f(h_{t-1}, x_t)\\)</li> <li>Output vector sequence \\(o_1, \\dots, o_t\\), where \\(o_t = g(h_t)\\)</li> </ul> <p>Often outputs of current state are used as input for next hidden state (and thus output)</p>"},{"location":"CS_Electives/Deep_Learning/04_RNN/#output-decoding","title":"Output Decoding","text":"\\[ P(y|o) \\propto \\exp(V_y^T \\ o) = \\exp(o[y]) \\]"},{"location":"CS_Electives/Deep_Learning/04_RNN/#gradients","title":"Gradients","text":"<p>Long chain of dependencies for back-propagation</p> <p>Need to keep a lot of intermediate values in memory</p> <p>Gradients can have problems</p>"},{"location":"CS_Electives/Deep_Learning/04_RNN/#accuracy","title":"Accuracy","text":"<p>Accuracy is usually measured in terms of log-likelihood. However, this makes outputs of different length incomparable (bad model on short output has higher likelihood than excellent model on very long output).</p> <p>Hence, we normalize log-likelihood to sequence length</p> \\[ \\begin{aligned} \\pi &amp;= - \\textcolor{hotpink}{\\frac{1}{T}} \\sum_{t=1}^T \\log P(y_t|\\text{model}) \\\\ \\text{Perplexity} &amp;= \\exp(\\pi) \\end{aligned} \\] <p>Perplexity is effectively number of possible choices on average</p>"},{"location":"CS_Electives/Deep_Learning/04_RNN/#truncated-bptt","title":"Truncated BPTT","text":"<p>Back-Propagation Through Time</p> Truncation Style None CostlyDivergent Fixed-Intervals Standard ApproachApproximationWorks well Variable Length Exit after reweighingDoesn\u2019t work better in practice Random Variable <p></p>"},{"location":"CS_Electives/Deep_Learning/05_GRU/","title":"05 GRU","text":"<p>Gated Recurrent Unit</p> <p>Not all observations are equally relevant, so we need a mechanism to </p> Update gate\u00a0\\(Z\\) Pay attention Reset gate\u00a0\\(R\\) Forget"},{"location":"CS_Electives/Deep_Learning/05_GRU/#operations","title":"Operations","text":""},{"location":"CS_Electives/Deep_Learning/05_GRU/#gating","title":"Gating","text":"\\[ \\begin{aligned} R_t &amp;= \u03c3(X_t W_{xr} + H_{t\u22121} W_{hr} + b_r) \\\\ Z_t &amp;= \u03c3(X_t W_{xz} + H_{t\u22121} W_{hz} + b_z) \\\\ \\tilde H_t &amp;= \\tanh \\Big( X_t W_{xh} + (R_t \\odot H_{t\u22121}) W_{hh} + b_h \\Big) \\\\ H t &amp;= \\Big[ Z_t \\odot H_{t\u22121} \\Big] + \\Big[ (1\u2212Z_t) \\odot \\tilde H_t \\Big] \\end{aligned} \\]"},{"location":"CS_Electives/Deep_Learning/06_LSTM/","title":"06 LSTM","text":"<p>Long Short-Term Memory</p> <p>RNN only has Short-Term memory, which does not work for well long sentences, and hence for use-cases such as Grammar Checking, we prefer LSTM</p>"},{"location":"CS_Electives/Deep_Learning/06_LSTM/#gates","title":"Gates","text":"Gate Function Forget Control forgetting/retaining info currently in memory Input Control whether to add new info to memory Output Control effect of hidden state on output \\[ \\begin{aligned} It &amp;= \\sigma(X_t W_{xi} + H_{t\u22121} W_{hi} + b_i) \\\\ Ft &amp;= \\sigma(X_t W_{xf} + H_{t\u22121} W_{hf} + b_f ) \\\\ O_t &amp;= \\sigma(X_t W_{xo} + H_{t\u22121} W_{ho} + b_o) \\\\ \\tilde C_t &amp;= \\tanh(X_t W_{xc} + H_{t\u22121} W_{hc} + b_c) \\\\ C_t &amp;= F_t \\odot C_{t\u22121} + I_t \\odot \\tilde C_t \\\\ H_t &amp;= O_t \\odot \\tanh(C_t) \\end{aligned} \\] <ul> <li>\\(\\tilde C_t\\) is the candidate memory</li> <li>\\(C_t\\)\u00a0is the long-term memory</li> </ul>"},{"location":"CS_Electives/Deep_Learning/07_Advanced_RNN/","title":"07 Advanced RNN","text":""},{"location":"CS_Electives/Deep_Learning/07_Advanced_RNN/#non-linear-units","title":"Non-Linear Units","text":"<p>Replace \\(\\phi\\) of updates with MLP</p> <p>\u2705 Keeps structure of latent space</p> <p>\u274c Costly (due to more complex gradients)</p>"},{"location":"CS_Electives/Deep_Learning/07_Advanced_RNN/#deep-rnn","title":"Deep RNN","text":"<p>Rather than using just 1 hidden layer, we use more hidden layers, ie, each time stamp of RNN has multiple cells</p> <p></p> \\[ \\begin{aligned} H_t^j &amp;= f_j(H_{t-1}^j, H_t^{j\u22121}) \\\\ O_t &amp;= g(H_t^L) \\end{aligned} \\]"},{"location":"CS_Electives/Deep_Learning/07_Advanced_RNN/#bidirectional-rnn","title":"Bidirectional RNN","text":""},{"location":"CS_Electives/Deep_Learning/07_Advanced_RNN/#context","title":"Context","text":"<ul> <li>I am happy</li> <li>I am very hungry</li> <li>I am so hungry, I could eat 2 plates of rice</li> </ul> <p>Very different words to fill in, depending on past and future context of a word</p> <p>Traditional RNNs only look at the past. In interpolation (fill in) we also use the future.</p>"},{"location":"CS_Electives/Deep_Learning/07_Advanced_RNN/#implementation","title":"Implementation","text":"<ul> <li>One RNN forward</li> <li>One RNN backward</li> <li>Combine both hidden states for output generation</li> </ul>"},{"location":"CS_Electives/Deep_Learning/07_Advanced_RNN/#idk","title":"idk","text":"<p>Bi-RNN does not work for sequence generation</p> Training Testing <p>However, we can still use it to encode the sequence</p>"},{"location":"CS_Electives/Deep_Learning/07_Advanced_RNN/#residual-rnn","title":"Residual RNN","text":"<p>Input of every second layer is also added to its output (residual connection)</p> <p></p> \\[ \\bar H_t^{(2i)} = H_t^{(2i)} + H_t^{(2i)\u22121} \\]"},{"location":"CS_Electives/Deep_Learning/07_Advanced_RNN/#adding-layers","title":"Adding Layers","text":"<p>Adding a layer to a model changes function \u2028class.</p> <p></p> <p>We want to add to the function class, using Taylor expansion\u2028 style parametrization</p> \\[ f(x) = x + g(x) \\] <p></p>"},{"location":"CS_Electives/Deep_Learning/07_Advanced_RNN/#variants","title":"Variants","text":"<ul> <li>Simple addition</li> <li>Nonlinearity before addition</li> <li>Could also concatenate</li> </ul>"},{"location":"CS_Electives/Deep_Learning/07_Advanced_RNN/#densenet-rnn","title":"DenseNet RNN","text":"<p>Concatenate outputs of previous layers as input to next layer, with occasional transition layers to reduce dimensionality</p> <p></p> \\[ \\bar H_t^{(t)} = [H_t^{(t)}, \\bar H_t^{t\u22121}] \\]"},{"location":"CS_Electives/Deep_Learning/08_Encoder_Decoder/","title":"08 Encoder Decoder","text":"<ul> <li>Encoder processes inputs</li> <li>Decoder generates outputs</li> </ul>"},{"location":"CS_Electives/Deep_Learning/08_Encoder_Decoder/#seq2seq","title":"Seq2Seq","text":"<p>Used for language translation</p> <p></p>"},{"location":"CS_Electives/Deep_Learning/08_Encoder_Decoder/#encoder","title":"Encoder","text":"<p>Reads input sequence</p> <p>Standard RNN model without output layer</p> <p>Encoder\u2019s hidden state in last time step is used as the decoder\u2019s initial hidden state</p>"},{"location":"CS_Electives/Deep_Learning/08_Encoder_Decoder/#decoder","title":"Decoder","text":"<p>RNN that generates output</p> <p>Fed with the targeted sentence during training</p>"},{"location":"CS_Electives/Deep_Learning/08_Encoder_Decoder/#search-algorithms-for-picking-weights","title":"Search Algorithms for Picking Weights","text":"<p>Let</p> <ul> <li>\\(n =\\) output vocabulary size</li> <li>\\(T = L =\\) max sequence length</li> </ul> Search Algorithm Time Complexity Greedy Used in seq2seq model during predictionIt could be suboptimal \\(O(nT)\\) Exhaustive Compute probability for every possible sequencePick the best sequence \\(O(n^T)\\)\u274c computationally infeasible Beam We keep the best \\(k\\) (beam size) candidates for each timeExamine \\(kn\\) sequences by adding new item to a candidate, and then keep the top-\\(k\\) onesFinal score of each candidate\\(= \\frac{1}{L_\\alpha} \\log P(y_1, \\dots, y_L)\\)$= \\frac{1}{L_\\alpha} \\sum_{t=1}^L \\log P(y_t y_1, \\dots, y_{t-1}, c)$Often, \\(\\alpha = 0.7\\) <p></p> <p></p>"},{"location":"CS_Electives/Deep_Learning/08_Encoder_Decoder/#disadvantage","title":"Disadvantage","text":"<p>Not suitable for large sentences, since the context vector might not be able to encapsulate the effect of very much previous words.</p>"},{"location":"CS_Electives/Deep_Learning/09_Attention_Mechanism/","title":"09 Attention Mechanism","text":"<p>Key idea: Generate context as a function as all hidden states, not just the last one.</p> <p></p> <p></p>"},{"location":"CS_Electives/Deep_Learning/10_YOLO/","title":"10 YOLO","text":"<p>You only look once (YOLO) at an image to predict what objects are present and where they are.</p>"},{"location":"CS_Electives/Deep_Learning/10_YOLO/#abstract","title":"Abstract","text":"<p>Traditional object detection repurposes classifiers to perform detection; YOLO frames object detection as a regression problem to spatially separated bounding boxes and associated class probabilities</p> <p>A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance</p> <p>YOLO makes more localization errors but is less likely to predict false positives on background</p>"},{"location":"CS_Electives/Deep_Learning/10_YOLO/#working","title":"Working","text":"<p>A single convolutional network simultaneously predicts multiple bounding boxes and class probabilities for those boxes. YOLO trains on full images and directly optimizes detection performance.</p>"},{"location":"CS_Electives/Deep_Learning/10_YOLO/#idk","title":"IDK","text":"<p>YOLO sees the entire image during training and test time, so it implicitly encodes contextual information about classes as well as their appearance. </p> <p>Meanwhile, Fast R-CNN mistakes background patches in an image for objects because it can\u2019t see the larger context</p>"},{"location":"CS_Electives/Deep_Learning/10_YOLO/#advantages","title":"Advantages","text":"<ol> <li>Fast</li> <li>Reasons globally</li> <li>Better understanding of general representations of objects, compared to other models; ie, real-life, artwork, etc</li> </ol>"},{"location":"CS_Electives/Deep_Learning/10_YOLO/#disadvantages","title":"Disadvantages","text":"<ul> <li>Lower accuracy than state-of-the-art systems; [however, higher accuracy (better mAP) than many models]</li> <li>Struggles with small objects that appear in groups, such as flocks of birds</li> <li>Each grid cell only predicts two boxes and can only have one class</li> <li>This spatial constraint limits no of nearby objects that the model can predict</li> <li>Struggles to generalize to objects in unseen aspect ratios/configurations</li> <li>Loss function treats errors the same in small and large bounding boxes</li> <li>Errors due to Incorrect localizations</li> </ul>"},{"location":"CS_Electives/Deep_Learning/10_YOLO/#detection","title":"Detection","text":"<p>System divides the input image into an \\(S \\times S\\) grid. If the center of an object falls into a grid cell, that grid cell is responsible for detecting that object.</p> <p>Each grid cell predicts B bounding boxes and confidence scores for those boxes; each bounding box consists of 5 predictions: \\(x, y, w, h\\), and confidence. These confidence scores reflect how confident the model is that the box contains an object and how accurate it thinks the box is that it predicts.</p> \\[ \\text{Confidence} = P(\\text{Object}) \\times \\text{IOU} \\] <p>Each grid cell also predicts C conditional class probabilities \\(P(\\text{Class}_i|\\text{Object})\\)</p> \\[ P(\\text{Class}_i|\\text{Object}) \\times P(\\text{Object}) \\times \\text{IOU} = Pr(\\text{Class}_i) \\times \\text{IOU} \\]"},{"location":"CS_Electives/Deep_Learning/10_YOLO/#network-design","title":"Network Design","text":"<p>Architecture is inspired by the GoogLeNet model for image classification. However, YOLO does not use inception modules used by GoogLeNet</p> <p>Pretrain the convolutional layers on the ImageNet classification task at half the resolution (224 \u00d7 224 input image) and then double the resolution for detection</p> Default YOLO Fast YOLO Convolutional Layers 24 9 Convolutional Layers Size \\(3 \\times 3\\) Same Reduction Layers Size \\(1 \\times 1\\) Same Fully-Connected Layers 2 Same Reduction \\(1 \\times 1\\) Same Final Output \\(7 \\times 7 \\times 30\\) Same"},{"location":"CS_Electives/Deep_Learning/10_YOLO/#activation-function","title":"Activation Function","text":"Layer Function Final Layer Linear Other layers \\(\\phi(x) = \\begin{cases} x, &amp; x&gt;0 \\\\ 0.1x, &amp; \\text{otherwise} \\end{cases}\\)"},{"location":"CS_Electives/Deep_Learning/10_YOLO/#error-function","title":"Error Function","text":"<p>Sum-Squared error</p> <p>Easy to optimize, but not aligned with the goal of maximizing mAP</p>"},{"location":"CS_Electives/Deep_Learning/11_ResNet/","title":"Residual Network","text":"\\[ \\hat y_{t+k} = y_t + f_k(y_t) \\]"},{"location":"CS_Electives/Deep_Learning/99_Deep_Learning_Systems/","title":"Deep Learning Systems","text":"<p>Easy-to-use automatic differentiation tools (Keras, Tensorflow, PyTorch) have been the biggest cause of rapid development in DL</p> <p></p>"},{"location":"CS_Electives/Deep_Learning/99_Deep_Learning_Systems/#why-learn","title":"Why Learn?","text":"<p>Understand internals of deep learning systems helps</p> <ul> <li>build/improve deep learning systems</li> <li>so that you can use them better</li> <li>not too difficult</li> </ul>"},{"location":"CS_Electives/Deep_Learning/99_Deep_Learning_Systems/#frameworks","title":"Frameworks","text":"Paradigm PyTorch Imperative chainer Imperative Tensorflow 1.0 Declarative Theano Declarative Caffe Forward &amp; Backward layers Jax mxnet Paradigm Advantage Disadvantage Imperative Easy to debugAllow mixing of programming control flow and computational graph construction Eager execution Declarative Lazy execution Hard to debug Forward &amp; Backward layers"},{"location":"CS_Electives/DevOps/","title":"DevOps","text":""},{"location":"CS_Electives/DevOps/01_Types_of_Apps/","title":"Types of Apps","text":"Advantages Disadvantages Monolithic Straighforward Hard to scale Multi-Tier Microservices Hard to pinpoint cause of errors due to interdependency between APIs"},{"location":"CS_Electives/DevOps/01_Unit_Testing/","title":"Unit Testing","text":""},{"location":"CS_Electives/DevOps/01_Unit_Testing/#test-naming-convention","title":"Test Naming Convention","text":"<pre><code>test_component_scenario_expectedOutcome\n</code></pre> <pre><code>def test_DataFrame_WhenDataFrameIsValid_ReturnColumns():\n  pass\n</code></pre>"},{"location":"CS_Electives/DevOps/01_Unit_Testing/#fixtures","title":"Fixtures","text":"<p>Template Object</p>"},{"location":"CS_Electives/DevOps/01_Unit_Testing/#mocking","title":"Mocking","text":""},{"location":"CS_Electives/Federated_Learning/","title":"Federated Learning","text":""},{"location":"CS_Electives/Federated_Learning/#references","title":"References","text":"<ul> <li> CS-E4740 Federated Learning | Alexander Jung | Aalto University (Finland)</li> <li> Lectures</li> <li> Material</li> <li> Google Workshop on Federated Learning and Analytics</li> <li> 2020</li> <li> 2021</li> </ul>"},{"location":"CS_Electives/Fintech/","title":"Fintech","text":""},{"location":"CS_Electives/Fintech/#references","title":"References","text":"<ul> <li>MIT 15.S08 FinTech: Shaping the Financial World</li> </ul>"},{"location":"CS_Electives/Fintech/05_Blockchain/","title":"Introduction","text":""},{"location":"CS_Electives/Fintech/05_Blockchain/#traditional-banks","title":"Traditional Banks","text":"<p>Banks maintain ledger</p> <p>Ledger is a record of the </p> <p>Bank ledgers are not auditable</p> <pre><code>flowchart LR\n\nsubgraph Different Banks\ndirection LR\nA2((A)) --&gt;\nba2[[Bank A]] --&gt;\nbb2[[Bank B]] --&gt;\nB2((B))\nend\n\nsubgraph Same Banks\ndirection LR\nA1((A)) --&gt;\nba[[Bank]] --&gt;\nB1((B))\nend</code></pre>"},{"location":"CS_Electives/Fintech/05_Blockchain/#bad-incidents","title":"Bad Incidents","text":""},{"location":"CS_Electives/Fintech/05_Blockchain/#cyprus","title":"Cyprus","text":"<p>Take 30% of everyone\u2019s funds, just like that</p>"},{"location":"CS_Electives/Fintech/05_Blockchain/#india-demonetization","title":"India demonetization","text":""},{"location":"CS_Electives/Fintech/05_Blockchain/#charge-backs","title":"Charge-backs","text":"<p>Reversed transaction within 24-48hrs</p> <p>But counter-party can sue the payer</p>"},{"location":"CS_Electives/Fintech/05_Blockchain/#2008-recession","title":"2008 Recession","text":"<p>The taxpayers\u2019 money was used for bailing out banks</p>"},{"location":"CS_Electives/Fintech/05_Blockchain/#pre-requisities","title":"Pre-Requisities","text":""},{"location":"CS_Electives/Fintech/05_Blockchain/#symmetric-keys","title":"Symmetric Keys","text":""},{"location":"CS_Electives/Fintech/05_Blockchain/#asymmetric-keys","title":"Asymmetric Keys","text":"<ul> <li>Public key helps</li> <li>encrypt data</li> <li>verify signature</li> <li>Private key helps</li> <li>decrypt data</li> <li>sign messages</li> </ul>"},{"location":"CS_Electives/Fintech/05_Blockchain/#steps","title":"Steps","text":"<ol> <li>A takes B\u2019s public key</li> <li>A encrypt the file</li> <li>B receives file</li> <li>B uses B\u2019s private key to decrypt the file</li> </ol>"},{"location":"CS_Electives/Fintech/05_Blockchain/#blockchain","title":"Blockchain","text":"<p>Continuous database split into blocks</p> <p>Basically like a doubly-linked list</p>"},{"location":"CS_Electives/Fintech/05_Blockchain/#concensus","title":"Concensus","text":"<p>Permission</p> <p>Centralized party cannot edit </p>"},{"location":"CS_Electives/Fintech/05_Blockchain/#hash-function","title":"Hash Function","text":"<p>Compression function that tranforms data into a simpler form</p> <p>Helps with verification, by serving as signature to data</p> <pre><code>flowchart LR\n\ngb[Genesis Block] --&gt;\nb1[Block 1] --&gt;\nb2[Block 2]</code></pre>"},{"location":"CS_Electives/Fintech/05_Blockchain/#components","title":"Components","text":"<ol> <li>Hash of current block</li> <li>Hash of previous block (except genesis block)</li> <li>Transactions and signatures</li> <li>Sender\u2019s signature, as they are the one initiating the transaction</li> <li>Timestamp</li> <li>Proof of Work</li> </ol>"},{"location":"CS_Electives/Fintech/05_Blockchain/#wallet","title":"Wallet","text":"<p>Front-end that keeps your details and interacts with the blockchain</p>"},{"location":"CS_Electives/Fintech/05_Blockchain/#bitcoin","title":"Bitcoin","text":"<p>Peer-to-peer network</p> <p>Trillion-Dollar Market</p>"},{"location":"CS_Electives/Fintech/05_Blockchain/#advantages","title":"Advantages","text":"<ul> <li>Resistant to inflation</li> <li>Overall supply </li> <li>Resistance to fraudulent</li> <li>Pseudo-anonymous</li> <li>You can find out the </li> <li>But you can\u2019t find out who it is</li> <li>Low transaction fees</li> </ul>"},{"location":"CS_Electives/Fintech/05_Blockchain/#parts-of-blockchain","title":"Parts of Blockchain","text":"<ol> <li>Users    The ones who send/receive transactions</li> <li>Nodes    Autonoumous computers which validate and timestamp transactions</li> <li>Miners    Subset of nodes, high computational, confirm transactions    Monitor transations</li> </ol>"},{"location":"CS_Electives/Fintech/05_Blockchain/#mining","title":"Mining","text":"<p>Every transcation has a transaction fee, which depends on </p> <p>The Bitcoin protocol rewards only the fastest miner solving a \u2018hash puzzle\u2019. The difficulty gets automatically adjusted by the network algorithm.</p> <p>Winner gets a freshly-minted bitcoin and the commission fee by the payer.</p> <p>There can only be 21M bitcoins total in the total. Every year, the reward gets divided into half. Currently, the reward is \\(6.25 \\text{ BTC}\\)</p> <p>Protocol is a set of rules</p>"},{"location":"CS_Electives/Fintech/05_Blockchain/#mempool","title":"Mempool","text":"<p>Memory pool</p> <p>On every 2 weeks, the difficulty algorithm updates the difficulty level</p>"},{"location":"CS_Electives/Fintech/05_Blockchain/#limitations","title":"Limitations","text":"<ul> <li>Time-Consuming</li> <li>10 min confirmation time</li> </ul>"},{"location":"CS_Electives/Fintech/05_Blockchain/#solutions","title":"Solutions","text":""},{"location":"CS_Electives/Fintech/05_Blockchain/#increase-the-block-size","title":"Increase the block size","text":"<ul> <li>So that more transactions fit into a block</li> <li>Introduce a second layer</li> <li>But not that great</li> </ul>"},{"location":"CS_Electives/Fintech/05_Blockchain/#lightning-network","title":"Lightning Network","text":"<ul> <li>Without this, throughput is only 7</li> <li>Depends on the internet speed of the nodes</li> <li>~ 3.9M transactions per second</li> </ul> <p>You just need to record 2 transactions</p> <ul> <li>Opening channel</li> <li>Closing channel</li> </ul> <p>Only disadvantage is that it depends on liquidity (of what?) ; hence, it is only recommend only for small transactions</p> <p>Another features relayed transactions through shortest paths between payer and receiver</p>"},{"location":"CS_Electives/Fintech/05_Blockchain/#stages-of-currencydigital-currency","title":"Stages of Currency/Digital Currency","text":"<pre><code>flowchart LR\n1[Collectible] --&gt;\n2[Store of Value] --&gt;\n3[Medium of Exchange]</code></pre>"},{"location":"CS_Electives/Fintech/05_Blockchain/#economic-majority","title":"Economic Majority","text":""},{"location":"CS_Electives/Fintech/05_Blockchain/#ring-signatures","title":"Ring Signatures","text":"<p>(not relevant here)</p>"},{"location":"CS_Electives/Fintech/05_Blockchain/#softwareswebsites-used","title":"Softwares/Websites Used","text":""},{"location":"CS_Electives/Fintech/05_Blockchain/#electrum","title":"Electrum","text":"<pre><code>open /Applications/Electrum.app --args --testnet\n</code></pre>"},{"location":"CS_Electives/Fintech/05_Blockchain/#other","title":"Other","text":"<ul> <li>mempool.emzy.de</li> <li>Bitcoin Testnet Faucet</li> <li>Parallel blockchain where developers use it for testing</li> </ul>"},{"location":"CS_Electives/Generative_AI/","title":"Generative AI","text":""},{"location":"CS_Electives/Generative_AI/#references","title":"References","text":"<ul> <li> Generative Adversarial Networks (GANs) Specialization</li> <li> Deep Generative Models (Cornell Tech CS 6785, Spring 2023) | Volodymyr Kuleshov</li> <li> Deep Generative Models | Stanford</li> <li> Intro to Deep Learning and Generative Models | Sebastian Raschka</li> <li> Generative AI for Everyone | Andrew Ng | Coursera</li> </ul>"},{"location":"CS_Electives/Generative_AI/01_Introduction/","title":"Introduction","text":""},{"location":"CS_Electives/Generative_AI/01_Introduction/#generative-ai","title":"Generative AI","text":"<p>AI systems that can produce high quality unstructured content: text, images, audio</p> <p></p>"},{"location":"CS_Electives/Generative_AI/01_Introduction/#impact-on-jobs","title":"Impact on Jobs","text":"<p>More effect on</p> <ul> <li>higher-paid jobs </li> <li>knowledge workers</li> </ul> <p></p> <p></p> <p></p>"},{"location":"CS_Electives/Generative_AI/01_Introduction/#lifecycle-of-genai-project","title":"Lifecycle of GenAI Project","text":"<ol> <li>Scoping</li> <li>Build prototype</li> <li>Internal evaluation</li> <li>Improve system</li> <li>Deploy</li> <li>Monitor</li> </ol>"},{"location":"CS_Electives/Generative_AI/01_Introduction/#llms","title":"LLMs","text":"<p>Large Language Models</p> <p>Supervised learning to repeatedly predict the next word</p>"},{"location":"CS_Electives/Generative_AI/01_Introduction/#applications","title":"Applications","text":"<ul> <li>Finding new information</li> <li>Writing</li> <li>Assistant</li> <li>Translation</li> <li>Reading</li> <li>Proof reading</li> <li>Summarization</li> <li>Chatting</li> </ul> <p>Advice for chatbots: Start with internal-facing that works with staff</p>"},{"location":"CS_Electives/Generative_AI/01_Introduction/#what-an-llm-can-do","title":"What an LLM can do","text":"<p>Rule of thumb</p> <p>Whatever a fresh undergraduate can do with the given prompt and</p> <ul> <li>No internet/other resources</li> <li>No training specific to your prompt</li> <li>No memory of previous tasks</li> </ul>"},{"location":"CS_Electives/Generative_AI/01_Introduction/#prompting-tips","title":"Prompting Tips","text":"<ol> <li>Be detailed: Give LLM sufficient context &amp; information required to task at hand</li> <li>Be specific</li> <li>Guide the model to think through its answer: Suggest steps for performing task</li> <li>Experiment and Iterate</li> </ol>"},{"location":"CS_Electives/Generative_AI/01_Introduction/#objective","title":"Objective","text":"<p>Helpful, Honest, Harmless</p>"},{"location":"CS_Electives/Generative_AI/01_Introduction/#how-it-works","title":"How it works","text":"<ul> <li>Instruction tuning</li> <li>RLHF: Reinforcement Learning from Human Feedback</li> <li>Train another Supervised Learning model for answer quality rewards</li> <li>Train LLM to generate responses with high response scores</li> </ul> <p>Can be used to reduce bias</p>"},{"location":"CS_Electives/Generative_AI/01_Introduction/#tool-use","title":"Tool-Use","text":""},{"location":"CS_Electives/Generative_AI/01_Introduction/#action","title":"Action","text":""},{"location":"CS_Electives/Generative_AI/01_Introduction/#reasoning","title":"Reasoning","text":""},{"location":"CS_Electives/Generative_AI/01_Introduction/#agents","title":"Agents","text":"<ul> <li>Use LLM to close and carry out complex sequence of actions</li> <li>Not ready at the time of typing this</li> </ul>"},{"location":"CS_Electives/Generative_AI/01_Introduction/#image-generation","title":"Image Generation","text":"<p>Diffusion Model</p> <p>Noise + Prompt -&gt; Generated Image</p>"},{"location":"CS_Electives/Generative_AI/01_Introduction/#limitations","title":"Limitations","text":"<ul> <li>Knowledge cut-off</li> <li>Hallucinations: LLM can produce confident responses which are completely false</li> <li>Prompt size is limited</li> <li>Does not work with structured data</li> <li>Does not do arithmetic well</li> <li>Bias &amp; Toxicity</li> </ul>"},{"location":"CS_Electives/Generative_AI/01_Introduction/#caveats","title":"Caveats","text":"<ul> <li>Be careful with confidential information</li> <li>Double-check: LLMs do not necessarily give reliable information</li> <li>For user service, better to have confirmation dialog before performing transaction</li> </ul>"},{"location":"CS_Electives/Generative_AI/01_Introduction/#cost-of-llm","title":"Cost of LLM","text":"<p>Relatively cheap to use</p> <p>4 tokens ~ 3 words</p>"},{"location":"CS_Electives/Generative_AI/01_Introduction/#rag","title":"RAG","text":"<ol> <li>Given question, search relevant documents for answer</li> <li>Incorporate retrieved text into updated prompt</li> <li>Generate answer with new prompt with additional context</li> </ol>"},{"location":"CS_Electives/Generative_AI/01_Introduction/#fine-tuning","title":"Fine-Tuning","text":"<ol> <li>To carry out a task that isn\u2019t easy to define in a prompt</li> <li>To help LLM gain specific knowledge</li> <li>To get a smaller model to perform a task</li> </ol>"},{"location":"CS_Electives/Generative_AI/01_Introduction/#pre-training","title":"Pre-Training","text":"<ul> <li>Very costly</li> <li>Requires large amount of data</li> </ul> <p>For building a specific application, pre-training is the last resort</p>"},{"location":"CS_Electives/Generative_AI/01_Introduction/#llm-model-size","title":"LLM Model Size","text":"<p>General guidelines</p> Parameters Capability Example 1B Pattern-matchingBasic knowledge of the word Restaurant review sentiment 10B Greater world knowledgeCan follow basic instructions Food order chatbot &gt; 100B Rich world knowledgeComplex reasoning Brainstorming"},{"location":"CS_Electives/IoT/","title":"Internet of Things","text":"<p>Covers solid technical knowledge and skills to build Internet of Things (IoT) systems. IoT has evolved due to the convergence of multiple technologies - embedded systems, sensor technology, real-time data analytics, machine learning, etc. Traditional fields of embedded systems, wireless sensor networks, control systems, automation (including home and building automation), and others all contribute to enabling the IoT.</p> <p>This course comprehensively covers various technologies and tools used for enabling IoT solutions. Knowledge of various topics required for building IoT prototypes like sensors and actuators/ Communications and networking and data management is also imparted in this course. This course would also help the students understand the various IoT security challenges and solutions to address them.</p> <p>The course will also give the students exposure to how various real-world problems are being solved by IoT-based solutions (like in applications for smart cities smart farming, etc.). There would also be some hands-on sessions where students would learn how to build and program IoT systems and make end-to-end solutions for different applications.</p>"},{"location":"CS_Electives/IoT/#prerequisites","title":"Prerequisites","text":"<ul> <li> Computer Architecture</li> <li> Computer Networks</li> </ul>"},{"location":"CS_Electives/IoT/#references","title":"References","text":"<ul> <li> Internet of Things | Gary Holness | Clark University</li> <li> Introduction to Internet of Things | IIT</li> <li> Design for internet of things | IIS Bangalore</li> <li> Intro to Industry 4.0 and Industrial Internet of Things | IITK</li> <li> Advanced IOT Applications | IIS Bangalore</li> <li> IoT Summer School</li> <li> Paul McWhorter | Arduino Tutorials</li> <li> Paul McWhorter | Raspberry Pi Tutorials for Absolute Beginners</li> </ul>"},{"location":"CS_Electives/IoT/01_Introduction/","title":"Introduction","text":"<p>Network of physical objects embedded with electronics, software, sensors and network connectivity that enables these objects to collect and exchange data</p>"},{"location":"CS_Electives/IoT/01_Introduction/#applications","title":"Applications","text":"<ul> <li>Wearable tech</li> <li>Healthcare devices</li> <li>Smart appliances</li> </ul>"},{"location":"CS_Electives/IoT/01_Introduction/#outputs","title":"Outputs","text":"Digital Analog Type Binary(0/1) Continuous(Usually 0-5v or 0-3.3v) Usually mapped to digital 0-255 or 0-1023 Example On/off LED Pulse-Width Modulation- Intensity of light- Brightness of LED- Speed of Motor"},{"location":"CS_Electives/IoT/01_Introduction/#standard-controllers-for-iot","title":"Standard Controllers for IoT","text":"Arduino Node MCU Raspberry Pi Type RISC RISC CISC Size of Projects Embedded Systems Small-Size Large-Size Cost WiFi \u274cNeed external ESP8266 module \u2705 \u2705 Programming C++ C++Lua PythonJavaC++ Flash Memory 32 KB 128 MB - GPIOGeneral Purpose I/O 13 10 17 ADCMeaning 6 1 0 Operating Voltage 5 v 3.3 v 5 v Clock Speed 16 MHz 26-52 MHz 1.2 GHz Supported Wifi Bands 2.4 GHz 2.4 GHz 2.4 GHz5 GHz (RPi 4 only)"},{"location":"CS_Electives/IoT/02_Protocols/","title":"Protocols","text":"HTTP HTTPS MQTT COAP Full Form Hyper Text Transfer Protocol HTTP Secure Message Queue Telemetry Transfer Constraint Application Protocol Client-Server Protocol Methodology Document-Centric Data-Centric Architecture Request/Response Protocol Publish/Subscribe Participants - Client- Server - Publisher- Broker- Subscriber Complexity Complex Simple Data Format ASCII Binary with 2B header Message size Large Small Port # 808080 443 1883 Secure \u274c \u2705 \u2705 Methods GETPUTPOST SimpleLightweight Designed for constrained devices, low-bandwidth, high-latency, unreliable networks"},{"location":"CS_Electives/IoT/03_Cloud_Platforms/","title":"Cloud Platforms","text":""},{"location":"CS_Electives/IoT/03_Cloud_Platforms/#arduino-iot-cloud","title":"Arduino IoT Cloud","text":"<p>HTTP</p> <ul> <li>Install Arduino create agent</li> <li>Go to create.arduino.cc/iot/</li> </ul>"},{"location":"CS_Electives/IoT/03_Cloud_Platforms/#things","title":"Things","text":"<ol> <li>Create thing</li> <li>Create variables</li> <li>Select devices</li> <li>Select network</li> <li>Set timezone</li> <li>Full code editor</li> </ol>"},{"location":"CS_Electives/IoT/03_Cloud_Platforms/#dashboards","title":"Dashboards","text":"<ol> <li>Create dashboard</li> <li>Add widgets</li> </ol>"},{"location":"CS_Electives/IoT/03_Cloud_Platforms/#mobile-app","title":"Mobile App","text":"<p>Arduino IoT Cloud Remote</p>"},{"location":"CS_Electives/IoT/03_Cloud_Platforms/#uploading-code-locally","title":"Uploading code locally","text":"<p>Install <code>ArduinoIoTCloud</code> library</p>"},{"location":"CS_Electives/IoT/03_Cloud_Platforms/#blynk","title":"Blynk","text":"<p>HTTP</p> <p>blynk.cloud</p> <ol> <li>Create account</li> <li>Create new template</li> <li>Create new data stream &gt; Virtual Pin</li> <li>Digital: Input only</li> <li>Analog: Input only</li> <li>Virtual Pin: Input/Output</li> <li>Install Blynk library</li> </ol>"},{"location":"CS_Electives/IoT/03_Cloud_Platforms/#thingspeak","title":"ThingSpeak","text":"<p>Developed by MathWorks, the same company that developed Matlab</p> <p>Uses HTTP Read and Write requests</p>"},{"location":"CS_Electives/IoT/03_Cloud_Platforms/#adafruit-io","title":"AdaFruit IO","text":"<p>Uses MQTT</p>"},{"location":"CS_Electives/IoT/03_Cloud_Platforms/#subscribe","title":"Subscribe","text":"<pre><code>Adafruit_MTT_Client mqtt(&amp;client, server, port, user, key);\nAdafruit_MTT_Subscribe toggle = Adafruit_MTT_Subscribe(\n  &amp;mqtt,\n  user\"/feeds/led\"\n);\n\nvoid mqtt_connect() {\n  if(mqtt.connected()){\n    return ;\n  }\n\n  Serial.println(\"Connecting to MQTT...\");\n\n  int retries = 3, status;\n\n  while(\n    (status = mqtt.connect()) != 0\n  ) {\n    Serial.println(mqtt.connectErrorString(status));\n    Serial.println(\"Retrying after 5sec\");\n\n    delay(5000);\n    retries--;\n\n    if(retries == 0) {\n      while(1); // reset NodeMCU\n    }\n  }\n\n  Serial.println(\"MQTT connected\");\n  mqtt.subscribe(&amp;toggle);\n}\n\nvoid setup() {\n  Serial.begin(9600);\n  pinMode(D2, OUTPUT);\n  Serial.println(\"Connecting to WiFi...\");\n  WiFi.begin(ssid, pass);\n  while(WiFi.status() != WL_CONNECTED) {\n    Serial.print(\".\");\n    delay(1000);\n  }\n  Serial.println(\"WiFi connected!\")\n}\n\nvoid loop() {\n  mqtt_connect();\n  Adafruit_MTT_Subscribe *subscription;\n\n  while(subscription = mqtt.readSubscription(5000)) {\n    if(subscription == &amp;toggle) {\n      char* data = (char*) toggle.lastread;\n      Serial.println(data);\n    }\n  }\n}\n</code></pre>"},{"location":"CS_Electives/IoT/03_Cloud_Platforms/#publish","title":"Publish","text":"<pre><code>Adafruit_MTT_Client mqtt(&amp;client, server, port, user, key);\nAdafruit_MTT_Publish gauge = Adafruit_MTT_Publish(\n  &amp;mqtt,\n  user\"/feeds/sensor\"\n);\n\nint data;\n\nvoid mqtt_connect() {\n  if(mqtt.connected()){\n    return ;\n  }\n\n  Serial.println(\"Connecting to MQTT...\");\n\n  int retries = 3, status;\n\n  while(\n    (status = mqtt.connect()) != 0\n  ) {\n    Serial.println(mqtt.connectErrorString(status));\n    Serial.println(\"Retrying after 5sec\");\n\n    delay(5000);\n    retries--;\n\n    if(retries == 0) {\n      while(1); // reset NodeMCU\n    }\n  }\n\n  Serial.println(\"MQTT connected\");\n}\n\nvoid setup() {\n  Serial.begin(9600);\n  Serial.println(\"Connecting to WiFi...\");\n  WiFi.begin(ssid, pass);\n  while(WiFi.status() != WL_CONNECTED) {\n    Serial.print(\".\");\n    delay(1000);\n  }\n  Serial.println(\"WiFi connected!\")\n}\n\nvoid loop() {\n  mqtt_connect();\n\n  data = 100;\n\n  if(gauge.publish(data)) {\n    Serial.println(\"Published successful: \" + String(data));\n  } else {\n    Serial.println(\"Published failed: \" + String(data));\n  }\n  delay(5000);\n}\n</code></pre>"},{"location":"CS_Electives/IoT/03_Cloud_Platforms/#ifttt","title":"IFTTT","text":"<ul> <li>WebHooks for HTTP Requests</li> </ul>"},{"location":"CS_Electives/IoT/03_Cloud_Platforms/#aws-iot-core","title":"AWS IoT Core","text":""},{"location":"CS_Electives/IoT/04_Custom_IoT_Server_Side_Solution/","title":"Custom IoT Server-Side Solution","text":"<p>This section will cover how to make our own IoT platform, using AWS EC2 (Amazon Web Services - Elastic Compute Cloud)</p> <p>Virtual Private Server</p> <p>Warning: Be careful about billings; only create one server and only use whatever services you are sure about</p>"},{"location":"CS_Electives/IoT/04_Custom_IoT_Server_Side_Solution/#alternatives-to-aws","title":"Alternatives to AWS","text":"<ul> <li>Azure</li> <li>Google Cloud</li> <li>Digital Ocean</li> <li>Linode</li> </ul>"},{"location":"CS_Electives/IoT/04_Custom_IoT_Server_Side_Solution/#setup","title":"Setup","text":"<ul> <li>EC2 dashboard</li> <li>Change datacenter location to preferred location</li> <li> <p>Launch Instance</p> </li> <li> <p>Choose AMI (Amazon Machine Image)</p> </li> <li>Operating system</li> <li>CPU architecture</li> <li>Choose Instance Type</li> <li>hardware configurations</li> <li>Choose instance details: You may skip this</li> <li>Add storage</li> <li>Add tags: you may skip this</li> <li>Configure security group</li> <li>Add rule: SSH</li> <li>Add rule: HTTP</li> <li>Review instance launch</li> <li>Create a new key pair</li> </ul>"},{"location":"CS_Electives/IoT/04_Custom_IoT_Server_Side_Solution/#operating-system","title":"Operating System","text":"<ul> <li>Windows</li> <li>Linux (preferred)</li> <li>FOSS</li> <li>Secure</li> <li>Fast</li> </ul>"},{"location":"CS_Electives/IoT/04_Custom_IoT_Server_Side_Solution/#installations","title":"Installations","text":"<ol> <li>Select instance</li> <li>Click <code>Connect</code></li> <li>Get public IP address</li> <li>Click <code>Connect</code></li> <li>Paste the below into the terminal Terminal</li> </ol> Purpose Option Web Server Apache Server/NGINX Server API Programming Language PHP/Python Database MySQL Database Management Tool PHPMyAdmin <pre><code># update all packages\nsudo apt-get update\n\n# install apache server\nsudo apt-get install apache2\n\n# start apache server\nsudo service apache2 start\n\n# install php\nsudo apt-get install php-dev libmcrypt-dev gcc make autoconf libc-dev pkg-config\n\n# restart apache server\nsudo service apache2 restart\n\n# change directory\ncd /etc/apache2/\n\n# instal mysql server\nsudo apt-get install mysql-server\n\n# Setup MySQL security\nsudo mysql_secure_installation\n# Press N for validate password component\n# Create password\n# Press Y for remaining questions\n\n# install phpmyadmin\nsudo apt-get install phpmyadmin\n# click yes for the questions\n\n# link phpmyadmin to mysql\nsudo ln -s /etc/phpmyadmin/apache.conf /etc/apache2/conf-available/phpmyadmin.conf\nsudo a2enconf phpmyadmin.conf\nsudo service apache2 reload\nsudo systemctl restart apache2\nsudo chmod -R 777 /var/www/html\n\n# login to mysql\nsudo mysql -uroot -p\n\n# create admin user with password for phpmyadmin\nCREATE USER 'admin'@'localhost' IDENTIFIED BY 'give_good_password';\nGRANT ALL PRIVILEGES ON *.* TO 'admin'@'localhost';\n\n# exit\nexit\n</code></pre>"},{"location":"CS_Electives/IoT/04_Custom_IoT_Server_Side_Solution/#phpmyadmin","title":"phpmyadmin","text":"<p>Verify phpmyadmin</p> <ol> <li>Go to <code>http://public_ip/phpmyadmin</code></li> <li>Put username and password from what was inputted for phpmyadmin in terminal</li> </ol> <p>IDK</p> <ol> <li>Create database</li> <li>Create table</li> <li>Create columns</li> </ol>"},{"location":"CS_Electives/IoT/04_Custom_IoT_Server_Side_Solution/#api","title":"API","text":""},{"location":"CS_Electives/IoT/04_Custom_IoT_Server_Side_Solution/#basicphp","title":"<code>basic.php</code>","text":"<pre><code>&lt;?php\n\n$host   = \"localhost\";\n$user   = \"admin\";\n$pass   = \"password\";\n$db     = \"iot\";\n\n// connect to mysql\n$con = mysqli_connect(\n    $host,\n  $user,\n  $pass,\n  $db\n);\n\nif ($con) {\n  echo \"Connection successful!\";\n} else {\n  echo \"Connection failed!\";\n}\n\n?&gt;\n</code></pre>"},{"location":"CS_Electives/IoT/04_Custom_IoT_Server_Side_Solution/#insertphp","title":"<code>insert.php</code>","text":"<p><code>http://.../insert.php?query_param=100&amp;query_param_2=200</code></p> <pre><code>&lt;?php\ndata_default_timezone_set(\"Asia/Kolkata\");\n\n$host   = \"localhost\";\n$user   = \"admin\";\n$pass   = \"password\";\n$db     = \"iot\";\n\n// connect to mysql\n$con = mysqli_connect(\n    $host,\n  $user,\n  $pass,\n  $db\n);\n\nif ($con) {\n\n  $query_param = $_GET[\"query_param\"];\n\n  if ($query_param) {\n    // to ensure empty values not inserted\n\n    $date = date(\"Y-m-d\");\n    $time = date(\"H:i:s\");\n\n    $query = \"\n    insert into table_name\n    (date, time, data)\n    values(\n    '$date', '$time', $data\n    );  \n    \";\n\n    if (mysqli.query($con, $sql)) {\n      echo \"Data inserted!\";\n    } else {\n      echo \"Insert failed!\";\n    }\n  } else {\n    echo \"Missing query parameter (s)\";\n  }\n\n} else {\n  echo \"Connection failed!\";\n}\n\n?&gt;\n</code></pre>"},{"location":"CS_Electives/IoT/04_Custom_IoT_Server_Side_Solution/#get_readingsphp","title":"<code>get_readings.php</code>","text":"<p><code>http://.../get_readings.php</code></p> <pre><code>&lt;html&gt;\n&lt;head&gt;\n    &lt;meta http-equiv=\"refresh\" content=\"5\"&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;table&gt;\n&lt;tbody&gt;\n\n&lt;?php\n$host   = \"localhost\";\n$user   = \"admin\";\n$pass   = \"password\";\n$db     = \"iot\";\n\n// connect to mysql\n$con = mysqli_connect(\n    $host,\n  $user,\n  $pass,\n  $db\n);\n\nif ($con) {\n\n  $query = \"\n  select * from table_name\n  order by id desc\n  limit 100\n  \";\n\n  $query_result = mysqli.query($con, $sql)\n\n  if ($query_result) {\n\n    while (\n      $row = mysqli_fetch_array($query_result)\n    ){\n      // print_r($row);\n      echo \"\n      &lt;tr&gt;\n      &lt;td&gt;$row['data']&lt;/td&gt;\n      &lt;/tr&gt;\n      \"\n    }\n\n  } else {\n    echo \"Query failed!\";\n  }\n\n} else {\n  echo \"Connection failed!\";\n}\n\n?&gt;\n&lt;/tbody&gt;\n&lt;/table&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"CS_Electives/IoT/04_Custom_IoT_Server_Side_Solution/#set_statusphp","title":"<code>set_status.php</code>","text":"<pre><code>&lt;html&gt;\n&lt;title&gt;Cloud Server Controlled LED&lt;/title&gt;\n&lt;body&gt;\n&lt;center&gt;\n&lt;h2 style='margin-top:50px;color:#123456;'&gt;Cloud Server Controlled LED&lt;/h2&gt;\n&lt;a href='?status=on'&gt;\n    &lt;button style='background-color:green;\n      font-size:20px;color:white;margin:10px;padding:5px;'&gt;\n      &lt;b&gt;LED ON&lt;/b&gt;\n    &lt;/button&gt;\n&lt;/a&gt;\n&lt;a href='?status=off'&gt;\n    &lt;button style='background-color:red;font-size:20px;color:white;\n    margin:10px;padding:5px;'&gt;\n    &lt;b&gt;LED OFF&lt;/b&gt;\n    &lt;/button&gt;\n&lt;/a&gt;\n&lt;/center&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\n&lt;?php\nif(isset($_GET['status'])){\n    date_default_timezone_set(\"Asia/Kolkata\");\n\n  $host = \"localhost\";\n    $user = \"iot_user\";\n    $pass = \"iot@1122\";\n    $db = \"iot\";\n\n  $con = mysqli_connect($host,$user,$pass,$db);\n\n  $date = date(\"d-m-Y\");    // 06-01-2022\n    $time = date(\"H:i:s\");\n    $status = $_GET['status'];\n    $query = \"\n    insert into led\n    (date,time,status)\n    values('$date','$time','$status')\n    \";\n\n  mysqli_query($con, $query);\n}\n\n?&gt;\n</code></pre>"},{"location":"CS_Electives/IoT/04_Custom_IoT_Server_Side_Solution/#get_statusphp","title":"<code>get_status.php</code>","text":"<pre><code>&lt;?php\n\n$host = \"localhost\";\n$user = \"iot_user\";\n$pass = \"iot@1122\";\n$db = \"iot\";\n\n$con = mysqli_connect($host,$user,$pass,$db);\n\n$query = \"\nselect *\nfrom table_name\norder by id desc\nlimit 1\n\";\n\n$query_result = mysqli_query($con, $query);\n\nwhile(\n  $row = mysqli_fetch_array($result)\n) {\n  echo $status;\n\n  break; // only one row any ways\n}\n\n?&gt;\n</code></pre>"},{"location":"CS_Electives/IoT/04_Custom_IoT_Server_Side_Solution/#uploading-api-files","title":"Uploading API files","text":"<p>Using: Filezilla</p> <ul> <li>General</li> <li>File &gt; Site Manager</li> <li>New Site<ul> <li>Protocol: SFTP</li> <li>Host: ip of the remote machine</li> <li>Logon type: Key file</li> </ul> </li> <li>Advanced</li> <li>Default remote directory: <code>/var/www/html</code></li> <li>Upload php files</li> </ul>"},{"location":"CS_Electives/IoT/Devices/","title":"Devices","text":""},{"location":"CS_Electives/IoT/Devices/#sensors-input","title":"Sensors (Input)","text":"<p>Devices that detects the state of a physical environment, and quantitatively provides a corresponding output as an electrical/optical signal.</p>"},{"location":"CS_Electives/IoT/Devices/#sensor-fusion","title":"Sensor Fusion","text":"<p>Combining measurements of the same quantity from multiple sensors, to obtain a combined information with lower uncertainty than any of the individual sensors. Using multiple sensors for the quantity also allows us to verify each sensor wrt others.</p> <p>If we have \\(s\\) sensors, $$ \\begin{aligned} \\mu_\\text{S} &amp;= \\left( \\sum \\limits_s^S \\dfrac{\\mu_s}{\\sigma^2_s} \\right) \\sigma^2_{S} \\ \\sigma^2_\\text{S} &amp;= \\dfrac{1}{\\sum \\limits_s^S \\dfrac{1}{\\sigma^2_s} } \\end{aligned} $$ where \\(S\\)\u00a0refers to the combination of all the sensors</p>"},{"location":"CS_Electives/IoT/Devices/#effectors-output","title":"Effectors (Output)","text":"<p>Devices that perform some action such as emitting light, sound, motor, etc</p>"},{"location":"CS_Electives/IoT/Devices/Color_Sensor/","title":"Color Sensor","text":"Sensor IR Filter Accuracy ISL29125 \u2705 Best TCS34725 \u2705 TCS3414 \u2705 TCS3200 \u274c TCS230 \u274c"},{"location":"CS_Electives/IoT/Devices/Color_Sensor/#ts230","title":"TS230","text":""},{"location":"CS_Electives/IoT/Devices/Color_Sensor/#calibration","title":"Calibration","text":"<pre><code>/*\n  Color Sensor Calibration\n  color-sensor-calib.ino\n  Calibrate RGB Color Sensor output Pulse Widths\n  Uses values obtained for RGB Sensor Demo sketch \n\n  DroneBot Workshop 2020\n  https://dronebotworkshop.com\n*/\n\n// Define color sensor pins\n\n#define S0 4\n#define S1 5\n#define S2 6\n#define S3 7\n#define sensorOut 8\n\n// Variables for Color Pulse Width Measurements\n\nint redPW = 0;\nint greenPW = 0;\nint bluePW = 0;\n\nvoid setup() {\n\n  // Set S0 - S3 as outputs\n  pinMode(S0, OUTPUT);\n  pinMode(S1, OUTPUT);\n  pinMode(S2, OUTPUT);\n  pinMode(S3, OUTPUT);\n\n  // Set Sensor output as input\n  pinMode(sensorOut, INPUT);\n\n  // Set Pulse Width scaling to 20%\n  digitalWrite(S0,HIGH);\n  digitalWrite(S1,LOW);\n\n  // Setup Serial Monitor\n  Serial.begin(9600);\n}\n\nvoid loop() {\n\n  // Read Red Pulse Width\n  redPW = getRedPW();\n  // Delay to stabilize sensor\n  delay(200);\n\n  // Read Green Pulse Width\n  greenPW = getGreenPW();\n  // Delay to stabilize sensor\n  delay(200);\n\n  // Read Blue Pulse Width\n  bluePW = getBluePW();\n  // Delay to stabilize sensor\n  delay(200);\n\n  // Print output to Serial Monitor\n  Serial.println(\n    \"RGB(\" +\n    String(redPW) + \",\" +\n    String(greenPW) + \",\" +\n    String(bluePW) +\n    \")\"\n  );\n}\n\n\n// Function to read Red Pulse Widths\nint getRedPW() {\n\n  // Set sensor to read Red only\n  digitalWrite(S2,LOW);\n  digitalWrite(S3,LOW);\n  // Define integer to represent Pulse Width\n  int PW;\n  // Read the output Pulse Width\n  PW = pulseIn(sensorOut, LOW);\n  // Return the value\n  return PW;\n\n}\n\n// Function to read Green Pulse Widths\nint getGreenPW() {\n\n  // Set sensor to read Green only\n  digitalWrite(S2,HIGH);\n  digitalWrite(S3,HIGH);\n  // Define integer to represent Pulse Width\n  int PW;\n  // Read the output Pulse Width\n  PW = pulseIn(sensorOut, LOW);\n  // Return the value\n  return PW;\n\n}\n\n// Function to read Blue Pulse Widths\nint getBluePW() {\n\n  // Set sensor to read Blue only\n  digitalWrite(S2,LOW);\n  digitalWrite(S3,HIGH);\n  // Define integer to represent Pulse Width\n  int PW;\n  // Read the output Pulse Width\n  PW = pulseIn(sensorOut, LOW);\n  // Return the value\n  return PW;\n\n}\n</code></pre>"},{"location":"CS_Electives/IoT/Devices/Color_Sensor/#get-values","title":"Get Values","text":"<pre><code>/*\n  RGB Color Sensor Demonstration\n  rgb-color-sensor-demo.ino\n  Read RGB values from Color Sensor\n  Must use calibration values from Color Sensor Calibration Sketch\n\n  DroneBot Workshop 2020\n  https://dronebotworkshop.com\n*/\n\n// Define color sensor pins\n\n#define S0 4\n#define S1 5\n#define S2 6\n#define S3 7\n#define sensorOut 8\n\n// Calibration Values\n// Get these from Calibration Sketch\n\nint redMin = 28; // Red minimum value pulse width from calibration\nint redMax = 204; // Red maximum value pulse width from calibration\nint greenMin = 30; // Green minimum value pulse width from calibration\nint greenMax = 242; // Green maximum value pulse width from calibration\nint blueMin = 26; // Blue minimum value pulse width from calibration\nint blueMax = 220; // Blue maximum value pulse width from calibration\n\n// Variables for Color Pulse Width Measurements\n\nint redPW = 0;\nint greenPW = 0;\nint bluePW = 0;\n\n// Variables for final Color values\n\nint redValue;\nint greenValue;\nint blueValue;\n\nvoid setup() {\n\n  // Set S0 - S3 as outputs\n  pinMode(S0, OUTPUT);\n  pinMode(S1, OUTPUT);\n  pinMode(S2, OUTPUT);\n  pinMode(S3, OUTPUT);\n\n  // Set Sensor output as input\n  pinMode(sensorOut, INPUT);\n\n  // Set Frequency scaling to 20%\n  digitalWrite(S0,HIGH);\n  digitalWrite(S1,LOW);\n\n  // Setup Serial Monitor\n  Serial.begin(9600);\n}\n\nvoid loop() {\n\n  // Read Red value\n  redPW = getRedPW();\n  // Map to value from 0-255\n  redValue = map(redPW, redMin,redMax,255,0);\n  // Delay to stabilize sensor\n  delay(200);\n\n  // Read Green value\n  greenPW = getGreenPW();\n  // Map to value from 0-255\n  greenValue = map(greenPW, greenMin,greenMax,255,0);\n  // Delay to stabilize sensor\n  delay(200);\n\n  // Read Blue value\n  bluePW = getBluePW();\n  // Map to value from 0-255\n  blueValue = map(bluePW, blueMin,blueMax,255,0);\n  // Delay to stabilize sensor\n  delay(200);\n\n  // Print output to Serial Monitor\n  Serial.println(\n    \"RGB(\" +\n    String(redValue) + \",\" +\n    String(greenValue) + \",\" +\n    String(blueValue) +\n    \")\"\n  );\n\n}\n\n\n// Function to read Red Pulse Widths\nint getRedPW() {\n\n  // Set sensor to read Red only\n  digitalWrite(S2,LOW);\n  digitalWrite(S3,LOW);\n  // Define integer to represent Pulse Width\n  int PW;\n  // Read the output Pulse Width\n  PW = pulseIn(sensorOut, LOW);\n  // Return the value\n  return PW;\n\n}\n\n// Function to read Green Pulse Widths\nint getGreenPW() {\n\n  // Set sensor to read Green only\n  digitalWrite(S2,HIGH);\n  digitalWrite(S3,HIGH);\n  // Define integer to represent Pulse Width\n  int PW;\n  // Read the output Pulse Width\n  PW = pulseIn(sensorOut, LOW);\n  // Return the value\n  return PW;\n\n}\n\n// Function to read Blue Pulse Widths\nint getBluePW() {\n\n  // Set sensor to read Blue only\n  digitalWrite(S2,LOW);\n  digitalWrite(S3,HIGH);\n  // Define integer to represent Pulse Width\n  int PW;\n  // Read the output Pulse Width\n  PW = pulseIn(sensorOut, LOW);\n  // Return the value\n  return PW;\n\n}\n</code></pre>"},{"location":"CS_Electives/IoT/Devices/Color_Sensor/#isl29125","title":"ISL29125","text":""},{"location":"CS_Electives/IoT/Devices/Color_Sensor/#calibration_1","title":"Calibration","text":"<pre><code>/******************************************************************************\nISL29125_basics.ino\nSimple example for using the ISL29125 RGB sensor library.\nJordan McConnell @ SparkFun Electronics\n11 Apr 2014\nhttps://github.com/sparkfun/ISL29125_Breakout\n\nThis example declares an SFE_ISL29125 object called RGB_sensor. The \nobject/sensor is initialized with a basic configuration so that it continuously\nsamples the light intensity of red, green and blue spectrums. These values are\nread from the sensor every 2 seconds and printed to the Serial monitor.\n\nDeveloped/Tested with:\nArduino Uno\nArduino IDE 1.0.5\n\nRequires:\nSFE_ISL29125_Library\n\nThis code is beerware.\nDistributed as-is; no warranty is given. \n******************************************************************************/\n\n#include &lt;Wire.h&gt;\n#include \"SFE_ISL29125.h\"\n\n// Declare sensor object\nSFE_ISL29125 RGB_sensor;\n\nvoid setup()\n{\n  // Initialize serial communication\n  Serial.begin(115200);\n\n  // Initialize the ISL29125 with simple configuration so it starts sampling\n  if (RGB_sensor.init())\n  {\n    Serial.println(\"Sensor Initialization Successful\\n\\r\");\n  }\n}\n\n// Read sensor values for each color and print them to serial monitor\nvoid loop()\n{\n  // Read sensor values (16 bit integers)\n  unsigned int red = RGB_sensor.readRed();\n  unsigned int green = RGB_sensor.readGreen();\n  unsigned int blue = RGB_sensor.readBlue();\n\n  // Print out readings, change HEX to DEC if you prefer decimal output\n  Serial.print(\"Red: \"); Serial.println(red,DEC);\n  Serial.print(\"Green: \"); Serial.println(green,DEC);\n  Serial.print(\"Blue: \"); Serial.println(blue,DEC);\n  Serial.println();\n  delay(2000);\n}\n</code></pre>"},{"location":"CS_Electives/IoT/Devices/Color_Sensor/#get-values_1","title":"Get Values","text":"<pre><code>/*\n  ISL29125 RGB sensor test\n  isl29125-test.ino\n  Displays RGB values for ISL29125 RGB sensor\n  Uses values obtained with Sparkfun ISL29125 RGB sensor basic demo\n  Uses Sparkfun ISL29125 Library\n\n  DroneBot Workshop 2020\n  https://dronebotworkshop.com\n*/\n\n// Include I2C Library\n#include &lt;Wire.h&gt;\n\n// Include Sparkfun ISL29125 Library\n#include \"SFE_ISL29125.h\"\n\n// Declare sensor object\nSFE_ISL29125 RGB_sensor;\n\n// Calibration values\n\nunsigned int redlow = 0;\nunsigned int redhigh = 0;\nunsigned int greenlow = 0;\nunsigned int greenhigh = 0;\nunsigned int bluelow = 0;\nunsigned int bluehigh = 0;\n\n// Declare RGB Values\nint redVal = 0;\nint greenVal = 0;\nint blueVal = 0;\n\n\nvoid setup()\n{\n  // Initialize serial communication\n  Serial.begin(115200);\n\n  // Initialize the ISL29125 with simple configuration so it starts sampling\n  if (RGB_sensor.init())\n  {\n    Serial.println(\"Sensor Initialization Successful\\n\\r\");\n  }\n}\n\n\nvoid loop()\n{\n  // Read sensor values (16 bit integers)\n  unsigned int red = RGB_sensor.readRed();\n  unsigned int green = RGB_sensor.readGreen();\n  unsigned int blue = RGB_sensor.readBlue();\n\n  // Convert to RGB values\n  int redV = map(red, redlow, redhigh, 0, 255);\n  int greenV = map(green, greenlow, greenhigh, 0, 255);\n  int blueV = map(blue, bluelow, bluehigh, 0, 255);\n\n  // Constrain to values of 0-255\n  redVal = constrain(redV, 0, 255);\n  greenVal = constrain(greenV, 0, 255);\n  blueVal = constrain(blueV, 0, 255);\n\n  Serial.print(\"Red: \"); \n  Serial.print(redVal);\n  Serial.print(\" - Green: \");\n  Serial.print(greenVal);\n  Serial.print(\" - Blue: \"); \n  Serial.println(blueVal);\n\n  // Delay for sensor to stabilize\n  delay(2000);\n}\n</code></pre>"},{"location":"CS_Electives/IoT/Devices/Color_Sensor/#references","title":"References","text":"<ul> <li>Arduino Color Sensing - TCS230 &amp; ISL29125</li> <li>#322 12 Light Sensors Tested: Measuring Light with Microcontrollers Arduino or ESP8266, ESP32 - YouTube </li> </ul>"},{"location":"CS_Electives/IoT/Devices/DHT/","title":"Digital Humidity and Temperature","text":"<p>Measure humidity and temperature</p> <p>2 variants</p> DHT 11 DHT 22 Temperature Range 0 - 50C -40 - 125C Temperature Accuracy \u00b1 2C \u00b1 0.5C Humidity Range 20-80% 0-100% Humidity Accuracy \u00b1 5% \u00b1 2-5% Sampling Rate(readings per second) 1Hz 0.5Hz Body Size 15.5 x 12 x 5.5 mm 15.1 x 25 x 7.7 mm Operating Voltage 3-5V 3-5V Max Current during measurement 2.5mA 2.5mA"},{"location":"CS_Electives/IoT/Devices/DHT/#working","title":"Working","text":"\\[ \\text{HH} \\ \\text{LH} \\ \\text{HT} \\ \\text{LT} \\ \\text{CP} \\] <p>where</p> <ul> <li>\\(HH=\\) High Humidity -&gt; Humidity Reading in %</li> <li>\\(LH=\\) Low Humidity</li> <li>\\(HT=\\) High Temperature -&gt; Temperature Reading</li> <li>\\(LT=\\) Low Temperature</li> <li>\\(CP=\\) Checksum Parity</li> </ul>"},{"location":"CS_Electives/IoT/Devices/DHT/#code","title":"Code","text":"<pre><code>#include &lt;DHT.h&gt; // not in-built\n\nDHT dht(pin_name, type_of_sensor);\n\ndht.begin();\n\ndht.readTemperature(); // returns Temperature in C\ndht.readTemperature(True); // returns Temperature in F\n// returns nan for invalid value \n\ndht.readHumidity() // returns Humidity %\n// returns nan for invalid value\n</code></pre> <pre><code>#include &lt;DHT.h&gt;\n\nDHT dht(D1, DHT11);\nfloat hum, temp;\n\nvoid setup() {\n  dht.begin();\n  Serial.begin(9600);\n}\nvoid loop() {\n  hum = dht.readHumidity();\n  temp = dht.readTemperature();\n\n  Serial.println(\n    String(hum) + \" \" + String(temp)\n  );\n\n  delay(5000);\n}\n</code></pre>"},{"location":"CS_Electives/IoT/Devices/DHT/#dependencies","title":"Dependencies","text":"<ul> <li>AdaFruit Unified Sensor</li> <li>DHT_Sensor</li> <li>Time</li> <li>TinyGSM</li> </ul>"},{"location":"CS_Electives/IoT/Devices/DHT/#sensor","title":"Sensor","text":"<p>3 pins</p> <ul> <li>VCC</li> <li>GND</li> <li>DOUT/Data/Signal</li> </ul>"},{"location":"CS_Electives/IoT/Devices/GPS/","title":"GPS","text":"<p>Receiver devices continuously receive signals from satellites and help calculate distance between receiver devices and network of satellites. The distance estimated with the help of 4/more satellites present in outer space help locate the exact position of the object.</p>"},{"location":"CS_Electives/IoT/Devices/IC/","title":"Integrated Chips","text":""},{"location":"CS_Electives/IoT/Devices/IC/#comparator-ic","title":"Comparator IC","text":"<p>Compares 2 voltages/currents and switch at the output to indicate which is larger</p> <p>It is an op-amp (operation amplifier)</p>"},{"location":"CS_Electives/IoT/Devices/IC/#lm358","title":"LM358","text":"<p>8-pins</p> <ul> <li>2 x 2 inputs</li> <li>2 outputs</li> <li>Ground</li> <li>VCC</li> </ul> <p></p>"},{"location":"CS_Electives/IoT/Devices/IMU/","title":"IMU","text":"<p>Inertial Measurement Unit</p> <p>Measure velocity, orientation, and gravitational forces together.</p>"},{"location":"CS_Electives/IoT/Devices/IMU/#components","title":"Components","text":"Component Detect Accelerometer Accelerations in \\(X, Y, Z\\) directions, using static &amp; dynamic forces Gyroscope Angular momentum orientation Magnetic Compass Direction"},{"location":"CS_Electives/IoT/Devices/IR/","title":"Infrared Sensor","text":"<p>Working principle: Reflection of light</p>"},{"location":"CS_Electives/IoT/Devices/IR/#uses","title":"Uses","text":"<ul> <li>Obstacle detection</li> <li>Differentiate between colors </li> </ul>"},{"location":"CS_Electives/IoT/Devices/IR/#components","title":"Components","text":"<ul> <li>Transmitter: IR LED</li> <li>Receiver: Photodiode (Reverse LED)</li> <li>Comparator IC for voltage comparison</li> <li>Potentiometer to set sensitivity of sensor, by controlling voltage threshold for comparator</li> <li>\\(V_s \\ne \\{0, V_{cc} \\}\\)</li> </ul>"},{"location":"CS_Electives/IoT/Devices/IR/#working-steps","title":"Working steps","text":"<ol> <li>Transmitter emits IR rays</li> <li>Light gets reflected by obstacle</li> <li>Receiver gets the reflected light</li> <li>Received light converted into voltage</li> </ol>"},{"location":"CS_Electives/IoT/Devices/IR/#limitations","title":"Limitations","text":"<ul> <li>Cannot obtain position of obstacle; only for Object-Detection (binary)</li> <li>Obstacle detection only works for light-colored obstacle</li> <li>dark colored objects will absorb light</li> </ul>"},{"location":"CS_Electives/IoT/Devices/IR/#circuit-diagram","title":"Circuit Diagram","text":""},{"location":"CS_Electives/IoT/Devices/IR/#code","title":"Code","text":"<pre><code>int objected_detected\n\nvoid setup(){\n  pinMode(D1, INPUT);       // sensor\n  pinMode(D2, OUTPUT);  // output device (LED)\n\n  Serial.begin(9600);\n}\nvoid loop(){\n  objected_detected = digitalRead(D1);\n\n  Serial.println(objected_detected);\n\n    if (objected_detected == 1) {\n    digitalWrite(D2, HIGH);\n  } else {\n    digitalWrite(D2, LOW);\n  }\n\n  delay(500);\n}\n</code></pre>"},{"location":"CS_Electives/IoT/Devices/LCD_Display/","title":"LCD Display","text":"<pre><code>/*\n  LiquidCrystal Library - Hello World\n\n Demonstrates the use a 16x2 LCD display.  The LiquidCrystal\n library works with all LCD displays that are compatible with the\n Hitachi HD44780 driver. There are many of them out there, and you\n can usually tell them by the 16-pin interface.\n\n This sketch prints \"Hello World!\" to the LCD\n and shows the time.\n\n  The circuit:\n * LCD RS pin to digital pin 12\n * LCD Enable pin to digital pin 11\n * LCD D4 pin to digital pin 5\n * LCD D5 pin to digital pin 4\n * LCD D6 pin to digital pin 3\n * LCD D7 pin to digital pin 2\n * LCD R/W pin to ground\n * LCD VSS pin to ground\n * LCD VCC pin to 5V\n * 10K resistor:\n * ends to +5V and ground\n * wiper to LCD VO pin (pin 3)\n\n This example code is in the public domain.\n\n https://docs.arduino.cc/learn/electronics/lcd-displays\n\n*/\n\n// include the library code:\n#include &lt;LiquidCrystal.h&gt;\n\n// initialize the library by associating any needed LCD interface pin\n// with the arduino pin number it is connected to\nconst int rs = 12, en = 11, d4 = 5, d5 = 4, d6 = 3, d7 = 2;\nLiquidCrystal lcd(rs, en, d4, d5, d6, d7);\n\nvoid setup() {\n  // set up the LCD's number of columns and rows:\n  lcd.begin(16, 2);\n  // Print a message to the LCD.\n  lcd.print(\"hello, world!\");\n}\n\nvoid loop() {\n  // set the cursor to column 0, line 1\n  // (note: line 1 is the second row, since counting begins with 0):\n  lcd.setCursor(0, 1);\n  // print the number of seconds since reset:\n  lcd.print(millis() / 1000);\n}\n</code></pre>"},{"location":"CS_Electives/IoT/Devices/LDR/","title":"LDR","text":"<p>Light Dependent Resistor</p> <p>Output \\(\\in [0, 1023]\\) $$ \\begin{aligned} R &amp;\\propto \\dfrac{1}{L_V} \\ \\implies I &amp;\\propto L_V \\end{aligned} $$ where</p> <ul> <li>\\(R =\\) Resistance</li> <li>\\(L_V =\\) Intensity of incident light</li> <li>\\(I =\\) Current</li> </ul>"},{"location":"CS_Electives/IoT/Devices/LDR/#applications","title":"Applications","text":"<ul> <li>Smart street lights</li> <li>Auto-brightness on Phone</li> </ul> <p>No polarity, as resistors don\u2019t have any polarity</p>"},{"location":"CS_Electives/IoT/Devices/LDR/#connection","title":"Connection","text":""},{"location":"CS_Electives/IoT/Devices/LDR/#code","title":"Code","text":"<pre><code>int reading;\n\nvoid setup(){\n  pinMode(A0, INPUT); // LDR\n  pinMode(D2, OUTPUT); // LED\n\n\n  Serial.begin(9600);\n}\nvoid loop(){\n  reading = analogRead(A0);\n\n  Serial.println(reading);\n\n  // if incident light too low -&gt; environment too dark\n  // then turn on output light\n  if (reading &lt; 50){\n    digitalWrite(D2, HIGH);\n  }\n  else {\n    digitalWrite(D2, LOW);\n  }\n\n  delay(500);\n}\n</code></pre>"},{"location":"CS_Electives/IoT/Devices/LED/","title":"LED","text":"<p>Light-Emitting Diode</p> <p>Polarized component (+ve Anode and -ve Cathode terminals)</p> <p></p>"},{"location":"CS_Electives/IoT/Devices/LED/#rgb-led","title":"RGB LED","text":"<p>Inputs Red, Green, Blue values</p>"},{"location":"CS_Electives/IoT/Devices/LiDar/","title":"LiDar","text":""},{"location":"CS_Electives/IoT/Devices/LiDar/#lidar_1","title":"LiDaR","text":"<p>Light Detection and Ranging</p> <p>Remote sensing method using light in the form of pulsed laser to measure ranges (variable distances).</p> <p>These light pulses generate precise, 2D/3D maps.</p> <p>Lidar instrument consists of a laser, scanner, specialized receiver.</p>"},{"location":"CS_Electives/IoT/Devices/Odometer/","title":"Odometer","text":"<p>Use of data from motion sensors to estimate change in position over time. It is used in robotics by some legged/wheeled robots to estimate their position wrt starting location.</p> <p>Types</p> <ul> <li>Wheel odometry</li> <li>Laser/Ultrasonic odometry</li> <li>GPS</li> <li>INS (Interval navigation system)</li> <li>Visual Odometry (VO)</li> </ul> <p>Encoders are fundamental robotics motion control as they provide accurate and precise feedback about angle, position, and speed.</p>"},{"location":"CS_Electives/IoT/Devices/Radar/","title":"Radar","text":"<p>Uses radio waves to detect vehicles and other obstructions in the environment.</p> <p>The duration of pulse returning can be used to determine the other objects\u2019 speed and direction of motion.</p>"},{"location":"CS_Electives/IoT/Devices/Relay/","title":"Relay","text":"<p>Electromagnetic switch</p> <p>Working principle: Electromagnetic induction</p> <p></p> <p>Be careful when working with relays, as they may short-circuit. Don\u2019t touch the back side and don\u2019t keep it near other conductors and electronics</p>"},{"location":"CS_Electives/IoT/Devices/Relay/#pins","title":"Pins","text":"<ul> <li>VCC: +5v</li> <li>GND</li> <li>IN: Input (D2)</li> <li>NC: Normally-closed/connected</li> <li>NO: Normally-open</li> <li>COM: Common</li> </ul> <p>Internal connections</p> <ul> <li>NC &amp; COM with spring</li> <li>NO &amp; IN</li> </ul> IN NO NC - COM NO - COM On Behaves like a magnet Break Attracted Off No effect RevertMakes a \u2018tick-tick\u2019 sound Break <p>If there is no \u2018tick-tick\u2019 sound, then the relay isn\u2019t getting sufficient voltage</p>"},{"location":"CS_Electives/IoT/Devices/Stereo_Camera/","title":"Stereo Cameras","text":"<p>Single camera/ordinary multi-camera system can only help in basic obstacle detection/surround view. However, in order to measure depth to infer and analyze distance between objects, cameras ned to act as a stereo pair</p> <p>Usually contain 2 cameras placed horizontally next to each other. It helps cameras to view the same area and assess the depth and distance of the object using pixel disparity technique.</p> <p>They are often paired with LiDar sensor to improve reliability and accuracy.</p> <p></p>"},{"location":"CS_Electives/IoT/Devices/Ultrasonic/","title":"Ultrasonic","text":"<p>Technical name: HC-SR04</p> <p>More precise than IR sensor</p> <p>Working principle: Reflection of soundwaves</p> <p>Ultrasonic waves travel faster than speed of audible sound</p> <p>Range: 3cm to 4m</p> <p>Accuracy: 3mm</p>"},{"location":"CS_Electives/IoT/Devices/Ultrasonic/#components","title":"Components","text":"<ul> <li>Emitter</li> <li> <p>Emitter sends out an Ultrasonic pulse at 40 kHz which travels through the air, and returns if it bounces back from an object.</p> </li> <li> <p>Receiver</p> </li> </ul>"},{"location":"CS_Electives/IoT/Devices/Ultrasonic/#measurement","title":"Measurement","text":"<p>Measures distance of object by emitting Ultrasonic sound waves, and converts reflected sound into electrical signal, and then by calculating the travel time &amp; speed of sound, the distance can be calculated. $$ D = S \\times \\dfrac{t}{2} $$</p>"},{"location":"CS_Electives/IoT/Devices/Ultrasonic/#pins","title":"Pins","text":"<ul> <li>VCC: +5v</li> <li>GND</li> <li>TRIG: Emitter (D3)</li> <li>ECHO: Receiver </li> </ul>"},{"location":"CS_Electives/IoT/Devices/Ultrasonic/#code","title":"Code","text":"<pre><code>int emit_duration = 10; // microseconds\nlong speed = 330; // m/s\nlong time;\n\nvoid setup() {\n  pinMode(2, OUTPUT); // trig\n  pinMode(3, INPUT); // echo\n\n  Serial.begin(9600);\n}\nvoid loop() {\n  // emit ultrasonic waves for 10 microsec\n  digitalWrite(2, HIGH);\n  delayMicroseconds(emit_duration);\n\n  digitalWrite(2, LOW);\n  delayMicroseconds(emit_duration);\n\n  time = pulseIn(3, HIGH); // microseconds\n  time /= 1000 * 1000; // seconds\n\n  distance = speed * (time/2);\n\n  delay(5000);\n}\n</code></pre>"},{"location":"CS_Electives/LLM/","title":"Large Language Models","text":""},{"location":"CS_Electives/LLM/#references","title":"References","text":"<ul> <li> Introduction to large language models</li> </ul>"},{"location":"CS_Electives/Machine_Learning/","title":"Machine Learning","text":"<p>An exciting sub-area of Artificial Intelligence focused on designing machines that can learn and improve their performance from experience. This course introduces students to the key algorithms and theories that form the core of machine learning. </p> <p>Students are expected to have a foundational understanding of several key concepts, including probability theory, decision theory, information theory, linear algebra, and optimization and search techniques.</p>"},{"location":"CS_Electives/Machine_Learning/#key-learning-objectives-include","title":"Key learning objectives include:","text":"<ul> <li>Understanding major approaches to learning (supervised, unsupervised, semi-supervised, and reinforcement learning).</li> <li>Exploring techniques made feasible by increased computational power and large volumes of data.</li> <li>Covering fundamental topics such as regression, decision trees, support vector machines, and artificial neural networks.</li> <li>Examining Bayesian techniques and Hidden Markov models.</li> <li>Introducing advanced topics like active learning, deep learning, and topological learning.</li> </ul> <p>This course provides students with a solid foundation in machine learning principles and prepares them for practical applications and further studies in artificial intelligence.</p>"},{"location":"CS_Electives/Machine_Learning/#references","title":"References","text":"<ul> <li> Machine Learning | Dr. Pranav | BITS Pilani Dubai Campus</li> <li> Modern Data Analysis for Economics</li> <li> Machine Learning From Data</li> <li> Machine Learning From Data | Prof Yaser Abu Mostafa | Caltech</li> <li> Machine Learning From Data | Rensselaer Prof. Malik Magdon-Ismail</li> <li> Machine Learning | Gary Holness | Clark University</li> <li> Machine Learning From Data | Uzma Mushtaque</li> <li> Machine Learning From Data | Course Handbook</li> <li> Machine Learning | Stanford</li> <li> Machine Learning| Andrew Ng Coursera too introductory</li> <li> Stanford CS229: Machine Learning | Andrew Ng | 2018</li> <li> Stanford CS229: Machine Learning | 2022</li> <li> Stanford CS229M: Machine Learning Theory</li> <li> MIT 6.S897 Machine Learning for Healthcare, Spring 2019</li> <li> Advanced Machine Learning | Sergey Plis | Georgia State University</li> <li> Online Machine Learning | IIT Bombay</li> <li> Machine Learning | mathematicalmonk</li> <li> MIT 9.520/6.860S - Statistical Learning Theory and Applications</li> <li> Machine Learning | NUS School of Computing</li> <li> Statistical Learning with Python | Stanford Online</li> <li> Intro to Machine Learning and Statistical Pattern Classification</li> <li> Machine Learning | WQU Saul Leung</li> <li> Machine Learning With Large Datasets | CMU 10-605</li> <li> Multimodal Machine Learning | CMU Fall 2022</li> <li> MLOps | Andrew Ng Coursera</li> <li> Machine Learning Yearning | Andrew Ng</li> <li> Advanced Machine Learning | Florian Marquardt</li> <li> Applied Machine Learning (Cornell Tech CS 5787, Fall 2020) | Volodymyr Kuleshov</li> <li> Machine Learning for Intelligent Systems | Kilian Weinberger | Cornell</li> <li> Applied Machine Learning | Andreas Mueller</li> <li> Spring 2019</li> <li> Spring 2020</li> <li> Probabilistic Machine Learning | T\u00fcbingen Machine Learning | Philipp Hennig</li> <li> 2021</li> <li> 2023</li> <li> Towards Bayesian Regression | Kapil Sachdeva</li> <li> Machine Learning Concepts (Simply Explained) | Pedram Jahangiry</li> <li> Machine Learning Codes and Concepts (2023) | Pedram Jahangiry</li> <li> Machine Learning in finance - 2021 | Pedram Jahangiry</li> <li> Business Analytics Using Data Mining (BADM) | Galit Shmueli</li> <li> Steve Brunton</li> <li> Data Driven Science &amp; Engineering | Machine Learning, Dynamical Systems, and Control</li> <li> Machine Learning for Fluid Dynamics | Steve Brunton</li> <li> Quantitative Social Science Methods, I (Gov2001 at Harvard University)</li> <li> Machine Learning | WIT Solapur - Professional Learning Community</li> <li> The Foundations of Machine Learning | Ion Petre</li> <li> Advanced Machine Learning 2020, CSE, IIT Kharagpur</li> <li> Introductory Applied Machine Learning | University of Edinburgh | Victor Lavrenko</li> <li> Machine Learning | University of Utah</li> <li> Machine Learning | VU University Amsterdam</li> <li> CPSC 330: Applied Machine Learning | University of British Columbia</li> <li> CPSC 340: Machine Learning and Data Mining (2018) | University of British Columbia</li> <li> Machine Learning Techniques | IIT Madras</li> <li> Statistical Machine Learning | University of Guelph</li> <li> Dimensionality Reduction and Manifold Learning | University of Toronto</li> <li> Gaussian Processes</li> <li> Gaussian Processes | Imperial College</li> <li> <p> Gaussian Process Summer School</p> </li> <li> <p> Gaussian Processes | University College London</p> </li> <li> From Data to Decisions: Measurement, Uncertainty, Analysis and Modeling | Chris Mack | University of Texas</li> <li> IE343 Statistical Learning | Se Young Yun</li> <li> Learning Theory | Se Young Yun</li> <li> Machine Learning Explainability | Stanford</li> <li> Machine Learning with Graphs | Stanford</li> <li> Intro to Machine Learning and Statistical Pattern Classification | Sebastian Raschka</li> <li> Fairness in Machine Learning | MIT</li> </ul>"},{"location":"CS_Electives/Machine_Learning/#current-video","title":"Current Video","text":""},{"location":"CS_Electives/Machine_Learning/01_Intro/","title":"Introduction","text":"<p>This introductory page is a big long, but that's because all the below concepts are common to every upcoming topic.</p>"},{"location":"CS_Electives/Machine_Learning/01_Intro/#machine-learning","title":"Machine Learning","text":"<p>Field of study that enables computers to learn without being explicitly programmed; machine learns how to perform task \\(T\\) from experience \\(E\\) with performance measure \\(P\\).</p> <p>Machine learning is necessary when it is not possible for us to make rules, ie, easier for the machine to learn the rules on its own</p> <p></p> <pre><code>flowchart LR\n\nsubgraph Machine Learning\n    direction LR\n    i2[Past&lt;br/&gt;Input] &amp; o2[Past&lt;br/&gt;Output] --&gt;\n    a(( )) --&gt;\n    r2[Derived&lt;br/&gt;Rules/&lt;br/&gt;Functions]\n\n    r2 &amp; ni[New&lt;br/&gt;Input] --&gt;\n    c(( )) --&gt;\n    no[New&lt;br/&gt;Output]\nend\n\nsubgraph Traditional Programming\n    direction LR\n    r1[Standard&lt;br/&gt;Rules/&lt;br/&gt;Functions] &amp; i1[New&lt;br/&gt;Input] --&gt;\n    b(( )) --&gt;\n    o1[New&lt;br/&gt;Output]\nend</code></pre>"},{"location":"CS_Electives/Machine_Learning/01_Intro/#why-do-we-need-ml","title":"Why do we need ML?","text":"<p>To perform tasks which are easy for humans, but difficult to generate a computer program for it</p>"},{"location":"CS_Electives/Machine_Learning/01_Intro/#requirements","title":"Requirements","text":"<ol> <li>\\(\\exists\\) pattern</li> <li>If \\(\\not \\exists\\) pattern and its just noise, it is impossible to model it</li> <li>We cannot quantify pattern mathematically</li> <li>\\(\\exists\\) data</li> </ol>"},{"location":"CS_Electives/Machine_Learning/01_Intro/#guiding-principles","title":"Guiding Principles","text":"Principle Questions Relevance Is the use of ML in a given context solving an appropriate problem Representativeness Is the training data appropriately selected Value - Do the predictions inform human decisions in a meaningful way- Does the machine learning model produce more accurate predictions than alternative methods- Does it explain variation more completely than alternative methods Explainability - Data selection, Model selection, (un)intended consequences- How effectively is use of ML communicated Auditability Can the model's decision process be queried/monitored by external actors Equity The model should benefit/harm one group disproportionately Accountability/Responsibility Are there mechanisms in place to ensure that someone will be responsible for responding to feedback and redressing harms, if necessary?"},{"location":"CS_Electives/Machine_Learning/01_Intro/#learning-problem","title":"Learning Problem","text":"<p>Given training examples and hypothesis set of candidate models, generate a hypothesis function using a learning algorithm to estimate an unknown target function</p> <p></p> <p>\\(P(x)\\) quantifies relative importance of \\(x\\)</p> <p>Learning model</p> <ul> <li>Learning algorithm</li> <li>Hypothesis set</li> </ul>"},{"location":"CS_Electives/Machine_Learning/01_Intro/#stages-of-machine-learning","title":"Stages of Machine Learning","text":"<pre><code>flowchart LR\ntd[Task&lt;br/&gt;Definition] --&gt;\ncd[(Collecting&lt;br/&gt;Data)] --&gt;\nl[Learning&lt;br/&gt;Type] --&gt;\nc[Define Cost] --&gt;\nOptimize --&gt;\nEvaluate --&gt;\nTune --&gt;\nsave[/Save Model/] --&gt;\nd[/Deploy/] --&gt; Model\ncd --&gt; ad\nld[(Live &lt;br/&gt;Data)] --&gt; ad[Anomaly&lt;br/&gt;Detection] --&gt; Model</code></pre>"},{"location":"CS_Electives/Machine_Learning/01_Intro/#3-dimensions-of-prediction","title":"3 Dimensions of Prediction","text":"<ul> <li>Point estimate</li> <li>Time</li> <li>Probabilistic</li> <li>Intervals</li> <li>Density</li> <li>Trajectories/Scenarios</li> </ul>"},{"location":"CS_Electives/Machine_Learning/01_Intro/#good-prediction-characteristics","title":"Good Prediction Characteristics","text":"<ul> <li>Forecast/Prediction consistency: Forecasts/Predictions should correspond to forecaster\u2019s best judgement on future events, based on the knowledge available at the time of issuing the Forecasts/Predictions</li> <li>Forecast/Prediction quality (accuracy): Forecasts/Predictions should describe future events as good as possible, regardless of what these Forecasts/Predictions may be used for</li> <li>Forecast/Prediction value: Forecasts/Predictions should bring additional benefits (monetary/others) when used as input to decision-making</li> </ul> <p>Hence, sometimes you may choose the Forecast/Prediction with the better value even if its quality is not the best</p>"},{"location":"CS_Electives/Machine_Learning/01_Intro/#performance-vs-parsimony","title":"Performance vs Parsimony","text":"<ul> <li>Parsimonious models are more explainable</li> <li>Parsimonious models generalize better</li> <li>Small gains with deep models may disappear with dataset shift/non-stationary</li> </ul>"},{"location":"CS_Electives/Machine_Learning/01_Intro/#aspects","title":"Aspects","text":"Aspect Equivalent in Marco Polo game Loss Goal Model Class Map Optimization Search Data Sound"},{"location":"CS_Electives/Machine_Learning/01_Intro/#open-source-tools","title":"Open-source Tools","text":"Scikit-Learn TensorFLow Keras PyTorch MXNet CNTK Caffe PaddlePaddle Weka"},{"location":"CS_Electives/Machine_Learning/01_Intro/#doesnt-do-well-for-forecasting","title":"Doesn\u2019t do well for Forecasting","text":"<p>Machine Learning cannot provide reliable time-series forecasting, without causal reasoning. This is why AI/ML cannot be blindly trusted for stock price prediction.</p> <p>Related topics</p> <ul> <li>Model ends up being a Naive forecaster: just blindly predicts \\(\\hat y_{t+h} = y_t\\)</li> <li>Counter-factual simulation: Never-before-seen events, such as</li> <li>declining house prices</li> <li>Negative oil prices</li> <li>Distribution drift</li> <li>Turkey problem</li> </ul> <p>In the face of external factors that is not factored into the model, human intervention is required</p>"},{"location":"CS_Electives/Machine_Learning/02_Task_T/","title":"Task \\(T\\)","text":"<p>Process of learning itself is not the task; learning is the means of attaining ability to perform the task</p> <p>Usually described in terms of how the machine learning system should process an instance (collection of features), which is usually represented as a vector.</p>"},{"location":"CS_Electives/Machine_Learning/02_Task_T/#tasks","title":"Tasks","text":"Function Mapping Example Regression Predicting a continuous numerical output \\({\\mathbb R}^n \\to {\\mathbb R}\\) Stock value prediction Classification Categorizing input into a discrete outputor outputing a probability dist over classesDerived from regressionIf binary and very imbalanced dataset, use anomaly detection instead \\({\\mathbb R}^n \\to [1, C]\\) Categorizing imagesFraud detection Anomaly Detection Identify abnormal events \\(\\mathbb R \\to [0, 1]\\) Fraud detection Classification w/ missing inputs Learn distribution over all variables, solve by marginalizing over missing variables \\({\\mathbb R}^n \\to [1, C]\\) Clustering Assigning class label to set of unclassified items, to group observations into clusters \\(\\hat y = \\hat f(x) = \\text{Cluster}(x)\\)\\({\\mathbb R}^n \\to [1, C ]\\) Grouping similar images Density estimation Estimating probability distribution from data \\({\\mathbb R}^n \\to P(x)\\) Transcription Convert unstructured data intro discrete textual form OCRSpeech Recognition Machine Translation Convert itinto a sequence of symbols into another language Natural Language Translation Structured Output Output data structure hasrelationships between elements ParsingImage segmentationImage captioning Synthesis &amp; Sampling Generate new samples similar to those intraining data Texture generationSpeech synthesisSupersampling images Data Imputation Predict values of missing entries Denoising Predict clean output from corrupt input Image/Video denoising Density Estimation Identify underlying probability distribution of set of inputs"},{"location":"CS_Electives/Machine_Learning/02_Task_T/#types-of-predictions","title":"Types of Predictions","text":"\\(x_\\text{new}\\) Uncertainty Intrapolation? \\(\\in X_\\text{train}\\) Low Interpolation \\(\\in [X_{\\text{train}_\\text{min}}, X_{\\text{train}_\\text{max}}]\\) Moderate Extrapolation \\(\\not \\in [X_{\\text{train}_\\text{min}}, X_{\\text{train}_\\text{max}}]\\) High Smoothing"},{"location":"CS_Electives/Machine_Learning/02_Task_T/#regression","title":"Regression","text":""},{"location":"CS_Electives/Machine_Learning/02_Task_T/#probabilistic-regression","title":"Probabilistic Regression","text":"<ul> <li>Probability of prediction is required</li> <li>Understand impact of input</li> <li>Regression target is the sum of individual binary outcomes</li> </ul> \\[ y'_i = p_i = \\dfrac{y_i}{n_i} \\]"},{"location":"CS_Electives/Machine_Learning/02_Task_T/#binary-aggregate-outcomes","title":"Binary Aggregate Outcomes","text":"\\[ \\begin{aligned} y_i &amp;\\sim \\text{Binomial}(n_i, p_i) \\\\ p_i &amp;= \\sigma(\\beta x_i) \\\\  \\implies y_i &amp;\\sim \\text{Bernoulli}\\Big( \\sigma(x_i' \\beta) \\Big) \\end{aligned} \\] <pre><code>    temperature fields cultivated percentCultivated\n1 13.18475               63         49  0.7777778\n2 12.35680              165        147  0.8909091\n3 17.57882               38         30  0.7894737\n4 20.86867              152         95  0.6250000\n5 13.88084               88         69  0.7840909\n6 17.18088              191        141  0.7382199\n</code></pre>"},{"location":"CS_Electives/Machine_Learning/02_Task_T/#multiple-aggregate-outcomes","title":"Multiple Aggregate Outcomes","text":"\\[ \\begin{aligned} y_i &amp;\\sim \\text{Multinomial}(n, p) \\\\ p_j &amp;= \\text{Softmax}(\\beta_j x) \\\\ &amp;= \\dfrac{\\exp(\\beta_j x)}{\\sum_k^K \\exp(\\beta_k x) } \\end{aligned} \\] <p>When \\(n_i=1,\\) this becomes multi-class classification</p> <p>Example</p> <pre><code>         temperature  rainfall fields noncrop corn wheat rice\n1    13.18475           75.26666     63       8   31    17    7\n2    12.35680           102.37572    165       7  100    30   28\n3    17.57882           101.61363     38       1   26     3    8\n4    20.86867           64.35788    152      45   78    12   17\n5    13.88084           107.54101     88       4   54    15   15\n</code></pre>"},{"location":"CS_Electives/Machine_Learning/02_Task_T/#classification","title":"Classification","text":""},{"location":"CS_Electives/Machine_Learning/02_Task_T/#decision-boundarysurface","title":"Decision Boundary/Surface","text":"<p>The boundary/surface that separates different classes</p> <p>Generated using decision function</p> <p>If we have \\(d\\) dimensional data, our decision boundary will have \\((d-1)\\) dimensions</p>"},{"location":"CS_Electives/Machine_Learning/02_Task_T/#linear-separability","title":"Linear Separability","text":"<p>Means the ability to separate points of different classes using a line, with/without a non-linear activation function</p> \\[ f(u) = \\begin{cases} 1, &amp; u \\ge 0 \\\\ 0, &amp; \\text{otherwise} \\end{cases} \\] Logic Gate Linearly-Separable? Comment AND \u2705 OR \u2705 XOR \u274c Linearly separable if we add \\((x \\cdot y)\\) as a feature XNOR \u274c Linearly separable if we add \\((x \\cdot y)\\) as a feature <p></p>"},{"location":"CS_Electives/Machine_Learning/02_Task_T/#linearly-non-separable","title":"Linearly Non-Separable","text":"Slightly Seriously"},{"location":"CS_Electives/Machine_Learning/02_Task_T/#discriminant-function","title":"Discriminant Function","text":"<p>Functions which takes an input vector \\(x\\) and assigns it to one of the \\(k\\) classes</p>"},{"location":"CS_Electives/Machine_Learning/02_Task_T/#multi-class-classification","title":"Multi-Class Classification","text":"One-vs-Rest One-vs-One No of classifiers \\(k\\) \\(\\frac{k(k-1)}{2}\\) Retains valid probabilistic interpretation \u2705 \u274c Limitation Some point may have multiple classes/no classes at all Multiple classes assigned to some points"},{"location":"CS_Electives/Machine_Learning/02_Task_T/#clustering","title":"Clustering","text":""},{"location":"CS_Electives/Machine_Learning/02_Task_T/#analyzing-clusters","title":"Analyzing clusters","text":"<p>Understand the characteristics of each cluster</p> <ol> <li>Select features \\(x_j\\)</li> <li>Do not use all features to perform clustering</li> <li>where \\(x_j=\\) sensible features such as<ul> <li>Spending habits</li> </ul> </li> <li>Perform clustering</li> <li>Perform statistical analysis on \\(x_{\\centernot{j}}\\)</li> </ol> <p>For example: 4 clusters</p> \\(x_1=0\\) \\(x_1=1\\) \\(x_2=0\\) A B \\(x_2=1\\) C D"},{"location":"CS_Electives/Machine_Learning/03_Model/","title":"Model","text":"\\[ \\hat y = \\hat f(x) + u \\\\  \\hat f(x) = E[y \\vert x] \\] <p>where</p> Denotation Term Comment \\(x\\) input; feature; predictor \\(y\\) output; target; response \\(\\hat y\\) prediction \\(E[y \\vert x]\\) CEF (Conditional Expectation Function) \\(\\hat f\\) Target functionHypothesisModel Gives mapping b/w \\(x\\) and \\(y\\) to obtain CEF \\(p(y \\vert x)\\) Target distribution/Posterior distribution of \\(y\\) Gives mapping b/w \\(x\\) and \\(y\\) to obtain Conditional Distribution \\(u\\) Random component"},{"location":"CS_Electives/Machine_Learning/03_Model/#idk","title":"IDK","text":"<pre><code>flowchart LR\nd[Data&lt;br/&gt;Generation] --&gt;\n|Input| m[Modelling] --&gt;\n|Analysis| si[Scientific&lt;br/&gt;Investigation]\n\nsi --&gt;\n|Improve Model| m\n\nsi --&gt;\n|Improve DoE/Data Generation| d</code></pre>"},{"location":"CS_Electives/Machine_Learning/03_Model/#desired-properties","title":"Desired Properties","text":"<ul> <li>Unbiased: Mean of residuals = 0</li> <li>Efficient: Variance of residuals and learnt parameters is min</li> <li>Maximum likelihood \\(P(D, \\theta)\\)</li> <li>Robust</li> <li>Consistent: \\(n \\to \\infty \\implies E[u_i] \\to 0\\)</li> </ul> <p>Attributes of probabilistic forecast quality</p> <ol> <li>Reliable: probabilistic calibration</li> <li>For quantile forecasts with level \\(\\alpha\\), observations \\(y_{t+k}\\) should be less than \\(\\hat y_{t+k}\\) \\(\\alpha\\) times</li> <li>For interval forecasts with coverage \\(p\\), observations \\(y_{t+k}\\) should be within the interval \\(p\\) times</li> <li>For predictive densities composed of \\(m+1\\) quantile forecasts with nominal levels \\(\\alpha_0, \\alpha_1, \\dots, \\alpha_m\\), all these quantile forecasts are evaluated individually using the above</li> <li>Q-Q Plots</li> <li>Sharp: informative</li> <li>Concentration of probability: how tight the predictive densities are</li> <li>Perfect probabilistic forecast gives a probability of 100% on a single value</li> <li>CRPS<ol> <li>Average of each predictive density and corresponding observation</li> <li>\\(\\text{CRPS}_{t, h} = \\int_y \\ \\Big( \\hat F_{t+h \\vert t} - 1(y_{t+h} \\le y) \\Big)^2 \\ \\cdot dy\\)</li> <li>\\(\\text{CRPS}_h = \\text{avg}(\\text{CRPS}_{t, h})\\)</li> </ol> </li> <li>Skilled</li> <li>High resolution</li> </ol>"},{"location":"CS_Electives/Machine_Learning/03_Model/#note","title":"Note","text":"<p>Every model is only limited to its \u2018scope\u2019, which should be clearly documented</p>"},{"location":"CS_Electives/Machine_Learning/03_Model/#idk_1","title":"IDK","text":"<ul> <li> <p>\"If you understand your solution better than the problem, then you are doing something wrong\" ~ Vincent Warmerdam</p> </li> <li>Think more about system design rather than just machine learning</li> <li>Simple linear models work. Most of the times non-linear/ensembles/deep learning models are not required</li> </ul>"},{"location":"CS_Electives/Machine_Learning/03_Model/#model-types","title":"Model Types","text":"Ideal Non-Parametric(Nearest Neighbor) Semi-Parametric Parametric \\(\\hat y\\) \\(\\text{Mean}(y \\vert x)\\) \\(\\text{Mean} \\Big(y \\vert x_i \\in N(x) \\Big)\\)\\(N(x)\\) is neighborhood of \\(x\\) \\(f(x)\\) Functional Form assumption None None Assumes functional form with a finite &amp; fixed number of parameters, before data is observed Advantages Perfect accuracy - learns complex patterns- in a high-dimensional space- without being specifically directed- learns interactions Compression of model into a single function Limitation Not possible to obtain Suffers from curse of dimensionality: Requires large dataset, especially when \\(k\\) is largeBlack box: Lacks interpretabilityLarge storage cost: Stores all training recordsComputationally-expensive Lost information? Visualization Space Complexity is function of Training set size Number of function parameters Number of function parameters Example Nearest Neighbor averaging Spline Linear Regression <p>Fundamentally, a parametric model can be though of data compression</p>"},{"location":"CS_Electives/Machine_Learning/03_Model/#modelling-types","title":"Modelling Types","text":"Discriminative/Reduced Form Generative/Structural/First-Principles Hybrid/Discrepancy Characteristic Mathematical/Statistical Theoretical(Scientific/Economic) Mix of first principles &amp; machine learning Effect ModifiersRead more Assumes that effect modifiers will remain same as during learning Incorporates effect modifiers Goal 1. \\(\\hat p(y \\vert x)\\)2. \\(\\hat y = \\hat E(y \\vert x)\\) 1. \\(\\hat p(x \\vert y)\\)2. \\(\\hat p(x, y)\\)3. \\(\\hat p(y \\vert x)\\)4. \\(\\hat y = \\hat E(y \\vert x)\\) \\(\\hat y = \\text{g}(x) + d(x)\\) This model defines a \u201cstory\u201d for how the data was generated. To obtain a data point1. Sample class \\(y \\sim \\text{Categorical}(p_1, p_2, \\dots, p_C)\\) with class proportions given by \\(p_c\\)2. Then, we sample \\(x\\) from the gaussian distribution \\(\\mathcal N(\\mu_c, \\Sigma_c)\\) for each class Includes Causal Theory \u274c \u2705 Same as Structural Intrapolation? \u2705 \u2705 \u2705 Interpolation \u26a0\ufe0f \u2705 \u26a0\ufe0f Extrapolation \u274c \u2705 \u274c Counter-factual simulation \u274c \u2705 \u274c Can adapt to data drift \u274c \u2705 \u26a0\ufe0f Stable for Equilibrium effects \u274c \u2705 \u26a0\ufe0f Synthetic data generation \u274c \u2705 \u274c Out-of-Sample Accuracy Low High(only for good theoretical model) Same as Structural Derivation Time 0 High Same as Structural Example models Non-Probabilistic classifiersLogistic regression Probabilistic classifiers (Bayesian/Gaussian) Comment The shortcoming of reduced form was seen in the 2008 RecessionThe prediction model for defaults was only for the case that housing prices go up, as there was data only for that. Hence, the model was not good for when the prices started going down. Learning \\(p(x, y)\\) can help understand \\(p(u, v)\\) if \\(\\{x, y \\}\\) and \\(\\{ u, v \\}\\) share a common underlying causal mechanismFor eg: Apples falling down trees and the earth orbiting around the sun both inform us of the gravitational constant. Example 1: General \\(f=\\sigma(kx), \\hat f = e^{kx}\\)\\(f = x^2, \\hat f = x\\)\\(f=e^x, \\hat f=x^2\\) \\(f = x^2, \\hat f = x^2\\) Example 2: Chemical Kinetics Fit curve to given data Solve the rate law equation for the given data Example 3: Astronomy Mars position wrt Earth, assuming that Mars revolves around the Earth Mars position wrt Earth, assuming that Mars &amp; Earth revolve around the Sun Example 4: Wage vs Education Relationship of wage vs education directly Relationship of wage vs education, with understanding of demand-supply curve (ie, effects of supply of college educated students in the market)eg: Kerala Example 5: Time-Series Forecasting Univariate model with lags and trends Multi-variate model with lags of \\(y\\) and \\(x\\)"},{"location":"CS_Electives/Machine_Learning/03_Model/#structural-vs-reduced-form","title":"Structural vs Reduced-Form","text":""},{"location":"CS_Electives/Machine_Learning/03_Model/#number-of-variables","title":"Number of Variables","text":"Univariate Regression Multi-Variate \\(\\hat y\\) \\(f(X_1)\\) \\(f(X_1, X_2, \\dots, X_n)\\) Equation \\(\\beta_0 + \\beta_1 X_1\\) \\(\\sum\\limits_{i=0}^n \\beta_i X_i\\) Best Fit Straight line Place"},{"location":"CS_Electives/Machine_Learning/03_Model/#degree-of-model","title":"Degree of Model","text":"Simple Linear Regression Polynomial Linear Regression Non-Linear Regression Equation \\(\\sum\\limits_{j=0}^k \\beta_j X_j\\) \\(\\sum \\limits_{j=0}^k \\sum\\limits_{i=0}^n \\beta_{ij} (X_j)^i\\) Any of the \\(\\beta\\) is not linear Example \\(\\beta_0 + \\beta_1 X_1 + \\beta_1 X_2\\) \\(\\beta_0 + \\beta_1 X_1 + \\beta_1 X_1^2 + \\beta_1 X_2^{10}\\) \\(\\beta_0 + e^{\\textcolor{hotpink}{\\beta_1} X_1}\\) Best Fit Straight line Curve Curve Solving method possible Closed-FormIterative Closed-FormIterative Iterative Limitations 1. Convergence may be slow2. Convergence to local minima vs global minima3. Solution may depend heavily on initial guess Comment You can alternatively perform transformation to make your regression linear, but this isn\u2019t best1. Your regression will minimize transformed errors, not your back-transformed errors (what actually matters). So the weights of errors will not be what is expected2. Transformed errors will be normal, but your back-transformed errors (what actually matters) won\u2019t be a normal <p>The term linear refers to the linearity in the coefficients \\(\\beta\\)s, not the predictors</p>"},{"location":"CS_Electives/Machine_Learning/03_Model/#jensens-inequality","title":"Jensen\u2019s Inequality","text":"\\[ E[\\log y] &lt; \\log (E[y]) \\] <p>Therefore $$ \\hat y = \\exp(\\beta_0 + \\beta_1 x) + u_i \\ E[y \\vert x] \\ne E[\\exp(\\beta_0 + \\beta_1 x)] $$ However, if you assume that \\(u \\sim N(0, \\sigma^2)\\) $$ E[y \\vert x] = \\exp(\\beta_0 + \\beta_1 x + \\dfrac{\\sigma^2}{2}) $$</p>"},{"location":"CS_Electives/Machine_Learning/03_Model/#multi-layer-models","title":"Multi-Layer Models","text":"<p>Boosting with different function and/or model class for each component of \\(f(x)\\) $$ \\hat f(x) = \\sum_{j=1}^k \\hat f_j(x_j) \\ \\hat f: x_j \\to u_{j-1} \\ u_0 = y $$ <pre><code>flowchart LR\nx1 --&gt; h1\ny ---&gt; u1\nh1 --&gt; u1\n\nx2 --&gt; h2\nu1 --&gt; u2\nh2 ---&gt; u2</code></pre></p> <p>Model in the following order to avoid fitting the noise:</p> <ol> <li>Components: low-frequency components first, then high-frequency components</li> <li>Model: low-variance models preferred, then high-variance models; or you can optimize the entire thing as a carefully-crafted neural network</li> </ol> <p>For eg: Time Series modelling</p> <ul> <li>Trend with LR: \\(\\hat f(t)\\)</li> <li>Seasonality with KNN</li> <li>Holidays with Decision Tree</li> </ul>"},{"location":"CS_Electives/Machine_Learning/03_Model/#categorial-inputs","title":"Categorial Inputs","text":""},{"location":"CS_Electives/Machine_Learning/03_Model/#binary","title":"Binary","text":"\\[ \\begin{aligned} \\hat y &amp;= \\beta_0 + \\beta_1 x + \\beta_2 T + + \\beta_3 x T + u \\\\ \\\\ T = 0 \\implies \\hat y &amp;= \\beta_0 + \\beta_1 x + u \\\\ T = 1 \\implies \\hat y &amp;= (\\beta_0 + \\beta_2) + (\\beta_1+\\beta_3) x + u \\end{aligned} \\] <p>where \\(T=\\) treatment/binary var</p>"},{"location":"CS_Electives/Machine_Learning/03_Model/#discrete-var","title":"Discrete Var","text":"<p>For \\(C\\) possible values of discrete var, you need to create \\((C-1)\\) dummy vars, as all zeros is also scenario</p> <p>Else, if you have \\(C\\) dummy vars, you will have perfect multi-collinearity (dangerous)</p>"},{"location":"CS_Electives/Machine_Learning/03_Model/#model-hints","title":"Model Hints","text":"<p>Known properties of \\(f\\) that can be used to improve \\(\\hat f\\), especially with small datasets</p> <ul> <li>Monotonicity</li> <li>Reflexivity</li> <li>Symmetry</li> <li>Rotational invariance</li> <li>Translational invariance</li> </ul> <p>Can be enforced through</p> <ul> <li>Modifying features</li> <li>Eg: using \\(\\vert x \\vert\\) instead of \\(x\\) for symmetry</li> <li>Regularization Penalty</li> <li>Data augmentation</li> </ul>"},{"location":"CS_Electives/Machine_Learning/03_Model/#latent-variable-models","title":"Latent Variable Models","text":""},{"location":"CS_Electives/Machine_Learning/03_Model/#examples","title":"Examples","text":"<ol> <li> <p>Image classification</p> </li> <li> <p>Contains variability due to gender, eye color, hair color, pose, etc</p> </li> <li> <p>Unless these images are annotated, these factors of variation are not explicitly available</p> </li> <li> <p>Classification</p> </li> <li> <p>Gaussian mixture models</p> </li> </ol>"},{"location":"CS_Electives/Machine_Learning/03_Model/#limitations","title":"Limitations","text":"<ul> <li>Computationally-expensive: requires approximations</li> </ul>"},{"location":"CS_Electives/Machine_Learning/04_Learning_Experience/","title":"Learning Experience \\(E\\)","text":""},{"location":"CS_Electives/Machine_Learning/04_Learning_Experience/#learning-objectives","title":"Learning Objectives","text":"Prediction Estimation of unseen data Modelling/Characterization/Inference How do inputs affect outputObtain the Sample CEF/Conditional Distribution which closely matches the Population CEF/Conditional Distribution Optimization What input values produce desired outputs (both mean and variance) Control How to adjust controlled inputs to maximize control of outputs Simulation Causal inference <p>Use ML models for developing structural models, and then let the structural models to make the predictions, not the ML models</p> <ul> <li>Why: Black swans can be predicted by theory, even if they cannot be predicted by ML</li> <li>How: Use a non-parametric ML to identify important variables and then develop a parametric structural form model.</li> </ul>"},{"location":"CS_Electives/Machine_Learning/04_Learning_Experience/#learning-paradigms","title":"Learning Paradigms","text":"Method Meaning Application Supervised Uses labelled data, to derive a mapping between input examples and target variable. \\(D_\\text{train} = X, y\\) Unsupervised Learning from unlabelled data \\(D_\\text{train} = X\\) Semi-Supervised \\(\\exists\\) labelled data and large amount of unlabelled data.Label the unlabelled data using the labelled data.For example, love is labelled as emotion, but lovely isn\u2019tCotraining, Semi-Supervised SVM Lazy/Instance-Based Store the training examples instead of training explicit description of the target function.Output of the learning algorithm for a new instance not only depends on it, but also on its neighbors.The best algorithm is KNN (K-Nearest Neighbor) Algorithm.Useful for recommender system. ActiveAL Learning system is allowed to choose the data from which it learns.There exists a human annotator.Useful for gene expression/cancer classification Multiple Instance Weakly supervised learning where training instances are arranged in sets.Each set has a label, but the instances don\u2019t Transfer Reuse a pre-trained model as the starting point for a model on a new related task Reinforcement LearningRL Learning in realtime, from experience of interacting in the environment, without any fixed input dataset.It is similar to a kid learning from experience.Best algorithm is Q-Learning algorithm. \\(D_\\text{train} = X, \\text{Feedback}\\) Game playing Bayesian Learning Conditional-probabilistic learning toolEach observed training expmle can incrementally inc/dec the estimated probability that a hypothesis is correct.Useful when there is chance of false positive.For eg: Covid +ve DeepDL Multi-Layered ANNs Computer Vision FederatedFL Distributed Privacy Online Streaming"},{"location":"CS_Electives/Machine_Learning/04_Learning_Experience/#training-method","title":"Training Method","text":"Advantage Batch \\(\\hat f: X \\to y\\) Better model Streaming/Online/Passive-Aggressive \\(\\hat f_b: X_{i \\le b} \\to y_{i \\le b}\\)where \\(b= \\text{Mini-batch}\\) - Adaptive to new data points- Computationally-cheap Hybrid - Batch training start of day- Online training intra-day"},{"location":"CS_Electives/Machine_Learning/04_Learning_Experience/#types-of-learners","title":"Types of Learners","text":"<p>They are not adapted by the ML algo itself, but we can use nested learning, where other algorithms optimize the hyperparameter for the ML algo.</p> Eager Learner Lazy Learner Training Learns relationship between class label &amp; attributes Stores training records Evaluation Perform computations to classify evaluation record Training Speed Slow Fast Evaluation Speed Fast Slow Example - Decision Tree- Rule-Based Classifier - Nearest-neighbor classifier"},{"location":"CS_Electives/Machine_Learning/04_Learning_Theory/","title":"Learning Theory","text":""},{"location":"CS_Electives/Machine_Learning/04_Learning_Theory/#objectives-of-learning","title":"Objectives of Learning","text":"<ol> <li>Ensure fit: \\(E_\\text{in} \\approx 0\\)</li> <li>Ensure generalization: \\(E_\\text{out} - E_\\text{in} \\approx 0\\)</li> </ol>"},{"location":"CS_Electives/Machine_Learning/04_Learning_Theory/#hypothesis","title":"Hypothesis","text":"<p>Estimated model \\(\\hat f(x) = \\hat y\\)</p>"},{"location":"CS_Electives/Machine_Learning/04_Learning_Theory/#hypothesis-set","title":"Hypothesis Set","text":"<p>Set of all hypotheses \\(H = \\{ \\hat f_i(x) \\}\\), both by</p> <ul> <li>machine</li> <li>human</li> </ul>"},{"location":"CS_Electives/Machine_Learning/04_Learning_Theory/#good-characteristics-of-hypotheses","title":"Good Characteristics of Hypotheses","text":""},{"location":"CS_Electives/Machine_Learning/04_Learning_Theory/#parsimony","title":"Parsimony","text":"<p>An explanation of the data should be made as simple as possible, but no simpler</p>"},{"location":"CS_Electives/Machine_Learning/04_Learning_Theory/#occams-razor","title":"Occam\u2019s Razor","text":"<p>The simplest model that fits the data is also the most plausible</p> <p>Simple</p> <ul> <li>Complexity of \\(h\\): MDL (Minimum Description Length)</li> <li>Complexity of \\(H\\): Entropy, VC Dimension</li> </ul> <p>\\(l\\) bits specify \\(h\\) \\(\\implies\\) \\(h\\) is one of the \\(2^l\\) elements of a set \\(H\\)</p> <p>Exception - Looks complex but is actually simple: SVM</p>"},{"location":"CS_Electives/Machine_Learning/04_Learning_Theory/#why-is-simpler-better","title":"Why is simpler better?","text":"<p>Simpler means out-of-sample performance</p> <p>Fewer simple hypotheses than complex ones: \\(m_H(N)\\) \\(\\implies\\) less likely to fit a given dataset: \\(m_h(N)/2^N\\) \\(\\implies\\) more significant when it happens</p>"},{"location":"CS_Electives/Machine_Learning/04_Learning_Theory/#falsifiability","title":"Falsifiability","text":"<p>If your data has chance of falsifying your assertion, then it does not provide any evidence for that assertion</p> <p>Fit that means nothing: linear regression fit with just 2 data points</p> <p></p>"},{"location":"CS_Electives/Machine_Learning/04_Learning_Theory/#sampling-bias","title":"Sampling Bias","text":"<p>If data is sampled in biased way, learning will produce a similarly biased outcome; problem for both causal and statistical learning</p> <p>Non-Random Sampling</p> <ul> <li>non-representative sample that is not a random sample of the population we are interested in</li> </ul> <p>or</p> <ul> <li>study population is different from the target population</li> </ul>"},{"location":"CS_Electives/Machine_Learning/04_Learning_Theory/#case-studies","title":"Case Studies","text":"<ul> <li>Presidential election results: Telephone: Truman vs Dewey</li> <li>Credit approval</li> <li>Creating portfolio based on long-term performance of currently-trading companies of S&amp;P 500</li> <li>You are looking at currently-trading stocks</li> <li>Sampling bias caused by \u2018snooping\u2019</li> <li>Solution: look &amp; trade explicitly wrt S&amp;P 500, not the comprising companies?</li> </ul>"},{"location":"CS_Electives/Machine_Learning/04_Learning_Theory/#types","title":"Types","text":"Censoring Truncation Given a random sample of individuals drawn from the population of interest, some variables \u2013 mainly the outcome \u2013 are observed only on individuals belonging to a subpopulation, while other variables are observed on all individuals in the sample If all variables are observed only on individuals belonging to a subpopulation greater information loss than censoring"},{"location":"CS_Electives/Machine_Learning/04_Learning_Theory/#solution-matching-distributions","title":"Solution: Matching distributions","text":"<p>Ensure that validation and test data matches the distribution of the true target population</p> <p>The train and dev set need not match the same distribution, but it is recommended to sub-sample them such that it matches the target population</p> <p>Doesn\u2019t work for</p> <ul> <li>Region with \\(p(x)=0\\) in-sample, but \\(p(x)&gt;0\\) out-of-sample</li> </ul> <p>How? Gaussian estimation/Adversarial validation</p> <ol> <li>Balancing using only train data</li> <li>Obtain probability \\(p\\) for each datapoint belonging to the train data</li> <li> <p>Weight these with \\(1/p\\) to be sampled again</p> </li> <li> <p>Distribution matching using target population</p> </li> <li>Obtain probability \\(p\\) for each datapoint belonging to the train data</li> <li>Weight these with \\(1/p\\) to be sampled again</li> </ol>"},{"location":"CS_Electives/Machine_Learning/04_Learning_Theory/#data-snooping","title":"Data Snooping","text":"<p>Also called p-hacking, specification search, data dredging, fishing</p> <p>Process of trying a series of models until we obtain a satisfactory result, without accounting for such.</p> <p>This will result in your model matching a particular dataset</p> <p>This includes</p> <ul> <li>Parameters: Coefficients/Weights of the model</li> <li>Hyper-Parameters: Parameters that affect the learning of the model</li> </ul> <p>It is possible to find a statistically significant result even if doesn\u2019t exist, if you try hard enough</p>"},{"location":"CS_Electives/Machine_Learning/04_Learning_Theory/#takeaways","title":"Takeaways","text":"<ul> <li>If a data set has affected any step in the learning process, its ability to assess the outcome of has been compromised. Hence it cannot be (fully) trusted in assessing the outcome.</li> <li>For a given problem type, if you perform an action which you would not do if the data were different, then you must penalize this action for generalization</li> <li>Using known properties of the target function does not have to be penalized, as it is not dataset-specific</li> </ul>"},{"location":"CS_Electives/Machine_Learning/04_Learning_Theory/#solutions","title":"Solutions","text":"<ul> <li>Avoid data snooping: Always use domain knowledge to create your hypothesis set before even looking at the training data</li> <li>Account for data snooping: If not possible to avoid, look at the data but make sure to account for this</li> </ul>"},{"location":"CS_Electives/Machine_Learning/04_Learning_Theory/#pitfalls","title":"Pitfalls","text":"<ul> <li>Explicit: Intentionally trying many models on the same dataset, thereby increasing size of hypothesis set</li> <li>Implicit</li> <li>Looking at the test data before choosing a model</li> <li> <p>Data leakage during feature engineering, such as normalization</p> </li> <li> <p>Adaptive analysis: When working with a public data set, we may already know what models work/don\u2019t work, so the Hypothesis space &gt; the model I formulate</p> </li> </ul> <p>For example:</p> <p></p> <p>If you look at the data beforehand $$ \\begin{aligned} H = &amp;{ \\ &amp; \\quad { 1, x_1, x_2, x_1 x_2, x_1^2, x_2^2 }, \\ &amp; \\quad { 1, x_1^2, x_2^2 }, \\ &amp; \\quad { 1, x_1^2 + x_2^2 }, \\ &amp; \\quad { x_1^2, x_2^2 - 0.6 } \\ &amp; } \\end{aligned} $$</p>"},{"location":"CS_Electives/Machine_Learning/04_Learning_Theory/#what-to-do","title":"What to do?","text":"<ul> <li>Formulate the research qn and fix the what model before seeing training data.</li> <li>If you intend on data snooping and choose a model based on the data, then you should decide on the set of models you are going to choose from before seeing the data, and account for the data snooping in your analysis by</li> <li>Adjusting the significance level of your hypothesis tests by, for example, using the Bonferroni correction</li> <li>Using a test data set to evaluate the performance of your final estimated model. The test set should be allocated at the beginning and only used at the end. Once a data set has been used, it should be treated as contaminated for evaluating test performance</li> </ul>"},{"location":"CS_Electives/Machine_Learning/04_Learning_Theory/#reporting-guidelines","title":"Reporting Guidelines","text":"<ul> <li>Aim for honesty &amp; transparency</li> <li>Clearly state research qn, research design, and reasoning behind model choice.</li> <li>Clearly state if analysis involves data snooping and how you have accounted for it.</li> <li>Report every hypothesis test you have performed relevant to the research question and highlight results that are robust across tests.</li> <li>Include a limitations section and point out any limitations and uncertainties in the analysis.</li> </ul>"},{"location":"CS_Electives/Machine_Learning/05_Statistics/","title":"Statistics","text":"<p>Statistical concepts such as Parameter estimation, Bias, Variance help in the aspects of generalization, over-fitting and under-fitting</p> <p>IID: Independent &amp; identically distributed</p>"},{"location":"CS_Electives/Machine_Learning/05_Statistics/#estimation-types","title":"Estimation Types","text":"Estimation Type Regression Output Classification Output Point \\(E(y \\vert X)\\) \\(E(c_i \\vert X)\\) Probabilistic \\(E(y \\vert X)\\)\\(\\sigma^2(y \\vert X)\\) \\(P(c_i \\vert X)\\)This is not the model confidence! This is the likelihood"},{"location":"CS_Electives/Machine_Learning/05_Statistics/#likelihood-vs-confidence","title":"Likelihood vs Confidence","text":"<ul> <li>Likelihood is the probability of classification being of a certain class</li> <li>Unreliable if input is unlike anything from training</li> <li>Confidence is the model\u2019s confidence that the likelihood is correct</li> </ul>"},{"location":"CS_Electives/Machine_Learning/05_Statistics/#idk","title":"IDK","text":"Expected deviation Bias from the true value Variance caused by any particular sample"},{"location":"CS_Electives/Machine_Learning/05_Statistics/#function-estimation","title":"Function Estimation","text":"<p>Estimation of relationship b/w input &amp; target variables, ie predict a variable \\(y\\) given input \\(x\\)</p> \\[ y = \\hat y + \\epsilon \\] <p>where \\(\\epsilon\\) is Bayes Error</p>"},{"location":"CS_Electives/Machine_Learning/05_Statistics/#statistical-learning-theory","title":"Statistical Learning Theory","text":"<p>Helps understand performance when we observe only the training set, through assumptions about training and test sets</p> <ul> <li> <p>i.i.d</p> </li> <li> <p>Training &amp; test data arise from same process</p> </li> <li>Observations in each data set are independent</li> <li>Training set and testing set are identically distributed</li> <li>\\(X\\) values are fixed in repeated sampling</li> <li> <p>No specification bias</p> </li> <li> <p>We need to use the correct functional form, which is theoretically consistent</p> </li> <li> <p>No Unbiasedness</p> </li> <li> <p>Independent vars should not be correlated with each other</p> </li> <li>If |correlation| &gt; 0.5 between 2 independent vars, then we drop one of the variables</li> <li> <p>High DOF</p> </li> <li> <p>Degree of freedom \\(= n - k\\), where</p> <ul> <li>\\(n =\\) number of observations</li> <li>\\(k =\\) no of independent variables</li> </ul> </li> <li>DOF \\(\\to\\) 0 leads to overfitting</li> <li> <p>High coefficient of variation in \\(X\\)</p> </li> <li> <p>We need more variation in values of \\(X\\)</p> </li> <li>Indian stock market is very volatile. But not in UAE; so it's hard to use it an independent var. Similarly, we cant use exchange rate in UAE as a regressor, as it is fixed to US dollars</li> <li>No variable interaction</li> <li> <p>Variable interaction: effect of \\(x_i\\) on \\(y\\) depends on \\(x_j\\)</p> </li> <li> <p>Solution: add interaction terms</p> </li> <li> <p>No collinearity</p> </li> <li>No multicollinearity</li> <li> <p>Homoskedasticity</p> </li> <li> <p>Constant variance</p> </li> <li> <p>\\(\\sigma^2 (y_i|x_i) = \\text{constant}\\) should be same \\(\\forall i\\)</p> </li> <li> <p>Causes of Heteroskedascity</p> </li> <li> <p>There is no measurement error \\(\\delta_i\\) in \\(X\\) or \\(Y\\)</p> </li> <li> <p>\\(X_\\text{measured} = X_\\text{true}\\)</p> </li> <li>\\(y_\\text{measured} = y_\\text{true}\\)</li> <li>\\(E(\\delta_i)=0\\)</li> <li>\\(\\text{var}(\\delta_i | x_i) = \\sigma^2 (\\delta_i|x_i) = \\text{constant}\\) should be same \\(\\forall i\\)</li> <li>\\(\\text{Cov}(\\delta_i, x_i) = 0, \\text{Cov}(\\delta_i, u_i) = 0\\)</li> </ul> <p>If there is measurement error, we need to perform correction</p> <ul> <li> <p>If there exists autocorrelation in time series, then we have to incorporate the lagged value of the dependent var as an explanatory var of itself</p> </li> <li> <p>For the Variance of distribution of potential outcomes, the range of distribution stays same over time</p> </li> <li> <p>\\(\\sigma^2 (x) = \\sigma^2(x-\\bar x)\\):   else, the variable is volatile; hard to predict; we cannot use OLS and hence have to use weighted regression</p> </li> <li> <p>if variance decreases, value of \\(y\\) is more reliable as training data</p> </li> <li> <p>if variance increases, value of \\(y\\) is less reliable as training data</p> <ul> <li>We use volatility modelling (calculating variance) to predict the pattern in variance</li> </ul> </li> </ul> <p>But rarely used in practice with deep learning, as</p> <ul> <li>bounds are loose</li> <li>difficult to determine capacity of deep learning algorithms</li> </ul>"},{"location":"CS_Electives/Machine_Learning/05_Statistics/#input-error","title":"Input Error","text":"<p>For higher order model, errors in \\(x\\) will look like heteroskedasticity</p>"},{"location":"CS_Electives/Machine_Learning/05_Statistics/#attenuation-bias","title":"Attenuation Bias","text":"<p>High measurement error \\(\\delta\\) and random noise \\(u\\) causes our estimated coefficients to be lower than the true coefficient</p> <p>Hence, for straight line model, error in \\(x\\)  will bias the OLS estimate of slope towards zero $$ \\begin{aligned} \\lim_{n \\to \\infty} \\hat \\beta &amp;= \\beta \\times \\text{SNR} \\ \\text{Signal-Noise Ratio: SNR} &amp;= \\dfrac{\\sigma<sup>2_x}{\\sigma</sup>2_x \\textcolor{hotpink}{+ \\sigma^2_u + \\sigma^2_\\delta}} \\end{aligned} $$</p>"},{"location":"CS_Electives/Machine_Learning/05_Statistics/#errors-in-measurement-correction","title":"Errors-in-Measurement Correction","text":"<p>This can be applied to</p> <ul> <li>any learning algorithm</li> <li>for regressors or response variables(s)</li> </ul> <p>Let\u2019s say true values of a regressor variable \\(X_1\\) was measured as \\(X_1^*\\) with measurement error \\(\\delta_1\\), where \\(\\delta_1 \\ne N(0, 1)\\). Here, we cannot ignore the error.</p>"},{"location":"CS_Electives/Machine_Learning/05_Statistics/#step-1-measurement-error","title":"Step 1: Measurement Error","text":"<p>Use an appropriate distribution to model the measurement error. Not necessary that the error is random.</p> <p>For eg, if we assume that the error is a skewed normal-distributed with variance \\(\\sigma^2_{X_1}\\) signifying the uncertainty.</p> \\[ \\delta_1 = N(\\mu_{X_1}, \\sigma^2_{X_1}, \\text{Skew}_{X_1}, \\text{Kurt}_{X_1}) \\]"},{"location":"CS_Electives/Machine_Learning/05_Statistics/#step-2-measurement","title":"Step 2: Measurement","text":"<p>Model the relationship between the error and the measured value.</p> <p>For eg, If we assume that the error is additive</p> \\[ \\begin{aligned} X_1^* &amp;= X_1 + \\delta_1 \\\\ \\implies X_1 &amp;= X_1^* \\textcolor{hotpink}{- \\delta_1} \\end{aligned} \\]"},{"location":"CS_Electives/Machine_Learning/05_Statistics/#step-3-model","title":"Step 3: Model","text":"<p>Since \\(X_1^*\\) is what we have, but we want the mapping with \\(X_1\\),</p> \\[ \\begin{aligned} \\hat y &amp;= f(X_1) \\\\ &amp;= f(X_1^* \\textcolor{hotpink}{- \\delta_1}) \\end{aligned} \\]"},{"location":"CS_Electives/Machine_Learning/05_Statistics/#example","title":"Example","text":"<p>Example: Modelling with linear regression using a regressor with measurement error $$ \\begin{aligned} \\implies \\hat y &amp;= \\theta_0 + \\theta_1 X_1 \\ &amp;= \\theta_0 + \\theta_1 (X_1^* - \\delta_1) \\ &amp;= \\theta_0 + \\theta_1 X_1^* \\textcolor{hotpink}{- \\theta_1 \\delta_1} \\end{aligned} $$</p>"},{"location":"CS_Electives/Machine_Learning/05_Statistics/#iia","title":"IIA","text":"<p>Independence of Irrelevant Alternatives</p> <p>The IIA property is the result of assuming that errors are independent of each other in a classification task</p> <p>The probability of \\(y = j\\) relative to \\(y = k\\) depends is not affected by the existence and the properties of other classes</p> <p>$$ \\begin{aligned} p(y=j \\vert x, z) &amp;= \\dfrac{ \\exp(\\beta_j x + \\textcolor{hotpink}{\\gamma_j z}) }{ \\sum_k^K \\exp(\\beta_k x + \\textcolor{hotpink}{\\gamma_k z} ) } \\ \\implies p(y=j \\vert x)  &amp;= \\int \\dfrac{ \\exp(\\beta_j x + \\textcolor{hotpink}{\\gamma_j z}) }{ \\sum_k^K \\exp(\\beta_k x + \\textcolor{hotpink}{\\gamma_k z} ) } f(z) \\cdot dz \\end{aligned} $$ where \\(\\gamma\\) is the effect of the other classes (substitutes/complementary goods)</p> <p>For IIA, \\(\\gamma=0\\)</p> <p>IIA property should be a desirable property for well-specified models</p> <ul> <li>the error for one alternative provides no information about the error for another alternative. This should be the property of a well-specified model such that the unobserved portion of utility is essentially \u201cwhite noise.</li> <li>However, when a model omits important unobserved variables that explain individual choice patterns, however, the errors can become correlated over alternatives</li> </ul>"},{"location":"CS_Electives/Machine_Learning/05_Statistics/#heteroskedascity","title":"Heteroskedascity","text":""},{"location":"CS_Electives/Machine_Learning/05_Statistics/#causes-of-heteroskedascity","title":"Causes of Heteroskedascity","text":"<ul> <li>Misspecified model</li> <li>If output is \\(\\bar y\\), but the sample size is different for each calculated mean</li> <li>\\(s_{\\bar y} = \\sigma_y/ \\sqrt{n}\\)</li> <li>Eg: Average income vs years of college</li> <li>Variance/standard error is relative to the \\(y\\)</li> <li>Eg: Precision of tool is relative to the observed value, such as weighing scale</li> <li>Variance has been experimentally determined for each \\(y\\) value</li> <li>Some distributions naturally have variance that is a function of the</li> <li>Mean: Poisson</li> <li>Mean &amp; Variance: Gamma</li> </ul>"},{"location":"CS_Electives/Machine_Learning/05_Statistics/#statistical-test","title":"Statistical Test","text":"<ul> <li>Sort residuals \\(u_i\\) wrt corresponding \\(\\vert y_i \\vert\\)</li> <li>Divide residuals (esr for fits) into \\(g\\) subgroups</li> <li>Test to see if sub-groups share same variance</li> <li>\\(H_0:\\) all groups have same variance</li> </ul> Distribution of statistic Null Hypothesis Formula Barlett Assumes normal distribution (sensitive to deviations from normality) \\(\\chi^2\\) distributed with \\((g-1)\\) DOF \\(k\\) sub-groups have equal variance \\(\\dfrac{(n-g) \\ln s^2_\\text{pool} - \\sum\\limits_{j=1}^g (n_j - 1) \\ln s^2_j }{ 1 + \\Big[ 1/[3(g-1)] \\Big] \\left[ \\Big( \\sum\\limits_{j=1}^g \\dfrac{1}{(n_j - 1)} \\Big) - \\dfrac{1}{n-g} \\right] }\\) Brown-Forsythe/Modified Levene compares deviations from median; it is robust to deviations from normality, but has lower power\\(n_j &gt; 25 \\quad \\forall j \\in k\\) \\(t\\) distribution with DOF = \\(n-g\\) Constant variance \\(\\dfrac{\\vert \\bar d_1 - \\bar d_2 \\vert}{s_\\text{pool} \\sqrt{\\dfrac{1}{n_1} + \\dfrac{1}{n_2}}}\\)\\(d_{ij}=\\vert x_{ij} - \\text{med}_j \\vert\\) White Test Perform linear regression of \\(u_i^2\\) with \\(x\\) and test \\(nR^2\\) as \\(X^2_{k-1}\\) Breusch-Pagan Variation of white test where \\(x\\) is replaced with any variable of interest Park Perform linear regression of \\(\\ln \\vert u_i^2 \\vert\\) vs \\(\\ln \\vert x \\vert\\) and test significance of slope different from 0 <p>where</p> <ul> <li>\\(n=\\) total number of data points</li> <li>\\(k=\\) number of subgroups</li> <li>\\(n_j=\\) sample size of \\(j\\)th sub-group</li> <li>\\(s^2_j =\\) variance of \\(j\\)th sub-group</li> <li>\\(s^2_\\text{pool} = \\dfrac{1}{n-k} \\sum\\limits_{j=1}^k (n_j - 1) s^2_j\\)</li> <li>\\(\\text{med}_j =\\) median of \\(j\\)th sub-group</li> </ul>"},{"location":"CS_Electives/Machine_Learning/05_Statistics/#correcting","title":"Correcting","text":"Dependence of variance on \\(y_i\\) Solution Known Weighted regression Data transformation Unknown GMM, generalized methods of moments estimation"},{"location":"CS_Electives/Machine_Learning/05_Statistics/#collinearity","title":"Collinearity","text":"<p>2 variables are correlated</p> <ul> <li>Can be inspected through correlation matrix of 2 variables</li> </ul>"},{"location":"CS_Electives/Machine_Learning/05_Statistics/#implication","title":"Implication","text":"<ul> <li>Adding/removing predictor variables changes the estimated effect of the vars (for eg: regression coefficients)</li> <li>Standard errors of coefficients become larger</li> <li>Individual regression coefficients may not be significant, even if the overall model is significant</li> <li>Some regression coefficients may be significantly different than expected (even opposing sign)</li> <li>There can be multiple solutions for \\(\\beta\\)</li> <li> <p>Both variables will be insignificant if both are included in the regression model</p> </li> <li> <p>Dropping one will likely make the other significant</p> </li> <li> <p>Hence we can\u2019t remove two (or more) supposedly insignificant predictors simultaneously: significance depends on what other predictors are included</p> </li> </ul>"},{"location":"CS_Electives/Machine_Learning/05_Statistics/#causes","title":"Causes","text":"Cause No data: Inappropriate sampling We only sample regions where predictors are correlated Inappropriate model If range of predictors is small: \\(r(x, x^2) \\ne 0\\) True Population Collinearity indeed exists in the true population (for eg, height and weight)"},{"location":"CS_Electives/Machine_Learning/05_Statistics/#multicollinearity","title":"Multicollinearity","text":"<p>Collinearity between 3 or more variables, even if no pair of variables are correlated</p> <p>eg: \\(r(x_1, x_2) = r(x_1, x_3) = 0\\), but \\(r(x_1, x_2+x_3) \\ne 0\\)</p>"},{"location":"CS_Electives/Machine_Learning/05_Statistics/#detection","title":"Detection","text":"Correlation matrix VIFVariance Inflation Factor How much is the variance of the \\(k\\)th model coefficient inflated compared to case of no inflation\\(\\text{VIF}(\\hat \\beta_j) = \\dfrac{1}{1 - R^2_{x_j \\vert x_{j'} }} = (\\tilde X^T \\tilde X)_{jj}^{-1}  \\\\ j' \\in [0, k] - \\{ j \\}\\)\\(R^2_{x_j \\vert x_{j'}}\\) is \\(R^2\\) when \\(x_j\\) is regressed against all other predictor vars\\(1/\\text{VIF}_{x_j \\vert x_{j'}}=\\) \u201ctolerance\u201d\\(\\text{VIF}_{x_j \\vert x_{j'}} \\ge 4 \\implies\\) Investigate\\(\\text{VIF}_{x_j \\vert x_{j'}} \\ge 10 \\implies\\) Act\\(E[\\text{VIF}_{x_j \\vert x_{j'}}] \\quad \\forall j &gt; 1 \\implies\\) Problematic Eigensystem Analysis Find eigenvalues of correlation matrix, ie \\(\\tilde X^T \\tilde X\\)If all eigenvalues are about the same magnitude, no multicollinearityElse calculate condition number\\(\\kappa = \\lambda_\\max/\\lambda_\\min\\)If \\(\\kappa &gt; 100 \\implies\\) problem"},{"location":"CS_Electives/Machine_Learning/05_Statistics/#solution","title":"Solution","text":"<ul> <li>Derive theoretical constraints relating input vars: helps simplify model; can be linear/non-linear</li> <li>If we only care about prediction, restrict scope of model for interpolation only, ie new inputs should coincide with range of predictor vars that exhibit the same pattern of multicollinearity</li> <li>Drop problematic variables, ie ones with highest VIF</li> <li>Collect more data that breaks pattern of multicollinearity</li> <li>Measure coefficients in separate experiment (then fix those coefficients)</li> <li>Regularization: Even for perfect multicollinearity, the ridge regression solution will always exist</li> <li>PCA</li> <li>Separates the high SE of coefficients from multicollinearity into components with low SE and high SE; you\u2019d only include the low SE components</li> <li>Helps identify unknown linear constraints</li> <li>Limitation: cannot help with non-linear relationship</li> </ul>"},{"location":"CS_Electives/Machine_Learning/06_Estimation/","title":"Estimation","text":"\\(\\theta\\) is thought of as Maximize Frequentist Unknown constant \\(p(D \\vert \\hat \\theta)\\)Likelihood Bayesian Unknown random variable with PDF \\(p(\\hat \\theta \\vert D)\\)"},{"location":"CS_Electives/Machine_Learning/06_Estimation/#mle","title":"MLE","text":"<p>Maximum Likelihood Estimation</p> <ol> <li>Predict a probability distribution \\(\\hat p(y \\vert x)\\)</li> <li>Get the likelihood of \\(\\hat p\\) wrt the data</li> <li>Update \\(\\hat p\\)\u00a0that maximizes the likelihood</li> <li>Go to step 1</li> </ol>"},{"location":"CS_Electives/Machine_Learning/06_Estimation/#idk","title":"IDK","text":"<p>To minimize the KL divergence between \\(\\hat p\\) and \\(p\\), and we maximize the likelihood of \\(\\hat p\\) $$ \\begin{aligned} \\arg \\min_{\\hat p} D_\\text{KL}(p \\vert \\vert \\hat p) &amp;= \\arg \\min_{\\hat p} E_{D \\sim p} \\ln \\left \\vert \\dfrac{\\hat p(x, y)}{p(x, y)} \\right \\vert \\ &amp;= \\arg \\min_{\\hat p} \\underbrace{E_{D \\sim p} \\ln p(x, y)}{\\mathclap {\\text{Constant}}} - \\underbrace{E} \\ln \\hat p(x, y){\\approx \\ln L(\\hat p),  n \\to \\infty} \\ &amp; \\approx \\arg \\min - L(\\hat p) = \\arg \\max_{\\hat p}  L(\\hat p) \\end{aligned} $$ Since we do not know \\(p\\), we can estimate \\(E_{D \\sim p} \\ln \\hat p(x, y)\\), using Monte-Carlo estimation and Law of Large numbers $$ E_{D \\sim p} \\ln \\hat p(x, y) {\\approx \\ln L(\\hat p),  n \\to \\infty} $$</p>"},{"location":"CS_Electives/Machine_Learning/06_Estimation/#likelihood","title":"Likelihood","text":"<p>Probability of observing data \\(x\\) according to pdf \\(p(x)\\)</p> \\[ \\begin{aligned} L(p) &amp;= Pr_q(x) \\\\ &amp;= \\prod_{i=1}^n p(x_i) \\\\ \\implies \\ln L(p) &amp;= \\sum_{i=1}^n \\ln p(x_i) \\\\ \\end{aligned} \\] \\[ \\begin{aligned} \\mathcal{L} &amp;= P(y_i \\vert x_i, \\hat \\theta) \\\\ &amp;= p(y_1, y_2, \\dots, y_n \\vert x_i,  \\hat \\theta) \\end{aligned} \\]"},{"location":"CS_Electives/Machine_Learning/06_Estimation/#estimation_1","title":"Estimation","text":"<p>Chooses a distribution \\(p(x)\\) that maximizes the (log) likelihood function for \\(x\\)</p> <p>Below example shows MLE for a single point</p> <p></p>"},{"location":"CS_Electives/Machine_Learning/06_Estimation/#mle-for-regression","title":"MLE for Regression","text":"<p>If we assume that the data is normally distributed, and we want unbiased prediction, then \\(u_i \\sim N(0, \\sigma^2_i)\\) $$ \\begin{aligned} \\mathcal{L} &amp;= P(u_1, u_2, \\dots, u_n \\vert \\hat \\theta) \\ &amp;= \\prod_i^n P(u_i) \\ &amp;= \\prod_i^n \\dfrac{1}{\\sqrt{2 \\pi \\sigma^2_i}} \\times \\exp \\left[\\dfrac{-1}{2} \\left(\\dfrac{u_i-\\mu_i}{\\sigma_i} \\right)^2 \\right]\\ &amp;= \\prod_i^n \\dfrac{1}{\\sqrt{2 \\pi \\sigma^2_i}} \\times \\exp \\left[\\dfrac{-1}{2} \\left(\\dfrac{u_i }{\\sigma_i} \\right)^2 \\right] &amp; (\\mu_u=0) \\end{aligned} $$</p> <p>$$ \\begin{aligned} \\ln \\vert \\mathcal{L} \\vert &amp;= \\dfrac{-1}{2} \\Big[ n \\ln \\vert 2 \\pi \\sigma^2_i \\vert + \\chi^2 \\Big] \\ \\implies -2 \\ln \\vert \\mathcal{L} \\vert &amp;= n \\ln \\vert 2 \\pi \\sigma^2_i \\vert + \\chi^2 \\</p> <p>&amp;= n \\ln \\vert 2 \\pi \\vert + n \\ln \\vert \\text{MSE} \\vert + n + \\sum_i^n \\ln \\vert w_i \\vert \\ &amp;\\approx n \\ln \\vert \\text{MSE} \\vert \\end{aligned} $$</p>"},{"location":"CS_Electives/Machine_Learning/06_Estimation/#optimization","title":"Optimization","text":"\\[ \\begin{aligned} \\implies \\max(\\mathcal{L}) &amp; \\propto \\min (-2 \\ln \\mathcal{L}) \\\\ &amp; \\propto \\min (\\chi^2_{n-k, 0}) \\\\ \\implies \\mathcal{L} &amp; \\propto e^{-\\chi^2_{n-k, 0}} \\end{aligned} \\] <p>By setting derivative to 0, we can also use this to derive expression for Matrix Normal Expression</p>"},{"location":"CS_Electives/Machine_Learning/06_Estimation/#note","title":"Note","text":"<p>$$ \\begin{aligned} E[\\chi^2] &amp;= n-p \\</p> <p>\\implies E[-2 \\ln \\vert \\mathcal{L} \\vert] &amp;= n \\ln \\vert 2 \\pi \\sigma^2_i \\vert + (n-p) \\end{aligned} $$</p>"},{"location":"CS_Electives/Machine_Learning/06_Estimation/#mle-for-classification","title":"MLE for Classification","text":"<p>Assume that \\(y \\sim \\text{Bernoulli}(p)\\) $$ \\mathcal L = $$</p>"},{"location":"CS_Electives/Machine_Learning/06_Estimation/#m-estimation","title":"M-Estimation","text":"<p>Minimize some other loss function weighted compare to MLE, such as MAE, Huber, etc</p>"},{"location":"CS_Electives/Machine_Learning/06_Estimation/#bayesian","title":"Bayesian","text":"<p>When is it justified?</p> <ul> <li>Prior is valid: Better than MLE</li> <li>Prior is irrelevant: Just a computational catalyst</li> </ul> \\[ \\begin{aligned} \\underbrace{P(\\hat f = f \\vert D)}_{\\mathclap{\\text{Posterior Distribution} \\qquad}} &amp;= \\frac{     \\overbrace{P(D \\vert \\hat f=f)}^{\\mathclap{\\text{Likelihood} }}     \\times     \\overbrace{P(\\hat f = f)}^{\\mathclap{\\qquad \\quad \\text{Prior Distribution}}} }{     \\underbrace{P(D)}_{\\mathclap{\\qquad \\text{Normalizing constant}}} } \\\\ &amp; \\propto P(D \\vert \\hat f=f) \\times P(\\hat f=f) \\end{aligned} \\] <ul> <li>\\(D = y \\vert x\\)</li> <li>\\(P(\\hat f = f)\\) is the prior belief of our understanding of</li> <li>Distribution of model parameters</li> <li>Distribution of residuals</li> </ul> <p>Usually we need to solve Bayes\u2019 equation numerically using MCMC: Markov Chain Monte Carlo sampling. Result is set of points from posterior distribution that we summarize</p> <p>Disadvantage: We need to calculate a lot of probabilities -&gt; Computationally-expensive</p> Hypothesis Maximum Likelihood Model that best explains the training data \\(h_\\text{ML} = \\underset{h_i \\in H}{\\arg \\max} \\ P(D \\vert  h_i)\\) Maximum A Posteriori Probability Model that is most probable given the training data \\(h_\\text{MAP} = something\\)"},{"location":"CS_Electives/Machine_Learning/06_Estimation/#steps","title":"Steps","text":"<ol> <li>Pick prior distribution</li> <li>Calculate likelihood function, similar to MLE</li> <li>Calculate posterior distribution, usually numerically</li> <li>Summarize posterior distribution</li> <li>MAP estimate</li> <li>Credible interval</li> </ol>"},{"location":"CS_Electives/Machine_Learning/06_Estimation/#prior-distribution","title":"Prior Distribution","text":"<p>\\(P(\\theta) = P(\\theta \\vert I)\\), where \\(I=\\) all info we have before data collection</p> Prior Uninformative/Objective/Baseline If we have no prior knowledge, then \\(P(\\theta \\vert I)=\\) constantHence, \\(P(\\hat \\theta \\vert D) = P(D \\vert \\hat \\theta)\\), so might as well perform MLE insteadeg: Uniform dist over expected range of possible values Informative/Substantive Based on previous data, experiments, knowledgeOne can assume that the prior for each parameter is independent of others \\(P(\\theta)=P(\\beta) P(\\sigma^2)\\), but usually a joint dist is requiredSetting prior to delta function fixes parameter independent of data (never done in practice, as it ignores the point of data) <p></p>"},{"location":"CS_Electives/Machine_Learning/06_Estimation/#reparametrization","title":"Reparametrization","text":"<p>Helps make prior assignment easy</p> <p>For eg: $$ \\begin{aligned} \\hat y_i &amp;= \\beta_0 + \\beta_1 x_i \\ \\implies \\hat y_i &amp;= \\beta_0' + \\beta_1 (x_i - \\bar x) \\ \\ \\text{Hence }  \\beta_0 &amp;= \\hat y_i \\vert (x_i=0) \\ \\implies \\beta_0' &amp;= \\hat y_i \\vert (x_i = \\bar x) \\end{aligned} $$</p> <ul> <li>This makes it easier to specify the prior for the intercept</li> <li>We can assume that \\(\\beta_0'\\) is independent of \\(\\beta_1\\)\u2019s prior</li> </ul>"},{"location":"CS_Electives/Machine_Learning/06_Estimation/#conjugate-priors","title":"Conjugate Priors","text":"<p>Special cases of priors only for which analytical solutions of the posterior distribution are possible with given likelihood distribution</p> <p>For eg:</p> <ul> <li>iid normal errors</li> <li>conjugate prior for \\(\\beta\\) is normal</li> <li>conjugate prior for \\(\\sigma^2_u\\) is inverse gamma</li> </ul>"},{"location":"CS_Electives/Machine_Learning/06_Estimation/#posterior-distribution","title":"Posterior Distribution","text":"<p>Output of Bayesian estimation is not best fit parameters, it is the posterior distribution: probability distribution for each parameter and \\(\\sigma_e\\)</p> <p>We need to summarize the distribution</p> <ul> <li>Best estimate: Summary statistic such as</li> <li>mode (Maximum a posteriori)</li> <li>mean</li> <li>median</li> <li>etc</li> <li>Credible interval: Quantiles</li> </ul> <p>Posterior distribution describes how much the data has changed our prior beliefs</p> <p></p>"},{"location":"CS_Electives/Machine_Learning/06_Estimation/#bernstein-von-mises-theorem","title":"Bernstein-von Mises Theorem","text":"<p>For very large \\(n\\), posterior distribution becomes independent of the prior distribution, as long as the prior \\(\\not \\in \\{0, 1 \\}\\)</p> <p>Posterior tends towards normal distribution equal to MLE (assuming iid)</p>"},{"location":"CS_Electives/Machine_Learning/06_Estimation/#problems","title":"Problems","text":"<ul> <li>Computationally-expensive</li> <li>Choosing appropriate prior</li> <li>Is it reasonable to treat every parameter as a random variable</li> </ul>"},{"location":"CS_Electives/Machine_Learning/06_Estimation/#mle-bayesian","title":"MLE + Bayesian","text":""},{"location":"CS_Electives/Machine_Learning/06_Estimation/#relationship","title":"Relationship","text":"<p>MLE = Bayesian with Jeffreys Prior</p>"},{"location":"CS_Electives/Machine_Learning/06_Estimation/#jeffreys-prior","title":"Jeffreys Prior","text":"\\[ P(\\beta, \\sigma^2_u) \\propto 1/\\sigma^2_u \\] <p>It is improper as it adds upto \\(\\infty\\), not \\(1\\)</p> <p>The resulting posterior distribution is \\(t\\) distributed about MLE parameter estimates</p>"},{"location":"CS_Electives/Machine_Learning/06_Estimation/#idk_1","title":"IDK","text":"<p>Taking \\(-\\ln\\) of Bayes\u2019 equation $$ \\begin{aligned} - \\ln p(\\hat \\theta \\vert D) &amp;= - \\ln L - \\ln p(\\hat \\theta) + \\underbrace{\\ln p(D)}\\text{constant} \\ \\min { - \\ln p(\\hat \\theta \\vert D) } &amp;= \\min { - \\ln L - \\ln p(\\hat \\theta) + \\cancel{\\ln p(D)} } \\ &amp;= \\min { \\chi^2 + \\sum \\left( \\dfrac{\\hat \\beta - \\mu_{\\beta}}{\\sigma_\\beta} \\right)^2 } \\ &amp;= \\text{Regularized Regression} \\end{aligned} $$}^{k</p>"},{"location":"CS_Electives/Machine_Learning/06_Estimation/#likelihood_1","title":"Likelihood","text":"<pre><code>def ll(X, y, pred):\n    # return log likelihood\n\n        mse = np.mean(\n      (y - pred)\n      **2\n    )\n\n    n = float(X.shape[0])\n    n_2 = n/2\n\n    return -n_2*np.log(2*np.pi) - n_2*np.log(mse) - n_2\n\ndef aic(X, y, pred):\n    p = X.shape[1]\n\n    return -2*ll(X, y, pred) + 2*p\n\nprint(aic(X, y, pred))\n</code></pre>"},{"location":"CS_Electives/Machine_Learning/07_Information_Theory/","title":"Information Theory","text":""},{"location":"CS_Electives/Machine_Learning/07_Information_Theory/#entropy","title":"Entropy","text":"<p>Entropy, as it relates to machine learning, is a measure of the randomness in the information being processed. The higher the entropy, the harder it is to draw any conclusions from that information. </p> <p>Information entropy is developed to describe the avg amount of info needed to specify the state of a RV</p> <p>Quantifies how much new information about a RV \\(x\\) is obtained when we observe a specific value \\(x_i\\)</p> <ul> <li>Depends on \u2018degree of surprise\u2019: highly improbable value conveys more information than likely one</li> <li>If we know an event is certain to happen, we would receive no information when we actually observe it happening</li> </ul> <p>Let \\(h(x)\\) denote the information content of an event \\(x\\), such that</p> <ul> <li>\\(p(x) \\propto \\dfrac{1}{p(x)}\\)</li> <li>For 2 independent events \\(A\\) and \\(B\\)</li> </ul> \\[ \\begin{aligned} p(A \\land B) &amp;= p(A) \\cdot p(B) \\\\ \\implies h(AB) &amp;= h(A) + h(B) \\end{aligned} \\] \\[ h(x) = \\log \\left \\vert \\dfrac{1}{p(x)} \\right \\vert \\]"},{"location":"CS_Electives/Machine_Learning/07_Information_Theory/#shannoninformation-entropy","title":"Shannon/Information Entropy","text":"<p>Entropy of a probability distribution \\(p(x)\\) is the average amount of information transmitted by discrete random variable \\(x\\) $$ \\begin{aligned} H(p) &amp;= E_p[h(x)] \\ &amp;= \\sum_x p(x) \\log \\left \\vert \\dfrac{1}{p(x)} \\right \\vert \\end{aligned} $$</p> <ul> <li>Distributions with sharp peaks around a few values will have a relatively low entropy</li> <li>Those spread evenly across many values will have higher entropy</li> </ul> <p>If we use \\(\\ln\\) instead of \\(\\log\\) for \\(H(p)\\), then \\(H(p)\\) is a lower bound on the avg no of bits needed to encode a RV with pdf \\(p\\). Achieving this bound requires using an optimal coding scheme designed for \\(p\\), which assigns</p> <ul> <li>shorter codes to higher probability events</li> <li>longer codes to less probable events</li> </ul>"},{"location":"CS_Electives/Machine_Learning/07_Information_Theory/#cross-entropy","title":"Cross Entropy","text":"<p>The average amount of information needed to specify \\(x\\) as a result of using pdf \\(q(x)\\) instead of true \\(p(x)\\)</p> <p>If we use \\(\\ln\\) instead of \\(\\log\\), it is the average no of bits needed to encode a RV using a coding scheme designed for \\(q(x)\\) instead of true \\(p(x)\\) $$ \\begin{aligned} H(p, q) &amp;= E_p \\left[ \\log \\left \\vert \\dfrac{1}{q(x)} \\right \\vert \\right] \\ &amp;= \\sum_x p(x) \\log \\left \\vert \\dfrac{1}{q(x)} \\right \\vert \\end{aligned} $$</p>"},{"location":"CS_Electives/Machine_Learning/07_Information_Theory/#kl-divergence","title":"KL Divergence","text":"<p>Dissimilarity measure of 2 probability distributions</p> <p>Represents the avg additional info required to  specify \\(x\\) due to using \\(q(x)\\) instead of true \\(p(x)\\) $$ \\begin{aligned}  D_\\text{KL}(p \\vert \\vert q) &amp;= \\text{Cross Entropy} - \\text{Entropy} \\ &amp;= H(p, q) - H(p) \\ &amp; = \\sum_x \\log \\left \\vert \\dfrac{p(x)}{q(x)} \\right \\vert \\cdot p(x) &amp; \\text{(Discrete)} \\ &amp; = \\int_x \\log \\left \\vert \\dfrac{p(x)}{q(x)} \\right \\vert \\cdot p(x) &amp; \\text{(Continuous)} \\end{aligned} $$ Propreties</p> <ul> <li>\\(D_\\text{KL}(p \\vert \\vert q) = 0 \\iff p=q\\)</li> <li>KL divergence is asymmetric \\(\\implies D_\\text{KL}(p \\vert \\vert q) \\ne D_\\text{KL}(q \\vert \\vert p)\\). Hence it is not a proper distance measure</li> <li>Non-Negative: \\(D_\\text{KL}(p \\vert \\vert q) \\ge 0\\)</li> </ul> <p>Given a fixed distribution \\(p\\), optimizing for a \\(q\\) for the following 3 goals are equivalent</p> <ul> <li>minimize \\(D_\\text{KL}(p \\vert \\vert q)\\)</li> <li>minimize \\(H(p, q)\\)</li> <li>maximize \\(L(q \\vert x)\\), where \\(x \\sim p(x)\\)</li> </ul>"},{"location":"CS_Electives/Machine_Learning/08_Cost/","title":"Cost Function","text":""},{"location":"CS_Electives/Machine_Learning/08_Cost/#error","title":"Error","text":"<p>Using MLE, we define error as</p> \\[ u_i = \\hat y_i - y_i \\\\ u_i' = \\dfrac{u_i}{\\sigma_{yi}} \\] <p>Usually we expect that \\(\\sigma_{yi}=1\\) and without any measurement noise</p> <p>Bayes\u2019 Error is the error incurred by an ideal model, which is one that makes predictions from true distribution \\(P(x,y)\\); even such a model incurs some error due to noise/overlap in the distributions</p>"},{"location":"CS_Electives/Machine_Learning/08_Cost/#total-regression","title":"Total Regression","text":"<p>Total Least Squares</p> <p>Types</p> <ul> <li>Deming Regression</li> <li>Orthogonal regression</li> <li>Geometric mean</li> <li>Method of moments</li> <li>Full total regression</li> </ul> <p>Useful for when data has noise due to</p> <ul> <li>Measurement error</li> <li>Need for privacy etc, such as when conducting a salary survey.</li> </ul> \\[ \\begin{aligned} L &amp;= \\left( \\dfrac{u_i}{\\sigma_{yi}} \\right)^2  + \\lambda \\left( \\dfrac{\\hat x_i - x_i}{\\sigma_{xi}} \\right)^2 \\\\ \\lambda &amp;= \\dfrac{\\sigma^2(\\text{known measurement error}_x)}{\\sigma^2(\\text{known measurement error}_y)} \\\\ \\hat y_i &amp;= \\hat \\beta_0 + \\hat \\beta_1 x_i \\\\ \\hat x_i &amp;= \\dfrac{y_i - \\beta_0}{\\beta_1} \\\\ \\text{or} \\\\ \\hat x_i &amp;= x_i + \\dfrac{y_i - \\hat y_i}{\\partial f/\\partial x_i} \\dfrac{(\\partial f/\\partial x_i)^2 \\sigma^2_{x_i}}{\\sqrt{1+\\sigma^2_{y_i, \\text{eff}}}} \\\\ \\sigma^2_{y_i, \\text{eff}} &amp;= \\sigma^2_{y_i} + \\left( \\dfrac{\\partial f}{\\partial x_i} \\right)^2 \\end{aligned} \\] <p>How to get \\(\\partial f/\\partial x_i\\)?</p> <ol> <li>Run regression ignoring \\(\\sigma_{xi}\\)</li> <li>Use this model fit to calculate \\(\\partial f/\\partial x_i \\quad \\forall i\\) </li> <li>Calculate the effective variance \\(\\sigma^2_{yi, \\text{eff}} \\quad \\forall i\\)</li> <li>Run weighted regression using 1/effective variance to weight \\(y_i\\)</li> <li>Repeated steps until parameters converge (usually takes 1-2 iterations)</li> </ol> Measurement Error of Regressor \\(\\lambda\\) 0 0 OLS Same as Response 1 Orthogonal <p></p> <p></p>"},{"location":"CS_Electives/Machine_Learning/08_Cost/#orthogonal-regression","title":"Orthogonal Regression","text":"<ul> <li>\\(\\sigma_x = \\sigma_y\\)</li> <li>Applied when you measure the same quantity with 2 different methods: for tool matching, calibration curves</li> <li>For straight-line model, \\(\\sigma^2_{y_i, \\text{eff}} \\propto (1 + {\\beta_1}^2)\\)</li> </ul> \\[ \\begin{aligned} \\sigma^2_{y_i, \\text{eff}} &amp;= \\sigma^2_y \\left[ 1 + \\left(\\dfrac{\\partial f}{\\partial x_i}\\right)^2 \\\\ \\right] \\\\ \\chi^2 &amp;= \\sum_{i=1}^n \\left[ \\dfrac{u_i}{\\sqrt{1+{\\beta_1}^2}} \\right]^2 \\end{aligned} \\]"},{"location":"CS_Electives/Machine_Learning/08_Cost/#geometric-mean-regression","title":"Geometric Mean Regression","text":"<ul> <li>Estimate slope as geometric mean of OLS slopes from \\(y \\vert x\\)  and \\(x \\vert y\\)</li> <li>\\(\\beta_1 = \\dfrac{s_y}{s_y}\\)</li> </ul>"},{"location":"CS_Electives/Machine_Learning/08_Cost/#method-of-moments","title":"Method of Moments","text":"<p>If we know measurement error in \\(x:\\) \\(\\sigma_{\\delta_x}\\)</p> <p>Only good when \\(n&gt;50\\)</p> \\[ \\begin{aligned} \\beta_1' &amp;= \\dfrac{\\beta_1}{1 - \\left(\\dfrac{s_{\\delta_x}}{s_x} \\right)^2} \\\\ \\text{SE}(\\beta_1') &amp;= \\dfrac{\\beta_1}{\\sqrt{n}} \\sqrt{\\dfrac{(s_x s_y)^2 + 2 (\\beta_1 s^2_{\\delta_x})^2}{(s_{xy})^2} - 1} \\end{aligned} \\]"},{"location":"CS_Electives/Machine_Learning/08_Cost/#deming-regression","title":"Deming Regression","text":"<p>OLS of \\(y_i \\vert \\hat x_i\\) produces same fit as Deming of \\(y_i \\vert x_i\\)</p> <p>Assumes that there is no model error: all uncertainty in \\(x\\) and \\(y\\) is due to measurement</p> <p>If Deming Regression and Method of Moments give different estimates, then the model specification may be incorrect</p>"},{"location":"CS_Electives/Machine_Learning/08_Cost/#mse-vs-chi-squared","title":"MSE vs Chi-Squared","text":"<ul> <li>MSE(\\(u_i\\)) may/may not \\(= \\chi^2_\\text{red}\\)</li> <li>MSE(\\(u_i'\\)) \\(= \\chi^2_\\text{red}\\)</li> </ul>"},{"location":"CS_Electives/Machine_Learning/08_Cost/#loss-functions-mathcal-ltheta","title":"Loss Functions \\({\\mathcal L}(\\theta)\\)","text":"\\[ \\text{Loss}_i = {\\mathcal L}(\\theta, u_i) \\] <ul> <li>Penalty for a single point (absolute value, squared, etc)</li> <li>Should always be tailor-made for each problem, unless impossible</li> <li>Need not be symmetric</li> <li>Regression: Under-prediction and over-prediction can be penalized differently</li> <li>Classification: False negative and false-positive can be penalized differently</li> </ul>"},{"location":"CS_Electives/Machine_Learning/08_Cost/#properties-of-loss-function","title":"Properties of Loss Function","text":"Non-negativity \\({\\mathcal L}(u_i) \\ge 0, \\quad \\forall i\\) No penalty for no error \\({\\mathcal L}(0)=0\\) Monoticity \\(\\vert u_i \\vert &gt; \\vert u_j \\vert \\implies {\\mathcal L}(u_i) &gt; {\\mathcal L}(u_j)\\) Differentiable Continuous derivative Symmetry \\({\\mathcal L}(-u_i)={\\mathcal L}(+u_i)\\) Not always required for custom loss functions"},{"location":"CS_Electives/Machine_Learning/08_Cost/#weighted-loss","title":"Weighted Loss","text":"<p>Related to weighted regression $$ {\\mathcal L}'(\\theta) = {\\mathcal L}(w_i \\theta) $$</p> \\[ \\begin{aligned} u'_i &amp;= u_i \\times \\sqrt[a]{w_i} \\\\ \\text{SE}(u') &amp;= s_{u'} \\\\ &amp;= \\sqrt{\\dfrac{\\sum_{i=1}^n w_i (u_i)^2}{n-k}} \\\\ &amp;= \\sqrt{\\dfrac{\\sum_{i=1}^n (u_i')^2}{n-k}} \\end{aligned} \\] <p>where</p> <ul> <li>\\({\\mathcal J}(\\theta)\\) = usual loss function</li> <li>\\(a=\\) exponent of the loss function (square, etc)</li> </ul> Goal: Address Action Prefer Comment Asymmetry of importance \\(w_i = \\Big( \\text{sgn}(u_i) - \\alpha \\Big)^2 \\\\ \\alpha \\in [-1, 1]\\) \\(\\begin{cases} \\text{Under-estimation}, &amp; \\alpha &lt; 0 \\\\ \\text{Over-estimation}, &amp; \\alpha &gt; 0 \\end{cases}\\) Observation ErrorMeasurement/ProcessHeteroskedasticity \\(\\sigma^2_{yi}\\)where \\(\\sigma^2_{y_i}\\) is the uncertainty associated with each observation Observations with low uncertainty Maximum likelihood estimation Input ErrorMeasurement/Process \\(w_i = \\dfrac{1}{\\sigma^2_{X}}\\)where \\(\\sigma^2_X\\) is the uncertainty associated Observations with high input measurement accuracy Observation Importance \\(w_i=\\) Importance Observations with high domain-knowledge importance For time series data, you can use \\(w_i = \\text{Radial basis function(t)}\\)- \\(\\mu = t_\\text{max}\\)- \\(\\sigma^2 = (t_\\text{max} - t_\\text{min})/2\\)"},{"location":"CS_Electives/Machine_Learning/08_Cost/#regression-loss","title":"Regression Loss","text":"Metric \\({\\mathcal L}(u_i)\\) Optimizing gives __ of conditional distribution Preferred Value Unit Range Signifies Advantages\u2705 Disadvantages\u274c Comment \\(\\alpha\\) of advanced family Breakdown Point Efficiency Indicator/Zero-One/Misclassification \\(\\begin{cases} 0, &amp; u_i = 0 \\\\ 1, &amp; \\text{o.w} \\end{cases}\\) Mode BE(Bias Error) \\(u_i\\) \\(\\begin{cases} 0, &amp; \\text{Unbiased} \\\\ &gt;0, &amp; \\text{Over-prediction} \\\\ &lt;0, &amp; \\text{Under-pred} \\end{cases}\\) Unit of \\(y\\) \\((-\\infty, \\infty)\\) Direction of error biasTendency to overestimate/underestimate Cannot evaluate accuracy, as equal and opposite errors will cancel each other, which may lead to non-optimal model having error=0 L1/AE(Absolute Error)/Manhattan distance \\(\\vert  u_i  \\vert\\) Median \\(\\downarrow\\) Unit of \\(y\\) \\([0, \\infty)\\) Robust to outliers Not differentiable at origin, which causes problems for some optimization algoThere can be multiple optimal fitsDoes not penalize large deviations MLE for \\(\\chi^2\\) for Laplacian dist \\(74 \\%\\)??? L2/SE (Squared Error)/Euclidean distance \\({u_i}^2\\) Mean \\(\\downarrow\\) Unit of \\(y^2\\) \\([0, \\infty)\\) Variance of errors with mean as MDEMaximum log likelihood Penalizes large deviations Sensitive to outliers MLE for \\(\\chi^2\\) for normal dist \\(\\approx 2\\) \\(1/n\\) \\(100 \\%\\) L3/Smooth L1/Pseudo-Huber/Charbonnier \\(\\begin{cases} \\dfrac{u_i^2}{2 \\epsilon}, &amp; \\vert u_i \\vert &lt; \\epsilon \\\\ \\vert u_i \\vert - \\dfrac{\\epsilon}{2}, &amp; \\text{o.w} \\end{cases}\\) \\(\\downarrow\\) \\([0, \\infty)\\) Balance b/w L1 &amp; L2Prevents exploding gradientsRobust to outliers Piece-wise combination of L1&amp;L2 1 Huber \\(\\begin{cases} \\dfrac{u_i^2}{2}, &amp; \\vert u_i \\vert &lt; \\epsilon \\\\ \\lambda \\vert u_i \\vert - \\dfrac{\\lambda^2}{2}, &amp; \\text{o.w} \\end{cases}\\)\\((\\lambda_\\text{rec} = 1.345 \\times \\text{MAD}_u)\\) \\(\\downarrow\\) \\([0, \\infty)\\) Same as Smooth L1 Computationally-expensive for large datasets\\(\\epsilon\\) needs to be optimizedOnly first-derivative is defined Piece-wise combination of L1&amp;L2\\(H = \\epsilon \\times {L_1}_\\text{smooth}\\)Solution behaves like a trimmed mean: (conditional) mean of two (conditional) quantiles \\(95 \\%\\) Log Cosh \\(\\Big\\{ \\log \\big( \\ \\cosh (u_i) \\ \\big) \\Big \\} \\\\ \\approx \\begin{cases} \\dfrac{{u_i}^2}{2}, &amp; \\vert u_i \\vert \\to 0 \\\\ \\vert u_i \\vert - \\log 2, &amp; \\vert u_i \\vert \\to \\infty \\end{cases}\\) \\(\\downarrow\\) Same as Smooth L1Doesn\u2019t require hyperparameter tuningDouble differentiable Cauchy/Lorentzian \\(\\dfrac{\\epsilon^2}{2} \\times \\log \\Big[ 1 + \\left( \\dfrac{{u_i}}{\\epsilon} \\right)^2 \\Big]\\) \\(\\downarrow\\) Same as Smooth L1 Not convex 0 Log-Barrier \\(\\begin{cases} - \\epsilon^2 \\times \\log \\Big(1- \\left(\\dfrac{u_i}{\\epsilon} \\right)^2 \\Big) , &amp; \\vert u_i \\vert \\le \\epsilon \\\\ \\infty, &amp; \\text{o.w} \\end{cases}\\) Solution not guaranteed Regression loss \\(&lt; \\epsilon\\), and classification loss further \\(\\epsilon\\)-insensitive \\(\\begin{cases} 0, &amp; \\vert u_i \\vert \\le \\epsilon \\\\ \\vert u_i \\vert - \\epsilon, &amp; \\text{otherwise} \\end{cases}\\) Non-differentiable Bisquare/Welsch/Leclerc \\(\\begin{cases} \\dfrac{\\lambda^2}{6 \\left(1- \\left[1-\\left( \\dfrac{u_i}{\\lambda} \\right)^2 \\right]^3 \\right)}, &amp; \\vert u_i \\vert &lt; \\lambda \\\\ \\dfrac{\\lambda^2}{6}, &amp; \\text{o.w} \\end{cases}\\)\\(\\lambda_\\text{rec} = 4.685 \\times \\text{MAD}_u\\) \\(\\downarrow\\) Robust to outliers Suffers from local minima (Use huber loss output as initial guess) Ignores values after a certain threshold \\(\\infty\\) Geman-Mclure -2 Quantile/Pinball \\(\\begin{cases} q \\vert u_i \\vert , &amp; \\hat y_i &lt; y_i \\\\ (1-q) \\vert u_i \\vert, &amp; \\text{o.w} \\end{cases}\\)\\(q = \\text{Quantile}\\) Quantile \\(\\downarrow\\) Unit of \\(y\\) Robust to outliers Computationally-expensive Expectile \\(\\begin{cases} e (u_i)^2 , &amp; \\hat y_i &lt; y_i \\\\ (1-e) (u_i)^2, &amp; \\text{o.w} \\end{cases}\\)\\(e = \\text{Expectile}\\) Expectable \\(\\downarrow\\) Unit of \\(y^2\\) Expectiles are a generalization of the expectation in the same way as quantiles are a generalization of the median APE(Absolute Percentage Error) \\(\\left \\lvert  \\dfrac{ u_i }{y_i}  \\right \\vert\\) \\(\\downarrow\\) % \\([0, \\infty)\\) Easy to understandRobust to outliers Explodes when \\(y_i \\approx 0\\)Division by 0 error when \\(y_i=0\\)Asymmetric: \\(\\text{APE}(\\hat y, y) \\ne \\text{APE}(y, \\hat y) \\implies\\) Penalizes over-prediction more than under-predictionSensitive to measurement units SMAPESymmetric APE \\(\\left \\lvert  \\dfrac{ u_i }{\\hat y_i + y_i}  \\right \\vert\\) Denominator is meant to be mean(\\(\\hat y, y\\)), but the 2 is cancelled for appropriate range SSE(Squared Scaled Error) \\(\\dfrac{ 1 }{\\text{SE}(y_\\text{naive}, y)} \\cdot u_i^2\\) \\(\\downarrow\\) % \\([0, \\infty)\\) ASE(Absolute Scaled Error) \\(\\dfrac{ 1 }{\\text{AE}(y_\\text{naive}, y)} \\cdot \\vert u_i \\vert\\) \\(\\downarrow\\) % \\([0, \\infty)\\) ASE Modified \\(\\left \\lvert \\dfrac{ u_i }{y_\\text{naive} - y_i} \\right \\vert\\) \\(\\downarrow\\) % \\([0, \\infty)\\) Explodes when \\(\\bar y - y_i \\approx 0\\)Division by 0 error when \\(\\bar y - y_i \\approx 0\\) WMAPE(Weighted Mean Absolute Percentage Error) \\(\\dfrac{1}{n} \\left(\\dfrac{ \\sum \\vert  u_i \\vert  }{\\sum \\vert  y_i  \\vert}\\right)\\) \\(\\downarrow\\) % \\([0, \\infty)\\) Avoids the  limitations of MAPE Not as easy to interpret MALE \\(\\vert \\ \\log_{1p} \\vert \\hat y_i \\vert - \\log_{1p} \\vert y_i \\vert \\ \\vert\\) MSLE(Log Error) \\((\\log_{1p} \\vert \\hat y_i \\vert - \\log_{1p} \\vert y_i \\vert)^2\\) \\(\\downarrow\\) Unit of \\(y^2\\) \\([0, \\infty)\\) Equivalent of log-transformation before fitting - Robust to outliers- Robust to skewness in response distribution- Linearizes relationship - Penalizes under-prediction more than over-prediction- Penalizes large errors very little, even lesser than  MAE (still larger than small errors, but weight penalty inc very little with error)- Less interpretability RAE(Relative Absolute Error) \\(\\dfrac{\\sum \\vert  u_i \\vert}{\\sum \\vert y_\\text{naive} - y_i  \\vert}\\) \\(\\downarrow\\) % \\([0, \\infty)\\) Scaled MAE RSE(Relative Square Error) \\(\\dfrac{\\sum  (u_i)^2 }{\\sum (y_\\text{naive} - y_i)^2 }\\) \\(\\downarrow\\) % \\([0, \\infty)\\) Scaled MSE Peer Loss \\({\\mathcal L}(\\hat y_i \\vert x_i, y_i) - L \\Big(y_{\\text{rand}_j} \\vert x_i, y_{\\text{rand}_j} \\Big)\\)\\({\\mathcal L}(\\hat y_i \\vert x_i, y_i) - L \\Big(\\hat y_j \\vert x_k, y_j \\Big)\\)Compare loss of actual prediction with predicting a random value Actual information gain Penalize overfitting to noise Winkler score \\(W_{p, t}\\) \\(\\dfrac{Q_{\\alpha/2, t} + Q_{1-\\alpha/2, t}}{\\alpha}\\) \\(\\downarrow\\) CRPS(Continuous Ranked Probability Scores) \\(\\overline Q_{p, t}, \\forall p\\) \\(\\downarrow\\) CRPS_SS(Skill Scores) \\(\\dfrac{\\text{CRPS}_\\text{Naive} - \\text{CRPS}_\\text{Method}}{\\text{CRPS}_\\text{Naive}}\\) \\(\\downarrow\\) <p>Robust estimators are only robust to non-influential outliers</p>"},{"location":"CS_Electives/Machine_Learning/08_Cost/#outlier-sensitivity","title":"Outlier Sensitivity","text":""},{"location":"CS_Electives/Machine_Learning/08_Cost/#advanced-loss","title":"Advanced Loss","text":"\\[ {\\mathcal L}(u, \\alpha, c) = \\frac{\\vert \\alpha - 2 \\vert}{\\alpha} \\times \\left[ \\left( \\dfrac{(u/c)^2}{\\vert \\alpha - 2 \\vert} + 1 \\right)^{\\alpha/2} -1 \\right] \\] <p>If you don\u2019t want to optimize for \\(c\\), default \\(c=1\\)</p> <p></p> <p></p> <ul> <li>Monotonic wrt \\(\\vert u \\vert\\) and \\(\\alpha\\): useful for graduated non-convexity</li> <li>Smooth wrt \\(u\\) and \\(\\alpha\\)</li> <li>Bounded first and second derivatives: no exploding gradients, easy preconditioning</li> </ul>"},{"location":"CS_Electives/Machine_Learning/08_Cost/#adaptive-loss","title":"Adaptive Loss","text":"<p>No hyper-parameter tuning!, as \\(\\alpha\\) is optimized for its most optimal value as well</p> <p>If the selection of \\(\\alpha\\) wants to discount the loss for outliers, it needs to increase the loss for inliers</p> \\[ \\begin{aligned} \\text{L}' &amp;= -\\log P(u \\vert \\alpha) \\\\ &amp;= {\\mathcal L}(u, \\alpha) + \\log Z(\\alpha) \\end{aligned} \\] <p></p>"},{"location":"CS_Electives/Machine_Learning/08_Cost/#classification-loss","title":"Classification Loss","text":"<p>Should be tuned to control which type of error we want to minimize</p> <ul> <li>overall error rate</li> <li>false positive rate (FPR)</li> <li>false negative rate (FNR)</li> </ul> <p>Imbalanced dataset: Re-weight loss function to ensure equal weightage for each target class</p> <ul> <li>Sample weight matching the probability of each class in the population data-generating distribution</li> <li> <p>For eg \\(\\sum_{i} w_{ic} = \\text{same}, \\forall c\\)</p> <ul> <li>\\(w_i = 1-f_c = 1-\\dfrac{n_c}{n}\\), where \\(n_i=\\) no of observations of class \\(c\\)</li> </ul> </li> <li> <p>Modify loss function</p> </li> <li>Under-sampling</li> </ul> Metric Formula Range Preferred Value Meaning Advantages Disadvantages Indicator/Zero-One/Misclassification \\(\\begin{cases} 0, &amp; \\hat y = y \\\\ 1, &amp; \\text{o.w} \\end{cases}\\) \\([0, 1]\\) \\(0\\) Produces a Bayes classifier that maximizes the posterior probability Easy to interpret Treats all error types equallyMinimizing is np-hardNot differentiable Modified Indicator \\(\\begin{cases} 0, &amp; \\hat y = y \\\\ a, &amp; \\text{FP} \\\\ b, &amp; \\text{FN} \\end{cases}\\) \\(0\\) Control on type of error to min Harder to interpret CrossEntropy/Log Loss/Negative Log Likelihood/Softmax \\(-\\sum\\limits_c^C p_c \\cdot \\ln \\hat p_c\\)such that \\(\\sum p_i = \\sum \\hat p_i\\)\\(-\\ln \\left( \\dfrac{\\exp(\\hat p_i)}{\\sum_{j=c}^C \\exp(\\hat p_c)} \\right)\\), where \\(i=\\) correct class\\(- \\hat p_i + \\ln \\sum_{c=1}^C \\exp(\\hat p_c)\\) \\([0, \\infty]\\) \\(\\downarrow\\) Minimizing gives us \\(p=q\\) for \\(n&gt;&gt;0\\) (Proven using Lagrange Multiplier Problem)Information Gain \\(\\propto \\dfrac{1}{\\text{Entropy}}\\)Entropy: How much information gain we have Binary cross entropy/Logistic \\(-\\log \\Big(\\sigma(-\\hat y \\cdot y_i) \\Big) = \\log(1 + e^{-\\hat y \\cdot y_i})\\)\\(y, \\hat y \\in \\{-1, 1 \\}\\) Gini Index \\(\\sum\\limits_c^C p_c (1 - \\hat p_c)\\) Hinge \\(\\max \\{ 0, 1 - y_i \\hat y_i \\}\\)\\(y \\in \\{ -1, 1 \\}\\) Equivalent to \\(L_1\\) loss but only for predicting wrong classMaximize margin - Insensitive to outliers: Penalizes errors \u201cthat matter\u201d - Loss is non-differentiable at point- Does not have probabilistic interpretation Exponential \\(\\exp (-\\hat y \\cdot y)\\)\\(y \\in \\{ -1, 1 \\}\\) Basic \\(e^{\\text{CE}}\\) Sensitive to outliers KL (Kullback-Leibler) Divergence/Relative entropy/Cross Entropy - Entropy \\(H(p, q) - H(p)\\) <p></p> <p>Example for \\(y = 1\\)</p> <p></p>"},{"location":"CS_Electives/Machine_Learning/08_Cost/#clustering-loss","title":"Clustering Loss","text":"\\[ {\\mathcal L}(\\theta) = D\\Big( x_i, \\text{centroid}(\\hat y_i) \\Big) \\]"},{"location":"CS_Electives/Machine_Learning/08_Cost/#proximity-measures","title":"Proximity Measures","text":"<ul> <li>Similarity</li> <li>Dissimilarity</li> <li>Distance measure (subclass)</li> </ul>"},{"location":"CS_Electives/Machine_Learning/08_Cost/#range","title":"Range","text":"<p>May be</p> <ul> <li>\\([0, 1], [0, 10], [0, 100]\\)</li> <li>\\([0, \\infty)\\)</li> </ul>"},{"location":"CS_Electives/Machine_Learning/08_Cost/#types-of-proximity-measures","title":"Types of Proximity Measures","text":""},{"location":"CS_Electives/Machine_Learning/08_Cost/#similarity","title":"Similarity","text":"<p>For document, sparse data</p> <ul> <li>Jacard Similarity</li> <li>Cosine Similarity</li> </ul>"},{"location":"CS_Electives/Machine_Learning/08_Cost/#dissimilarity","title":"Dissimilarity","text":"<p>For continuous data</p> <ul> <li>Correlation</li> <li>Euclidean</li> </ul>"},{"location":"CS_Electives/Machine_Learning/08_Cost/#idk","title":"IDK","text":"Attribute Type Dissimilarity Similarity Nominal \\(\\begin{cases} 0, &amp; p=q \\\\1, &amp;p \\ne q \\end{cases}\\) \\(\\begin{cases} 1, &amp; p=q \\\\ 0, &amp;p \\ne q \\end{cases}\\) Ordinal \\(\\dfrac{\\vert  p-q \\vert}{n-1}\\)Values mapped to integers: \\([0, n-1]\\), where \\(n\\) is the no of values \\(1- \\dfrac{\\vert  p-q  \\vert}{n-1}\\) Interval/Ratio \\(\\vert p-q \\vert\\) \\(-d\\) \\(\\dfrac{1}{1+d}\\) \\(1 - \\dfrac{d-d_\\text{min}}{d_\\text{max}-d_\\text{min}}\\)"},{"location":"CS_Electives/Machine_Learning/08_Cost/#dissimilarity-matrix","title":"Dissimilarity Matrix","text":"<p>Symmetric \\(n \\times n\\) matrix, which stores a collection of dissimilarities for all pairs of \\(n\\) objects</p> <ul> <li>\\(d(2, 1) = d(1, 2)\\)</li> </ul> <p>It gives the distance from every object to every other object</p> <p>Something</p> <p>Example</p> ObjectIdentifier Test 1 Tets 2 Test 3 <p>Compute for test 2</p> 1 2 3 4 1 2 3 4"},{"location":"CS_Electives/Machine_Learning/08_Cost/#distance-between-data-objects","title":"Distance between data objects","text":""},{"location":"CS_Electives/Machine_Learning/08_Cost/#minkowskis-distance","title":"Minkowski\u2019s distance","text":"<p>Let</p> <ul> <li>\\(a, b\\) be data objects</li> <li>\\(n\\) be no of attributes</li> <li>\\(r\\) be parameter</li> </ul> <p>The distance between \\(x,y\\) is</p> \\[ d(a, b) = \\left( \\sum_{k=1}^n \\vert  a_k - b_k  \\vert^r \\right)^{\\frac{1}{r}} \\] \\(r\\) Type of Distance \\(d(x, y)\\) Gives Magnitude of Distance Remarks 1 City blockManhattanTaxicab\\(L_1\\) Norm \\(\\sum_{k=1}^n \\vert  a_k - b_k  \\vert\\) Distance along axes Maximum 2 Euclidean\\(L_2\\) Norm \\(\\sqrt{ \\sum_{k=1}^n \\vert  a_k - b_k  \\vert^2 }\\) Perpendicular Distance Shortest We need to standardize the data first \\(\\infty\\) ChebychevSupremum\\(L_{\\max}\\) norm\\(L_\\infty\\) norm \\(\\max (\\vert  x_k - y_k  \\vert )\\) Medium Makowski <p>Also, we have squared euclidean distance, which is used sometimes</p> \\[ d(x, y) = \\sum_{k=1}^n |a_k - b_k|^2 \\]"},{"location":"CS_Electives/Machine_Learning/08_Cost/#properties-of-distance-metrics","title":"Properties of Distance Metrics","text":"Property Meaning Non-negativity \\(d(a, b) = 0\\) Symmetry \\(d(a, b) = d(b, a)\\) Triangular inequality \\(d(a, c) \\le d(a, b) + d(b, c)\\)"},{"location":"CS_Electives/Machine_Learning/08_Cost/#similarity-between-binary-vector","title":"Similarity between Binary Vector","text":"<p>\\(M_{00}\\) shows how often do they come together; \\(p, q\\) do not have 11 in the same attribute</p>"},{"location":"CS_Electives/Machine_Learning/08_Cost/#simple-matching-coefficient","title":"Simple Matching Coefficient","text":"\\[ \\text{SMC}(p, q) = \\frac{ M_{00} + M_{11} (\\text{Total no of matches}) }{ \\text{Number of attributes} } \\]"},{"location":"CS_Electives/Machine_Learning/08_Cost/#jaccard-coefficient","title":"Jaccard Coefficient","text":"<p>We ignore the similarities of \\(M_{00}\\)</p> \\[ \\text{JC}(p, q) = \\frac{M_{11}}{M_{11} + M_{01} + M_{10}} \\]"},{"location":"CS_Electives/Machine_Learning/08_Cost/#similarity-between-document-vectors","title":"Similarity between Document Vectors","text":""},{"location":"CS_Electives/Machine_Learning/08_Cost/#cosine-similarity","title":"Cosine Similarity","text":"\\[ \\begin{aligned} \\cos(x, y) &amp;= \\frac{ xy }{ \\vert  x \\vert  \\ \\ \\vert  y  \\vert  } \\sum_{i=1}^n x_i y_i \\\\ &amp;= x \\cdot y \\\\ \\vert  x  \\vert &amp;= \\sqrt{\\sum_{i=1}^n x_i^2} \\end{aligned}  \\] \\(\\cos (x, y)\\) Interpretation 1 Similarity 0 No similarity/Dissimilarity -1 Dissimilarity"},{"location":"CS_Electives/Machine_Learning/08_Cost/#document-vector","title":"Document Vector","text":"<p>Frequency of occurance of each term</p> \\[ \\cos(d_1, d_2) = \\frac{d_1 d_2}{ ||d_1|| \\ \\ ||d_2|| } \\sum_{i=1}^n d_1 d_2 \\]"},{"location":"CS_Electives/Machine_Learning/08_Cost/#tanimatto-coefficientextended-jaccard-coefficient","title":"Tanimatto Coefficient/Extended Jaccard Coefficient","text":"\\[ T(p, q) = \\frac{ pq }{ ||p||^2 + ||q||^2 - pq } \\]"},{"location":"CS_Electives/Machine_Learning/08_Cost/#costs-functions-mathcal-jtheta","title":"Costs Functions \\({\\mathcal J}(\\theta)\\)","text":"<p>Aggregated penalty for entire dataset (mean, median) which is calculated once for each epoch, which includes loss function and/or regularization</p> <p>This is the objective function on for our model to minimize $$ {\\mathcal J}(\\hat y, y) = f(  {\\mathcal L}(\\hat y, y)  ) $$ where \\(f=\\) summary statistic such as mean, etc</p> <p>You can optimize</p> <ul> <li>location: mean, median, etc</li> <li>scale: variance, IQR, etc</li> <li>Combination of both</li> </ul> <p>For eg:</p> <ul> <li>Mean(SE) = MSE, ie Mean Squared Error</li> <li>\\(\\text{RMSE} = \\sqrt{\\text{MSE}}\\)</li> <li>Normalized RMSE = \\(\\dfrac{\\text{RMSE}}{\\bar y}\\)</li> </ul>"},{"location":"CS_Electives/Machine_Learning/08_Cost/#rmse","title":"RMSE","text":"<p>RMSE is a good balance between MSE and MAE, as it is similar to</p> <ul> <li>MSE: penalizes large deviations</li> <li>MAE: is in the same unit as \\(y\\)</li> </ul>"},{"location":"CS_Electives/Machine_Learning/08_Cost/#bessels-correction","title":"Bessel\u2019s Correction","text":"<p>Penalize number of predictors $$ \\begin{aligned} \\text{Cost}\\text{corrected} &amp;= \\text{Cost}\\text{corrected} \\times \\dfrac{n}{\\text{DOF}} \\ \\text{DOF} &amp;= n-k-e \\end{aligned} $$</p> <ul> <li>where</li> <li>\\(n=\\) no of samples</li> <li>\\(k=\\) no of parameters</li> <li> <p>\\(e=\\) no of intermediate estimates (such as \\(\\bar x\\) for variance)</p> </li> <li> <p>Modify accordingly for squares/root metrics</p> </li> </ul>"},{"location":"CS_Electives/Machine_Learning/08_Cost/#robustness-to-outliers","title":"Robustness to Outliers","text":"<ul> <li>Median: Very robust, but very low efficiency</li> <li>Trimmed Mean: Does not work well for small sample sizes</li> <li>IQR</li> </ul>"},{"location":"CS_Electives/Machine_Learning/09_Optimization/","title":"Optimization","text":"<p>Refer to Optimization Algorithms</p>"},{"location":"CS_Electives/Machine_Learning/10_Generalization/","title":"Generalization","text":"<p>The ability of trained model to be able to perform well on unseen data. Better validation result \\(\\implies\\) Better generalization</p> <p>The generalization gap (error difference b/w seen and unseen data) should be small</p> <p>The more models you try, the worse your generalization wrt that data due to increase in \\(\\vert H \\vert\\) as \\(H = \\bigcup_i H_i\\) where \\(H_i\\) is the \\(H\\) for each model. This is the essence behind importance of train-dev-val-test split</p> <p>Note: Always try to overfit with a very small sample and then focus on generalization</p> \\[ E_\\text{out} \\le E_\\text{in} + \\dfrac{\\Omega}{n} \\] <p>where</p> <ul> <li>\\(\\Omega =\\) Overfit/Complexity Penalty</li> </ul> <p>Lesson: Match the \u2018model complexity\u2019 to the data resources, not to the target complexity, ie, pick a hypothesis set that you can afford to navigate for the given training dataset</p>"},{"location":"CS_Electives/Machine_Learning/10_Generalization/#noise","title":"Noise","text":"<p>Part of \\(y\\) we cannot model</p>"},{"location":"CS_Electives/Machine_Learning/10_Generalization/#types","title":"Types","text":"Stochastic/Bayes\u2019 Error Deterministic Given by \\(u\\) \\(f^* - f\\) Meaning Observational error Part of \\(f\\) that \\(H\\) cannot capture, even when there is no stochastic noise Cause \\(P(y \\vert x)\\) 1. Complexity of \\(f\\)2. \\(H\\) Frequency High Low Smooth \u274c \u2705 For a given \\(x\\) Randomly distributed Fixed constant Comment No point trying to capture it, as you will learn a false pattern, given limited training dataset When teaching a kid, better to use easy-to-understand examples rather than the actual science Effect on overfitting <ul> <li>For a finite \\(n\\), \\(H\\) tends to fit both stochastic and deterministic noise</li> <li>Deterministic and Stochastic noise affect the Variance by making the model more susceptible to overfitting</li> <li>In presence of stochastic and/or deterministic noise, it is better to prefer smoother/simpler hypotheses, so that the model can avoid fitting the noise</li> <li> <p>For time-series modelling, we can use averaging/smoothing filter a pre-processing step</p> </li> <li> <p>For target \\(f\\) of high complexity, train model \\(\\hat f\\) with smooth \\(\\tilde y\\) (for eg: Moving Average) to reduce effect of deterministic noise</p> </li> </ul>"},{"location":"CS_Electives/Machine_Learning/10_Generalization/#error-decomposition","title":"Error Decomposition","text":"Bias Variance\\(\\text{Var}(\\hat f)\\) Distribution Mismatch Indicates Inaccuracy Imprecision Data Sampling Issue Meaning Proximity of prediction is to true values Amount by which \\(\\hat y\\) would change for different training data Implication Underfitting Overfitting Denotation \\(\\hat f-f\\) \\(E \\Big[ \\ (\\hat f - f^*)^2 \\ \\Big]\\) Estimated through Train Error - Acceptable Error Dev error - Train Error Validation Error - Dev Error Reason for estimation In-sample performance Difference between in-sample and out-sample performance \\[ \\text{E}_\\text{out} = \\text{Bias}^2 + \\text{Variance} + \\text{Dist Mis} + \\text{DN} + \\text{SN} \\]"},{"location":"CS_Electives/Machine_Learning/10_Generalization/#prediction-bias-variance","title":"Prediction Bias &amp; Variance","text":"<p>We want low value of both</p> <p>If a measurement is biased, the estimate will include a constant systematic error</p> <p></p>"},{"location":"CS_Electives/Machine_Learning/10_Generalization/#bias-variance-tradeoff","title":"Bias-Variance Tradeoff","text":"<p>Usually U-Shaped</p> <p></p> <p>Each additional parameter adds the same amount of variance \\(\\sigma^2/n\\), regardless of whether its true coefficient is large or small (or zero).</p> <p>$$ \\begin{aligned} \\text{Variance} &amp;= \\sigma^2 \\left[ \\dfrac{1+k}{n} + 1 \\right] \\ &amp; \\approx O(k) \\end{aligned} $$ Hence, we can reduce variance by shrinking small coefficients to zero</p>"},{"location":"CS_Electives/Machine_Learning/10_Generalization/#tip","title":"Tip","text":"<p>When using feature selection/LASSO regularization, stop one standard deviation &gt; the optimal point, as even though bias has increased by a small amount, variance can be decreased a lot</p> <p></p>"},{"location":"CS_Electives/Machine_Learning/10_Generalization/#vc-analysis","title":"VC Analysis","text":"<p>Let</p> <ul> <li>\\(E_\\text{in} =\\) error on seen train data</li> <li>\\(E_\\text{test} =\\) error on unseen test data</li> <li>\\(E_\\text{out} =\\) theoretical error on the unseen population</li> <li>\\(\\vert h \\vert =\\) size of hypothesis, estimated through the number of Dichotomies</li> <li>\\(\\vert H \\vert =\\) size of hypothesis set, estimated through the number of Dichotomies</li> </ul> <p>For test data, \\(\\vert H \\vert = 1\\), as it is not biased and we do not choose a hypothesis that looks good on it.</p>"},{"location":"CS_Electives/Machine_Learning/10_Generalization/#pictorial-representation","title":"Pictorial Representation","text":""},{"location":"CS_Electives/Machine_Learning/10_Generalization/#idk","title":"IDK","text":""},{"location":"CS_Electives/Machine_Learning/10_Generalization/#dichotomies","title":"Dichotomies","text":"<p>Prediction of hypothesis on data</p> <p>No of dichotomies \\(\\le 2^n\\)</p>"},{"location":"CS_Electives/Machine_Learning/10_Generalization/#growth-function","title":"Growth Function","text":"<p>counts the most dichotomies on \\(n\\) points $$ \\begin{aligned} m_H(n) &amp;= \\max_{x_i} \\vert H(x_i) \\vert \\qquad \\forall i \\in [1, n] \\ \\implies m_H(n) &amp;\\le 2^n \\end{aligned} $$ The hypothesis set is said to \u201cshatter\u201d \\(n\\) points</p>"},{"location":"CS_Electives/Machine_Learning/10_Generalization/#break-point","title":"Break Point","text":"<p>If no dataset of size \\(n_b\\) can be shattered by \\(H\\), then \\(n_k\\) is a break point for \\(H\\), ie Point at which you fail to get all possible dichotomies. This also implies that dataset of \\(n &gt; n_k\\) cannot be shattered as well $$ m_H(k) = 2^k $$</p>"},{"location":"CS_Electives/Machine_Learning/10_Generalization/#vc-dimension","title":"VC Dimension","text":"<p>\\(d_\\text{VC}(H)\\) is the most points \\(H\\) can shatter</p> <p>\\(d_\\text{VC}(H)\\) measures the effective number of parameters $$ \\begin{aligned} d_\\text{VC}(H) &amp;= \\arg \\max_n m_H(n) = 2^n \\ &amp;= n_b-1 \\end{aligned} $$</p> \\[ \\begin{aligned} d_\\text{vc} (H) &amp;= \\sum_i d_\\text{vc} (h_i) \\end{aligned} \\] <p>Independent of</p> <ul> <li>Learning algorithm</li> <li>Input distribution</li> <li>Target function</li> </ul> <p>Dependent on</p> <ul> <li>Final hypothesis \\(\\hat f\\)</li> <li>Training examples</li> <li>Hypothesis set</li> </ul> <p>\\(d_\\text{VC}(H)\\) is finite \\(\\implies\\) \\(\\hat f \\in H\\) will generalize</p> <p>Usually, \\(d_\\text{VC} \\le\\) no of parameters in the model </p>"},{"location":"CS_Electives/Machine_Learning/10_Generalization/#sauers-lemma","title":"Sauer\u2019s Lemma","text":"\\[ \\begin{aligned} d_\\text{vc} (H) &lt; \\infty  \\implies m_H(n) \\le \\sum_{r=0}^{d_\\text{vc}(H)} nCr \\end{aligned} \\]"},{"location":"CS_Electives/Machine_Learning/10_Generalization/#conclusion","title":"Conclusion","text":"\\(m_H(n)\\) No break point \\(2^n\\) Any breakpoint \\(\\sum_{r=0}^{d_\\text{VC}(H)} n C_r\\)Polynomial in \\(n\\)"},{"location":"CS_Electives/Machine_Learning/10_Generalization/#hoeffdings-inequality","title":"Hoeffding\u2019s Inequality","text":"\\[ \\begin{aligned} P( \\vert E_\\text{out} - E_\\text{test} \\vert &gt; \\epsilon) &amp; \\le \\sum_{i=1}^{\\vert H \\vert} P( \\vert E_\\text{out}(h_i) - E_\\text{in}(h_i) \\vert &gt; \\epsilon) \\\\ &amp; \\le {\\vert H \\vert} \\cdot 2 \\exp(-2 n \\epsilon^2) \\end{aligned} \\] <p>But this is a very loose bound (better to loose, than incorrectly tight), as we assume that each hypothesis is disjoint</p> <p></p>"},{"location":"CS_Electives/Machine_Learning/10_Generalization/#vapnik-chervonenkis-inequality","title":"Vapnik-Chervonenkis Inequality","text":"\\[ \\begin{aligned} P( \\vert E_\\text{out} - E_\\text{test} \\vert &gt; \\epsilon) &amp; \\le {\\textcolor{hotpink}{2} \\cdot m_H(\\textcolor{hotpink}{2}n)} \\cdot \\exp \\left( -2 n \\left( \\dfrac{\\epsilon}{\\textcolor{hotpink}{4}} \\right)^2 \\right) \\\\ &amp; \\le \\underbrace{4 \\cdot m_H(2n) \\cdot \\exp \\left( \\frac{-1}{8} n \\epsilon^2 \\right)}_\\delta \\end{aligned} \\] <p>\\(\\delta\\) is like the significance level</p>"},{"location":"CS_Electives/Machine_Learning/10_Generalization/#why-difference-from-hoeffdings","title":"Why difference from Hoeffding\u2019s?","text":"<p>Empirical observation: The bounded quantity has the same monotonicity as the bound</p>"},{"location":"CS_Electives/Machine_Learning/10_Generalization/#generalization_1","title":"Generalization","text":"\\[ \\begin{aligned} \\epsilon &amp;= \\underbrace{\\sqrt{ \\dfrac{8}{n} \\ln \\left \\vert \\dfrac{4m_H(2n)}{\\delta} \\right \\vert }}_\\Omega  \\\\ &amp; = O \\left( \\sqrt{\\dfrac{d_{vc} \\ln n + \\ln \\vert 1/\\delta \\vert }{n}} \\right) \\\\ &amp; = O \\left( \\sqrt{d_{vc} \\dfrac{ \\ln \\vert n \\vert }{n}} \\right) \\end{aligned} \\] \\[ \\text{with prob} \\ge 1-\\delta, \\quad \\vert E_\\text{out} - E_\\text{in} \\vert \\le \\Omega(n, H, \\delta) \\]"},{"location":"CS_Electives/Machine_Learning/10_Generalization/#recommend-test-size","title":"Recommend Test Size","text":"<p>Rule of thumb to get good generalization $$ n_\\text{test} \\ge 10 \\times d_\\text{VC}(H) $$</p>"},{"location":"CS_Electives/Machine_Learning/10_Generalization/#curse-of-dimensionality","title":"Curse of Dimensionality","text":"<p>When \\(k\\) is very large, then the \\(\\vert H \\vert\\) gets very large and hence, generalization becomes very poor</p>"},{"location":"CS_Electives/Machine_Learning/10_Generalization/#training-size","title":"Training Size","text":"<p>Generalization improves with size of training set, until a saturation point, after which it stops improving.</p> More data \\(\\implies\\) Parametric asymptote to an error value exceeding Bayes error Non-Parametric better generalization until best possible error is achieved"},{"location":"CS_Electives/Machine_Learning/10_Generalization/#generalization-bound-vs-generalization-gap","title":"Generalization Bound vs Generalization Gap","text":"Generalization Gap Generalization Bound Associated with Model- Bias- Variance Testing method- Test Set Size- No of trials"},{"location":"CS_Electives/Machine_Learning/10_Generalization/#linear-regression","title":"Linear Regression","text":"<p>Consider true \\(f = \\text{Linear} + u; \\quad u \\sim N(0, \\sigma^2_u)\\)</p> <ul> <li>Best approximation error \\(= \\sigma^2_u\\)</li> <li>Expected in-sample error \\(= \\sigma^2_u \\left[1- \\dfrac{k}{n} \\right]\\)</li> <li>Expected out-sample error \\(= \\sigma^2_u \\left[ 1 + \\dfrac{k}{n} \\right]\\)</li> <li>Expected generalization gap \\(= 2 \\sigma^2_u \\left( \\dfrac{k}{n} \\right)\\)</li> </ul> <p></p>"},{"location":"CS_Electives/Machine_Learning/10_Generalization/#vc-vs-bias-variance","title":"VC vs Bias-Variance","text":""},{"location":"CS_Electives/Machine_Learning/10_Generalization/#scenarios","title":"Scenarios","text":"Scenarios Conclusion Desired Error &lt; Train Error Underfitting/Optimal Train Error \\(\\approx\\) Test Error \\(\\approx\\) Desired ErrorTrain Error &lt; Test Error &lt; Desired Error Optimal Train Error &lt;&lt; Test ErrorTrain Error &lt; Desired Error &lt; Test Error Overfitting"},{"location":"CS_Electives/Machine_Learning/10_Generalization/#fitting-capacity","title":"Fitting &amp; Capacity","text":"<p>We can control the fitting of a model, by changing hypothesis space, and hence changing its capacity</p> Underfitting Appropriate-Fitting Overfitting Capacity Low Appropriate Low Low Bias(Fitting Signal) \u274c \u2705 \u2705 Low Variance(Avoiding Noise) \u2705 \u2705 \u274c Implication Acceptable Optimal Harmful Steps toaddress Increase model complexityIncrease training dataRemove noise from dataInc no of features Cross-ValidationMore training dataFeature ReductionEarly StoppingRegularization Causes - Small train set- Stochastic noise- Deterministic noise- Excessive model capacity <p></p> <p>The capacity of a model increases with increased degree of polynomial</p>"},{"location":"CS_Electives/Machine_Learning/10_Generalization/#interesting-note","title":"Interesting Note","text":"<p>Even if the target function is known to be of high complexity, for a small training dataset, a low capacity model will generalize better.</p> <p></p> <p></p> <p></p>"},{"location":"CS_Electives/Machine_Learning/10_Generalization/#unsupervised-learning","title":"Unsupervised Learning","text":"<p>Harder to quantify than for supervised learning</p> <p>If model is probabilistic, we can detect overfitting by comparing in-sample and out-sample log-likelihood</p> <p>Detecting overfitting with larger datasets will be paradoxically harder</p>"},{"location":"CS_Electives/Machine_Learning/10_Generalization/#clustering","title":"Clustering","text":"<p>Overfitting: Fitting small, local clusters rather than the true global clusters</p>"},{"location":"CS_Electives/Machine_Learning/10_Generalization/#non-probabilistic","title":"Non-Probabilistic","text":""},{"location":"CS_Electives/Machine_Learning/10_Generalization/#probabilistic-models","title":"Probabilistic Models","text":"<p>Log likelihood on in-sample and out-sample data</p> <p></p>"},{"location":"CS_Electives/Machine_Learning/11_Data_Splitting/","title":"Data Splitting","text":""},{"location":"CS_Electives/Machine_Learning/11_Data_Splitting/#train-test","title":"Train-Test","text":"<p>The training set has an optimistic bias, since it is used to choose a hypothesis that looks good on it. Hence, we require a unseen set as it is not biased</p> <p>Once a data set has been used in the learning/validation process, it is \u201ccontaminated\u201d \u2013 it obtains an optimistic (deceptive) bias, and the error calculated on the data set no longer has the tight generalization bound.</p> <p>To simulate deployment, any data used for evaluation should be treated as if it does not exist at the time of modelling</p>"},{"location":"CS_Electives/Machine_Learning/11_Data_Splitting/#data-split-sets","title":"Data Split Sets","text":"Train Development(Inner Validation) Validation(Outer Validation) Test(Holdout) Recommend split % 40 20 20 20 In-Sample(\u2018Seen\u2019 by model) \u2705 \u274c \u274c \u274c EDA(\u2018Seen\u2019 by analyst) \u2705 \u274c \u274c \u274c Pre-Processing \u201clearning\u201d(Normalization, Standardization, \u2026) \u2705 \u274c \u274c \u274c Feature Engineering \u201clearning\u201d(Selection, Transformation, \u2026) \u2705 \u274c \u274c \u274c Underfit Evaluation \u2705 \u274c \u274c \u274c Model Tuning \u2705 \u274c \u274c \u274c Overfit Evaluation \u274c \u2705 \u274c \u274c Hyperparameter Tuning \u274c \u2705 \u274c \u274c Model Comparison/Selection \u274c \u274c \u2705 \u274c Performance Reporting \u274c \u274c \u274c \u2705 \\(\\hat f\\) \\({\\hat f}_\\text{in}\\) \\({\\hat f}_\\text{dev}\\) \\({\\hat f}_\\text{val}\\) \\({\\hat f}_\\text{test}\\) \\(\\hat f\\) trained on Train Train Train+Dev Train+Dev+Val \\(E\\) \\(E_\\text{in}\\) \\(E_\\text{dev}\\) \\(E_\\text{val}\\) \\(E_\\text{test}\\) Error Names Training error/In-Sample Error/Empirical Error/Empirical Risk Development Error Validation Error \\(\\hat E_\\text{out}\\)Expected errorPrediction errorRisk \\({\\vert H \\vert}_\\text{set}\\) \\(\\infty\\) \\(d_\\text{vc}\\) \\({\\vert H \\vert}_\\text{val}\\) \\(1\\) Comment Used for \u201ctraining\u201d on \u201cfinalist\u201d set of hypotheses Should not be used for any model decision making Color Scheme Below Green Yellow Orange Red \\[ \\begin{aligned} \\mathbb{E}[E_\\text{test}] &amp;= E_\\text{out} \\\\ \\text{var}[E_\\text{test}] &amp;= \\dfrac{\\sigma^2_{u}}{n_\\text{test}} \\\\ \\end{aligned} \\] \\[ E_\\text{out} \\le E_\\text{set} + O \\left( \\sqrt{\\dfrac{\\ln {\\vert H \\vert}_\\text{set}}{n_\\text{set}}} \\right) \\]"},{"location":"CS_Electives/Machine_Learning/11_Data_Splitting/#test-size-tradeoff","title":"Test-Size Tradeoff","text":"\\[ E_\\text{out}(\\hat f) \\underbrace{\\approx}_\\mathclap{n^*_\\text{test} \\downarrow} E_\\text{out}(\\hat f_\\text{test}) \\underbrace{\\approx}_\\mathclap{n^*_\\text{test} \\uparrow} E_\\text{test}(\\hat f_\\text{test}) \\] Small Large Low Model Bias \u2705 \u274c Small Generalization Bound \u274c \u2705 Reliable \\(\\hat E_\\text{out}\\)\\(E_\\text{out}(\\hat f_\\text{test})-E_\\text{test}(\\hat f_\\text{test})\\) \u274c \u2705 Tested model and final model are sameSmall \\(E_\\text{out}(\\hat f) - E_\\text{out}(\\hat f_\\text{test})\\) \u2705 \u274c Extreme caseModel performance reporting \u201cwith no certainty, the model is excellent\u201d \u201cwith high certainty, the model is crap\u201d"},{"location":"CS_Electives/Machine_Learning/11_Data_Splitting/#usage","title":"Usage","text":"<ol> <li>Training Data</li> <li>Get \\(E_\\text{in}\\)</li> <li>Overfit all models</li> <li>Beat baseline model(s)</li> <li>Dev data</li> <li>Get \\(E_\\text{dev}\\)</li> <li>Tune all models to generalize</li> <li>Beat baseline model(s)</li> <li>Validation data</li> <li>Compare all models on \\(E_\\text{val}\\)</li> <li>Must beat baseline model(s)</li> <li>Select best model \\(\\hat f_\\text{val}^*\\)</li> <li>Get accuracy estimate of \\(\\hat f_\\text{val}^*\\) on test data: \\(E_\\text{test}\\)</li> </ol> <p>Single metric</p> <ul> <li>Use RMS (Root Mean Squared) of train and dev error estimate to compare models</li> <li>Harmonic mean not applicable as it gives more weight to smaller value</li> </ul> <p></p>"},{"location":"CS_Electives/Machine_Learning/11_Data_Splitting/#sampling-types","title":"Sampling Types","text":"<p>Repeatedly drawing samples from a training set and refitting a model of interest on each sample in order to obtain additional information about the fitted model.</p> <p>Hence, these help address the issue of a simple validation: Results can be highly variable, depending on which observations are included in the training set and which are in the validation set</p> Sampling Comment Better for identifying uncertainty in model Bootstrapping w/ Replacement Better as we can have a large repetitions of folds parameters Cross Validation w/o Replacement accuracy"},{"location":"CS_Electives/Machine_Learning/11_Data_Splitting/#cross-validation-types","title":"Cross Validation Types","text":"Purpose Comment Regular \\(k\\) fold Obtain uncertainty of evaluation estimates Higher \\(k\\) recommended for small datasets Leave-One-Out For very small datasets\\(n &lt; 20\\) \\(k=n\\) Shuffled Random Permutation Stratified Ensures that Train, Validation &amp; Test sets have same distribution Stratified Shuffle Grouped Grouped - Leave One Group Out Grouped with Random Permutation Walk-Forward Expanding Window Walk-Forward Rolling Window Blocking Purging Remove train obs whose labels overlap in time with test labels Purging &amp; Embargo Prevent data leakage due to serial correlation \\(x_{\\text{train}_{-1}} \\approx x_{\\text{test}_{0}}\\)\\(y_{\\text{train}_{-1}} \\approx y_{\\text{test}_{0}}\\) CPCV(Combinatorial Purged)"},{"location":"CS_Electives/Machine_Learning/11_Data_Splitting/#bootstrapping-types","title":"Bootstrapping Types","text":"Random sampling with replacement IID ARIMA Bootstrap Parametric Moving Block Bootstrap Non-parametric Circular Block Bootstrap Non-parametric Stationary Bootstrap Non-parametric"},{"location":"CS_Electives/Machine_Learning/11_Data_Splitting/#validation-methods","title":"Validation Methods","text":"<p>Make sure to shuffle all splits for cross-sectional data</p> Type Cross-Sectional Time Series Comment Holdout \\(k\\)- Fold 1. Split dataset into \\(k\\) subsets2. Train model on \\((k-1)\\) subsets3. Evaluate performance on \\(1\\) subset4. Summary stats of all iterations Repeated \\(k\\)-Fold \u274c Repeat \\(k\\) fold with different splits and random seed Nested \\(k\\)-Fold Nested Repeated \\(k\\)-Fold \u274c"},{"location":"CS_Electives/Machine_Learning/11_Data_Splitting/#decision-parameter-k","title":"Decision Parameter \\(k\\)","text":"<p>There is a tradeoff</p> Small \\(k\\) Large \\(k\\) Train Size Small Large Test Size Large Small Bias High Low Variance Low High <p>Usually \\(k\\) is taken</p> <ul> <li>Large dataset: 4</li> <li>Small dataset: 10</li> <li>Tiny dataset: \\(k=n\\) , ie LOOCV (Leave-One-Out CV)</li> </ul> <p></p>"},{"location":"CS_Electives/Machine_Learning/11_Data_Splitting/#data-leakage","title":"Data Leakage","text":"<p>Cases where some information from the training set has \u201cleaked\u201d into the validation/test set. Estimation of the performances is likely to be optimistic</p> <p>Due to data leakage, model trained for \\(y_t = f(x_j)\\) is more likely to be \u2018luckily\u2019 accurate, even if \\(x_j\\) is irrelevant</p> <p>Causes</p> <ul> <li>Perform feature selection using the whole dataset</li> <li>Perform dimensionality reduction using the whole dataset</li> <li>Perform parameter selection using the whole dataset</li> <li>Perform model or architecture search using the whole dataset</li> <li>Report the performance obtained on the validation set that was used to decide when to stop training (in deep learning)</li> <li>For a given patient, put some of its visits in the training set and some in the validation set</li> <li>For a given 3D medical image, put some 2D slices in the train- ing set and some in the validation set</li> </ul>"},{"location":"CS_Electives/Machine_Learning/12_Evaluation/","title":"Model Evaluation","text":""},{"location":"CS_Electives/Machine_Learning/12_Evaluation/#goals","title":"Goals","text":"<ol> <li>Minimize bias</li> <li>Minimize variance</li> <li>Minimize generalization gap</li> </ol>"},{"location":"CS_Electives/Machine_Learning/12_Evaluation/#guidelines","title":"Guidelines","text":"<ul> <li>Always check if your model is able to learn from a synthetic dataset where you know the underlying data-generating process</li> <li>Metrics computed from test set may not be representative of the true population</li> <li>Always look at multiple metrics; never trust a single one alone</li> <li>false positives and false negatives are seldom equivalent</li> <li>understand the problem to known the right tradeoff</li> <li>Always monitor the worst-case prediction</li> <li>Maximum loss</li> <li>95<sup>th</sup> percentile loss</li> </ul>"},{"location":"CS_Electives/Machine_Learning/12_Evaluation/#baselinebenchark-models","title":"Baseline/Benchark models","text":"<p>Always establish a baseline</p> <ul> <li>Basic/Naive/Dummy predictions</li> <li>Regression<ul> <li>Mean</li> <li>Max</li> <li>Min</li> <li>Random</li> </ul> </li> <li>Classification<ul> <li>Mode</li> <li>Random</li> </ul> </li> <li>Time series specific<ul> <li>Persistence</li> <li>\\(\\hat y_{t+h} = y_t\\)</li> <li>Latest value available</li> <li>Great for short horizons</li> <li>Climatology</li> <li>\\(\\hat y_{t+h} = \\bar y_{i \\le t}\\)</li> <li>Average of all observations until present</li> <li>Great for short horizons</li> <li>Combination of Persistence and Climatology</li> <li>\\(\\hat y_{t+h} = \\beta_1 y_t + \\beta_2 \\bar y_{i \\le t}\\)</li> <li>Lag: \\(\\hat y_{t+h} = y_{t-k}\\)</li> <li>Seasonal Lag: \\(\\hat y_{t+h} = y_{t+h-m}\\)</li> <li>Moving average</li> <li>Exponential average</li> </ul> </li> <li>Human Level Performance</li> <li>Literature Review</li> <li>Performance of older system</li> </ul>"},{"location":"CS_Electives/Machine_Learning/12_Evaluation/#significance","title":"Significance","text":"<p>All the evaluation should be performed relative to the baseline.</p> <p>For eg: Relative RMSE = RMSE(model)/RMSE(baseline), with \u201cgood\u201d threshold as 1</p>"},{"location":"CS_Electives/Machine_Learning/12_Evaluation/#probabilistic-evaluation","title":"Probabilistic Evaluation","text":"<p>Now, we need to see if any difference in accuracy across models/hyperparameters is statistically-significant, or just a matter of chance.</p> <p>Summary Statistics: Don\u2019t just look at the mean evaluation metric of the multiple splits; also get the uncertainty associated with the validation process.</p> <ul> <li>Standard error of accuracy estimate</li> <li>Standard deviation</li> <li>Quantiles</li> <li>PDF</li> <li>VaR</li> </ul>"},{"location":"CS_Electives/Machine_Learning/12_Evaluation/#bessels-correction","title":"Bessel\u2019s Correction","text":"\\[ \\begin{aligned} \\text{Metric}_\\text{corrected} &amp;= \\text{Metric}_\\text{uncorrected} \\times \\dfrac{n}{\\text{DOF}} \\\\ \\text{DOF} &amp;= n-k-e \\end{aligned} \\] <ul> <li>where</li> <li>\\(n=\\) no of samples</li> <li>\\(k=\\) no of parameters</li> <li>\\(e=\\) no of intermediate estimates (such as \\(\\bar x\\) for variance)</li> <li>Do not perform this on metrics which are already corrected for degree of freedom (such as \\(R^2_\\text{adj}\\))</li> <li>Modify accordingly for squares/root metrics</li> </ul>"},{"location":"CS_Electives/Machine_Learning/12_Evaluation/#regression-evaluation","title":"Regression Evaluation","text":"Metric Formula Unit Range Preferred Value Signifies Advantages\u2705 Disadvantages\u274c Comment \\(R^2\\)(Coefficient of Determination) \\(1 - \\text{RSE}\\) Unitless \\([0, 1]\\) \\(1\\) Proportion of changes in dependent var explained by regressors.Proportion of variance in \\(y\\) explained by model wrt variance explained by meanDemonstrates ___ of regressors- Relevance- Power- Importance Cannot use to compare same model on different samples, as it depends on variance of sampleSusceptible to spurious regression, as it increases automatically when increasing predictors Adjusted \\(R^2\\) \\(1 - \\left[\\dfrac{(n-1)}{(n-k-1)} (1-R^2)\\right]\\) Unitless \\([0, 1]\\) \\(1\\) Penalizes large number of predictors Accuracy \\(100 - \\text{MAPE}\\) % \\([0, 100]\\) \\(100 \\%\\) \\(\\chi^2_\\text{reduced}\\) \\(\\dfrac{\\chi^2}{n-k} = \\dfrac{1}{n-k}\\sum \\left( u_i/\\sigma_i \\right)^2\\) \\([0, \\infty]\\) \\(1\\) \\(\\approx 1:\\) Good fit\\(\\gg 1:\\) Underfit/Low variance estimate\\(\\ll 1:\\) Overfit/High variance estimate Spearman\u2019s Correlation \\(\\dfrac{ r(\\ rg( \\hat y), rg(y) \\ ) }{ \\sigma(\\ rg(\\hat y) \\ ) \\cdot \\sigma( \\ rg(y) \\ ) }\\) Unitless \\([-1, 1]\\) \\(1\\) Very robust against outliersInvariant under monotone transformations of \\(y\\) DW(Durbin-Watson Stat) \\(&gt; 2\\) Confidence of error term being random process Not appropriate when \\(k&gt;n\\) Similar to \\(t\\) or \\(z\\) valueIf \\(R^2 &gt;\\) DW Statistic, there is Spurious Regression AICAkaike Information Criterion \\(-2 \\ln L + 2k\\) \\(0\\) Leave-one-out cross validation score Penalizes predictors more heavily than \\(R_\\text{adj}^2\\) For small values of \\(n\\), selects too many predictorsNot appropriate when \\(k&gt;n\\) AIC Corrected \\(\\text{AIC} + \\dfrac{2k(k+1)}{n-k-1}\\) \\(0\\) Encourages feature selection BIC/SBIC/SC(Schwarz\u2019s Bayesian Information Criterion) \\(-2 \\ln L + k \\ln n\\) \\(0\\) Penalizes predictors more heavily than AIC HQICHannan-Quinn Information Criterion \\(-2 \\ln L + 2k \\ln \\vert \\ln n \\vert\\) \\(0\\)"},{"location":"CS_Electives/Machine_Learning/12_Evaluation/#probabilistic-evaluation_1","title":"Probabilistic Evaluation","text":"<p>You can model the error such as MAE as a \\(\\chi^2\\) distribution with dof = \\(n-k\\)</p> <p>The uncertainty can be obtained from the distribution</p>"},{"location":"CS_Electives/Machine_Learning/12_Evaluation/#spurious-regression","title":"Spurious Regression","text":"<p>Misleading statistical evidence of a relationship that does not truly exist</p> <p>Occurs when we perform regression between</p> <ul> <li>2 independent variables</li> </ul> <p>and/or</p> <ul> <li>2 non-stationary variables</li> </ul> <p>(Refer econometrics)</p> <p>You may get high \\(R^2\\) and \\(t\\) values, but \\(u_t\\) is not white noise (it is non-stationary)</p> <p>\\(\\sigma^2_u\\) becomes infinite as we go further in time</p>"},{"location":"CS_Electives/Machine_Learning/12_Evaluation/#classification-evaluation","title":"Classification Evaluation","text":"<p>There is always a tradeoff b/w specificity and sensitivity</p> Metric Formula Preferred Value Unit Range Meaning Entropy of each classification \\(H_i = -\\sum \\hat y \\ln \\hat y\\) \\(\\downarrow\\) \\([0, \\infty)\\) Uncertainty in a single classification Mean Entropy \\(H_i = -\\sum \\hat y \\ln \\hat y\\) Uncertainty in classification of entire dataset Accuracy \\(1 - \\text{Error}\\)\\(\\dfrac{\\text{TP + TN}}{\\text{TP + FP + TN + FN}}\\) \\(\\uparrow\\) % \\([0, 100]\\) \\(\\dfrac{\\text{Correct Predictions}}{\\text{No of predictions}}\\) Error \\(\\dfrac{\\text{FP + FN}}{\\text{TP + FP + TN + FN}}\\) \\([0, 1]\\) \\(\\downarrow\\) \\(\\dfrac{\\text{Wrong Predictions}}{\\text{No of predictions}}\\) F ScoreF<sub>1</sub> ScoreF-Measure \\(\\dfrac{2}{\\dfrac{1}{\\text{Precision}} + \\dfrac{1}{\\text{Recall}}}\\)\\(2 \\times \\dfrac{P \\times R}{P + R}\\) \\(\\uparrow\\) Unitless \\([0, 1]\\) Harmonic mean between precision and recallClose to lower value ROC-AUCReceiver-Operator Characteristics-Area Under Curve Sensitivity vs (1-Specificity)= (1-FNR) vs FPR \\(\\uparrow\\) Unitless \\([0, 1]\\) AUC = Probability that algo ranks a +ve over a -veRobust to unbalanced dataset PrecisionPPV/Positive Predictive Value \\(\\dfrac{\\textcolor{hotpink}{\\text{TP}}}{\\textcolor{hotpink}{\\text{TP}} + \\text{FP}}\\) \\(\\uparrow\\) Unitless \\([0, 1]\\) How many actual +ve values were correctly predicted as +ve RecallSensitivityTrue Positive Rate \\(\\dfrac{\\textcolor{hotpink}{\\text{TP}}}{\\textcolor{hotpink}{\\text{TP}} + \\text{FN}}\\) \\(\\uparrow\\) Unitless \\([0, 1]\\) Out of actual +ve values, how many were correctly predicted as +ve SpecificityTrue Negative Rate \\(\\dfrac{\\textcolor{hotpink}{\\text{TN}}}{\\textcolor{hotpink}{\\text{TN}} + \\text{FP}}\\) \\(\\uparrow\\) Unitless \\([0, 1]\\) Out of actual -ve values, how many were correctly predicted as -ve NPVNegative Predictive Value \\(\\dfrac{\\textcolor{hotpink}{\\text{TN}}}{\\textcolor{hotpink}{\\text{TN}} + \\text{FN}}\\) Unitless \\([0, 1]\\) Out of actual -ve values, how many were correctly predicted as -ve \\(F_\\beta\\) Score \\(\\dfrac{(1 + \\beta^2)}{{\\beta^2}} \\times \\dfrac{P \\times R}{P + R}\\) \\(\\uparrow\\) Unitless [0, 1] Balance between importance of precision/recall FP Rate \\(\\begin{aligned}\\alpha &amp;= \\dfrac{\\textcolor{hotpink}{\\text{FP}}}{\\textcolor{hotpink}{\\text{FP}} + \\text{TN}} \\\\ &amp;= 1 - \\text{Specificity} \\end{aligned}\\) \\(\\downarrow\\) Unitless \\([0, 1]\\) Out of the actual -ve, how many were misclassified as Positive FN Rate \\(\\begin{aligned}\\beta &amp;= \\dfrac{\\textcolor{hotpink}{\\text{FN}}}{\\textcolor{hotpink}{\\text{FN}} + \\text{TP}} \\\\ &amp;= 1 - \\text{Sensitivity} \\end{aligned}\\) \\(\\downarrow\\) Unitless \\([0, 1]\\) Out of the actual +ve, how many were misclassified as Negative Balanced Accuracy \\(\\frac{\\text{Sensitivity + Specificity}}2{}\\) Unitless MCCMathews Correlation Coefficient \\(\\dfrac{\\text{TP} \\cdot \\text{TN} - \\text{FP}\\cdot \\text{FN} }{\\sqrt{(\\text{TP}+\\text{FP})(\\text{TP}+\\text{FN})(\\text{TN}+\\text{FP})(\\text{TN}+\\text{FN})}}\\) \\(\\uparrow\\) Unitless \\([-1, 1]\\) 1 = perfect classification0 = random classification-1 = perfect misclassification Markdedness PPV + NPV - 1 Brier Score Scaled Nagelkerke\u2019s \\(R^2\\) Hosmer-Lemeshow Test Calibration: agreement b/w obs and predicted"},{"location":"CS_Electives/Machine_Learning/12_Evaluation/#graphs","title":"Graphs","text":"Graph Preferred Error Rate ROC Curve How does the classifier compare to a classifier that predicts randomly with \\(p=\\text{TPR}\\)How well model can discriminate between \\(y=0\\) and \\(y=1\\) Top-LeftAt least higher than 45deg line Calibration Graph Create bins of different predicted probabilitiesCalculate the fraction of \\(y=1\\) for each binConfidence intervals (more uncertainty for bins with fewer samples)Histogram showing fraction of samples in each bin Along 45deg line Confusion Probabilities"},{"location":"CS_Electives/Machine_Learning/12_Evaluation/#tradeoff-for-threshold","title":"Tradeoff for Threshold","text":""},{"location":"CS_Electives/Machine_Learning/12_Evaluation/#probabilistic-evaluation_2","title":"Probabilistic Evaluation","text":"<p>Wilson score interval</p> <p></p> <p>You can model accuracy as a binomial distribution with</p> <ul> <li>\\(n=\\) Validation set size</li> <li>= No of predictions</li> <li>= No of k folds * Validation Set Size</li> <li>\\(p=\\) Obtained Accuracy of classifier</li> </ul> <p>Similar to confidence intervals for proportion</p> <p>The uncertainty can be obtained from the distribution</p> <p></p> <pre><code>for n in [100, 1_000, 10_000, 100_000]:\n  dist = stats.binom(n, 0.7)\n\n  alpha = 0.025\n\n  interval_width = dist.isf(alpha) - dist.isf(1-0.975)\n  print(f\"Size: {interval_width/n * 100}\")\n  # returns alpha % of observed accuracy that fall outside the inverval --&gt; This is the maximum further accuracy that is theoretically achievable\n</code></pre>"},{"location":"CS_Electives/Machine_Learning/12_Evaluation/#decision-boundary","title":"Decision Boundary","text":"<p>Plot random distribution of values</p> <p>For eg:</p> <p></p>"},{"location":"CS_Electives/Machine_Learning/12_Evaluation/#confusion-matrix","title":"Confusion Matrix","text":"<p>\\(n \\times n\\) matrix, where \\(n\\) is the number of classes</p>"},{"location":"CS_Electives/Machine_Learning/12_Evaluation/#binary-classification","title":"Binary Classification","text":""},{"location":"CS_Electives/Machine_Learning/12_Evaluation/#multi-class-classification","title":"Multi-Class Classification","text":"<p>Confusion Matrix with respect to A</p> A B C A TP FN FN B FP TN TN C FP TN TN"},{"location":"CS_Electives/Machine_Learning/12_Evaluation/#classification-accuracy-measures","title":"Classification Accuracy Measures","text":""},{"location":"CS_Electives/Machine_Learning/12_Evaluation/#jacquard-index","title":"Jacquard Index","text":"\\[ \\begin{aligned} J(y, \\hat y) &amp;= \\frac{|y \\cap \\hat y|}{|y \\cup \\hat y|} \\\\ &amp;= \\frac{|y \\cap \\hat y|}{|y| + |\\hat y| - |y \\cap \\hat y|} \\end{aligned} \\]"},{"location":"CS_Electives/Machine_Learning/12_Evaluation/#multi-class-averaging","title":"Multi-Class Averaging","text":"Micro-Average All samples equally contribute to average \\(\\dfrac{1}{C}\\sum_{c=1}^C \\dfrac{\\dots}{\\dots}\\) Macro-Average All classes equally contribute to average \\(\\dfrac{\\sum_{c=1}^C \\dots}{\\sum_{c=1}^C \\dots}\\) Weighted-Average Each class\u2019 contribution to average is weighted by its size \\(\\sum_{c=1}^C \\dfrac{n_c}{n}  \\dfrac{\\dots}{\\dots}\\)"},{"location":"CS_Electives/Machine_Learning/12_Evaluation/#inspection","title":"Inspection","text":"Inspection Identify Error Analysis Systematic errors Bias-Variance Analysis General errors"},{"location":"CS_Electives/Machine_Learning/12_Evaluation/#error-analysis","title":"Error Analysis","text":"<p>Residual Inspection</p> <p>Perform all the inspection on</p> <ul> <li>train and dev data</li> <li>externally-studentized residuals, to correct for leverage</li> </ul> <p>There should be no explainable unsystematic component</p> <ul> <li>Symmetric distribution for values of error terms for a given value \\(x\\)</li> <li>Not over time/different values of \\(x\\)</li> <li>This means that</li> <li>you have used up all the possible factors</li> <li>\\(u_i\\) only contains the non-systematic component</li> </ul> Ensure Meaning Numerical Graphical Implication if violated Action if violated Random residuals - No relationship between error and independent variables- No relationship between error and predictions- If there is correlation, \\(\\exists\\) unexplained system component Normality test$E(a b) = 0\\(&lt;br /&gt;\\)r(a, b) = 0\\(&lt;br/&gt;\\)a \\in [u_i, \\vert u_i \\vert , u_i^2]\\(&lt;br/&gt;\\)b \\in [ x_i, \\vert x_i \\vert , x_i^2, y_i, \\vert y_i \\vert , y_i^2 ]$ Q-Q PlotHistogram\\(u_i-x_i\\)\\(\\vert u_i \\vert -x_i\\)\\(u_i^2-x_i\\)\\(u_i-y_i\\)\\(\\vert u_i \\vert -y _i\\)\\(u_i^2-y_i\\) \u274c Unbiased parameters No autocorrelation - Random sequence of residuals should bounce between +ve and -ve according to a binomial distribution- Too many/few bounces may mean that sequence is not randomNo autocorrelation between \\(u_i\\) and \\(u_j\\) \\(r(u_i, u_j \\vert x_i, x_j )=0\\)Runs testDW Test Sequence Plot of \\(u_i\\) vs \\(t\\)Lag Plot of \\(u_i\\) vs \\(u_j\\) \u2705 Parameters remain unbiased\u274c MSE estimate will be lower than true residual varianceProperties of error terms in presence of \\(AR(1)\\) autocorrelation - \\(E[u_i]=0\\)- \\(\\text{var}(u_i)= \\sigma^2/(1-\\rho^2)\\)- \\(r(u_i, u_{i-p}) = \\rho^p \\text{var}(u_i) = \\rho^p \\sigma^2/(1-\\rho^2)\\) Fix model misspecificationIncorporate trendIncorporate lag (last resort)Autocorrelation analysis No effect of outliers Outlier removal/adjustment Low leverage &amp; influence of each point Data transformation Homoscedasticity(Constant variance) $\\sigma^2 (u_i x_i) = \\text{constant}$ should be same \\(\\forall i\\) Error in input variables Total regression Correct model specification Model building Goodness of fit - MLE Percentiles- Kolmogorov Smirnov Significance in difference in residuals for models/baselines Ensure that all the models are truly different, or is the conclusion that one model is performing better than another due to chance Comparing Samples"},{"location":"CS_Electives/Machine_Learning/12_Evaluation/#why-is-this-important","title":"Why is this important?","text":"<p>For eg: Running OLS on Anscombe\u2019s quartet gives</p> <ul> <li>same curve fit for all</li> <li>Same \\(R^2\\), RMSE, standard errors for coefficients for all</li> </ul> <p>But clearly the fit is not equally optimal</p> <ol> <li>Only the first one is acceptable</li> <li>Model misspecification</li> <li>Outlier</li> <li>Leverage</li> </ol> <p></p> <p>which is shown in the residual plot</p> <p></p>"},{"location":"CS_Electives/Machine_Learning/12_Evaluation/#aggregated-inspection","title":"Aggregated Inspection","text":"<ul> <li>Aggregate the data based on metadata</li> <li>Evaluate metrics on groups</li> </ul> \\[ u_i \\vert g(x_i) \\\\ u_i \\vert g(y_i) \\] <p>where \\(g()\\) is the group, which can be \\(x_{ij}, y_i\\) or combination of these</p> <ul> <li>Image blurry/flipped/mislabelled</li> <li>Gender</li> <li>Age</li> <li>Age &amp; Gender</li> </ul>"},{"location":"CS_Electives/Machine_Learning/12_Evaluation/#diebold-mariano-test","title":"Diebold-Mariano Test","text":"<p>Determine whether predictions of 2 models are significantly different</p> <p>Basically the same as Comparing Samples for residuals</p>"},{"location":"CS_Electives/Machine_Learning/12_Evaluation/#bias-variance-analysis","title":"Bias-Variance Analysis","text":""},{"location":"CS_Electives/Machine_Learning/12_Evaluation/#evaluation-curves","title":"Evaluation Curves","text":"<p>Related to Interpretation</p> <ul> <li>Always look at all curves with uncertainties wrt each epoch, train, hyper-parameter value.</li> <li>The uncertainty in-sample and out-sample should also be similar</li> <li>If train set metric uncertainty is small and test set metric uncertainty is large, this is bad even if the average loss metric is same</li> </ul> Learning Curve Loss Curve Validation Curve Loss vs Train Size Epochs Hyper-parameter value Comment Train Error increases with train size, because model overfits small train datasets Purpose: Detect BiasVarianceUtility of adding more data Optimization problemsUndertrainingOvertraining Model ComplexityOptimal Hyperparameters No retraining \u274c \u2705 \u274c No extra computation \u274c \u2705 \u274c"},{"location":"CS_Electives/Machine_Learning/12_Evaluation/#learning-curve","title":"Learning Curve","text":"<p>Based on the slope of the curves, determine if adding more data will help</p> Conclusion High Bias(Underfitting) High Variance(Overfitting) High BiasHigh Variance"},{"location":"CS_Electives/Machine_Learning/12_Evaluation/#loss-curve","title":"Loss Curve","text":""},{"location":"CS_Electives/Machine_Learning/12_Evaluation/#same-model-variable-learning-rate","title":"Same Model, Variable Learning Rate","text":""},{"location":"CS_Electives/Machine_Learning/12_Evaluation/#validation-curve","title":"Validation Curve","text":""},{"location":"CS_Electives/Machine_Learning/12_Evaluation/#neural-network","title":"Neural Network","text":"Recommended Value Activation distributions \\(N(0, 1)\\) Activation gradient distributions \\(N(0, 1)\\) Parameter weight distributions \\(N(0, 1)\\) Parameter gradient distributions \\(N(0, 1)\\) Parameter gradient:weight distributions \\(1\\) Parameter update:weight distributions \\(10^{-3}\\)"},{"location":"CS_Electives/Machine_Learning/13_Selection/","title":"Selection","text":""},{"location":"CS_Electives/Machine_Learning/13_Selection/#models","title":"Models","text":"Null \\(\\beta_0 + u\\) Subset \\(\\beta_0 + \\sum_j^k' \\beta_j x_j + u; k'&lt;k\\) Full \\(\\beta_0 + \\sum_j^k \\beta_j x_j + u\\)"},{"location":"CS_Electives/Machine_Learning/13_Selection/#model-selection","title":"Model Selection","text":"<ol> <li>Fit multiple models \\(g_i\\) on the training data</li> <li>Use interval validation data for hyper parameter tuning of each model \\(g_i\\)</li> <li>Use external validation data for model selection and obtain \\(g^*\\)</li> <li>Combine the training and validation data. Refit \\(g^*\\) on this set to obtain \\(g^{**}\\)</li> <li>Assess the performance of \\(g^{**}\\) on the test data</li> </ol> <p>Finally, train \\(g^{**}\\) on the entire data to obtain \\(\\hat f\\)</p>"},{"location":"CS_Electives/Machine_Learning/13_Selection/#feature-selection","title":"Feature Selection","text":"<p>Also called as Subset/Variable Selection</p> <p>For \\(p\\) potential predictors, \\(\\exists \\ 2^p\\) possible models. Comparing all subsets is computationally-infeasible</p> Selection Type Constraint Type Advantage Disadvantage Full Search Discrete Hard Brute-Force Global optima Computationally-expensive Forward stepwise selection Discrete Hard Starts with the null model, and then adds predictors one-at-a-time Computationally efficientLower sample size requirement Backward Stepwise Selection Discrete Hard Start with the full model and remove predictors one-at-a-time ExpensiveLarge sample size requirement LASSO Continuous Soft Refer to regularization <p>Neither selection method is guaranteed to find the best subset of predictors.</p>"},{"location":"CS_Electives/Machine_Learning/13_Selection/#forward","title":"Forward","text":"<ol> <li>Let M0 denote the null model.</li> <li>Fit all univariate models. Choose the one with the best in-sample fit (smallest RSS, highest R2) and add that variable \u2013 say x(1)\u2013 to M0. Call the resulting model M1.</li> <li>Fit all bivariate models that include x(1): y \u223c \u03b20 + \u03b2(1)x(1) + \u03b2j xj , and add xj from the one with the best in-sample fit to M1. Call the resulting model M2.</li> <li>Continue until your model selection rule (cross-validation error, AIC, BIC) is lower for the current model than for any of the models that add one variable.</li> </ol>"},{"location":"CS_Electives/Machine_Learning/13_Selection/#idk","title":"IDK","text":"<ul> <li>\\(\\alpha_\\text{add}\\) usually \\(0.05\\) or \\(0.10\\)</li> <li>\\(\\alpha_\\text{remove} &gt; \\alpha_\\text{add}\\)</li> </ul>"},{"location":"CS_Electives/Machine_Learning/13_Selection/#forward-stepwise-regression","title":"Forward Stepwise Regression","text":"<ol> <li>Write down full possible model with all predictors, functions, interactions, etc</li> <li>Regress \\(y\\) against each model term individually</li> <li>Pick \\(\\alpha_\\text{add}\\) and \\(\\alpha_\\text{remove}\\) such that \\(\\alpha_\\text{add}&lt;\\alpha_\\text{remove}\\)</li> <li>Pick best regressor<ol> <li>Calculate \\(t=\\beta_j/\\text{SE}(\\beta_j)\\)</li> <li>Calculate \\(p\\) value for each term</li> <li>Pick smallest \\(p\\)-value \\(p^*_j\\)</li> <li>\\(p^*_j&lt;\\alpha_\\text{add} \\implies\\) add parameter \\(j\\)</li> </ol> </li> <li>Check if previous term should be removed</li> <li>For all previously-added regressors, find the one with the lowest \\(t\\) score and hence highest \\(p\\) value \\(p^*_{j'}\\)</li> <li>If \\(p^*_{j'} &gt; \\alpha_\\text{remove}\\), remove \\(j'\\)</li> <li>Repeat step 4-5 until no improvement</li> </ol>"},{"location":"CS_Electives/Machine_Learning/13_Selection/#omitted-variable-bias","title":"Omitted Variable Bias","text":"<p>If a correct regressor \\(x_j\\) is missing from the model, then the remaining model parameters will be biased if \\(x_j\\) is related to the other vars</p> <p>The bias will be proportional to the correlation between the missing \\(x_j\\) and the regressors used in the model</p>"},{"location":"CS_Electives/Machine_Learning/13_Selection/#uncounted-dof","title":"Uncounted DOF","text":"<p>Every time you test a regressor term for the model, it is an addition to the degree of freedom, whether or not you include it in the model</p> <p>Causes data snooping</p> <p>DOF = \\(n-p +\\) no of trials</p>"},{"location":"CS_Electives/Machine_Learning/14_Model_Tuning/","title":"Model Tuning","text":"<p>Be data-driven with model tuning, by closely-examining actual performance</p> <ul> <li>Sometimes you need to decide if it is worth fixing certain type of error</li> </ul>"},{"location":"CS_Electives/Machine_Learning/14_Model_Tuning/#regularization","title":"Regularization","text":"<p>Methods that constrain the complexity of a model</p> <ul> <li>Goal: Improve out-of-sample error by reducing overfitting &amp; variance</li> <li>Tradeoff: Compromising on an increased bias</li> </ul> <p></p> <p>Regularization allows a continuous spectrum of model complexity, rather than discrete jumps</p>"},{"location":"CS_Electives/Machine_Learning/14_Model_Tuning/#reducing-capacity","title":"Reducing Capacity","text":"<p>Reduce the number of</p> <ul> <li>parameters (weights)</li> <li>layers (neural net)</li> <li>units per layers (neural net)</li> <li>Bottle neck layers (neural net)</li> </ul>"},{"location":"CS_Electives/Machine_Learning/14_Model_Tuning/#bottleneck-layers-for-neural-nets","title":"Bottleneck Layer(s) for neural nets","text":"<p>Let</p> <ul> <li>Let layer \\(i\\) and layer \\(j\\) be adjacent layers</li> <li>\\(w_i\\) be weights of layer \\(i\\)</li> <li>\\(\\vert w_i \\vert\\) be the number of units in a layer \\(i\\)</li> <li>\\(w_b\\) be the bottleneck layer</li> <li>effective only if \\(\\vert w_b \\vert &lt; \\arg \\min(\\vert w_i \\vert, \\vert w_j \\vert)\\)</li> </ul> \\[ \\begin{aligned} \\text{Before: } &amp; \\vert w_i \\vert \\cdot \\vert w_j \\vert \\\\ \\text{After: } &amp; \\vert w_i \\vert \\cdot \\vert w_b \\vert + \\vert w_b \\vert \\cdot \\vert w_j \\vert \\end{aligned} \\] <p>Example</p> <pre><code>flowchart LR\na[10&lt;br/&gt;units] --&gt;|100&lt;br/&gt;connections| b[10&lt;br/&gt;units]\nc[10&lt;br/&gt;units] --&gt;|10&lt;br/&gt;connections| d[1&lt;br/&gt;unit] --&gt;|10&lt;br/&gt;connections| e[10&lt;br/&gt;units]</code></pre>"},{"location":"CS_Electives/Machine_Learning/14_Model_Tuning/#increase-dof","title":"Increase DOF","text":"<ul> <li> <p>Reduce \\(k\\)</p> </li> <li> <p>Feature Selection</p> </li> <li> <p>Dimensionality Reduction</p> </li> <li> <p>Increase \\(n\\)</p> </li> <li>Data augmentation</li> </ul>"},{"location":"CS_Electives/Machine_Learning/14_Model_Tuning/#subsampling-at-each-iteration","title":"Subsampling at each iteration","text":"<ul> <li>columns</li> <li>rows</li> </ul>"},{"location":"CS_Electives/Machine_Learning/14_Model_Tuning/#weight-decay","title":"Weight Decay","text":"<p>Also called as</p> <ul> <li>Shrinkage Term</li> <li>Regularization Penalty</li> </ul> <p>Reduce errors by fitting the function appropriately on the given training set, to help reduce variance, and hence avoid overfitting, while minimally affecting bias.</p> <p>This is done by adding a penalty term in the cost function.</p> <p>Note:</p> <ul> <li>All input features must be standardized</li> <li>Intercept should not be penalized</li> <li>You can use different penalty for each term</li> <li>You can perform different regularizer and norm based on expected knowledge of distribution of parameter</li> </ul> \\[ \\begin{aligned} J'(\\theta) &amp;= J(\\theta) + \\dfrac{\\lambda}{n}  \\underbrace{\\textcolor{hotpink}{R(\\theta)}}_ {\\mathclap {\\qquad \\text{Reg Penalty}}} \\end{aligned} \\] <p>where</p> <ul> <li>\\(\\sigma_u =\\) Random Standard Deviation: Stochastic Noise</li> <li>\\(Q_f =\\) Target Complexity: Deterministic Noise</li> </ul> <p>This is similar to $$ \\begin{aligned} E_\\text{aug} &amp;= E_\\text{in} + \\dfrac{\\Omega}{n} \\ &amp;\\updownarrow \\ E_\\text{out} &amp;\\le E_\\text{in} + \\dfrac{\\Omega}{n} \\end{aligned} $$</p> <p>Hence, \\(E_\\text{aug}\\) is a better proxy for \\(E_\\text{out}\\) than \\(E_\\text{in}\\)</p> Regularizer Penalty Effect Robust to outliers Unique solution? Comments \\(\\hat \\beta\\) Limitations Bayesian Interpretation \\(L_0\\) \\(\\sum \\limits_{j=1}^k (\\beta_j \\ne 0)\\)Number of non-zero coefficients Enforces sparsity (Feature selection) Computationally-expensiveNot ConvexNo closed-form soln (requires grad descent) \\(L_1\\)(Lasso: Least Absolute Shrinkage &amp; Selection Operator) \\(\\sum \\limits_{j=1}^k \\gamma_j {\\left \\vert \\dfrac{{\\beta_j - \\mu_{\\beta^*_j} } }{\\sigma_{\\beta^*_j}} \\right \\vert}\\) Encourages sparsity (Feature selection)Eliminates low effect features completely \u2705 \u274c ConvexNo closed-form soln (requires grad descent) \\(\\begin{cases} \\text{sign}({\\hat \\beta}_\\text{OLS}) \\times \\left( \\vert {\\hat \\beta}_\\text{OLS} \\vert - \\lambda/2 \\right) , &amp; \\vert {\\hat \\beta}_\\text{OLS} \\vert &gt; \\lambda/2, \\\\ 0, &amp; \\text{otherwise} \\end{cases}\\) when \\(\\exists\\) highly-correlated features- Results can be random/arbitrary and unstable - Multiple solutions Laplace prior \\(L_2\\)(Rigde) \\(\\sum \\limits_{j=1}^k \\gamma_j \\left( \\dfrac{\\beta_j - \\mu_{\\beta^*_j}}{\\sigma_{\\beta_j^*}} \\right)^2\\) Scale down parametersReduces multi-collinearity \u274c \u2705 ConvexClosed-form soln exists \\(\\dfrac{{\\hat \\beta}_\\text{OLS}}{1 + \\lambda}\\) Normal prior \\(L_q\\) \\(\\sum \\limits_{j=1}^k \\gamma_j {\\left \\vert \\dfrac{{\\beta_j - \\mu_{\\beta^*_j} } }{\\sigma_{\\beta^*_j}} \\right \\vert^q}\\) \\(L_3\\)(Elastic Net) \\(\\alpha L_1 + (1-\\alpha) L_2\\) Not very \u2705 Entropy \\(\\sum \\limits_{j=1}^k - P(\\beta_j) \\ln P(\\beta_j)\\) Encourage parameters to be differentEncourages sparsityCause high variation in between parameters SR3(Sparse Relaxed) <p>where</p> <ul> <li>\\(\\mu_{\\beta^*_j}\\) is the prior-known most probable value of \\(\\beta_j\\)</li> <li>\\(\\sigma_{\\beta^*_j}\\) is the prior-known standard deviation of \\(\\beta_j\\)</li> <li>\\(\\gamma_j\\): Weightage of weight decay</li> <li>Penalize some parameters more than others</li> <li>Useful to penalize higher order terms with \\(\\gamma_j = 2^q\\), where \\(q=\\) complexity of term</li> </ul> <p>\\(\\mu_{\\beta^*_j}\\) and \\(\\sigma_{\\beta^*_j}\\) incorporate desirable Bayesian aspects in our model.</p>"},{"location":"CS_Electives/Machine_Learning/14_Model_Tuning/#bayesian-interpretation","title":"Bayesian Interpretation","text":"<p>Regularization amounts to the use of informative priors, where we introduce our knowledge or belief about the target function in the form of priors, and use them to \u201cregulate\u201d the behavior of the hypothesis we choose</p> <p>Incorporating \\(\\hat \\beta\\) into the regularization incorporates maximum likelihood estimate of the coefficients.</p> <p>Didn\u2019t understand: The standard deviation of the prior distribution corresponds to regularization strength \\(\\lambda\\)</p> <p>Example</p> \\(y\\) \\(\\hat \\beta\\) \\(\\beta x\\) 0 \\(x^\\beta\\) 1 <p></p>"},{"location":"CS_Electives/Machine_Learning/14_Model_Tuning/#idk","title":"IDK","text":""},{"location":"CS_Electives/Machine_Learning/14_Model_Tuning/#contours-of-regularizers","title":"Contours of Regularizers","text":""},{"location":"CS_Electives/Machine_Learning/14_Model_Tuning/#limitations","title":"Limitations","text":"<p>Magnitude of parameters may not always be the best estimate of complexity, especially for Deep Neural Networks</p>"},{"location":"CS_Electives/Machine_Learning/14_Model_Tuning/#why-is-standardization-required","title":"Why is standardization required?","text":"<ul> <li>magnitudes of parameters need to be comparable</li> <li>Penalized estimates are not scale equivariant: multiplying \\(x_j\\) by a constant \\(c\\) can cause a significant change in \\(\\hat \\beta\\) when using regularization</li> </ul>"},{"location":"CS_Electives/Machine_Learning/14_Model_Tuning/#feature-selection-paths","title":"Feature Selection Paths","text":""},{"location":"CS_Electives/Machine_Learning/14_Model_Tuning/#penalty-coefficient-magnitude","title":"Penalty Coefficient Magnitude","text":""},{"location":"CS_Electives/Machine_Learning/14_Model_Tuning/#penalty-coefficient-vs-noise","title":"Penalty Coefficient vs Noise","text":"Stochastic Noise Deterministic Noise \\[ \\begin{aligned} \\lambda^* &amp;\\propto \\sigma^2_u \\\\ \\lambda^* &amp;\\propto Q_f^2 \\end{aligned} \\] <p>where \\(\\lambda^* =\\) Optimal \\(\\lambda\\)</p>"},{"location":"CS_Electives/Machine_Learning/14_Model_Tuning/#frequentist-interpretation","title":"Frequentist Interpretation","text":"<p>\\(\\lambda\\) regulates the smoothness of the spline that solves $$ \\min_h \\left { J(\\theta, x_i, y_i) + \\lambda \\int [h''(t)]^2 \\cdot dt \\right } $$ Penalizing the squared \\(k\\)th derivative leads to a natural spline of degree \\(2k \u2212 1\\)</p> \\(\\lambda\\) Reduces Comment 0 Bias Interpolates every \\((x_i, y_i)\\) \\(\\infty\\) Variance Becomes linear least squares line"},{"location":"CS_Electives/Machine_Learning/14_Model_Tuning/#idk_1","title":"IDK","text":"\\[ \\begin{aligned} &amp;\\arg \\min J(\\theta) \\\\ &amp;\\text{subject to: } \\sum_{j=1}^k (\\beta_j)^2 \\le C \\end{aligned} \\] \\[ \\lambda \\propto \\dfrac{1}{C} \\]"},{"location":"CS_Electives/Machine_Learning/14_Model_Tuning/#optimization-equivalent","title":"Optimization Equivalent","text":"<p>This is equivalent to:</p> <p>At each iteration, shrink the weights by the gradient of the regularization penalty before taking the gradient step</p> <p>For L2: \\((1-\\nu \\lambda)\\) $$ \\begin{aligned} w_{t+1} &amp;= w_{t} - \\nu \\Big[ \\nabla w_t + \\nabla R(w_t) \\Big] \\ &amp;= \\Big[ 1 - \\nu  \\nabla R(w_t) \\Big] w_{t} - \\nu  \\nabla w_t \\</p> <p>\\implies \\text{With L2} &amp;= (1- \\nu \\lambda) w_{t} - \\nu g(t) \\end{aligned} $$</p> <p>Most deep learning libraries incorporate weight decay in the optimizer; however, this isn\u2019t the most conceptually best way to approach weight decay - better to incorporate in the loss function</p>"},{"location":"CS_Electives/Machine_Learning/14_Model_Tuning/#hinted-regularization","title":"Hinted Regularization","text":"<p>Penalize deviation from Model Hints </p>"},{"location":"CS_Electives/Machine_Learning/14_Model_Tuning/#multi-stage-regularization","title":"Multi-Stage Regularization","text":"Stage Goal 1 variable selection LASSO 2 estimation Any method <p>Intuition: Since vars in 2<sup>nd</sup> stage have less \"competition\" from noise variables, estimating using selected variables could give better results</p>"},{"location":"CS_Electives/Machine_Learning/14_Model_Tuning/#system-equation-penalty","title":"System Equation Penalty","text":"<p>Useful if you know the underlying systematic differential equation</p> <p>$$ J'(\\theta) = J(\\theta) + \\text{DE} \\ \\text{RHS(DE)} = 0 $$ Refer to PINNs for more information</p>"},{"location":"CS_Electives/Machine_Learning/14_Model_Tuning/#idk_2","title":"IDK","text":""},{"location":"CS_Electives/Machine_Learning/14_Model_Tuning/#early-stopping","title":"Early Stopping","text":"<ul> <li>This applies to all evaluation curves </li> <li>Stopping criteria</li> <li>In-Sample error &lt; Out-Sample error</li> <li>Evaluation metric<ul> <li>RMSE \\(\\le \\sigma_u\\)</li> <li>MAPE \\(\\le 1-\\text{Accuracy}_\\text{req}\\)</li> </ul> </li> <li>Stop few iterations/value before the optimal point, to reduce variance further: \\(e_\\text{stop} &lt; e^*\\)</li> </ul>"},{"location":"CS_Electives/Machine_Learning/14_Model_Tuning/#dropout","title":"Dropout","text":"<p>Dropout is applied on the output of hidden fully-connected layers</p> <ul> <li>Makes networks \u201crobust\u201d to missing activations</li> <li>Stochastic approximation</li> </ul> <p></p>"},{"location":"CS_Electives/Machine_Learning/14_Model_Tuning/#training","title":"Training","text":"<p>Stochastically drop out units with probability \\(p_\\text{drop}\\) and keep with \\(p_\\text{keep}=1-p_\\text{drop}\\)</p> \\[ (w'_t)_j = \\begin{cases} 0, &amp; \\text{with prob } p \\\\ (w_t)_j, &amp; \\text{o.w} \\end{cases} \\] <p>Annealed dropout</p>"},{"location":"CS_Electives/Machine_Learning/14_Model_Tuning/#evaluationproduction","title":"Evaluation/Production","text":"<p>At inference time, dropout is inactive, as we should not predict stochastically</p> Approach Time Advantages Disadvantage Naive approach Test Simply not use dropout All units receive \\((1/p_\\text{drop})\\) times as many incoming signals compared to training, so responses will be different Test-Time Rescaling Test Multiply weights by \\(p_\\text{keep}\\) Comparing  similar architectures w/ and w/o dropout requires implementing 2 different networks at test time Dropout Inversion(preferred) Train Divide weights by \\(p_\\text{keep}\\) Overcome limitations of Test-Time RescalingAllows for annealed dropout"},{"location":"CS_Electives/Machine_Learning/14_Model_Tuning/#ensembling","title":"Ensembling","text":"<p>Reduces variance from high-variance models, such as trees</p>"},{"location":"CS_Electives/Machine_Learning/14_Model_Tuning/#noise-injection","title":"Noise Injection","text":"<ul> <li>Add noise to inputs</li> <li>Add noise to outputs</li> </ul> <p>Behaves similar to L2 regularization</p>"},{"location":"CS_Electives/Machine_Learning/14_Model_Tuning/#label-smoothing","title":"Label Smoothing","text":"<p>The output of \\(\\sigma, \\tanh\\) never actually really output the maximum/minimum range values. So the model will keep trying to make the predictions go to exact \\(0/1\\) (which is never attainable), making it prone to overfitting</p> <p>Using label smoothing, model becomes less confident with extremely confident labels (which we want to avoid). Now, the penalty given to a model due to an incorrect prediction will be slightly lower than using hard labels which would result in smaller gradients</p> Modify Target \\(y' = \\begin{cases} y - \\epsilon, &amp; y = y_\\text{true} \\\\ y + \\dfrac{\\epsilon}{C-1}, &amp; \\text{o.w} \\end{cases}\\) Loss \\(L' = (1-\\epsilon) L_i + \\dfrac{\\epsilon}{C-1} \\sum_j L_j\\) <p>where</p> <ul> <li>\\(y_\\text{true}\\) is the true label</li> <li>\\(C=\\) total number of classes/labels</li> <li>\\(\\epsilon \\approx 0\\)</li> </ul> <p>Extreme cases for \\(\\epsilon\\)</p> <ul> <li>\\(\\epsilon=0\\): original</li> <li>\\(\\epsilon=1\\): uniform</li> </ul>"},{"location":"CS_Electives/Machine_Learning/14_Model_Tuning/#active-teaching","title":"Active Teaching","text":"<pre><code>flowchart LR\nc(( ))\nm[ML]\n\nd[(Dataset)] --&gt; m &amp; EDA\n\nEDA --&gt;\nrb[Rule-Based System]\n\nsubgraph Active Teaching\n  rb &amp; m &lt;--&gt;|Compare &amp; Update| c\nend</code></pre>"},{"location":"CS_Electives/Machine_Learning/14_Model_Tuning/#neural-network","title":"Neural Network","text":"Phase Hessian Mode Connectivity Model Similarity Treatment 1 Large -ve Low Larger network 2 Large +ve Low Smaller learning rate 3 Small -ve Low Larger network 4-A Small \\(\\approx 0\\) Low Increase train size 4-B Small \\(\\approx 0\\) High \u2705"},{"location":"CS_Electives/Machine_Learning/15_Hyperparameter_Tuning/","title":"Hyper-Parameter Tuning","text":"Advantage Disadvantage Manual Time-Consuming Grid Search Computationally-expensive Random Search Non-deterministic Evolutionary Randomization, Natural Selection, Mutation Bayesian Probabilistic model of relationship b/w cost function and hyper-parameters, using information gathered from trials Gradient-Based Treat hyper parameter tuning like parameter fitting Early-Stopping Focus resources on settings that look promisingeg: Successive Halving"},{"location":"CS_Electives/Machine_Learning/15_Hyperparameter_Tuning/#speed-up","title":"Speed Up","text":"<ul> <li>Parallelizing</li> <li>Caching</li> <li>Random sampling: Won\u2019t work with caching</li> </ul>"},{"location":"CS_Electives/Machine_Learning/15_Hyperparameter_Tuning/#clustering","title":"Clustering","text":""},{"location":"CS_Electives/Machine_Learning/15_Hyperparameter_Tuning/#elbow-method","title":"Elbow Method","text":"<p>Plot cost function as function of no of clusters</p> <p></p>"},{"location":"CS_Electives/Machine_Learning/16_Statistical_Inference/","title":"16 Statistical Inference","text":""},{"location":"CS_Electives/Machine_Learning/16_Statistical_Inference/#statistical-inference","title":"Statistical Inference","text":"<p>Deals with the problem of uncertainty in estimates due to sample variability</p> <p>Does not deal with</p> <ul> <li>Whether model specification is correct</li> <li>Whether \\(x\\) has causal effect on \\(y\\)</li> <li>Whether model is good for describing causal effect of \\(x\\) on \\(y\\)</li> </ul>"},{"location":"CS_Electives/Machine_Learning/16_Statistical_Inference/#hypothesis-testing","title":"Hypothesis Testing","text":"\\[ \\begin{aligned} &amp; H_0: \\beta_j = 0 \\\\ &amp; {\\tiny \\text{ that includes other all predictors and nothing else}} \\\\ \\\\ &amp; H_1: \\text{o.w} \\end{aligned} \\] <ul> <li>Rejection of \\(H_0 \\centernot \\implies \\beta\\)  significantly different from 0. Therefore, to assess magnitude of \\(\\beta\\), confidence intervals are more useful than \\(p\\)-values</li> <li>Rejection of \\(H_0\\) does not mean that \\(x\\) has a significant causal effect on \\(y\\). Statistical significance \\(\\centernot \\implies\\) scientific, real-world significance. The most important variables are not those with the smallest p-values.</li> <li>The t\u2212test can be thought of as checking whether adding \\(x_j\\) really improves predictions in a model that contains other specified predictors</li> <li>95% CI = \\(\\text{LL}, \\text{UL} \\centernot \\implies \\text{Pr}(\\beta \\in [\\text{LL, UL}]) = 0.95\\)</li> <li>Correct interpretation: a 95% CI for \\(\\beta\\) means that if     we estimate our model on many independent random samples drawn     from the same population and construct \\(\\text{CI}_m = [\\text{LL}_m, \\text{UL}_m]\\) on each sample, then 95% of these \\(\\{ CI_m \\}\\) will contain \\(\\beta\\)</li> </ul> \\[ L(H1, \\hat H_1) = \\begin{cases} 0, &amp; H_1 = \\hat H_1 \\\\ 95, &amp; H_1 = 0, \\hat H_1 = 1 \\\\ 5, &amp; H_1 = 1, \\hat H_1 = 0 \\end{cases} \\]"},{"location":"CS_Electives/Machine_Learning/16_Statistical_Inference/#p-value","title":"P-Value","text":"<p>P-value is not the conditional probability of \\(H_0\\). It is actually the probability of \\(H_0\\) being true based only on the observed data set (without incorporating prior knowledge) $$ \\begin{aligned} p \\text{-value} &amp;\\ne P(H_0 = \\text{True} \\vert D) \\ p \\text{-value} &amp;= P(D \\vert H_0 = \\text{True} )\\ &amp;= \\text{Pr}(\\vert t \\vert \\ge \\vert t(\\hat \\beta)  \\vert H_0) \\end{aligned} $$</p> \\[ \\begin{aligned} \\text{What actually} &amp; \\text { needed}\\\\ P(H_0 = \\text{True} \\vert D) &amp;= \\dfrac{P(D \\vert H_0) \\cdot P(H_0)}{P(D)} \\\\ &amp;= \\dfrac{p \\cdot P(H_0)}{P(D)} \\\\ &amp;= \\dfrac{p \\cdot P(H_0)}{p \\cdot P(H_0) + (1-p) \\cdot P(H_1)} \\end{aligned} \\] <p>where \\(D\\) is the data</p> <p>When \\(P(H_1) &lt; 0.1\\), we may need the p\u2212value to be much smaller than the conventional threshold of \\(\\alpha = 0.05\\) in order to \u201cconfidently\u201d reject \\(H_0\\)</p> <ul> <li>For example, concluding that a coin is biased would require a significant number of one-sided results </li> </ul> <p>Hypothesis tests are only valid for large sample size, as they are based on the asymptotic properties of test statistics.  Hence, Bootstrapping can be used to obtain more accurate p\u2212value estimates</p> <p></p>"},{"location":"CS_Electives/Machine_Learning/16_Statistical_Inference/#information-content-of-statistical-nonsignificance","title":"Information Content of Statistical (Non)Significance","text":"<p>Statistical result is informative only when it has the potential to substantially change our beliefs. The discrepancy between a prior and a posterior distribution thus provides a basic measure of the informativeness of a statistical result.</p> <p>Using this measure, non-significant results are often more informative than significant results in scenarios common in empirical economics.</p> <p>Hence, null need not always be \\(H_0: \\beta = 0\\). It can be what is prior known. This can be implemented in ridge regression by using a prior known value</p> <ul> <li>Beliefs on the causal effect of a policy intervention are usually better described by a continuous distribution rather than a distribution with significant probability mass at point zero.</li> </ul> <p>When \\(P(H_0)\\) is low, statistical significance often carries little information; non-significance is highly informative, because in this case, non-significance is more \u201csurprising\u201d and induces a larger change in the posterior belief $$ \\underbrace{1 - \\dfrac{p(\\beta \\vert R=0)}{p(\\beta)}}\\text{INS} \\ = \\dfrac{P(R=1)}{P(R=0)} \\times \\underbrace{1 - \\dfrac{p(\\beta \\vert R=1)}{p(\\beta)}}\\text{IS} $$ where</p> <ul> <li>\\(R=H_0 \\text{ rejected}\\) at given significance level</li> <li>\\(P(R = 1)\\) is the prior probability of rejection of the null</li> <li>\\(P(R = 1) = \\int P(R = 1 \\vert \\beta)  \\cdot p(\\beta) \\cdot d\\theta\\)</li> <li>\\(\\text{INS}\\) = Informativeness of non-significance</li> <li>\\(\\text{IS}\\) = Informativeness of significance</li> </ul>"},{"location":"CS_Electives/Machine_Learning/16_Statistical_Inference/#takeaways","title":"Takeaways","text":"<ul> <li>Non-significance is more informative than significance as long as   \\(P(R = 1) &gt; 0.5\\)</li> <li>As \\(n\\) inc and \\(p(\\beta=0)\\) dec, \\(p(R=1)\\) increases</li> <li>Thus, as datasets get larger, and because there are rarely reasons to put significant priors on \\(\\theta=0\\), non-significant results will be more informative in empirical studies in economics</li> <li>When \\(n\\) is very large, without prior probability mass at the point null, significance carries no information</li> </ul>"},{"location":"CS_Electives/Machine_Learning/16_Statistical_Inference/#statistical-significance-filter","title":"Statistical Significance Filter","text":"<p>Publication Bias</p> <p>Only the extreme significant cases of the study make it through to the publication, and hence are not a representative sample of all empirical findings.</p> <p></p> <p>\\(E[\\hat \\beta \\vert \\text{significant} &gt;&gt; \\beta]\\)</p> <p>The power of test is low; The null hypothesis is false, but fails to be rejected \\((1-\\alpha \\% )\\) of the time</p> <p>Lower power leads to high exaggeration ratios, ie if the estimate is statistically significant, it must be at least \\(a\\) times higher than the true effect size</p> <p>Type \\(S\\) error probability: if the error is statistically-significant, but has the wrong sign</p>"},{"location":"CS_Electives/Machine_Learning/16_Statistical_Inference/#multiple-testing","title":"Multiple Testing","text":"<p>Multiple comparisons</p> <p>Assuming each test is independent, under \\(H_0\\) of all tests</p> <p>If you perform multiple hypothesis tests, the probability of at least one producing a statistically-significant result at the significance level \\(\\alpha\\) due to chance, is necessarily greater than \\(\\alpha\\)</p>"},{"location":"CS_Electives/Machine_Learning/16_Statistical_Inference/#fwer","title":"FWER","text":"<p>Joint Type 1 Error/FWER (Family-wise Error Rate) is the probability of making at least 1 type 1 error when simultaneously performing \\(m\\) hypothesis tests $$ P(\\ge 1 \\text{ false positive}) = 1 - (1-\\alpha)^m $$ where \\(m\\) is the number of tests conducted (ie model specifications tried)</p> <p></p>"},{"location":"CS_Electives/Machine_Learning/16_Statistical_Inference/#bonferroni-correction","title":"Bonferroni Correction","text":"<p>Bounds the FWER at below \\(\\alpha\\) by setting the significance threshold for each individual test as \\(\\alpha/m\\) $$ 1 - \\left(1 - \\dfrac{\\alpha}{m} \\right)^m \\le \\alpha $$ It is conservative, as it is assumes independent tests.</p> <p>For large \\(m\\), it leads to a significant loss of power, ie higher probability of false negative</p>"},{"location":"CS_Electives/Machine_Learning/16_Statistical_Inference/#selective-inference","title":"Selective Inference","text":"<p>Assessing strength of evidence after obtaining the \u2018best model\u2019 through searching from a large number of models</p> <p>If not taken into account, the effects of selection can greatly exaggerate the apparent strengths of relationships.</p> <p>Also called as Post-Selection Inference</p>"},{"location":"CS_Electives/Machine_Learning/16_Statistical_Inference/#context","title":"Context","text":"<p>To conduct statistical inference for procedures that involve model selection, such as forward stepwise regression or the lasso, it is tempting to look only at the final selected model. However, such inference is generally invalid</p> <p>The problem is essentially the same as those of specification search and data-snooping: an observed correlation of 0.9 between x and y may be noteworthy. However, if x is found by searching over 100 variables looking for the one with the highest observed correlation with y, then the finding is no longer as impressive and could well be due to chance</p> <p></p> <p></p>"},{"location":"CS_Electives/Machine_Learning/16_Statistical_Inference/#solution-conditional-coverage","title":"Solution: Conditional Coverage","text":"<p>Make the inference conditional on the model selected</p> <p>Construct CI for \\(\\beta_{j, M}\\) conditional on model \\(M\\) being selected: $$ P(\\beta_{j, M} \\in C_{j,m} \\vert M \\text{ selected}) \\ge 1 - \\alpha $$</p>"},{"location":"CS_Electives/Machine_Learning/17_Uncertainty/","title":"Uncertainty","text":""},{"location":"CS_Electives/Machine_Learning/17_Uncertainty/#types-of-uncertainty","title":"Types of Uncertainty","text":"Others\u2019 knowledgeOur knowledge Known Unknown Known Things we are certain of We know there are things we can\u2019t predicteg: Random Process Unknown Others know but you don\u2019t knoweg: Insufficient data Completely unexpected/unforeseeable eventseg: Unknown distribution Aleatoric Epistemic Uncertainty in Data Model Cause Noisy input dataMeasurement errors Missing training data Describes confidence in Input data Prediction Reducible through more training data \u274c \u2705 Can be learnt by model \u2705 \u274c Solution Better instruments/measurements Get more data"},{"location":"CS_Electives/Machine_Learning/17_Uncertainty/#uncertainty-intervals","title":"Uncertainty Intervals","text":"<p>You can obtain uncertainty using</p> Concept Limitations Asymptotic approach Central limit theorem - Requires large sample size to satisfy asymptotic condition- Assumes normal distribution of errors- Assumes homoscedascity- Requires appropriate formula for calculating standard error (not possible for complex models) Bootstrapping(preferred) Random sampling with replacement Higher computation cost \\[ \\hat y \\pm t_{\\alpha/2} \\times \\text{SE} \\] Coefficient Confidence Interval Response Confidence Interval Response Prediction Interval Denotation \\(\\sigma_{\\hat \\beta}\\) \\(\\sigma\\Big(\\hat \\mu(y \\vert x) \\Big)\\) \\(\\sigma\\Big( \\hat y \\vert x \\Big)\\) The upper and lower bound for estimated __ at a given level of significance \\(\\hat \\beta\\) \\(\\hat \\mu(y \\vert x)\\) \\(\\hat y \\vert x\\) SE (Standard Error) for Univariate Regression(Asymptotic Approach) \\(\\dfrac{\\text{RMSE}}{\\sqrt{\\sum (x_{\\text{pred}_\\text{cent}} )^2}}\\) \\(\\text{RMSE} \\times \\sqrt{ \\dfrac{1}{n} + \\dfrac{(x_{\\text{pred}_\\text{cent}} )^2}{n \\sigma_x^2}}\\) \\(\\text{RMSE}  \\times \\sqrt{ \\textcolor{hotpink}{1 +} \\dfrac{1}{n} + \\dfrac{(x_{\\text{pred}_\\text{cent}})^2}{n \\sigma_x^2}}\\) SE (Standard Error) for Multivariate Regression(Asymptotic Approach) \\({\\text{CovMatrix}_\\beta}_{ii}\\) \\(\\text{RMSE} \\times \\sqrt{\\dfrac{1}{n} + J'_{{x_\\text{pred}}_\\text{cent}} \\ \\text{CovMatrix}_{X} \\ J'_{{x_\\text{pred}}_\\text{cent}}}\\) \\(\\text{RMSE} \\times \\sqrt{\\textcolor{hotpink}{1+} \\dfrac{1}{n} + x'_{\\text{pred}_\\text{cent}} \\ \\text{CovMatrix}_{X} \\ x_{\\text{pred}_\\text{cent}}}\\) \\[ \\begin{aligned} x'_{\\text{pred}_\\text{cent}} &amp;= x_\\text{pred} - \\bar X \\\\ X_\\text{cent} &amp;= X - \\bar X \\\\ \\\\ \\text{CovMatrix}_{\\beta} &amp;= \\text{RMSE} \\times \\sqrt{\\text{CovMatrix}_{X}} \\\\ \\text{CovMatrix}_{X} &amp;= H^{-1} \\\\ H^{-1} &amp;\\approx (J' J)^{-1} \\\\ J &amp;= X \\text{ for degree 1} \\end{aligned} \\] <p>Where</p> <ul> <li>\\(J\\): Jacobean matrix</li> <li>\\(H\\): Hessian matrix</li> </ul> <p>High values for non-diagonal elements of \\(\\text{Cov}_\\beta\\) means that the errors of \\(\\beta\\) are correlated with each other.</p> <p>Degree of freedom \\(= n - k - 1\\), where</p> <ul> <li>\\(n =\\) sample size</li> <li>\\(k=\\) no of input variables</li> </ul> <p>Confidence and prediction intervals are narrowest at \\(X = \\bar X\\), and get wider further from this point.</p> <p></p> <p>Under homoskedasticity, $$ \\begin{aligned} \\hat V(\\hat \\beta) &amp;= (X' X)^{-1} \\hat \\sigma^2 \\ &amp;=\\dfrac{\\hat \\sigma^2}{\\hat u_j' \\hat u_j} \\end{aligned} $$</p>"},{"location":"CS_Electives/Machine_Learning/17_Uncertainty/#note","title":"Note","text":"<ul> <li>RMSE = RMSE of validation data</li> <li>If your validation error distribution is not normal, or you have a lot of data, you can use the quantiles of validation error distribution for the confidence intervals</li> </ul>"},{"location":"CS_Electives/Machine_Learning/17_Uncertainty/#intervals-using-models-prediction","title":"Intervals using Models\u2019 Prediction","text":"<p>For each data point, take __ of multiple models</p> <ul> <li>average</li> <li>5<sup>th</sup> quantile</li> <li>95<sup>th</sup> quantile</li> </ul>"},{"location":"CS_Electives/Machine_Learning/17_Uncertainty/#predictive-density","title":"Predictive Density","text":"<p>Describes the full probabilistic distribution \\(\\forall x\\)</p> <p></p>"},{"location":"CS_Electives/Machine_Learning/17_Uncertainty/#trajectoriesscena-rios","title":"Trajectories/Scena rios","text":"<p>Equally-likely samples of multivariate predictive densities</p> <p></p>"},{"location":"CS_Electives/Machine_Learning/17_Uncertainty/#uncertainty-propagation","title":"Uncertainty Propagation","text":"<p>This table shows the variances and standard deviations of simple functions of the real variables \\(A,B\\!\\), with standard deviations \\(\\sigma_A, \\sigma_B,\\,\\) covariance \\(\\sigma_{AB}=\\rho_{AB}\\sigma_A\\sigma_B\\,\\), and correlation \\(\\rho_{AB}\\). The real-valued coefficients \\(a\\) and \\(b\\) are assumed exactly known (deterministic), i.e., \\(\\sigma_a = \\sigma_b = 0\\).</p> <p>In the columns \"Variance\" and \"Standard Deviation\", A and B should be understood as expectation values (i.e. values around which we're estimating the uncertainty), and \\(f\\) should be understood as the value of the function calculated at the expectation value of \\(A,B\\!\\).</p> Function Variance \\(aA\\) \\(= a^2\\sigma_A^2\\) \\(aA + bB\\) \\(= a^2\\sigma_A^2 + b^2\\sigma_B^2 + 2ab\\,\\sigma_{AB}\\) \\(aA - bB\\) \\(= a^2\\sigma_A^2 + b^2\\sigma_B^2 - 2ab\\,\\sigma_{AB}\\) \\(AB\\) \\(\\approx f^2 \\left[\\left(\\frac{\\sigma_A}{A}\\right)^2 + \\left(\\frac{\\sigma_B}{B}\\right)^2 + 2\\frac{\\sigma_{AB}}{AB} \\right]\\)[<sup>1][</sup>2] \\(\\frac{A}{B}\\) \\(\\approx f^2 \\left[\\left(\\frac{\\sigma_A}{A}\\right)^2 + \\left(\\frac{\\sigma_B}{B}\\right)^2 - 2\\frac{\\sigma_{AB}}{AB} \\right]\\)[^3] \\(\\frac{A}{A+B}\\) \\(\\approx \\frac{f^2}{\\left(A+B\\right)^2} \\left(\\frac{B^2}{A^2}\\sigma_A^2  +\\sigma_B^2 - 2\\frac{B}{A} \\sigma_{AB} \\right)\\) \\(a A^{b}\\) \\(\\approx \\left( {a}{b}{A}^{b-1}{\\sigma_A} \\right)^2 = \\left( \\frac{{f}{b}{\\sigma_A}}{A} \\right)^2\\) \\(a \\ln(bA)\\) \\(\\approx \\left(a \\frac{\\sigma_A}{A} \\right)^2\\)[^4] \\(a \\log_{10}(bA)\\) \\(\\approx \\left(a \\frac{\\sigma_A}{A \\ln(10)} \\right)^2\\)[^5] \\(a e^{bA}\\) \\(\\approx f^2 \\left( b\\sigma_A \\right)^2\\)[^6] \\(a^{bA}\\) \\(\\approx f^2 (b\\ln(a)\\sigma_A)^2\\) \\(a \\sin(bA)\\) \\(\\approx \\left[ a b \\cos(b A) \\sigma_A \\right]^2\\) \\(a \\cos \\left( b A \\right)\\) \\(\\approx \\left[ a b \\sin(b A) \\sigma_A \\right]^2\\) \\(a \\tan \\left( b A \\right)\\) \\(\\left[ a b \\sec^2(b A) \\sigma_A \\right]^2\\) \\(A^B\\) \\(\\approx f^2 \\left[ \\left( \\frac{B}{A}\\sigma_A \\right)^2 +\\left( \\ln(A)\\sigma_B \\right)^2 + 2 \\frac{B \\ln(A)}{A} \\sigma_{AB} \\right]\\) \\(\\sqrt{aA^2 \\pm bB^2}\\) \\(\\approx \\left(\\frac{A}{f}\\right)^2 a^2\\sigma_A^2 + \\left(\\frac{B}{f}\\right)^2 b^2\\sigma_B^2 \\pm 2ab\\frac{AB}{f^2}\\,\\sigma_{AB}\\) <p>For uncorrelated variables (\\(\\rho_{AB}=0\\), \\(\\sigma_{AB}=0\\)) expressions for more complicated functions can be derived by combining simpler functions. For example, repeated multiplication, assuming no correlation, gives \\(f = ABC; \\qquad \\left(\\frac{\\sigma_f}{f}\\right)^2 \\approx \\left(\\frac{\\sigma_A}{A}\\right)^2 + \\left(\\frac{\\sigma_B}{B}\\right)^2+ \\left(\\frac{\\sigma_C}{C}\\right)^2.\\)</p> <p>For the case \\(f = AB\\) we also have Goodman's expression[^7] for the exact variance: for the uncorrelated case it is \\(V(XY)= E(X)^2 V(Y) + E(Y)^2 V(X) + E((X-E(X))^2 (Y-E(Y))^2)\\) and therefore we have: \\(\\sigma_f^2 = A^2\\sigma_B^2 + B^2\\sigma_A^2 +  \\sigma_A^2\\sigma_B^2\\)</p>"},{"location":"CS_Electives/Machine_Learning/17_Uncertainty/#effect-of-correlation-on-differences-effect_of_correlation_on_differences","title":"Effect of correlation on differences [effect_of_correlation_on_differences]","text":"<p>If A and B are uncorrelated, their difference A-B will have more variance than either of them. An increasing positive correlation (\\(\\rho_{AB}\\to 1\\)) will decrease the variance of the difference, converging to zero variance for perfectly correlated variables with the same variance. On the other hand, a negative correlation (\\(\\rho_{AB}\\to -1\\)) will further increase the variance of the difference, compared to the uncorrelated case.</p> <p>For example, the self-subtraction f=A-A has zero variance \\(\\sigma_f^2=0\\) only if the variate is perfectly autocorrelated (\\(\\rho_A=1\\)). If A is uncorrelated, \\(\\rho_A=0\\), then the output variance is twice the input variance, \\(\\sigma_f^2=2\\sigma^2_A\\). And if A is perfectly anticorrelated, \\(\\rho_A=-1\\), then the input variance is quadrupled in the output, \\(\\sigma_f^2=4\\sigma^2_A\\) (notice \\(1-\\rho_A=2\\) for f = aA \u2212 aA in the table above).</p>"},{"location":"CS_Electives/Machine_Learning/17_Uncertainty/#value-at-risk-models","title":"Value at Risk Models","text":"<ul> <li>Derive the risk profile of the firm</li> <li>Protect firm against unacceptably large concentrations</li> <li>Quantify potential losses</li> </ul> <ol> <li>Collect data</li> <li>Graph the data to inspect data quality</li> <li>Transform prices data into returns form (percentage diff of prices)</li> <li>Look at the frequency distribution</li> <li>Obtain the standard deviation (volatility)</li> <li>Multiply volatility with one-sided \\(Z_1\\) to estimate 99% worst-case loss</li> </ol>"},{"location":"CS_Electives/Machine_Learning/18_Interpretation/","title":"Model Interpretation","text":"<p>Association \\(\\ne\\) Causation</p>"},{"location":"CS_Electives/Machine_Learning/18_Interpretation/#classification-of-inference-techniques","title":"Classification of Inference Techniques","text":"<ul> <li>IDK</li> <li>Model-Specific</li> <li>Model-Agnostic</li> <li>Scope</li> <li>Global: Explanation for entire dataset</li> <li>Local: Explanation for single data point</li> </ul>"},{"location":"CS_Electives/Machine_Learning/18_Interpretation/#inference-techniques","title":"Inference Techniques","text":"IDK Scope Simple Linear Regression\\(y = \\beta_0 + \\beta_j x_j\\) Model-Specific Global For every unit increase in \\(x_j\\), \\(y\\) changes by \\(\\beta_j\\) units \\(\\ln \\vert y \\vert = \\beta_0 + \\beta_j x_j\\) Model-Specific Global For every unit increase in \\(x_j\\), percentage change in \\(y\\) is \\(\\beta_j\\) units \\(\\ln \\vert y \\vert = \\beta_0 + \\beta_j \\ln \\vert x_j \\vert\\) Model-Specific Global Elasticity of \\(y\\) wrt \\(x_j\\) is given by \\(\\beta_j\\)\\(\\beta_j = \\dfrac{\\% \\Delta y}{\\% \\Delta x_j}\\) SAGE Model-Agnostic Global Variable/Feature Importance Model-Agnostic Global Decrease of in-sample error due to splits over \\(x\\), averaged over all trees of ensemble Partial Dependence Model-Agnostic Global Partial derivative of \\(y\\) wrt \\(x\\): Marginal effect of \\(x\\) on \\(y\\) after integrating out all other vars SHAP Model-Agnostic Local LIME Model-Agnostic Local"},{"location":"CS_Electives/Machine_Learning/19_Compression/","title":"Model Compression","text":"Quantization Reducing precision from Float64 to Int8 Pruning Removing unnecessary aspects of the modelRemoving neurons in ANN"},{"location":"CS_Electives/Machine_Learning/20_Production/","title":"Production","text":"<p>Stage after deploying the model to work with live data</p> <p>Refer to https://www.youtube.com/watch?v=2wXA1jQqJJ4&amp;list=PLdfopzFjkPz9shHCeH9poe9sbAn0pIojX</p>"},{"location":"CS_Electives/Machine_Learning/20_Production/#drift","title":"Drift","text":"Type Meaning Identification Solution Concept Drift Data Drift Train &amp; Test data not from same distribution Adversarial Validation Anomaly Detection"},{"location":"CS_Electives/Machine_Learning/20_Production/#adversarial-validation","title":"Adversarial Validation","text":"<p>Create a new feature in the dataset as \u201cSet\u201d, which signifies if the data belongs to training/test set</p> <p>Train a classifier to predict which set</p> <p>ROC-AUC signifies how accurately the classifier can distinguish between the sets. Higher values \\(\\ge 0.8\\) imply that Train &amp; Test data not from same distribution.</p>"},{"location":"CS_Electives/Machine_Learning/20_Production/#deployment-checklist","title":"Deployment Checklist","text":"<ul> <li>Realtime or batch training</li> <li>Cloud vs Edge/Browser</li> <li>Computer resources (CPU/GPU/Memory)</li> <li>Latency, throughput (QPS)</li> <li>Logging</li> <li>Security &amp; Privacy</li> </ul>"},{"location":"CS_Electives/Machine_Learning/20_Production/#scenarios-of-deployment","title":"Scenarios of Deployment","text":"<ul> <li>New product/capability</li> <li>Automate/assist with manual task</li> <li>Replace previous ML system</li> </ul>"},{"location":"CS_Electives/Machine_Learning/20_Production/#types-of-deployment","title":"Types of Deployment","text":"Type Canary Roll out to small fraction of traffic initiallyMonitor system and ramp up traffic gradually Blue-Green Fully deploy new version (green)Keep old model dormant, and rollback to it if required (blue)"},{"location":"CS_Electives/Machine_Learning/20_Production/#degrees-of-automation","title":"Degrees of Automation","text":"Human-Only Shadow Mode AI Assistance Partial Automation Full automation"},{"location":"CS_Electives/Machine_Learning/20_Production/#monitoring","title":"Monitoring","text":"<ul> <li>Brainstorm potential problems</li> <li>Brainstorm appropriate metrics to identify the problems</li> <li>Software Metrics<ul> <li>Memory</li> <li>Compute</li> <li>Latency</li> <li>Throughput</li> <li>Server load</li> </ul> </li> <li>Input Metrics<ul> <li>Average Input length</li> <li>Fraction of rows with missing values</li> <li>Average image brightness</li> </ul> </li> <li>Output metrics<ul> <li>Missing outputs</li> <li>No of times user redoes search</li> <li>CTR (ClickThrough Rate): No of clicks that your ad receives divided by the number of times your ad</li> </ul> </li> </ul>"},{"location":"CS_Electives/Machine_Learning/20_Production/#model-serving","title":"Model Serving","text":""},{"location":"CS_Electives/Machine_Learning/20_Production/#handling-data-drift","title":"Handling Data Drift","text":"<ul> <li>Use a batch-streaming hybrid</li> <li>Works when we have the label associated with every data point, such as in Recommender Systems</li> <li>Give higher sample weight to recent datapoints</li> </ul>"},{"location":"CS_Electives/Machine_Learning/21_Performance_Optimization/","title":"Performance Optimization","text":"<ul> <li>Subsample</li> <li>Multi-Threading</li> <li>Caching</li> <li>Partial fit</li> </ul>"},{"location":"CS_Electives/Machine_Learning/22_Learning_Algorithms/","title":"Learning Algorithms","text":"Learning Type Task Algorithm Comment Probabilistic Parametric Scope \\(d_\\text{VC}\\) Bias Variance Generalization Advantages Disadvantages Supervised Regression OLS \u274c \u2705 Global \\(k+1\\) High Low Good\\(n &gt;&gt; k\\) Classification Logistic \u2705 \u2705 Global \\(k+1\\) High Low Good\\(n &gt;&gt; k\\) Regression/Classification Piecewise Constant \u274c \u274c Local Regression/Classification Piecewise Polynomial \u274c \u274c Local Regression/Classification SVM Margin-Based \u274c \u2705 Computationally-expensive Regression/Classification Gaussian Processes \u2705 \u2705 Regression/Classification KNNNearest Neighbor Good baseline modelCan use mean, median, mode for regressionCan use weightage, voting for classification \u274c \u274c Regression/Classification Decision Tree Automatic Piecewise ConstantExactly opposite in characteristics wrt to OLS \u274c \u274c Local Low High - Highly-interpretable- Auto-detect non-linear relationships- Auto-model variable interactions- Fast evaluation: Traversal only occurs on subset of attributes - Poor regressive performance- Unstable: Tree struct sensitive to train data; changing train data changes tree- Require large no of splits for even simple relationships Regression/Classification Linear Tree Automatic Piecewise Polynomial \u274c \u274c Local Regression/Classification Random Forest Bagged Trees \u274c \u274c Local Regression/Classification XGBoost Boosted Trees \u274c \u274c Local Regression/Classification CatBoost Boosted Trees \u274c \u274c Local Regression/Classification LightGBM Boosted Trees \u274c \u274c Local Unsupervised Clustering K-MeansK-Medoids \u274c \u274c Clustering Gaussian Mixtures Clustering Hierarchical Clustering Clustering One-Many Clustering Clustering Graph Clustering Anomaly Detection Kernel Density Estimation \u2705 Anomaly Detection Isolation Forest \u274c Re-Inforcement Learning Q-Learning \u274c \u274c"},{"location":"CS_Electives/Machine_Learning/22_Learning_Algorithms/#curse-of-dimensionality","title":"Curse of Dimensionality","text":"<p>As the no of dimensions increases, relative distances tend to 0</p> <p>Distance-based models are the most affected</p> <ul> <li>KNN</li> <li>K-Means</li> <li>Tree-based classification</li> <li>SVM?</li> </ul>"},{"location":"CS_Electives/Machine_Learning/23_OLS_Regression/","title":"OLS Regression","text":"<p>OLS: Ordinary Least Squares</p> \\[ \\hat y = \\hat \\beta_0 + \\sum_{j=1}^k \\hat \\beta_j X_j \\] <ul> <li>\\(\\hat \\beta_0\\) is the value of \\(y\\) when \\(x_j=0, \\forall j \\in [1, k]\\)</li> <li>\\(\\hat \\beta_j\\) shows the change in \\(y\\) associated (not necessarily caused) with an increase of \\(X_j\\) by 1 unit</li> </ul> \\[ \\begin{aligned} \\hat \\beta &amp;= \\dfrac{\\text{Cov}(X, y)}{V(X)} \\\\ \\hat \\beta_0 &amp;= E[y] - E[X]' \\hat \\beta \\\\ \\text{Simple model} \\implies \\hat \\beta_1 &amp;= \\dfrac{\\sigma_{xy}}{\\sigma_x} \\\\ \\hat \\beta_0 &amp;= \\bar y - \\beta_1 \\bar x \\\\ \\end{aligned} \\] \\[ \\text{Frisch-Waugh-Lovell} \\\\ \\implies \\hat \\beta_j  = \\dfrac{\\sigma_{u_j, y}}{\\sigma_{u_j}} \\] <p>where \\(u_j\\) is the residual from a regression of \\(x_j\\) with all other features</p> <p>In vector form, $$ \\begin{aligned} \\hat \\beta &amp;= (X'X)^{-1} X' Y \\ \\hat \\beta_j &amp;=\\dfrac{{\\hat u_j}' Y}{{\\hat u_j}' \\hat u_j} \\end{aligned} $$</p>"},{"location":"CS_Electives/Machine_Learning/23_OLS_Regression/#properties","title":"Properties","text":"<ul> <li> <p>Regression is performed with linear parameters</p> </li> <li> <p>Easy computation, just from the data points</p> </li> <li> <p>Point estimators (specific; not internal)</p> </li> <li> <p>Regression Line passes through \\((\\bar x, \\bar y)\\)</p> </li> <li> <p>Mean value of estimated values = Mean value of actual values \\(E(\\hat y) = E(y)\\)</p> </li> <li> <p>Mean value of error/residual terms = 0: \\(\\sum u_i = 0\\)</p> </li> <li> <p>Predicted value and residuals are not correlated with each other: \\(\\sum \\hat u_i \\hat y_i = 0\\)</p> </li> <li> <p>Error terms are uncorrelated \\(x\\): \\(\\sum \\hat u_i x_i = 0\\)</p> </li> <li> <p>Each \\(\\hat \\beta_j\\) is the slope coefficient on a scatter plot with \\(y\\) on the \\(y\\)-axis and \\(u_j^*\\) on the x-axis</p> </li> <li> <p>\\(u_j^*\\) isolates the value of \\(x_j\\) from other \\(x_i, i \\ne j\\)</p> </li> <li> <p>OLS is BLUE (Best Linear Unbiased Estimator)</p> </li> <li> <p>Gauss Markov Theorem</p> </li> <li>Linearity of OLS Estimators</li> <li>Unbiasness of OLS Estimators</li> <li>Minimum variance of OLS Estimators</li> <li> <p>OLS estimators are consistent</p> <p>They will converge to the true value as the sample size increases \\(\\to \\infty\\)</p> </li> <li> <p>Gives the MLE with \\(u \\sim N(0, \\text{MSE})\\)</p> </li> </ul>"},{"location":"CS_Electives/Machine_Learning/23_OLS_Regression/#geometric-interpretation","title":"Geometric Interpretation","text":"<p>OLS fit \\(\\hat y\\) is the projection of \\(y\\) onto the linear space spanned by \\(\\{ 1, x_1, \\dots , x_k \\}\\)</p> <p></p> <p>Projection/Hat Matrix $$ \\begin{aligned} \\hat Y &amp;= HY \\ H &amp;= X (X' X)^{-1} X' \\ H^2 &amp;= H \\ (I-H)^2 &amp;= (I-H) \\ \\text{trace}(H) &amp;= 1+p \\end{aligned} $$</p>"},{"location":"CS_Electives/Machine_Learning/23_OLS_Regression/#asymptotic-variance-of-estimator","title":"Asymptotic Variance of Estimator","text":"<p>Using central limit theorem, $$ \\sqrt{n}(\\hat \\beta - \\beta) \\sim N(0, \\sigma_{\\hat \\beta}) \\ \\implies  \\dfrac{(\\hat \\beta - \\beta)}{\\sigma_{\\hat \\beta}} \\sim N(0, 1) $$</p> \\[ \\begin{aligned} \\sigma_{\\hat \\beta} &amp;= (X' X)^{-1} (X' \\ohm X) (X'X)^{-1} \\\\ \\ohm &amp;= \\text{diag}(\\hat e_1^2, \\dots, \\hat e^2_n) \\end{aligned} \\] <p>Assuming homoskedascity of errors $$ \\begin{aligned} \\sigma_{\\hat \\beta} &amp;= \\dfrac{\\text{MSE}}{\\hat u_j \\hat u_j} \\ &amp;= (X' X)^{-1} \\cdot \\text{MSE} \\end{aligned} $$</p>"},{"location":"CS_Electives/Machine_Learning/23_OLS_Regression/#correlation-vs-r2","title":"Correlation vs \\(R^2\\)","text":"Correlation \\(R^2\\) Range \\([-1, 1]\\) \\([0, 1]\\) Symmetric? \u2705 \u274c \\(r(x, y) = r(y, x)\\) \\(R^2(x, y) \\ne R^2(y, x)\\) Independent on scale of variables? \u2705 \u2705 \\(r(kx, y) = r(x, y)\\) \\(R^2(kx, y) = R^2(x, y)\\) Independent on origin? \u274c \u2705 \\(r(x-c, y) \\ne r(x, y)\\) \\(R^2(x-c, y) \\ne R^2(x, y)\\) Relevance for non-linear relationship? \u274c \u2705 \\(r(\\frac{1}{x}, y) \\approx 0\\) \\(R^2(\\frac{1}{x}, y)\\) not necessarily 0 Gives direction of causation/association(not exactly the value of causality) \u274c \u2705"},{"location":"CS_Electives/Machine_Learning/23_OLS_Regression/#isotonic-regression","title":"Isotonic Regression","text":"<p>Minimizes error ensuring increasing/decreasing trend only</p> <p></p>"},{"location":"CS_Electives/Machine_Learning/24_IDK/","title":"IDK","text":"<p>Deriving the coefficients as a distribution</p> <p>Limitation: only works for a single parameter $$ n_\\text{eff} = \\dfrac{n(n-1)}{2} $$</p>"},{"location":"CS_Electives/Machine_Learning/24_IDK/#linear-fit","title":"Linear Fit","text":"\\[ \\begin{aligned} y &amp;= mx + c \\\\ \\implies m &amp;= \\dfrac{y_i-y_j}{x_i - x_j} \\sim \\text{pdf}(m) \\end{aligned} \\]"},{"location":"CS_Electives/Machine_Learning/24_IDK/#exponential-curve","title":"Exponential Curve","text":"\\[ \\begin{aligned} c_t &amp;= c_0 e^{kt} \\\\ k &amp;= \\dfrac{-\\ln \\vert c_i / c_j \\vert}{i-j} \\sim \\text{pdf}(k) \\end{aligned} \\]"},{"location":"CS_Electives/Machine_Learning/24_Piecewise_Regression/","title":"Piecewise Regression","text":"<p>Breaks input space into distinct regions and fits different relationship for each region.</p>"},{"location":"CS_Electives/Machine_Learning/24_Piecewise_Regression/#linear-basis-function-models","title":"Linear Basis Function Models","text":"\\[ \\begin{aligned} \\hat y &amp;= \\sum_{m=1}^M \\hat \\beta_m \\phi_m + u \\\\ &amp;= \\beta' \\Phi + u \\\\ &amp;= (\\Phi' \\Phi)^{-1} \\Phi' Y \\end{aligned} \\] <p>where \\(\\phi(x) =\\) basis function</p>"},{"location":"CS_Electives/Machine_Learning/24_Piecewise_Regression/#piecewise-constant-regression","title":"Piecewise Constant Regression","text":"<p>For every \\(x \\in R_m\\), we make the same prediction, which is simply the mean of the response values for the training observations in \\(R_m\\)</p> <ul> <li>Divide range of \\(x\\) into \\(m\\) regions by creating \\((m-1)\\) knots (cut-points) \\(\\epsilon_1, \\dots, \\epsilon_{M-1}\\)</li> <li>Construct dummy variables: \\(\\phi_m(x) = I(\\epsilon_{m-1} \\le x &lt; \\epsilon_m), \\forall m \\in M\\)</li> <li>Fit model \\(\\hat y = \\sum_{m=1}^M \\hat \\beta_m \\phi_m + u\\)</li> <li>\\(\\hat y = \\sum_{m=1}^M \\hat \\beta_m \\phi_m\\) is called as step function/piece-wise constant function</li> <li>\\(\\hat \\beta_m = \\bar y_m = \\dfrac{\\sum_{x_i \\in R_m} y_i}{n_m}\\)</li> </ul>"},{"location":"CS_Electives/Machine_Learning/24_Piecewise_Regression/#piecewise-polynomial-regression","title":"Piecewise Polynomial Regression","text":""},{"location":"CS_Electives/Machine_Learning/24_Piecewise_Regression/#splines","title":"Splines","text":"<p>So we specify \\(\\alpha_{10} + \\alpha_{11} \\epsilon = \\alpha_{20}\\)</p> <p>Spline with one knot $$ \\hat y = \\beta_0 + \\beta_1 x + \\beta(x- \\epsilon)_+ + u $$ where</p> <ul> <li>\\(\\beta_0 = \\alpha_{10}, \\beta_1 = \\alpha_{11}, \\beta_2 = \\alpha_{21}-\\alpha_{11}\\)</li> <li>\\((x-\\epsilon)_+ = (x-\\epsilon) I (x \\ge \\epsilon)\\)</li> </ul> <p></p> <p></p> Limitation Spline Degree \\(d\\) spline is a piece-wise degree \\(d\\) polynomial with continuity in derivatives up to degree \\((d-1)\\) at each knot- Continuous- Smooth \\((d+m)\\) degrees of freedom\\((m-1)\\) knots High variance at boundary Natural Spline Function is linear beyond the boundary knots, to produce more stable estimates"},{"location":"CS_Electives/Machine_Learning/24_Piecewise_Regression/#generalized-additive-models","title":"Generalized Additive Models","text":"\\[ \\begin{aligned} \\hat y &amp;= w_0 + \\sum_{i=1}^k w_i(x_i) + u \\\\ w_j(x_j) &amp;= \\sum_{m=1}^{M_j} \\beta_{jm} \\phi_{jm} (x_j) \\end{aligned} \\] <p>Allow for flexible nonlinear relationships in each dimension of the input space while maintaining additive structure of linear models</p> <p>Can be fit using least squares, as it is a linear basis function model</p>"},{"location":"CS_Electives/Machine_Learning/25_Generalized_Linear_Model/","title":"Generalized Linear Model","text":"<p>Why? For non-normal distribution, OLS \\(\\ne\\) MLE</p> <p>Steps</p> <ol> <li> <p>Let \\(y\\) have a probability distributions as long as it is from the exponential family</p> </li> <li> <p>Included</p> <ul> <li>Normal, log-normal, exponential, gamma, chi-squared, beta, Bernoulli, poisson, binomial, etc</li> </ul> </li> <li> <p>Not included:</p> <ul> <li> <p>Student\u2019s \\(t\\) due to heavy tails</p> </li> <li> <p>Mixed distributions (with different location/scale parameters)</p> </li> </ul> </li> <li> <p>Allow for any transformation (link function) of \\(y\\), such that transformation is monodic and differentiable</p> </li> <li> <p>Write linear parameters</p> </li> <li> <p>Derive MLE</p> </li> </ol> Distribution Typical Uses Link Name Link Function\\(g(y)\\) Bernoulli/Binomial Outcome of single yes/no occurence Logit(Logistic) \\(\\ln \\left \\vert \\dfrac{y}{1-y} \\right \\vert\\) Exponential/Gamma Exponential response dataScale parameters Inverse \\(1/y\\) Normal/Gaussian Linear response data Identity \\(y\\) Inverse Gaussian Poisson Count of occurrences in fixed amount of time/space Log \\(\\ln \\vert y \\vert\\) Quasi Normal with constant variance Quasi-binomial Binomial with constant variance Quasi-poisson Poisson with constant variance"},{"location":"CS_Electives/Machine_Learning/25_Logistic_Regression/","title":"Logistic Regression","text":"<p>Logistic regression minimizes the cross-entropy error</p> <p>Uses Sigmoid Function and binary cross-entropy loss</p> <p>Decision of such a discriminant function is always singly-connected and convex. $$ \\begin{aligned} p(y=k) &amp;= \\dfrac{     \\exp(\\beta_k x) }{     \\sum_k^K \\exp(\\beta_k x) } \\ \\beta_0 &amp;= 0, \\beta_1 = 1 \\ \\implies p(y=1) &amp;= \\dfrac{     e^{x} }{     1 + e^{\\beta x} } \\ p(y=0) &amp;= \\dfrac{     1 }{     1 + e^{\\beta x} } \\ \\implies \\ln \\dfrac{P(y=j)}{P(y=k)} &amp;= (\\beta_j-\\beta_k)' X \\end{aligned} $$</p> \\[ \\begin{aligned} p &amp;= P(y=1|x) \\\\ &amp;= \\sigma(\\beta^T x) \\\\ &amp;= \\frac{1}{1 + \\exp(-\\beta^T x)} \\\\ \\implies \\beta^T x &amp;= \\text{logit}(p) \\\\ &amp;= \\ln \\left \\vert \\dfrac{p}{1-p} \\right \\vert \\end{aligned} \\] <p>$$ \\begin{aligned} \\hat \\beta &amp;= \\arg \\max_\\beta L(\\beta) \\</p> <p>&amp;= \\arg \\min_\\beta -\\log L(\\beta) &amp;= \\sum_{i=1}^n \\ln P(y_i \\vert x_i; \\beta) \\ -\\log L(\\beta) &amp;= \\sum_{i=1}^n y_i \\log \\sigma (x_i' \\beta) + (1-y_i) \\log(1- \\sigma(x_i' \\beta)) \\end{aligned} $$</p> <p>Logistic regression assumes that the log odds is a linear function</p> <p>We need to choose class \\(0\\) to be the reference level, and normalize \\(\\beta_0=0\\)</p> <p>Meaning: \\(\\exp( \\beta x')\\) is the probability of \\(y=1\\) relative to \\(y=0\\)</p>"},{"location":"CS_Electives/Machine_Learning/25_Logistic_Regression/#disadvantages","title":"Disadvantages","text":"<ol> <li>Decision boundary dependent on order of training data in mini-batches</li> <li>Suboptimal decision boundary: Does not give maximum-margin decision boundary</li> </ol>"},{"location":"CS_Electives/Machine_Learning/25_Logistic_Regression/#classification","title":"Classification","text":""},{"location":"CS_Electives/Machine_Learning/25_Logistic_Regression/#binary","title":"Binary","text":"\\[ \\begin{aligned} \\hat y &amp;= \\begin{cases} 1, &amp; p \\ge p_1 \\\\ 0, &amp; p \\le p_0 \\\\ \\text{Unsure} &amp; \\text{o.w} \\end{cases} \\\\ p_1 &amp;= \\dfrac{1}{1 + c} \\end{aligned} \\] <p>where</p> <ul> <li>\\(c =\\) relative cost of FN wrt FP (misclassifying \\(y=1\\) wrt \\(y=0\\))</li> <li>\\(c =\\) usually 1</li> <li>\\(p_0, p_1 =\\) thresholds</li> <li>\\(p_0, p_1\\) are commonly taken as 0.5</li> </ul>"},{"location":"CS_Electives/Machine_Learning/25_Logistic_Regression/#multi-class","title":"Multi-Class","text":"<p>Softmax function = generalized logistic function for multi-class</p> <p>Linear decision boundary $$ \\begin{aligned} p(y=j) &amp;= \\dfrac{\\exp(\\beta_j x)}{\\sum_k^K \\exp(\\beta_k x) } \\ \\ \\ln \\dfrac{P(y=j)}{P(y=k)} &amp;= (\\beta_j-\\beta_k)' X \\ \\implies \\ln \\dfrac{P(y=j)}{P(y=k)} &amp;= (\\beta_j)' X &amp; (\\beta_k = 0) \\end{aligned} $$ where \\(K\\) = number of classes</p> <p></p> <p>We need to choose one class \\(k\\) to be the reference level, and normalize \\(\\beta_k=0\\)</p> <p>Meaning: \\(\\exp( \\beta_j x')\\) is the probability of \\(y=j\\) relative to \\(y=k\\)</p>"},{"location":"CS_Electives/Machine_Learning/25_Logistic_Regression/#decision-boundary","title":"Decision Boundary","text":"<p>Decision boundary is obtained by solving for \\(P(y=k_i) = P(y=k_j)\\)</p> <p>The decision boundary between 2 classes does not depend on another class</p>"},{"location":"CS_Electives/Machine_Learning/25_Logistic_Regression/#class-share","title":"Class Share","text":"<p>Consider alternative \\(j\\) $$ \\begin{aligned} P(y_i=j) &amp;= \\int P(y_i=j \\vert x_i) f(x_i) \\cdot d x_i \\ &amp; \\approx \\dfrac{1}{n} \\sum_{i=1}^n P(y_i=j \\vert x_i) \\end{aligned} $$</p> <p>We can average individual conditional choice probabilities to get an estimate of the class share of each alternative in the population.</p>"},{"location":"CS_Electives/Machine_Learning/25_Logistic_Regression/#counterfactual-analysis","title":"Counterfactual Analysis","text":"<p>Logistic regression assumes IIA, hence does not take into account similarity of classes for evaluating class share when there is a class is added/dropped</p> <p>This assumption is fine for in-sample prediction, but inappropriate for counterfactual analysis</p>"},{"location":"CS_Electives/Machine_Learning/25_Logistic_Regression/#probabilistic-regression","title":"Probabilistic Regression","text":""},{"location":"CS_Electives/Machine_Learning/25_Logistic_Regression/#binary_1","title":"Binary","text":""},{"location":"CS_Electives/Machine_Learning/25_Logistic_Regression/#multi","title":"Multi","text":""},{"location":"CS_Electives/Machine_Learning/25_Logistic_Regression/#ols-vs-logistic","title":"OLS vs Logistic","text":"<p>OLS is not appropriate for classification</p> <ul> <li>OLS is not probabilistic, as it\u2019s range can be outside \\([0, 1]\\)</li> <li>OLS assumes \\(\\sigma^2_{y \\vert x} = \\sigma^2\\), but \\(\\sigma^2_{y \\vert x} = p(1-p)\\) in true Bernoulli distribution and hence is not constant</li> <li>OLS tries to find \\(\\hat y_i\\) close to \\(y_i\\), even though we just need \\(y_i = (\\hat y &gt; 0.5)\\)</li> <li>OLS is sensitive to outliers, due to large deviation and high cost function</li> <li>OLS penalizes cases where \\(y_i=1\\) and \\(\\hat y&gt;1\\), ie it penalizes predictions that are \u201ctoo correct\u201d</li> <li>OLS penalizes observations with large positive margins and hence is not a suitable loss function for classification</li> </ul> <p></p> <p></p> <ul> <li>Classes can be masked by others, especially when no of classes is large and no of predictors is small</li> <li>OLS estimates normal linear model, but classification has a distribution very different from Gaussian</li> <li><ul> <li>Decision boundaries produced by linear regression between 1 and 2 and between 2 and 3 are the same, so we would never predict class 2</li> </ul> </li> </ul>"},{"location":"CS_Electives/Machine_Learning/25_Logistic_Regression/#idk","title":"IDK","text":"<p>Don\u2019t think of logistic/softmax regression as linear regression followed by logistic/softmax function</p> <p>Think of the logistic/softmax function embedded in the loss function itself</p>"},{"location":"CS_Electives/Machine_Learning/25_Logistic_Regression/#odds","title":"Odds","text":"\\[ \\begin{aligned} \\text{Odds}  &amp;= \\dfrac{p(1)}{p(0)} \\\\ &amp;= \\dfrac{p}{1-p} \\end{aligned} \\]"},{"location":"CS_Electives/Machine_Learning/26_Bayesian_Learning/","title":"Bayesian Learning","text":"<p>Focus marginalization rather than optimization</p> <p>Rather than use a single setting of parameters \\(w\\), use all settings weighted by their posterior probabilities in a Bayesian model average</p> <p></p>"},{"location":"CS_Electives/Machine_Learning/26_Bayesian_Learning/#advantages","title":"Advantages","text":"<p>Automatically calibrated complexity even with highly flexible models</p>"},{"location":"CS_Electives/Machine_Learning/26_Bayesian_Learning/#limitations","title":"Limitations","text":"<p>Computationally-expensive for high dimensions</p>"},{"location":"CS_Electives/Machine_Learning/26_Bayesian_Learning/#bayes-optimal-classifier","title":"Bayes Optimal Classifier","text":"<p>Given new instance \\(x\\)</p> <p>Consider \\(v=\\{v_1, v_2 \\}=\\{\\oplus, \\ominus \\}\\)</p> <p>The optimal classifier is given by</p> \\[ \\underset{v_j \\in V}{\\arg \\max} \\sum_{h_i \\in H} \\textcolor{hotpink}{P(v_j | h_i)} \\ P(h_i | D) \\]"},{"location":"CS_Electives/Machine_Learning/26_Bayesian_Learning/#disadvantage","title":"Disadvantage","text":"<p>Very costly to implement. We need to calculate a lot of probabilities</p>"},{"location":"CS_Electives/Machine_Learning/26_Bayesian_Learning/#gibbs-algorithm","title":"Gibbs Algorithm","text":"<p>Consider we have multiple independent hypotheses</p> <ol> <li>Choose one hypothesis at random, according to \\(P(h|D)\\)</li> <li>Use this to classify new instance</li> </ol>"},{"location":"CS_Electives/Machine_Learning/26_Bayesian_Learning/#disadvantage_1","title":"Disadvantage","text":"<p>Lower accuracy</p> <p>One more point in slide</p>"},{"location":"CS_Electives/Machine_Learning/26_Bayesian_Learning/#naive-bayes","title":"Naive Bayes","text":""},{"location":"CS_Electives/Machine_Learning/26_Bayesian_Learning/#bayesian-belief-network","title":"Bayesian Belief Network","text":""},{"location":"CS_Electives/Machine_Learning/26_Bayesian_Learning/#bayesian-nn","title":"Bayesian NN","text":""},{"location":"CS_Electives/Machine_Learning/26_Bayesian_Learning/#bayesian-classifier","title":"Bayesian Classifier","text":"<p>Called as \u2018Naive\u2019 classifier, due to following assumptions</p> <ul> <li>Empirically-proven</li> <li>Scales very well</li> </ul>"},{"location":"CS_Electives/Machine_Learning/26_Bayesian_Learning/#naive-bayes-classification","title":"Naive Bayes Classification","text":"<p>Calculate posterior probability, based on assumption that all input attributes are conditionally-independent</p>"},{"location":"CS_Electives/Machine_Learning/26_Bayesian_Learning/#drawbacks","title":"Drawbacks","text":"<ol> <li>Doesn\u2019t work for continuous independent variable</li> <li>We need to use Gaussian Classifier</li> <li>Violation of Independence Assumption</li> <li>Zero outlook</li> </ol>"},{"location":"CS_Electives/Machine_Learning/27_SVM/","title":"Support Vector Machine","text":"<p>Goal: obtain hyperplane farthest from all sample points</p> <p>Larger margins \\(\\implies\\) fewer dichotomies \\(\\implies\\) smaller \\(d_\\text{vc}\\)</p> <p>Margin = Distance b/w boundary and edge point closest to it</p> <p>Note: \\(y \\in \\{ -1, 1 \\}\\), not \\(\\{ 0, 1 \\}\\)</p>"},{"location":"CS_Electives/Machine_Learning/27_SVM/#hard-margin","title":"Hard Margin","text":"<p>Linearly-separable</p> <p>Consider \\(x_s\\) be the nearest data point to the plane \\(\\theta^T X + b=0\\)</p> <p>Constrain \\(\\theta: \\vert \\theta^T x_s + b \\vert = 1\\), so that we get a unique plane $$ \\underset{\\theta, \\gamma}{{\\arg\\max}}  \\gamma \\ \\text{subject to } \\min_{s} \\vert \\hat y_s \\vert = 1 %% \\text{subject to } y \\dfrac{\\hat y}{\\vert \\vert \\theta \\vert \\vert} \\ge \\gamma $$ Since \\(\\theta\\) is \\(\\perp\\) to the plane in the \\(x\\) space, margin = distance between \\(x_i\\) and the plane \\(\\theta^T X + b=0\\) is given by $$ \\begin{aligned} \\gamma &amp;= \\vert \\hat \\theta^T(x_i - x) \\vert \\ &amp;= \\left\\vert \\dfrac{\\theta}{\\vert \\vert \\theta \\vert \\vert} (x_i - x) \\right\\vert \\ &amp;= \\dfrac{1}{\\vert \\vert \\theta \\vert \\vert} \\vert \\theta^T x_i - \\theta^T x \\vert \\ &amp;= \\dfrac{1}{\\vert \\vert \\theta \\vert \\vert} \\vert (\\theta^T x_i + b) - (\\theta^T x + b) \\vert \\ &amp;= \\dfrac{1}{\\vert \\vert \\theta \\vert \\vert} \\vert 1 - 0 \\vert \\ &amp;= \\dfrac{1}{\\vert \\vert \\theta \\vert \\vert} \\end{aligned} $$</p> \\[ \\vert \\hat y_s \\vert = y_s \\hat y_s \\] <p>Optimization problem can be re-written as $$ \\begin{aligned} \\underset{\\theta}{\\arg \\max} \\frac{1}{\\vert \\vert \\theta\\vert \\vert} &amp; \\ \\text{Subject to constraint: } &amp;(\\hat y y) \\ge 1 \\quad \\forall i \\ =&amp; \\begin{cases} \\hat y \\ge 1, &amp; y_i &gt; 0 \\ \\hat y \\le -1, &amp; y_i &lt; 0  \\end{cases} \\end{aligned} $$</p> <p>Re-writing again $$ \\begin{aligned} \\underset{\\theta}{\\arg \\min} {\\vert\\vert \\theta \\vert\\vert}^2 &amp; \\ \\text{Subject to constraint: } &amp;(\\hat y y) \\ge 1 \\quad \\forall i \\ =&amp; \\begin{cases} \\hat y \\ge 1, &amp; y_i &gt; 0 \\ \\hat y \\le -1, &amp; y_i &lt; 0  \\end{cases} \\end{aligned} $$</p> <p>Re-writing again $$ \\begin{aligned} &amp;\\underset{\\theta}{\\arg \\min}  {\\vert\\vert \\theta \\vert\\vert}^2 - \\sum_\\mathclap{s \\in \\text{Sup Vec}}\\alpha_s \\Big( \\hat y y - 1 \\Big) \\ &amp; \\max \\alpha_s \\ge 0 \\quad \\forall s \\end{aligned} $$</p>"},{"location":"CS_Electives/Machine_Learning/27_SVM/#limitations","title":"Limitations","text":"<ul> <li>Does not work for Non-separable problems</li> </ul>"},{"location":"CS_Electives/Machine_Learning/27_SVM/#soft-marginhinge-loss","title":"Soft Margin/Hinge Loss","text":"<p>Non-separable</p> <p>Quantify margin violation: \\(y_i \\hat y_i \\le 1\\) not satisfied $$ y_i \\hat y_i \\ge 1-\\epsilon_i \\ \\epsilon_i \\ge 0 \\ \\implies \\text{Total violation} = \\sum_{i=1}^n \\epsilon_i $$</p> \\[ \\arg \\min_\\theta \\vert \\vert \\theta \\vert \\vert^2 + C \\sum_{i=1}^n \\epsilon_i \\\\ \\text{Subject to: } \\epsilon_i = \\max \\{ 0, 1 - y_i \\hat y_i \\} \\quad \\forall i \\] \\[ \\arg \\min_\\theta \\vert \\vert \\theta \\vert \\vert^2 + C \\sum_{i=1}^n \\max \\{ 0, 1 - y_i \\hat y_i \\} \\] <p>Since it doesn\u2019t matter which term we multiply by \\(c&gt;0\\), this is equivalent to $$ \\arg \\min_\\theta \\underbrace{L(y, \\hat y)}{\\mathclap{\\text{Hinge Loss}}} + \\underbrace{\\dfrac{\\lambda}{2}\\vert \\vert \\theta \\vert \\vert^2} $$}}</p> <p>Regularization optimizes for max margin</p>"},{"location":"CS_Electives/Machine_Learning/27_SVM/#gradient","title":"Gradient","text":"\\[ \\nabla J(\\theta) = \\begin{cases} -y \\cdot x, &amp; y \\hat y &gt; 1 \\\\ 0, &amp; \\text{o.w} \\end{cases} \\]"},{"location":"CS_Electives/Machine_Learning/27_SVM/#types","title":"Types","text":"<ul> <li>Primal: better for large \\(n\\)</li> <li>Dual: better for large \\(k\\)</li> </ul>"},{"location":"CS_Electives/Machine_Learning/27_SVM/#types-of-svs","title":"Types of SVs","text":"Margin SV Non-Margin SV \\(\\alpha_s\\) \\(\\in (0, C)\\) \\(= C\\) \\(\\epsilon_i\\) \\(=0\\) \\(&gt;0\\)"},{"location":"CS_Electives/Machine_Learning/27_SVM/#generalization","title":"Generalization","text":"<p>Since the complexity of the plane only depends on the support vectors, \\(d_\\text{vc} = \\text{\\# of SVs}\\) $$ \\mathbb{E} [E_\\text{out}] \\le \\dfrac{\\mathbb{E}  [\\text{# of SV}]}{n-1} $$</p>"},{"location":"CS_Electives/Machine_Learning/27_SVM/#kernel-trick","title":"Kernel Trick","text":"<p>Complex \\(h\\), but still simple \\(H\\), as complexity of plane only depends on support vectors</p> <p>Kernel function: \\(\\phi(x, x')\\) is valid \\(\\iff\\)</p> <ul> <li>It is symmetric</li> <li>Mercer\u2019s condition: Matrix \\([k(x_i, x_j)]\\) is +ve semi-definite</li> </ul> <p>Linear transformation function for Non-Linearly-Separable</p> <p>For eg, to increase the dimensionality, we can use \\(\\phi(x) = (x, x^2)\\)</p> Kernel Function \\(\\phi(x)\\) Linear \\(x\\) Polynomial \\((mx+c)^n\\) Gaussian \\(\\exp \\left( \\dfrac{-\\vert  x-y  \\vert^2}{2 \\sigma^2} \\right)\\)  where \\(\\sigma^2 =\\) Variance of sample RBF(Radial Basis Function)Most powerful, but not necessary in most cases \\(\\exp( -\\gamma \\vert  x_i - x_j  \\vert^2 )\\) <p>Interesting observation</p> <ul> <li>Features \\(\\phi(x)\\) are never used</li> <li>Only dot product \\(\\phi(x)^T \\phi(x)\\) is used</li> </ul> <p>We can compute dot product between \\(O(k^2)\\) features in \\(O(k)\\) time</p> <p>Implication</p> <ul> <li>Faster for high-dimensions</li> <li>Can be applied for any model class that use dot products</li> <li>Supervised: SVM, Linear Regression, Logistic regression</li> <li>Unsupervised: PCA, Density Estimation</li> </ul>"},{"location":"CS_Electives/Machine_Learning/27_SVM/#advantages","title":"Advantages","text":"<ul> <li>Can handle high-dimensionality with lower computational cost</li> </ul>"},{"location":"CS_Electives/Machine_Learning/27_SVM/#disadvantage","title":"Disadvantage","text":"<ul> <li>Still computationally-expensive: \\(O(n^2)\\), as we need compute distance \\(\\phi(x_i, x_j) \\quad \\forall i, j\\)</li> </ul>"},{"location":"CS_Electives/Machine_Learning/28_Decision_Trees/","title":"Decision Trees","text":"<p>Piecewise constant model that adaptively learns to divide predictor space into different regions, and then fits a model in each region, without human intervention for deciding the cuts</p> <p>Can be applied for regression and classification</p>"},{"location":"CS_Electives/Machine_Learning/28_Decision_Trees/#advantages","title":"Advantages","text":"<ul> <li>Interpretable, for small trees</li> <li>Little data processing required</li> </ul>"},{"location":"CS_Electives/Machine_Learning/28_Decision_Trees/#disadvantages","title":"Disadvantages","text":"<ul> <li>Small trees are not powerful</li> <li>Large trees tend to overfit, and are hard to regularize</li> <li>Do not work well for modelling linear relationships</li> <li>Cannot extrapolate well</li> <li>Decision regions tend to be highly-fragmented</li> <li>Resulting function (also Decision boundary for classification) is very non-smooth and blocky</li> </ul>"},{"location":"CS_Electives/Machine_Learning/28_Decision_Trees/#terms","title":"Terms","text":"Term Meaning Internal Nodes Labelled by attributes names, to be tested on an attributeContain the splitting attributes Leaf/Terminal Nodes Labelled by class labels Edges determined by the nuo of outcomes on n attribute test conditions Size Number of leaves"},{"location":"CS_Electives/Machine_Learning/28_Decision_Trees/#goal","title":"Goal","text":"<p>Find cuts to obtain \\(R_m\\) and improve in-sample fit, while minimizing out-of-sample loss</p> <p>Since it is computationally-infeasible to consider every possible partition of the predictor space, we adopt a forward stepwise procedure</p>"},{"location":"CS_Electives/Machine_Learning/28_Decision_Trees/#decision-tree-approaches","title":"Decision Tree Approaches","text":"Approach Steps Disadvantage Greedy Divide the problem into different steps, take decisions at each stepBuild tree in Top-Down mannerSplit train set into purer subsets (the best split) @ every node Backtracking is not possible Recursive"},{"location":"CS_Electives/Machine_Learning/28_Decision_Trees/#regression-tree","title":"Regression Tree","text":"\\[ \\begin{aligned} \\hat y &amp;= \\sum_{m=1}^M \\hat y_m \\cdot I \\{ x \\in R_m \\} \\\\ \\hat y_m &amp;= \\underset{x_i \\in R_m}{\\text{mean}} \\ (y_i) \\end{aligned} \\] <p>where</p> <ul> <li>\\(M =\\) no of leaves = no of regions</li> <li>\\(R_m =\\) region \\(m\\)</li> <li>\\(n_m=\\) no of obs in \\(R_m\\)</li> </ul> <p>For every observation that falls into the region Rm, we make the same prediction, which is simply the mean of the response values for the training observations in \\(R_m\\)</p>"},{"location":"CS_Electives/Machine_Learning/28_Decision_Trees/#cart-algorithm","title":"CART Algorithm","text":"<p>Greedy recursive partitioning algorithm that divides predictor space through successive binary splits, until a stopping criterion is reached.</p> <p>This is greedy because at each step of the tree-building process, the optimal split is made at that particular step, rather than looking ahead and picking a split that will lead to a better tree in some future step</p>"},{"location":"CS_Electives/Machine_Learning/28_Decision_Trees/#steps","title":"Steps","text":"<ol> <li> <p>At each step, generate binary split to divide predictor space into two regions that achieves the biggest reduction in \\(E_\\text{in}\\)</p> </li> <li> <p>The 2 regions are as homogenous in response \\(y\\) as possible</p> <ul> <li>\\(R_\\text{left} = \\{ x \\in R: x_j &lt; s \\}\\)</li> <li>\\(R_\\text{right} = \\{ x \\in R: x_j \\ge s \\}\\)</li> </ul> </li> <li> <p>For regression this means choosing \\(j, s\\) such that</p> <p>\\(\\arg \\min\\limits_{j, s} \\left \\{ \\sum \\limits_{x_j \\in R_\\text{left}} L(y_i, \\hat y_\\text{left}) + \\sum \\limits_{x_j \\in R_\\text{right}} L(y_i, \\hat y_\\text{right}) \\right \\}\\)</p> </li> <li> <p>Prune the tree by choosing a subtree \\(T \\subset T_0\\) that minimizes</p> </li> </ol> <p>\\(\\sum \\limits_{m=1}^{\\vert T \\vert} \\sum \\limits_{x_j \\in R_m} L(y_i, \\hat y_m) + \\alpha \\vert T \\vert, \\quad \\forall \\alpha \\ge 0\\)</p> <ul> <li>\\(\\vert T \\vert =\\) size of tree</li> <li>\\(\\alpha\\) controls tradeoff b/w subtree\u2019s complexity and fit to training data<ul> <li>\\(\\alpha=0 \\implies T=T_0\\)</li> <li>\\(\\vert T \\vert \\propto \\dfrac{1}{\\alpha}\\)</li> <li>Optimal \\(\\alpha\\) obtained through cross-validation</li> </ul> </li> </ul>"},{"location":"CS_Electives/Machine_Learning/28_Decision_Trees/#classification-tree","title":"Classification Tree","text":"\\[ \\begin{aligned} \\hat y &amp;= \\sum_{m=1}^M \\hat c_m \\cdot I (x \\in R_m) \\\\ \\hat c_m &amp;= \\underset{x_i \\in R_m}{\\text{mode}} \\ (y_i) \\end{aligned} \\] <p>\\(\\hat c_m =\\) most frequent class in \\(R_m\\) </p>"},{"location":"CS_Electives/Machine_Learning/28_Decision_Trees/#node-impurity","title":"Node Impurity","text":"Misclassification Rate \\(1 - {\\hat p}_m\\) Gini-Index \\(\\sum \\limits_c^C \\hat p_c^m (1 - \\hat p_c^m)\\) Cross-Entropy \\(- \\sum \\limits_c^C \\hat p_c^m \\cdot \\ln \\hat p_c^m\\)"},{"location":"CS_Electives/Machine_Learning/28_Decision_Trees/#steps_1","title":"Steps","text":"<ol> <li> <p>Pick an independent variable</p> </li> <li> <p>Find Entropy of all classes of that independent variable</p> </li> </ol> \\[ H(C_i) = -P_\\text{Pos} \\log_2 (P_\\text{Pos}) -P_\\text{Neg} \\log_2 (P_\\text{Neg}) \\] <ol> <li>Calculate gain of each independent variable for current set/subset of data</li> </ol> \\[ \\begin{aligned} &amp;\\text{Gain}\\Big( \\text{Value}(C_1), C_2 \\Big) \\\\ =&amp; H \\Big(\\text{Value}(C_1) \\Big) \\\\    &amp; - \\left[ \\sum_{i=1} \\frac{n (C_2=\\text{Value}_i)}{n \\Big(\\text{Value}(C_1) \\Big)} \\times H(C_2=\\text{Value}_i) \\right] \\end{aligned} \\] <ol> <li> <p>Pick the independent variable with the highest gain</p> </li> <li> <p>Recursively repeat for each independent variable</p> </li> </ol>"},{"location":"CS_Electives/Machine_Learning/28_Decision_Trees/#hunts-algorithm","title":"Hunt\u2019s Algorithm","text":"<p>Let</p> <ul> <li>\\(t\\) be a node</li> <li>\\(D_t\\) be train dataset @ node \\(t\\)</li> <li> <p>\\(y\\) be set of class labels \\(\\{ C_1, C_2, \\dots, C_n \\}\\)</p> </li> <li> <p>Make node \\(t\\) into a leaf node. Label \\(t\\) with class label \\(y_t\\)</p> </li> <li>Split using appropriate attribute</li> <li>Apply splitting criteria for each attribute and obtain impurity of split using that attribute</li> <li>Pick the attribute that gives the lowest impurity</li> <li>Label the node \\(t\\) with this attribute</li> <li>Determine the outcomes</li> <li>Create nodes for each outcome</li> <li>Draw the edges</li> <li>Split the dataset</li> <li>Repeat the steps for each subtree now</li> </ul>"},{"location":"CS_Electives/Machine_Learning/28_Decision_Trees/#additional-cases","title":"Additional Cases","text":""},{"location":"CS_Electives/Machine_Learning/28_Decision_Trees/#empty-subset","title":"Empty Subset","text":"<p>Let\u2019s say when splitting your data, you end up having an empty subset of the data</p> <ol> <li>Make node \\(t\\) into a leaf node</li> <li>\\(t\\) is labelled as the majority class of the parent dataset</li> </ol>"},{"location":"CS_Electives/Machine_Learning/28_Decision_Trees/#all-records-have-identical-attribute-values","title":"All records have identical attribute values","text":"<ol> <li>Make \\(t\\) into a leaf node</li> <li>\\(t\\) is labelled as the majority class represented in the current subset</li> </ol>"},{"location":"CS_Electives/Machine_Learning/28_Decision_Trees/#number-of-records-fall-below-minimum-threshold-value","title":"Number of records fall below minimum threshold value","text":"<ul> <li>Make \\(t\\) into a leaf node</li> <li>\\(t\\) is labelled as the majority class represented in the current subset</li> </ul>"},{"location":"CS_Electives/Machine_Learning/28_Decision_Trees/#splitting-continuous-attributes","title":"Splitting Continuous Attributes","text":""},{"location":"CS_Electives/Machine_Learning/28_Decision_Trees/#binary-split","title":"Binary Split","text":"<p>Choose a splitting value that results in a purer partition</p>"},{"location":"CS_Electives/Machine_Learning/28_Decision_Trees/#multi-way-split","title":"Multi-Way Split","text":"<ul> <li>Apply discretization</li> <li>Each bin will be a different node</li> </ul> <pre><code>flowchart LR\n\nsubgraph Multi-Way Split\ndirection TB\ns2((Salary))\n\na2(( ))\nb2(( ))\nc2(( ))\nd2(( ))\ne2(( ))\n\ns2 --&gt;|&lt; 10k| a2\ns2 --&gt;|10k - 30k| b2\ns2 --&gt;|30k - 50k| c2\ns2 --&gt;|50k - 80k| d2\ns2 --&gt;| &gt; 80k| e2\nend\n\nsubgraph Binary\ndirection TB\ns1((Salary))\n\nend</code></pre>"},{"location":"CS_Electives/Machine_Learning/28_Decision_Trees/#challenges","title":"Challenges","text":"<ul> <li>How to split training records?</li> <li>Find best splitting attribute (Test on Attribute)</li> <li>Measure for goodness of split</li> <li>Stopping conditions   Stop at situations that result in fully-grown tree (the tree has learnt all the important characteristics, which may not be optimal; in that case we may require some other stopping conditions)<ul> <li>When all records at a node have the same attribute value</li> <li>When all records at a node have the same class label value</li> <li>When a node receives an empty subset</li> </ul> </li> </ul>"},{"location":"CS_Electives/Machine_Learning/28_Decision_Trees/#attribute-test-condition-and-outcomes","title":"Attribute Test Condition and Outcomes","text":""},{"location":"CS_Electives/Machine_Learning/28_Decision_Trees/#nominal","title":"Nominal","text":"<p>Binary split will have \\(2^{k-1} - 1\\) combinations of the binary split, where \\(k=\\) no of values. For eg:</p> <pre><code>flowchart LR\n\nsubgraph 1\ndirection TB\naa[Attribute] --&gt; v1a[v1] &amp; notv1a[\"Not v1 &lt;br&gt; (v2 or v3 or v4)\"]\nend\n\nsubgraph 2\ndirection TB\nab[Attribute] --&gt; v1b[v1 or v2] &amp; notv1b[\"Not (v1 or v2) &lt;br&gt; (v3 or v4)\"]\nend</code></pre>"},{"location":"CS_Electives/Machine_Learning/28_Decision_Trees/#missed-this-class","title":"Missed this class","text":"<p>Lec20.pdf </p>"},{"location":"CS_Electives/Machine_Learning/28_Decision_Trees/#terms_1","title":"Terms","text":"<p>Variables</p> \\[ \\Delta = I(Parent) - \\sum_{j=1}^k \\underbrace{ \\frac{N(V_j)}{N} I (V_j) }_\\text{Weighted Average Impurity} \\] <ul> <li>\\(I(\\text{Parent}) =\\) Impurity at parent</li> <li>\\(N =\\) no of records before split (parent)</li> <li>\\(k =\\) no of splits</li> <li>\\(V_j =\\) denote a split</li> <li>\\(N(V_j) =\\) no of records at split \\(V_j\\)</li> <li>\\(I(V_k) =\\) impurity at child(split) \\(V_j\\)</li> </ul>"},{"location":"CS_Electives/Machine_Learning/28_Decision_Trees/#steps-to-choose-splitting-attribute","title":"Steps to choose splitting attribute","text":"<ul> <li>Compute degree of impurity of parent node (before splitting)</li> <li>Compute degree of impurity of child nodes (after splitting)</li> <li>Compute weighted average impurity of split</li> <li>Compute gain \\((\\Delta)\\)   Larger the gain, better the test condition</li> <li>Choose attribute with largest gain</li> <li>Repeat for all attributes</li> </ul>"},{"location":"CS_Electives/Machine_Learning/28_Decision_Trees/#note","title":"Note","text":"<p>Information gain means the impurity measure is entropy</p>"},{"location":"CS_Electives/Machine_Learning/29_Gaussian_Classifier/","title":"Gaussian Classifier","text":"\\[ \\begin{aligned} P(y=c|x) &amp; = \\dfrac{P(x, y=c)}{P(x)} \\\\ &amp; = \\dfrac{P(x|y=c) \\times P(y=c)}{P(x)} \\\\ &amp; \\propto P(x|y=c) \\times P(y=c) \\end{aligned} \\] <p>We assume Normally-distributed</p>"},{"location":"CS_Electives/Machine_Learning/29_Gaussian_Classifier/#gaussian-mixture","title":"Gaussian Mixture","text":"<p>A mixture of \\(K\\) Gaussians is a distribution \\(p(x)\\) of the form $$ p(x) = \\sum_{k=1}^K p_k N(x; \\mu_k, \\Sigma_k) $$ where</p> <ul> <li>\\(N\\) is a multi-variate Gaussian distribution</li> <li>\\(\\Sigma_k =\\) covariance</li> <li>\\(p_k =\\) probability of \\(x\\)</li> </ul> <p></p>"},{"location":"CS_Electives/Machine_Learning/29_Gaussian_Classifier/#gaussian-discriminant-analysis","title":"Gaussian Discriminant Analysis","text":"<p>Also called Quadratic Discriminant Analysis, as the shape of the decision boundary is quadratic</p> <p>Hence, if we have \\(C\\) classes $$ \\begin{alignedat}{1} p(x, y) &amp;= \\sum_{c=1}^c \\hat p(y=c) &amp;&amp;\\cdot \\hat p(x \\vert y=c) \\ &amp;= \\sum_{c=1}^C p_c &amp;&amp;\\cdot N(x; \\mu_c, \\Sigma_c) \\end{alignedat} $$ Guessing parameters</p> <p></p> <p>For \\(C\\) Classes, there are \\(3C\\) parameters $$ \\begin{aligned} \\hat \\theta &amp; = { \\ &amp; \\mu_1, \\Sigma_1, p_1 \\ &amp; \\dots \\ &amp; \\mu_C, \\Sigma_C, p_C \\ } \\end{aligned} $$</p> \\[ \\begin{aligned} \\mu_c &amp;= E[x \\vert y = c] \\\\ \\Sigma_c &amp;= \\Sigma[x \\vert y = c] \\\\ p_c &amp;= \\dfrac{n_c}{n} \\end{aligned} \\]"},{"location":"CS_Electives/Machine_Learning/29_Gaussian_Classifier/#special-cases","title":"Special Cases","text":"<ul> <li>LDA: \\(\\Sigma_k = \\text{same}\\)</li> <li>Gaussian Naive Bayes: \\(\\Sigma_k = \\text{diagonal}\\)</li> </ul>"},{"location":"CS_Electives/Machine_Learning/29_Gaussian_Classifier/#bernoulli-naive-bayes","title":"Bernoulli Naive Bayes","text":"<ul> <li>\\(P(y):\\) categorical distribution</li> <li>\\(P(x_j \\vert y):\\) Bernoulli distribution</li> </ul>"},{"location":"CS_Electives/Machine_Learning/29_Gaussian_Classifier/#assumption","title":"Assumption","text":"<p>Assume that every input var is independent of each other $$ \\begin{aligned} &amp;p(x_j \\vert y)  \\perp p(x_{\\centernot j} \\vert y) \\ \\implies &amp;p(x \\vert y) = \\prod_{j=1}^k p(x_j \\vert y) \\end{aligned} $$ \\(p(x_j \\vert y)\\) is assumed as Bernoulli distribution, hence there is only one parameter for each input var</p> <p>\\(p(x \\vert y)\\) has only \\(k\\)\u00a0parameters in total</p>"},{"location":"CS_Electives/Machine_Learning/29_Gaussian_Classifier/#why","title":"Why?","text":"<p>To handle discrete input data of high dimensionality</p> <p>Solution: assume that \\(x\\) is sampled from a categorical distribution that assigns a probability to each possible state of \\(x\\)</p> <p>However, if the dimensionality of \\(x\\) is too high, \\(x\\) can take a large domain of values. Hence, we would need to specify \\((C_j)^k-1\\) parameters for the categorical distribution, where</p> <ul> <li>\\(C_j=\\) no of classes in discrete variable \\(x_j\\)</li> <li>\\(k=\\) no of dimensions</li> </ul>"},{"location":"CS_Electives/Machine_Learning/29_Gaussian_Classifier/#limitations","title":"Limitations","text":"<ul> <li>This is not a perfect assumption, as inputs may be correlated with each other for eg in NLP</li> <li>\u201cDoctor\u201d will be accompanied with \u201cPatient\u201d in the same \u2018bag of words\u2019</li> </ul>"},{"location":"CS_Electives/Machine_Learning/29_Gaussian_Classifier/#idk","title":"IDK","text":"\\[ \\begin{aligned} \\ln \\mathcal{L}(x \\vert C) &amp;= \\ln \\mathcal{L}(x \\vert \\mu_c, \\sigma_c^2) \\\\ &amp;= \\ln P(x \\vert \\mu_c, \\sigma_c^2) \\\\ \\end{aligned} \\] \\[ \\ln \\underbrace{\\mathcal{L} (C|x)}_{\\mathclap {\\text{Posterior}}} = \\ln \\underbrace{\\mathcal{L}(x|C)}_{\\mathclap {\\text{Likelihood}}} + \\ln \\underbrace{\\mathcal{L} (C)}_{\\mathclap{\\text{Posterior}}} \\]"},{"location":"CS_Electives/Machine_Learning/29_Gaussian_Classifier/#2-classes","title":"2 Classes","text":"\\[ \\begin{aligned} \\ln \\frac{P(C_1 | x)}{P(C_2 | x)} &amp;= \\ln P(C_1 | x) - \\ln P(C_2 | x) \\\\ &amp;= \\frac{-1}{2} () \\end{aligned} \\] <ul> <li>If log ratio \\(\\ge 0\\), assign to \\(C_1\\)</li> <li>If log ratio \\(&lt;0\\), assign to \\(C_2\\)</li> </ul> <p>We need to ensure that we have equal sample of both classes, so that the prior probabilities of both the classes in the formula is the same.</p>"},{"location":"CS_Electives/Machine_Learning/30_KNN/","title":"\\(k\\) Nearest Neighbor","text":"<p>represents each record as a datapoint with \\(m\\) dimensional space, where \\(m\\) is the number of attributes</p> <p>Similarity-based, non-parametric</p> <p>Assumes that underlying relationship b/w \\(x\\) and \\(y\\) is continuous</p>"},{"location":"CS_Electives/Machine_Learning/30_KNN/#requirements","title":"Requirements","text":"<ul> <li>Set of labelled records</li> <li>Normalized Proximity/Distance metric</li> <li>Min-Max normalization</li> <li>\\(Z\\)-normalization</li> <li>Odd value of \\(k\\)</li> </ul>"},{"location":"CS_Electives/Machine_Learning/30_KNN/#choosing-k","title":"Choosing \\(k\\)","text":"<p>Similar to bias-variance tradeoff $$ \\begin{aligned} \\text{Flexibility} &amp;\\propto \\dfrac{1}{k} \\</p> <p>\\implies \\text{Bias} &amp;\\propto k \\ \\text{Variance} &amp;\\propto \\dfrac{1}{k} \\end{aligned} $$</p> <ul> <li>For K = 1, the training error rate is 0. Bias is low and   variance is high</li> <li>As K inc, model becomes less flexible and produces a linear-like decision boundary, with lower variance and higher bias</li> </ul> <p></p> <p></p>"},{"location":"CS_Electives/Machine_Learning/30_KNN/#classification","title":"Classification","text":"<p>Class label is the majority label of \\(k\\) nearest neighbors</p> <p></p>"},{"location":"CS_Electives/Machine_Learning/30_KNN/#steps","title":"Steps","text":"<ul> <li>Compute distance between test record and all train records</li> <li>Identify \\(k\\) neighbors of test records   (Low distance=high similarity)</li> <li>Use majority voiting to find the class label of test sample</li> </ul>"},{"location":"CS_Electives/Machine_Learning/30_KNN/#choosing-value-of-k","title":"Choosing Value of \\(k\\)","text":"\\(k\\) Problems too small OverfittingSusceptible to noise too large UnderfittingSusceptible to far-off points <ol> <li>Use a test set</li> <li>Let \\(k = 3\\)</li> <li>Record error rate of classifier/accuracy</li> <li>\\(k = k+2\\)</li> <li>Repeat steps 3 and 4, until value of \\(k\\) for which</li> <li>error is min</li> <li>accuracy is maximum</li> </ol>"},{"location":"CS_Electives/Machine_Learning/30_KNN/#regression","title":"Regression","text":"<p>KNN can be used for regression as well</p> <p>Predicted value will be the average of the continuous labels of \\(k\\)-nearest neighbors</p>"},{"location":"CS_Electives/Machine_Learning/30_KNN/#distance-weighted-knn-classifier","title":"Distance-Weighted KNN Classifier","text":"<p>Closer points are given larger weights for the majority voting scheme</p>"},{"location":"CS_Electives/Machine_Learning/30_KNN/#knn-classifier","title":"KNN CLassifier","text":"<p>\\(k\\)-nearest neighbor</p> <ul> <li>Pick a value of \\(k\\)</li> <li>Calculate distance between unknown item from all other items</li> <li>Seect \\(k\\) observations in the training data are nearest to the unknown data point</li> <li>Predict the response of the unknown data point using the most popular response value from \\(k\\) nearest neighbors</li> </ul> <p>Lazy Learning: It does not build models explicitly KNN builds a model for each test element</p>"},{"location":"CS_Electives/Machine_Learning/30_KNN/#value-of-k","title":"Value of \\(k\\)","text":"\\(k\\) too Small \\(k\\) too Large Sensitive to noise Neighborhood includes points from other classes"},{"location":"CS_Electives/Machine_Learning/30_KNN/#distances","title":"Distances","text":"<ul> <li>Manhattan distance</li> <li>Euclidian distance</li> <li>Makowski distance</li> </ul> <p>Refer to data mining distances</p>"},{"location":"CS_Electives/Machine_Learning/30_KNN/#advantages","title":"Advantages","text":"<ul> <li>No training period</li> <li>Easy to implement</li> <li>NEw data can be added any time</li> <li>Multi-class, not just binary classification</li> </ul>"},{"location":"CS_Electives/Machine_Learning/30_KNN/#disadvantages","title":"Disadvantages","text":"<ul> <li>We have to calculate the distance for all testing dataset, wrt all records of the training dataset</li> <li>Does not work well with large dataset</li> <li>Does not work well with high dimensional dataset</li> <li>Sensitive to noisy and mssing data</li> <li>Attributes need to scaled to prevent distance measures from being dominated by one of the attributes</li> </ul>"},{"location":"CS_Electives/Machine_Learning/31_Ensemble_Learning/","title":"Ensemble Learning","text":"<p>Reduce variance of models</p> <p>Why do they work</p> <ul> <li>Statistical: Average predictions</li> <li>Computational: Average local optima</li> <li>Representational: Extend hypothesis space</li> </ul> <p></p>"},{"location":"CS_Electives/Machine_Learning/31_Ensemble_Learning/#advantages","title":"Advantages","text":"<ul> <li>Improved accuracy</li> <li>Reduced variance</li> <li>Noisy useless signals will average out and have no effect</li> </ul>"},{"location":"CS_Electives/Machine_Learning/31_Ensemble_Learning/#disadvantages","title":"Disadvantages","text":"<ul> <li>Not interpretable</li> <li>Do not work with unstructured data (images, audio)</li> <li>Computationally-expensive</li> </ul>"},{"location":"CS_Electives/Machine_Learning/31_Ensemble_Learning/#steps","title":"Steps","text":"<ol> <li>Divide dataset into subsets</li> <li>For each subset, apply a model<ul> <li>This model is usually decision tree</li> </ul> </li> <li>Aggegrate the results</li> </ol>"},{"location":"CS_Electives/Machine_Learning/31_Ensemble_Learning/#stability-of-classifier","title":"Stability of Classifier","text":"<p>For unstable models, we have to change model when adding new point</p> <p>For stable models, not required</p>"},{"location":"CS_Electives/Machine_Learning/31_Ensemble_Learning/#learning-techniques","title":"Learning Techniques","text":"Single Bagging(Boostrap aggregation) Boosting Blending/Stacking Training sequence N/A Parallel Sequential Parallel/Sequential Forward stage-wise also to fit an adaptive additive model (adaptive basis functions) \\(\\hat f = \\sum_{m=1}^{M} \\alpha_i \\hat f_i\\) Individual Learners Overfitting UnderfittingSlightly better than average No of learners 1 \\(n\\) \\(n\\) \\(n\\) Training Complete training Random sampling with replacement Random sampling with replacement over weighted data Aggregage the results at the end Only pass over the mis-classified pointsWe boost the probability of mis-classified points to be picked again Preferred for Linear Data Non-Linear Data Example Random forest XGBoost Comment - Only effective for low-bias, high-variance models- Only effective if misclassification rate of individual classifiers &lt;0.5 Advantages Fast (parallel training) Disadvantages OverfittingSlow to train"},{"location":"CS_Electives/Machine_Learning/31_Ensemble_Learning/#bagging","title":"Bagging","text":"<p>\u201cWisdom of the crowds\u201d</p> <p>Bagged classifier\u2019s misclassification rate behaves like a binomial distribution</p> <p>Bagging a good classifier can improve predictive accuracy, but bagging a bad one hurts $$ \\text{MSE}' = \\dfrac{1}{k} \\text{MSE} + \\dfrac{k-1}{k} C $$ where \\(C=\\) covariance between each bagging classifier</p>"},{"location":"CS_Electives/Machine_Learning/31_Ensemble_Learning/#classification","title":"Classification","text":"\\(\\hat y_i\\) Majority/Hard Vote Soft Voting"},{"location":"CS_Electives/Machine_Learning/31_Ensemble_Learning/#training-speed","title":"Training Speed","text":"<p>It cannot be said that boosting is slower than bagging, just because it is sequential and bagging is parallel.</p> <p>This is because, boosting may end in just 10 iterations, but you may need 1000 classifiers for bagging.</p>"},{"location":"CS_Electives/Machine_Learning/31_Ensemble_Learning/#random-forest","title":"Random Forest","text":"<p>Bagging with reduced correlation b/w sampled trees, through random selection of input variables \\(m&lt;&lt;k\\) for each split</p> <p>Usually \\(m = \\sqrt{p}\\)</p>"},{"location":"CS_Electives/Machine_Learning/31_Ensemble_Learning/#boosting","title":"Boosting","text":"<p>\\(\\lambda\\) is the learning rate</p>"},{"location":"CS_Electives/Machine_Learning/31_Ensemble_Learning/#regression","title":"Regression","text":"<ol> <li> <p>Set \\(\\hat f(x) = 0 \\implies u_i = y_i \\quad \\forall i\\)</p> </li> <li> <p>For \\(b=1, \\dots, B:\\)</p> </li> <li> <p>Fit a regression model \\(\\hat f_b(x)\\) to the training data to obtain \\(\\hat y_b\\)</p> </li> <li> <p>Update \\(\\hat y\\) with a shrunken version of \\(\\hat f_b\\): \\(\\hat y = \\hat y + \\lambda \\hat y_b\\),</p> </li> <li> <p>Update the residuals: \\(u_i = u_i - \\lambda \\hat y_b\\)</p> </li> <li> <p>Output: \\(\\hat y = \\sum_{b=1}^B \\lambda \\hat y_b\\)</p> </li> </ol> <p>In each iteration, we fit the model to residuals: this enables re-weighting training data so that obs that did not fit well (\\(r_i\\) large)  become more imp in next iteration.</p>"},{"location":"CS_Electives/Machine_Learning/31_Ensemble_Learning/#classification_1","title":"Classification","text":"<p>**Ada**ptive **Boost**ing</p> <p>The boosted classifier is a weighted sum of individual classifiers, with weights proportional to each classifier\u2019s accuracy on the training set (good classifiers get more weight)</p> <p>In AdaBoost, if an individual classifier has accuracy &lt; 50%, we flip the sign of its predictions and turn it into a classifier with accuracy &gt; 50%. This is achieved by making \\(\\alpha_b\\) &lt; 0 so that the classifier enters negatively into the final hypothesis.</p> <p>In each iteration, we re-weight the obs in the training data such that misclassified points in the previous round see their weights increase compared to correctly classified points. Hence, successive classifiers focus more on misclassified points.</p>"},{"location":"CS_Electives/Machine_Learning/31_Ensemble_Learning/#steps_1","title":"Steps","text":"<ol> <li> <p>Let \\(y \\in \\{ -1, 1 \\}\\)</p> </li> <li> <p>Let \\(w_i = 1/n \\quad \\forall i\\)</p> </li> <li> <p>For \\(b= 1, \\dots, B\\)</p> </li> <li> <p>Fit a classifier \\(\\hat f_b\\) to the training data by minimizing the weighted error</p> <p>\\(\\dfrac{\\sum_{i=1}^n w_i (\\hat y_b \\ne y_i)}{\\sum_{i=1}^n w_i}\\)</p> </li> <li> <p>Let \\(\\alpha_b = \\log \\vert (1-\\epsilon_b)/\\epsilon_b \\vert\\) where \\(\\epsilon_b\\) is the weighted error of \\(\\hat f_b (x)\\)</p> </li> <li> <p>\\(L_i = \\exp \\Big( \\alpha_b (\\hat y_b \\ne y_i) \\Big)\\)</p> </li> <li> <p>Update \\(w_i\\)</p> <p>\\(w_i = w_i \\cdot L_i\\)</p> </li> <li> <p>Output: \\(\\hat y = \\text{sign} (\\sum_{b=1}^B \\alpha_b \\cdot \\hat y_b)\\)</p> </li> </ol>"},{"location":"CS_Electives/Machine_Learning/31_Ensemble_Learning/#optimization","title":"Optimization","text":"<p>Instead of doing a global minimization, the boosting strategy follows a forward stage-wise procedure by adding basis functions one by one</p>"},{"location":"CS_Electives/Machine_Learning/31_Ensemble_Learning/#stage-wise-vs-step-wise","title":"Stage-wise vs step-wise","text":"Stage-wise Step-wise Coefficients updated at each step One All Optimality Worse Better Computation Cost Low High"},{"location":"CS_Electives/Machine_Learning/31_Ensemble_Learning/#additive-boosting","title":"Additive Boosting","text":"\\[ \\hat f = \\sum_{l=1}^L \\alpha_l \\hat f_l(x; \\theta_l) \\] <p>Where \\(\\hat f_l\\) is linear/non-linear</p>"},{"location":"CS_Electives/Machine_Learning/31_Ensemble_Learning/#fsam","title":"FSAM","text":"<p>Forward Stage-wise Additive Modeling</p> <p>At each iteration for learner \\(l\\)</p> \\[ \\arg \\min \\limits_{\\alpha_l, \\theta_l} \\sum_{i=1}^n \\mathcal L \\Big( y_i, \\underbrace{\\hat f_{[\\small {1, l-1}]}(x_i)}_{\\mathclap{\\text{Constant}}} +  \\alpha_l \\hat f_{l}(x_i, \\theta_l) \\Big) \\]"},{"location":"CS_Electives/Machine_Learning/31_Ensemble_Learning/#limitations","title":"Limitations","text":"<ul> <li>Greedy search: local search; We may miss something better</li> <li>May overfit</li> <li>Optimization is computationally expensive</li> </ul>"},{"location":"CS_Electives/Machine_Learning/31_Ensemble_Learning/#gradient-boosting","title":"Gradient Boosting","text":"<p>Performs functional optimization of the cost function: Functional gradient descent with approximate gradients $$ \\begin{aligned} \\hat f_e(x) &amp; \\leftarrow \\hat f_{e-1}(x) - \\eta \\nabla_f J(f) \\ \\hat f_0(x) &amp;= 0 \\end{aligned} $$ Works with any differentiable loss function</p>"},{"location":"CS_Electives/Machine_Learning/32_KMeans_Clustering/","title":"K-Means","text":"<p>Input: \\(k =\\) number of clusters</p> <p>Output: Set of \\(k\\) clusters</p>"},{"location":"CS_Electives/Machine_Learning/32_KMeans_Clustering/#steps","title":"Steps","text":"<ol> <li> <p>Normalize \\(X\\)</p> </li> <li> <p>Randomly choose \\(k\\) objects from the dataset as the initial cluster centroids    (centroid = center of a cluster)</p> </li> <li> <p>For each of the objects</p> </li> <li> <p>Compute distance between current objects and \\(k\\) cluster centroids</p> </li> <li> <p>Assign current object to that cluster to which it is closest</p> <p>If distance of a point between 2 clusters is same, then we assign the point to first centroid.</p> </li> <li> <p>Compute \u2018cluster centers\u2019 \\(m\\) of each cluster. These become the new cluster centroids</p> </li> </ol> \\[ \\begin{aligned} m_k &amp;= \\Big(\\text{mean}(X), \\text{mean}(Y) \\Big) \\\\ X &amp;= \\text{List of } x \\text{ coordinates} \\\\ Y &amp;= \\text{List of } y \\text{ coordinates} \\end{aligned} \\] <ol> <li> <p>Repeat steps 2-3 until convergence criterion is satisfied</p> </li> <li> <p>Stop</p> </li> </ol>"},{"location":"CS_Electives/Machine_Learning/32_KMeans_Clustering/#convergence-criterion","title":"Convergence Criterion","text":"<ul> <li>Particular number of iterations   We can derive this by testing and plotting graph of accuracy vs iterations</li> </ul> <p>or</p> <ul> <li>when clusters don\u2019t change over subsequent iterations</li> </ul>"},{"location":"CS_Electives/Machine_Learning/32_KMeans_Clustering/#limitations","title":"Limitations","text":"<ul> <li>Clustering stuck in local minima: convergence dependent on initialization</li> <li>Measuring clustering quality is hard &amp; relies on heuristics</li> <li>Non-probabilistic: Cluster assignment is binary</li> </ul>"},{"location":"CS_Electives/Machine_Learning/33_Gaussian_Mixture_Clustering/","title":"Gaussian EM K-Means Clustering","text":"<p>Probabilistic clustering which requires K means as well; the output of k means is fed into this model</p>"},{"location":"CS_Electives/Machine_Learning/33_Gaussian_Mixture_Clustering/#em-for-gaussian-mixture","title":"EM for Gaussian Mixture","text":"<p>Alternates between 2 steps</p> <ol> <li>E-Step: Given an estimate \\(\\theta_e\\)\u00a0of the weights, compute \\(P_\\theta(y \\vert x_i)\\) and use it to \u2018hallucinate\u2019 expected cluster assignments \\(z_i\\)</li> <li>M-Step: Find a new \\(\\theta_{e+1}\\) that maximizes the marginal log-likelihood by optimizing \\(P_\\theta(x_i, z_i)\\) given the \\(z_i\\) from step 1</li> </ol> \\[ \\begin{aligned} \\theta_{e+1} &amp;= \\arg \\max_\\theta \\sum_{i=1}^n \\mathbb E_{z_i \\sim P_{\\theta_e} (y \\vert x_i)} \\log P_\\theta(x_i, z_i) \\\\ &amp;= \\arg \\max_\\theta \\sum_{i=1}^n \\sum_{c=1}^C P_{\\theta_e}(y=c \\vert x_i) \\log P_\\theta(x_i, y=k) \\end{aligned} \\] \\[ \\begin{aligned} P_\\theta(y=c \\vert x) &amp;= \\dfrac{P_\\theta (y=c, x)}{P_\\theta(x)} \\\\ &amp;= \\dfrac{P_\\theta (y=c) P_\\theta (x \\vert y=c)}{ \\sum_{c=1}^C P_\\theta (y=c) P_\\theta (x \\vert y=k) } \\end{aligned} \\] <ol> <li>Go to step 1</li> </ol> <p>This process increases the marginal likelihood at each step and eventually converges</p>"},{"location":"CS_Electives/Machine_Learning/33_Gaussian_Mixture_Clustering/#advantages","title":"Advantages","text":"<ul> <li>Easy to implement</li> <li>Guaranteed to converge?</li> <li>Works for many ML models</li> </ul>"},{"location":"CS_Electives/Machine_Learning/33_Gaussian_Mixture_Clustering/#limitations","title":"Limitations","text":"<ul> <li>Can get stuck in local optima</li> <li>May not be able to compute \\(P_{\\theta_e} (y \\vert x_i)\\)\u00a0in each model</li> </ul>"},{"location":"CS_Electives/Machine_Learning/33_Gaussian_Mixture_Clustering/#steps","title":"Steps","text":"<p>Expectation Maximization</p> <ol> <li> <p>Initialize gaussian parameters \\(\\mu_k, \\Sigma_k, \\pi_k\\)</p> </li> <li> <p>\\(\\mu_k \\leftarrow \\mu_k\\)</p> </li> <li>\\(\\Sigma_k \\leftarrow \\text{cov \\(\\Big(\\) cluster(\\)k\\() \\(\\Big)\\)}\\)</li> <li> <p>\\(\\pi_k = \\frac{\\text{No of points in } k}{\\text{Total no of points}}\\)</p> </li> <li> <p>E Step: Assign each point \\(X_n\\) an assignment score \\(\\gamma(z_{nk})\\) for each cluster \\(k\\)</p> </li> </ol> \\[ \\gamma(z_{nk}) = \\frac{ \\pi_k N(x_n|\\mu_k, \\Sigma_k) }{ \\sum_{i=1}^K \\pi_i N(x_n|\\mu_i, \\Sigma_i) } \\] <ol> <li>M Step: Given scores, adjust \\(\\mu_k, \\pi_k, \\Sigma_k\\) for each cluster \\(k\\)</li> </ol> \\[ \\begin{aligned} \\text{Let } N_k &amp;= \\sum_{n=1}^N \\gamma(z_{nk}) \\\\ N &amp;= \\text{Sample Size} \\\\ \\mu_k^\\text{new} &amp;= \\frac{1}{N_k} \\sum_{n=1}^N \\gamma(z_{nk}) x_n \\\\ \\Sigma_k^\\text{new} &amp;= \\frac{1}{N_k} \\sum_{n=1}^N \\gamma(z_{nk}) (x_n - \\mu_k^\\text{new}) (x_n - \\mu_k^\\text{new})^T \\\\ \\pi_k^\\text{new} &amp;= \\frac{N_k}{N} \\end{aligned} \\] <ol> <li>Evaluate log likelihood</li> </ol> \\[ \\ln p(X| \\mu, \\Sigma, \\pi) = \\sum_{n=1}^N \\ln \\left| \\sum_{k=1}^K \\pi_k N(x_n | \\mu_k, \\Sigma_k) \\right| \\] <ol> <li>Stop if likelihood/parameters converge</li> </ol>"},{"location":"CS_Electives/Machine_Learning/33_Gaussian_Mixture_Clustering/#k-means-vs-gaussian-mixture","title":"K Means vs Gaussian Mixture","text":"K means Gaussian Mixture Classifier model Probability model Probabilistic? \u274c \u2705 Classifier Type Hard Soft Pamater to fit to data \\(\\mu_k\\) \\(\\mu_k, \\Sigma_k, \\pi_k\\) \u274c Disadvantage If a class may belong to multiple clusters, we have to assign the first oneIf sample size is too small, initial grouping determines clusters significantly Complex \u2705 Advantage SimpleFast and efficient, with \\(O(tkn),\\) where- \\(t =\\) no of iterations- \\(k =\\) no of clusters- \\(n =\\) no of sample points If a class may belong to multiple clusters, we have a probability to back it up"},{"location":"CS_Electives/Machine_Learning/33_KDE/","title":"Density Estimation","text":""},{"location":"CS_Electives/Machine_Learning/33_KDE/#histogram-de","title":"Histogram DE","text":"<p>Limitations</p> <ul> <li>No of grid cells inc exponentially with dimensionality \\(k\\)</li> <li>Histogram is not \u2018smooth\u2019</li> <li>Shape of histogram depends on bin positions</li> </ul>"},{"location":"CS_Electives/Machine_Learning/33_KDE/#kde","title":"KDE","text":"<p>Kernel Density Estimation</p> <p>Hyperparameter: Bandwidth \\(w\\)</p> <p></p> <p>Histogram has \\(b\\) bins of width \\(w\\) at fixed positions</p> <p>KDE effectively places a bin of width \\(w\\) at each \\(x \\in \\mathcal X\\)</p> <p>To obtain \\(P(x)\\), we count the % of points that fall in the bin centered at \\(x\\)</p>"},{"location":"CS_Electives/Machine_Learning/33_KDE/#tophat-kde","title":"Tophat KDE","text":"\\[ \\begin{aligned} P_w(x) &amp;= \\dfrac{n(x ; w)}{n} \\\\ n(x ; w) &amp;= \\Bigg\\vert \\Big\\{ x_i: \\vert \\vert x_i - x \\vert \\vert \\le w/2 \\Big\\} \\Bigg\\vert \\end{aligned} \\] <p>\\(n(x; w)=\\) no points that are within a bin of width \\(w\\) centered at \\(x\\)</p> <p>To make it smooth, replace histogram counts with weighted averages $$ P(x) \\propto \\sum_{i=1}^n K(x, x_i) $$ Interpretation</p> <ul> <li>We count the no of points \u2018near\u2019 \\(x\\), but each \\(x_i\\) has a weight \\(K(x, x_i)\\) that depends on the similarity between \\(x\\) and \\(x_i\\)</li> <li>We place a \u2018micro-density\u2019 \\(K(x, x_i)\\) at each \\(x_i\\); the final density \\(P(x)\\)\u00a0is their sum</li> </ul>"},{"location":"CS_Electives/Machine_Learning/33_KDE/#common-kernels","title":"Common Kernels","text":"<ul> <li>Linear</li> <li>Gaussian</li> <li>Tophat</li> <li>Epanechnikov</li> <li>Exponential</li> <li>Cosine</li> </ul>"},{"location":"CS_Electives/Machine_Learning/33_KDE/#advantages","title":"Advantages","text":"<ul> <li>Can approximate any data distribution arbitrarily well</li> </ul>"},{"location":"CS_Electives/Machine_Learning/33_KDE/#disadvantages","title":"Disadvantages","text":"<ul> <li>No of datapoints required for a good fit increases exponentially with dimensionality</li> <li>High space complexity: Need to store entire dataset to make queries</li> </ul>"},{"location":"CS_Electives/Machine_Learning/34_Anomaly_Detection/","title":"Anomaly Detection","text":""},{"location":"CS_Electives/Machine_Learning/34_Anomaly_Detection/#note","title":"Note","text":"<p>When using AD as a secondary model to filter data for the primary model, you need not use the same input features for both</p> <p>Let</p> <ul> <li>\\(\\mathcal{X}, \\mathcal{y} \\in \\mathcal{D}\\) be all the data you have</li> <li>\\(\\mathcal{X}_a\\) be used for primary model</li> <li>\\(X_b\\) be used for anomaly detection</li> </ul> <p>Then, all these perfectly reasonable - \\(\\vert \\mathcal{X}_a \\vert  = \\vert \\mathcal{X}_b \\vert\\) - \\(\\vert \\mathcal{X}_a \\vert &gt; \\vert \\mathcal{X}_b \\vert\\) - \\(\\vert \\mathcal{X}_a \\vert &lt; \\vert \\mathcal{X}_b \\vert\\)</p>"},{"location":"CS_Electives/Machine_Learning/34_Anomaly_Detection/#density-estimation","title":"Density Estimation","text":""},{"location":"CS_Electives/Machine_Learning/34_Anomaly_Detection/#procedure-methodology","title":"Procedure Methodology","text":"Training Only non-anomalous samples Validation Verify with known values, then validate, and then update model Testing Verify with known values and then test"},{"location":"CS_Electives/Machine_Learning/34_Anomaly_Detection/#anomaly-detection-vs-classification","title":"Anomaly Detection vs Classification","text":"Anomaly Detection Classification Anomalous training samples requirement None(only required for tuning) Large Non-anomalous training samples requirement Large Large Can handle novelties \u2705 \u274c Example Unseen defectsFraud Known defects (scratches)Spam mail"},{"location":"CS_Electives/Machine_Learning/34_Anomaly_Detection/#feature-engineering","title":"Feature Engineering","text":"<p>Include features that have very small/large values for anomalies</p> <p>If anomalies don\u2019t have such values, then try to find a combination of features such as \\(x_1 \\cdot x_2\\) to achieve it</p>"},{"location":"CS_Electives/Machine_Learning/34_Anomaly_Detection/#dealing-with-non-gaussian-features","title":"Dealing with Non-Gaussian Features","text":"<p>Transformation of training, validation, and test set.</p> <p></p> <p>If you have x values as 0, then \\(\\log(x)\\) as \\(\\log(0)\\) is undefined. So you use \\(\\log(x+c)\\), where \\(c&gt;0\\)</p>"},{"location":"CS_Electives/Machine_Learning/36_Active_Learning/","title":"Active Learning","text":"<pre><code>---\ntitle: Active Learning\n---\nflowchart LR\n1[Label&lt;br/&gt;small sample] --&gt;\n2[Train&lt;br/&gt;model] --&gt;\n3[Predict&lt;br/&gt;unlabelled] --&gt;\n4[\"Query Selection&lt;br/&gt;(Rank poor predictions)\"] --&gt;\n1</code></pre>"},{"location":"CS_Electives/Machine_Learning/36_Active_Learning/#query-selection-strategies","title":"Query Selection Strategies","text":"Type Strategy Measure Advantage Heuristic Uncertainty Sampling Least confidentSmallest marginHighest entropy QBC (Query By Committee) Vote entropy of enseble models Expected Model Change Gradient of loss function wrt parameters Expected Error Reduction Variance Reduction Density Weighted Methods Average similarity to entire unlabelled pool of examples Outliers ignored Approximate Posterior Monte-Carlo Dropout Stochastic Weight Modelling"},{"location":"CS_Electives/Machine_Learning/37_Gaussian_Processes/","title":"Gaussian Processes","text":""},{"location":"CS_Electives/NLP/","title":"Natural Language Processing","text":""},{"location":"CS_Electives/NLP/#references","title":"References","text":"<ul> <li> Natural Language Processing Specialization</li> <li> Natural Language Processing | University of Utah</li> <li> Natural Language Processing with Deep Learning | Stanford</li> <li> Natural Language Understanding | Stanford XCS224U</li> <li> Natural Language Processing | Michael Collins</li> <li> Natural Language Processing | University of Michigan</li> <li> Text Mining and Analytics [FULL COURSE] | UIUC</li> <li> Text Retrieval and Search Engines | UIUC</li> </ul>"},{"location":"CS_Electives/NLP/01_Text_Classification/","title":"Text Classification","text":"<ul> <li>Devise features by hand: Does the message contain \u201cchurch\u201d. Does the email contain an Indian organization\u2019s domain</li> <li>Bag of words: Count of occurrences off each word of a pre-defined \u2018vocabulary\u2019</li> </ul> <p>Pre-Processing</p> <ul> <li>Stemming: only keep the root of the word</li> <li>\u201cslowly\u201d and \u201cslow\u201d both mapped to \u201cslow\u201d</li> <li>Filtering</li> <li>Stopwords: articles</li> <li>Filler words</li> <li>rare words</li> </ul>"},{"location":"CS_Electives/NNFL/","title":"Neural Networks &amp; Fuzzy Logic","text":""},{"location":"CS_Electives/NNFL/#references","title":"References","text":"<ul> <li> NOC Jan 2019: Fuzzy Logic and Neural Networks | IIT Kharagpur</li> </ul>"},{"location":"CS_Electives/Optimization_for_AI/","title":"Optimization Algorithms","text":""},{"location":"CS_Electives/Optimization_for_AI/#references","title":"References","text":"<ul> <li> Adaptive and Cooperative Algorithms; Search Algorithms; Game Theory | University of Waterloo</li> <li> Optimization Methods for Machine Learning and Engineering | Julius Pfrommer</li> <li> Optimization in Machine Learning 2022 | Ganesh Ramakrishnan</li> <li> Optimization | CMU Machine Learning</li> <li> https://mmas.github.io/optimization-scipy</li> <li> https://scipy-lectures.org/advanced/mathematical_optimization/</li> <li> Deep Learning Optimizers</li> <li> Optimization Techniques - W2023</li> <li> Convex Optimization for AI | Han Dean | CMU</li> <li> Optimization | Se Young Yun</li> <li> Convex Optimization | Stanford</li> </ul>"},{"location":"CS_Electives/Optimization_for_AI/01_Introduction/","title":"Optimization","text":"<p>Find a set of parameters that minimizes the loss function for the given data and algorithm $$ \\underset{\\theta}{\\arg \\min}  L(  y, \\hat f_\\theta(D)  ) $$</p>"},{"location":"CS_Electives/Optimization_for_AI/01_Introduction/#idk","title":"IDK","text":"<p>Always test optimization procedure on known solution</p>"},{"location":"CS_Electives/Optimization_for_AI/01_Introduction/#backpropagation-steps","title":"Backpropagation Steps","text":"<ol> <li>Forward pass</li> <li>Calculate loss </li> <li>Backward pass</li> </ol> <p>Basically just chain rule + intelligent caching of intermediate results</p> <p>Computationally-cheap, but large storage required for caching intermediate results</p> <p>Each layer needs to be able to perform vector Jacobian product: multiply the \u201cincoming backward gradient\u201d by its derivatives $$ \\begin{aligned} \\dfrac{\\partial J}{\\partial \\theta_l} &amp;= \\dfrac{\\partial J}{\\partial y_L} \\left( \\prod_{i=l}^L \\dfrac{\\partial y_i}{\\partial y_{i-1}} \\right) \\dfrac{\\partial y_l}{\\partial \\theta_l} \\end{aligned} $$ </p>"},{"location":"CS_Electives/Optimization_for_AI/01_Introduction/#training-process","title":"Training Process","text":"<pre><code>flowchart LR\ni[Initialize \u03b8] --&gt;\nj[Calculate cost function] --&gt;\na{Acceptable?} --&gt;\n|Yes| stop([Stop])\n\na --&gt;\n|No| change[Update \u03b8] --&gt;\nj</code></pre>"},{"location":"CS_Electives/Optimization_for_AI/01_Introduction/#steps","title":"Steps","text":"<ul> <li>Forward pass</li> <li>Backward pass</li> <li>Weights update</li> </ul>"},{"location":"CS_Electives/Optimization_for_AI/01_Introduction/#optimization-parameters","title":"Optimization Parameters","text":""},{"location":"CS_Electives/Optimization_for_AI/01_Introduction/#objective-function","title":"Objective Function","text":"<p>An objective function has a unique global minimum if it is</p> <ul> <li>differentiable</li> <li>convex</li> </ul> <p></p>"},{"location":"CS_Electives/Optimization_for_AI/01_Introduction/#hard-constraints-and-bounds","title":"Hard Constraints and Bounds","text":"<p>Useful if you know the underlying systematic differential equation</p> <p>$$ \\text{DE} = 0 $$ Refer to PINNs for more information</p> <p>When it is not possible to use a discontinuous hard constraint/bound (such as \\(\\beta \\ge k\\)), you can add a barrier function to the cost function $$ J' = J + B $$ where \\(B\\) can be</p> <ul> <li>Exponential barrier: \\(\\pm \\exp \\{ m (\\beta - k) \\}\\)</li> <li>where \\(m=\\) barrier coefficient</li> </ul> <p></p>"},{"location":"CS_Electives/Optimization_for_AI/01_Introduction/#soft-constraints-regularization","title":"Soft Constraints: Regularization","text":"<p>Encourages (not guaranteed) certain parameters to end in range values, through penalizing deviation from prior/preferred values</p>"},{"location":"CS_Electives/Optimization_for_AI/01_Introduction/#weights-initialization-algorithm","title":"Weights Initialization Algorithm","text":"<ul> <li>Zero (bad)</li> <li>Random</li> <li>Glorot (Xavier)</li> </ul>"},{"location":"CS_Electives/Optimization_for_AI/01_Introduction/#optimization-algorithms","title":"Optimization Algorithms","text":"<p>Optimization Algorithms</p>"},{"location":"CS_Electives/Optimization_for_AI/01_Introduction/#batch-size","title":"Batch Size","text":"Optimizer Meaning Comment Gradient-Free Weight Update Rule\\(w_{t+1}\\) Advantages Disadvantages BGD(Batch Gradient Descent) Update weights after viewing the entire dataset: \\(n\\) sample points \u274c Guaranteed convergence to local minimum Computationally-expensive for large datasetProne to getting stuck at non-optimal local minima for non-convex cost functions SGD(Stochastic Gradient Descent) Update weights after viewing every sample point \u274c \\(w_t - \\eta g(w_t)\\) Cheaper computationFaster updatesRandomization helps escape \u2018shallow\u2019 local minima May not converge to global minima for non-convex cost functionsNoisy/Oscillating/Erratic convergence MBGD(Mini-Batch Gradient Descent) Update weights after viewing the \\(b\\) sample points, where \\(b &lt; n\\)Usually \\(b=32\\) Middle ground between BGD and SGDGeneralizes better than Adam \u274c <p>Rule of thumb for SGD: recommended \\(\\eta = 0.1\\)</p>"},{"location":"CS_Electives/Optimization_for_AI/01_Introduction/#gradient-descent","title":"Gradient Descent","text":"<p>Similar to trial and error</p> <ol> <li>Start with some \\(\\theta\\) vector</li> <li>Keep changing \\(\\theta_0, \\theta_1, \\dots, \\theta_n\\) using derivative of cost function, until minimum for \\(J(\\theta)\\) is obtained - Simultaneously</li> </ol> \\[ \\theta_{\\text{new}} = \\theta_{\\text{prev}} - \\eta \\  {\\nabla J} \\] Meaning \\(\\theta_{\\text{new}}\\) Coefficients obtained from current iteration(Output of current iteration) \\(\\theta_{\\text{old}}\\) Coefficients obtained from previous iteration(Output of previous iteration) \\(\\eta\\) Learning Rate \\(\\nabla J\\) Gradient vector of \\(J (\\theta)\\)"},{"location":"CS_Electives/Optimization_for_AI/01_Introduction/#gradients-of-the-loss-function","title":"Gradients of the Loss Function","text":""},{"location":"CS_Electives/Optimization_for_AI/01_Introduction/#learning-rate-eta","title":"Learning Rate \\(\\eta\\)","text":"<p>\\(0 &lt; \\eta &lt; 1\\)</p> <ul> <li>Large value may lead to underfitting/overfitting</li> <li>Small value will lead to more time taken</li> </ul> <p>Can be</p> <ul> <li>constant</li> <li>time-based decay</li> </ul> <p></p>"},{"location":"CS_Electives/Optimization_for_AI/01_Introduction/#iterative-vs-normal-equation","title":"Iterative vs Normal Equation","text":"Iterative Normal Equation \\(\\alpha\\) not required \u274c \u2705 Feature scaling not required \u274c \u2705 Time Complexity \\(O(kn^2)\\) \\(O(n^3)\\) Performance Fast even for large \\(n\\) Slow if \\(n &gt; 10^4\\) Compatibility Works for all algorithms Doesn\u2019t work for classification No of features Works for all algorithms Doesn't work when \\(X^TX\\) is non-invertible Stop criteria None Convergence Slow Global Optimal guaranteed \u274c \u2705 Loss Function Should be double-differentiable <p>Gradient-based methods find min of a function by moving in the direction in which the function decreases most steeply</p>"},{"location":"CS_Electives/Optimization_for_AI/01_Introduction/#speed-up-training","title":"Speed Up Training","text":"<ul> <li>Subsetting</li> <li>Feature-scaling</li> <li>Pruning</li> <li>Good Weight initialization</li> <li>Good Activation functions</li> <li>Transfer learning: Re-use parts of pre-trained network</li> <li>Using mini-batch updates</li> <li>Learning rate scheduling</li> <li>Faster optimization algorithm</li> <li>Use GPU/TPU</li> </ul>"},{"location":"CS_Electives/Optimization_for_AI/01_Introduction/#subsetting","title":"Subsetting","text":"<ol> <li>Sample Size</li> <li>Mini-Batch</li> <li>Stochastic</li> <li>Input Features</li> </ol> <p>You can do either</p> <ul> <li>drop with both approaches</li> <li>Bagging with each sub-model using the subset</li> </ul>"},{"location":"CS_Electives/Optimization_for_AI/01_Introduction/#feature-scaling","title":"Feature Scaling","text":"<p>Helps to speed up gradient descent by making it easier for the algorithm to reach minimum faster</p> <p>Get every feature to approx \\(-1 \\le x_i \\le 1\\) range</p> <p>Atleast try to get \\(-3 \\le x_i \\le 3\\) or \\(-\\frac13 \\le x_i \\le \\frac13\\)</p>"},{"location":"CS_Electives/Optimization_for_AI/01_Introduction/#standardization","title":"Standardization","text":"\\[ \\begin{aligned} x'_i &amp;= z_i \\\\ &amp;= \\frac{ x_i - \\bar x }{s} \\end{aligned} \\]"},{"location":"CS_Electives/Optimization_for_AI/01_Introduction/#batch-normalization","title":"Batch Normalization","text":""},{"location":"CS_Electives/Optimization_for_AI/01_Introduction/#learning-rate","title":"Learning Rate","text":""},{"location":"CS_Electives/Optimization_for_AI/01_Introduction/#convex-function","title":"Convex Function","text":"<p>Convex function is one where $$ \\begin{aligned} f(\\alpha x + \\beta y) &amp;\\le \\alpha f(x) + \\beta f(y) \\ \\alpha + \\beta &amp;= 1; \\alpha, \\beta \\ge 0 \\end{aligned} $$</p>"},{"location":"CS_Electives/Optimization_for_AI/01_Introduction/#robust-optimization","title":"Robust Optimization","text":""},{"location":"CS_Electives/Optimization_for_AI/01_Introduction/#limitations","title":"Limitations","text":"<ul> <li>Parameters must be independent</li> <li>Cannot handle equality constraints</li> <li>Hard to estimate min and max value of parameter</li> <li>Method is extremely conservative</li> </ul>"},{"location":"CS_Electives/Optimization_for_AI/01_Introduction/#batch-size_1","title":"Batch Size","text":""},{"location":"CS_Electives/Optimization_for_AI/01_Introduction/#resources","title":"Resources","text":"<p>Let \\(b=\\) batch size $$ \\begin{aligned} \\text{Space Requirement} &amp;\\propto \\dfrac{1}{b} \\ \\text{Time Requirement} &amp;\\propto b \\end{aligned} $$</p> <ul> <li>Larger batch size means larger memory required to train a single batch at one time</li> <li>Larger batch size means fewer updates per epoch</li> </ul>"},{"location":"CS_Electives/Optimization_for_AI/01_Introduction/#generalization","title":"Generalization","text":"<p>The following is only empirically-proven $$ \\begin{aligned} \\text{Generalization} &amp;\\propto \\dfrac{1}{b} \\end{aligned} $$ The noise from smaller batch size helps escape suboptimal local minimum</p> <p></p> <p></p>"},{"location":"CS_Electives/Optimization_for_AI/01_Introduction/#learning-rate_1","title":"Learning Rate","text":"<p>Should be scaled according to batch size $$ \\text{LR}' = \\text{LR} \\times (b/32) $$</p>"},{"location":"CS_Electives/Optimization_for_AI/01_Introduction/#batching","title":"Batching","text":"<p>When training a neural network, we usually divide our data in mini-batches and go through them one by one. The network predicts batch labels, which are used to compute the loss with respect to the actual targets. Next, we perform backward pass to compute gradients and update model weights in the direction of those gradients.</p> <ul> <li>Full dataset does not fit in memory</li> <li>Faster convergence due to stochasticity</li> </ul>"},{"location":"CS_Electives/Optimization_for_AI/01_Introduction/#approaches-to-obtain-gradient","title":"Approaches to obtain gradient","text":"<ul> <li>Exact: Use matrix differential calculus, Jacobians, Kronecker products, &amp; vectorization</li> <li>Approximation</li> <li>Pretend everything everything is a scalar</li> <li>use typical chain rule</li> <li>rearrange/transpose matrices/vectors to make the sizes work</li> <li>verify result numerically </li> </ul>"},{"location":"CS_Electives/Optimization_for_AI/01_Introduction/#initialization","title":"Initialization","text":"<p>Initialization is very important: Weights don\u2019t move \u201cthat much\u201d, so weights tend often stay much closer to initial points than to the \u201cfinal\u201d point after optimization from different initial point</p> <p>If you initialize all the weights as 0, all your gradients will be 0 and ANN will not learn anything $$ \\begin{aligned} W_{t=0} &amp;= N(0, \\sigma^2 I) \\ \\sigma^2_\\text{recom RELU} &amp;= \\dfrac{2}{\\text{no of neurons}} \\ \\sigma^2_{\\text{recom } \\sigma} &amp;= \\dfrac{1}{\\text{no of neurons}} \\end{aligned} $$ Kaiming Normal Initialization: based on central limit theorem, we want the entire distribution to become \\(N(0, 1)\\)</p> <p>The choice of \\(\\sigma^2\\) will affect</p> <ol> <li>Magnitude/Norm of forward activations</li> <li>Magnitude/Norm of gradients</li> </ol> \\(\\sigma^2\\) Norms Too low Vanishing Optimal Right Too high Exploding <p></p> <p>Here \\(n=\\) no of neurons</p> <p>Why is \\(\\sigma^2 = 2/n\\) the best? Because ReLU will cause half the components of the activations to be set to 0, so we need twice the variance to achieve the same final variance</p> <p>Even when trained successfully, the effects/scales present at initialization persist throughout training</p> <p></p>"},{"location":"CS_Electives/Optimization_for_AI/01_Introduction/#solution","title":"Solution","text":"Layer Normalization Batch Normalization Normalize activations of each image at each layer Normalize activations of all images in each mini-batch at each layer \\(w'_{i+1}\\) \\(\\dfrac{w_{i+1} - E[w_{i+1}]}{\\sigma(w_{i+1}) + \\epsilon}\\) Limitation Harder to train standard FCN to low loss, because the relative sizes between activations is lost Inter-dependence of training samples (Soln: below) <p>Where</p> <ul> <li>\\(i=\\) layer number</li> <li>\\(\\epsilon={10}^{-5}\\)</li> </ul>"},{"location":"CS_Electives/Optimization_for_AI/01_Introduction/#soln","title":"Soln","text":"<ol> <li>Training: Compute running average of mean \\(\\hat \\mu_{i+1}\\) &amp; variance \\(\\hat \\sigma^2_{i+1}\\) for all features at each layer</li> <li>Inference: Normalize by these quantities</li> </ol> \\[ (w'_{i+1})_j = \\dfrac{(w_{i+1})_j - (\\hat \\mu_{i+1})_j}{(\\hat \\sigma_{i+1})_j + \\epsilon} \\]"},{"location":"CS_Electives/Optimization_for_AI/01_Introduction/#stopping-criteria","title":"Stopping Criteria","text":"<p>Use an <code>or</code> combination of the following</p> <ul> <li>\\(J(\\theta) \\le\\)  Cost Threshold</li> <li>\\(\\vert J(\\theta)_e - J(\\theta)_{e-1} \\vert \\le\\) Convergence threshold</li> <li>where \\(e=\\) epochs</li> <li> <p>Moving average of the previous 5?</p> </li> <li> <p>Evaluation metric \\(\\le\\) Evaluation threshold</p> </li> <li>This may be different from the cost function</li> <li>MSE for cost; MAPE for evaluation</li> <li>\\(n_{\\text{iter}} \\ge\\) Iter Threshold</li> <li>Time taken \\(\\ge\\)  Duration threshold</li> </ul>"},{"location":"CS_Electives/Optimization_for_AI/01_Introduction/#idk_1","title":"IDK","text":"<p>For each epoch, you can subsample the training set and then create batches</p> <ul> <li>Cheaper epochs</li> <li>More stochastic</li> </ul>"},{"location":"CS_Electives/Optimization_for_AI/01_Optimization_Algorithms/","title":"Optimization Algorithms","text":"Optimizer Meaning Comment Gradient-Free Weight Update Rule\\(w_{t+1}\\) Advantages Disadvantages L-BGFS Newton\u2019s method Not commonly used \u274c \\(w_{t+1} = w_t - \\eta H^{-1} g(w_t)\\) Full-step will optimize quadratic functions in one step Can\u2019t efficiently solve for Newton step, even using automatic differentiationFor non-convex optimization, it is very unclear if we even want to use Newton direction Nelder-Mead Simplex method \u2705 Gradient Descent Generalizes better than Adam \u274c \\(w_t - \\eta g(w_t)\\) GD + Momentum \u201cAveraging\u201d of step directions\u201cglobal\u201d structure similar to BGFS \u274c \\(w_t - \\eta u_{t}\\)\\(u_{t+1} = \\beta u_t + (1-\\beta) g(w_t); u_0 = 0\\)\\(\\beta \\in [0, 1):\\) momentum averaging parameter Smoothens out steps Can introduce oscillation/non-descent behavior GD + Unbiased Momentum \u274c \\(w_t - \\dfrac{\\eta u_{t}}{1 - \\beta^{t+1}}\\)Dividing by \\(1-\\beta^{t+1}\\) unbiases the update Ensures updates have equal expected magnitude across all iterations Sometimes you want the initial steps to be smaller than the later states GD + Nesterov Momentum \u274c \\(w_t - \\eta u_{t\\textcolor{hotpink}{-1}}\\)\\(u_{t+1} = \\beta u_t + (1-\\beta) g(w_t \\textcolor{hotpink}{- \\eta u_t}); u_0 = 0\\) AdaDelta \u274c \\(w_t + v_{t+1}\\)\\(v_{t+1} = \\rho v_t - \\eta g(w_t)\\)or\\(v_{t+1} = \\rho v_t - \\eta g(w_t + \\rho v_t)\\) AdaGrad Decreases the momentum for each parameter, based on how much that parameter has made progressCan only decrease the moment \u274c \\(w_{i, t+1} = w_{i, t} - \\dfrac{\\eta}{\\epsilon + \\sqrt{v_{i, t+1}}} g(w_{i, t})^2\\) \\(v_{i, t+1} = v_{i, t} + g(w_{i, t})^2\\)\\(\\epsilon &gt; 0\\) RMSProp Keeps a memory of previous gradientsCan increase/decrease the moment \u274c \\(w_{t+1} = w_{i, t} - \\dfrac{\\eta}{\\epsilon + \\sqrt{v_{t+1}}} g(w_{t})^2\\) \\(v_{t+1} = \\beta v_{t} + (1-\\beta) g(w_t)^2\\)\\(\\epsilon &gt; 0, \\beta \\in [0, 1]\\) Adam(Adaptive Moment Estimation) Scales the updates for each parameter differently \u274c \\(w_{t+1} = w_{t} - \\dfrac{\\eta}{\\epsilon + \\sqrt{\\hat v_{t+1}}} \\hat m_{t+1}\\)\\(\\hat m_{t+1} = \\dfrac{m_{t+1}}{1-{\\beta_1}^{t+1}}\\)\\(m_{t+1} = \\beta_1 m_t + (1-\\beta_1) g(w_t)\\)\\(\\hat v_{t+1} = \\dfrac{v_{t+1}}{1-{\\beta_2}^{t+1}}\\)\\(v_{t+1} = \\beta_2 v_t + (1-\\beta_2) g(w_t)^2\\)\\(\\epsilon &gt; 0; \\beta_1, \\beta_2 \\in [0, 1]\\)"},{"location":"CS_Electives/Optimization_for_AI/01_Optimization_Algorithms/#newtons-method","title":"Newton\u2019s Method","text":"<p>Integrates more \u201cglobal\u201d structure into optimization methods, which scales gradients according to the inverse of the Hessian Equivalent to approximating the function as quadratic using second-order Taylor expansion, then solving for optimal solution</p> <ul> <li>\\(\\eta=1:\\) Full step</li> <li>o.w: Damped Newton method</li> </ul>"},{"location":"CS_Electives/Optimization_for_AI/03/","title":"03","text":""},{"location":"CS_Electives/Optimization_for_AI/03/#worst-first-backpropagation","title":"Worst-First Backpropagation","text":"<p>Backpropagation is expensive, so only focus on Top k</p>"},{"location":"CS_Electives/Optimization_for_AI/03/#gradient-accumulation","title":"Gradient Accumulation","text":"<ul> <li>Use a small batch size</li> <li>Save the gradients at each batch</li> <li>Update network weights once every couple of batches</li> </ul> <p>Purpose</p> <ul> <li>Helps to imitate a larger batch size</li> <li>For large GPU memory intensive architectures</li> </ul> <p>Notes</p> <ul> <li>Some network architectures have batch-specific operations. For instance, batch normalization is performed on a batch level and therefore may yield slightly different results when using the same effective batch size with and without gradient accumulation</li> <li>It is important to also update weights on the last batch, to ensure that the last batches are not discarded and used for optimizing the network</li> </ul>"},{"location":"CS_Electives/Optimization_for_AI/03/#performance-improvement","title":"Performance Improvement","text":""},{"location":"CS_Electives/Optimization_for_AI/03/#evaluation-frequency","title":"Evaluation Frequency","text":"<ul> <li><code>train_eval_every</code></li> <li><code>dev_eval_every</code></li> </ul> <p>10 is a good number</p>"},{"location":"CS_Electives/Optimization_for_AI/03/#1-consider-using-another-learning-rate-schedule","title":"1. Consider using another learning rate schedule","text":"<p>The learning rate (schedule) you choose has a large impact on the speed of convergence as well as the generalization performance of your model.</p> <p>Cyclical Learning Rates and the 1Cycle learning rate schedule are both methods introduced by Leslie N. Smith (here and here), and then popularised by fast.ai's Jeremy Howard and Sylvain Gugger (here and here). Essentially, the 1Cycle learning rate schedule looks something like this:</p> <p></p> <p>Sylvain writes:</p> <p>[1cycle consists of]  two steps of equal lengths, one going from a lower learning rate to a higher one than go back to the minimum. The maximum should be the value picked with the Learning Rate Finder, and the lower one can be ten times lower. Then, the length of this cycle should be slightly less than the total number of epochs, and, in the last part of training, we should allow the learning rate to decrease more than the minimum, by several orders of magnitude.</p> <p>In the best case this schedule achieves a massive speed-up \u2013 what Smith calls Superconvergence \u2013 as compared to conventional learning rate schedules. Using the 1Cycle policy he needs ~10x fewer training iterations of a ResNet-56 on ImageNet to match the performance of the original paper, for instance). The schedule seems to perform robustly well across common architectures and optimizers.</p> <p>PyTorch implements both of these methods <code>torch.optim.lr_scheduler.CyclicLR</code> and <code>torch.optim.lr_scheduler.OneCycleLR,</code> see the documentation.</p> <p>One drawback of these schedulers is that they introduce a number of additional hyperparameters. This post and this repo, offer a nice overview and implementation of how good hyper-parameters can be found including the Learning Rate Finder mentioned above.</p> <p>Why does this work? It doesn't seem entirely clear but one possible explanation might be that regularly increasing the learning rate helps to traverse saddle points in the loss landscape more quickly.</p>"},{"location":"CS_Electives/Optimization_for_AI/03/#2-use-multiple-workers-and-pinned-memory-in-dataloader","title":"2. Use multiple workers and pinned memory in DataLoader","text":"<p>When using torch.utils.data.DataLoader, set <code>num_workers &gt; 0</code>, rather than the default value of 0, and <code>pin_memory=True</code>, rather than the default value of False. Details of this are explained here.</p> <p>Szymon Micacz achieves a 2x speed-up for a single training epoch by using four workers and pinned memory.</p> <p>A rule of thumb that people are using to choose the number of workers is to set it to four times the number of available GPUs with both a larger and smaller number of workers leading to a slow down.</p> <p>Note that increasing num_workerswill increase your CPU memory consumption.</p>"},{"location":"CS_Electives/Optimization_for_AI/03/#3-max-out-the-batch-size","title":"3. Max out the batch size","text":"<p>This is a somewhat contentious point. Generally, however, it seems like using the largest batch size your GPU memory permits will accelerate your training (see NVIDIA's Szymon Migacz, for instance). Note that you will also have to adjust other hyperparameters, such as the learning rate, if you modify the batch size. A rule of thumb here is to double the learning rate as you double the batch size.</p> <p>OpenAI has a nice empirical paper on the number of convergence steps needed for different batch sizes. Daniel Huynh runs some experiments with different batch sizes (also using the 1Cycle policy discussed above) where he achieves a 4x speed-up by going from batch size 64 to 512.</p> <p>One of the downsides of using large batch sizes, however, is that they might lead to solutions that generalize worse than those trained with smaller batches.</p>"},{"location":"CS_Electives/Optimization_for_AI/03/#4-use-automatic-mixed-precision-amp","title":"4. Use Automatic Mixed Precision (AMP)","text":"<p>The release of PyTorch 1.6 included a native implementation of Automatic Mixed Precision training to PyTorch. The main idea here is that certain operations can be run faster and without a loss of accuracy at semi-precision (FP16) rather than in the single-precision (FP32) used elsewhere. AMP, then, automatically decide which operation should be executed in which format. This allows both for faster training and a smaller memory footprint.</p> <p>In the best case, the usage of AMP would look something like this:</p> <pre><code>import torch\n# Creates once at the beginning of training\nscaler = torch.cuda.amp.GradScaler()\n\nfor data, label in data_iter:\n   optimizer.zero_grad()\n   # Casts operations to mixed precision\n   with torch.cuda.amp.autocast():\n      loss = model(data)\n\n   # Scales the loss, and calls backward()\n   # to create scaled gradients\n   scaler.scale(loss).backward()\n\n   # Unscales gradients and calls\n   # or skips optimizer.step()\n   scaler.step(optimizer)\n\n   # Updates the scale for next iteration\n   scaler.update()\n</code></pre> <p>Benchmarking a number of common language and vision models on NVIDIA V100 GPUs, Huang and colleagues find that using AMP over regular FP32 training yields roughly 2x \u2013 but upto 5.5x \u2013 training speed-ups.</p> <p>Currently, only CUDA ops can be autocast in this way. See the documentation here for more details on this and other limitations.</p> <p>u/SVPERBlA points out that you can squeeze out some additional performance (~ 20%) from AMP on NVIDIA Tensor Core GPUs if you convert your tensors to the Channels Last memory format. Refer to this section in the NVIDIA docs for an explanation of the speedup and more about NCHW versus NHWC tensor formats.</p>"},{"location":"CS_Electives/Optimization_for_AI/03/#5-consider-using-another-optimizer","title":"5. Consider using another optimizer","text":"<p>AdamW is Adam with weight decay (rather than L2-regularization) which was popularized by fast.ai and is now available natively in PyTorch as <code>torch.optim.AdamW</code>. AdamW seems to consistently outperform Adam in terms of both the error achieved and the training time. See this excellent blog post on why using weight decay instead of L2-regularization makes a difference for Adam.</p> <p>Both Adam and AdamW work well with the 1Cycle policy described above.</p> <p>There are also a few not-yet-native optimizers that have received a lot of attention recently, most notably LARS (pip installable implementation) and LAMB.</p> <p>NVIDA's APEX implements fused versions of a number of common optimizers such as Adam. This implementation avoid a number of passes to and from GPU memory as compared to the PyTorch implementation of Adam, yielding speed-ups in the range of 5%.</p>"},{"location":"CS_Electives/Optimization_for_AI/03/#6-turn-on-cudnn-benchmarking","title":"6. Turn on cudNN benchmarking","text":"<p>If your model architecture remains fixed and your input size stays constant, setting <code>torch.backends.cudnn.benchmark = True</code> might be beneficial (docs). This enables the cudNN autotuner which will benchmark a number of different ways of computing convolutions in cudNN and then use the fastest method from then on.</p> <p>For a rough reference on the type of speed-up you can expect from this, Szymon Migacz achieves a speed-up of 70% on a forward pass for a convolution and a 27% speed-up for a forward + backward pass of the same convolution.</p> <p>One caveat here is that this autotuning might become very slow if you max out the batch size as mentioned above.</p>"},{"location":"CS_Electives/Optimization_for_AI/03/#7-beware-of-frequently-transferring-data-between-cpus-and-gpus","title":"7. Beware of frequently transferring data between CPUs and GPUs","text":"<p>Beware of frequently transferring tensors from a GPU to a CPU using <code>tensor.cpu()</code> and vice versa using <code>tensor.cuda()</code> as these are relatively expensive. The same applies for <code>.item()</code> and <code>.numpy()</code> \u2013 use <code>.detach()</code> instead.</p> <p>If you are creating a new tensor, you can also directly assign it to your GPU using the keyword argument <code>device=torch.device('cuda:0')</code>.</p> <p>If you do need to transfer data, using <code>.to(non_blocking=True)</code>, might be useful as long as you don't have any synchronization points after the transfer.</p> <p>If you really have to, you might want to give Santosh Gupta's SpeedTorch a try, although it doesn't seem entirely clear when this actually does/doesn't provide speed-ups.</p>"},{"location":"CS_Electives/Optimization_for_AI/03/#8-use-gradientactivation-checkpointing","title":"8. Use gradient/activation checkpointing","text":"<p>Quoting directly from the documentation:</p> <p>Checkpointing works by trading compute for memory. Rather than storing all intermediate activations of the entire computation graph for computing backward, the checkpointed part does not save intermediate activations, and instead recomputes them in backward pass. It can be applied on any part of a model.</p> <p>Specifically, in the forward pass, function will run in torch.no_grad() manner, i.e., not storing the intermediate activations. Instead, the forward pass saves the inputs tuple and the functionparameter. In the backwards pass, the saved inputs and function is retrieved, and the forward pass is computed on function again, now tracking the intermediate activations, and then the gradients are calculated using these activation values.</p> <p>So while this will might slightly increase your run time for a given batch size, you'll significantly reduce your memory footprint. This in turn will allow you to further increase the batch size you're using allowing for better GPU utilization.</p> <p>While checkpointing is implemented natively as <code>torch.utils.checkpoint</code>(docs), it does seem to take some thought and effort to implement properly. Priya Goyal has a good tutorial demonstrating some of the key aspects of checkpointing.</p>"},{"location":"CS_Electives/Optimization_for_AI/03/#9-use-gradient-accumulation","title":"9. Use gradient accumulation","text":"<p>Another approach to increasing the batch size is to accumulate gradients across multiple <code>.backward()</code> passes before calling optimizer.step().</p> <p>Following a post by Hugging Face's Thomas Wolf, gradient accumulation can be implemented as follows:</p> <pre><code>model.zero_grad()                                   # Reset gradients tensors\nfor i, (inputs, labels) in enumerate(training_set):\n    predictions = model(inputs)                     # Forward pass\n    loss = loss_function(predictions, labels)       # Compute loss function\n    loss = loss / accumulation_steps                # Normalize our loss (if averaged)\n    loss.backward()                                 # Backward pass\n    if (i+1) % accumulation_steps == 0:             # Wait for several backward steps\n        optimizer.step()                            # Now we can do an optimizer step\n        model.zero_grad()                           # Reset gradients tensors\n        if (i+1) % evaluation_steps == 0:           # Evaluate the model when we...\n            evaluate_model()                        # ...have no gradients accumulate\n</code></pre> <p>This method was developed mainly to circumvent GPU memory limitations and I'm not entirely clear on the trade-off between having additional <code>.backward()</code> loops. This discussion on the fastai forum seems to suggest that it can in fact accelerate training, so it's probably worth a try.</p>"},{"location":"CS_Electives/Optimization_for_AI/03/#10-use-distributed-data-parallel-for-multi-gpu-training","title":"10. Use Distributed Data Parallel for multi-GPU training","text":"<p>Methods to accelerate distributed training probably warrant their own post but one simple one is to use <code>torch.nn.DistributedDataParallel</code> rather than <code>torch.nn.DataParallel</code>. By doing so, each GPU will be driven by a dedicated CPU core avoiding the GIL issues of DataParallel.</p> <p>In general, I can strongly recommend reading the documentation on distributed training.</p>"},{"location":"CS_Electives/Optimization_for_AI/03/#11-set-gradients-to-none-rather-than-0","title":"11. Set gradients to None rather than 0","text":"<p>Use <code>.zero_grad(set_to_none=True)</code> rather than <code>.zero_grad()</code>.</p> <p>Doing so will let the memory allocator handle the gradients rather than actively setting them to 0. This will lead to yield a modest speed-up as they say in the documentation, so don't expect any miracles.</p> <p>Watch out, doing this is not side-effect free! Check the docs for the details on this.</p>"},{"location":"CS_Electives/Optimization_for_AI/03/#12-use-as_tensor-rather-than-tensor","title":"12. Use .as_tensor() rather than .tensor()","text":"<p><code>torch.tensor()</code> always copies data. If you have a numpy array that you want to convert, use <code>torch.as_tensor()</code> or <code>torch.from_numpy()</code> to avoid copying the data.</p>"},{"location":"CS_Electives/Optimization_for_AI/03/#13-turn-on-debugging-tools-only-when-actually-needed","title":"13. Turn on debugging tools only when actually needed","text":"<p>PyTorch offers a number of useful debugging tools like the autograd.profiler, autograd.grad_check, and autograd.anomaly_detection. Make sure to use them to better understand when needed but to also turn them off when you don't need them as they will slow down your training.</p>"},{"location":"CS_Electives/Optimization_for_AI/03/#14-use-gradient-clipping","title":"14. Use gradient clipping","text":"<p>Originally used to avoid exploding gradients in RNNs, there is both some empirical evidence as well as some theoretical support that clipping gradients (roughly speaking: <code>gradient = min(gradient, threshold)</code>) accelerates convergence.</p> <p>Hugging Face's Transformer implementation is a really clean example of how to use gradient clipping as well as some of the other methods such as AMP mentioned in this post.</p> <p>In PyTorch this can be done using <code>torch.nn.utils.clip_grad_norm_</code>(documentation).</p> <p>It's not entirely clear to me which models benefit how much from gradient clipping but it seems to be robustly useful for RNNs, Transformer-based and ResNets architectures and a range of different optimizers.</p>"},{"location":"CS_Electives/Optimization_for_AI/03/#15-turn-off-bias-before-batchnorm","title":"15. Turn off bias before BatchNorm","text":"<p>This is a very simple one: turn off the bias of layers before BatchNormalization layers. For a 2-D convolutional layer, this can be done by setting the bias keyword to False: <code>torch.nn.Conv2d(..., bias=False, ...)</code>.  (Here's a reminder why this makes sense.)</p> <p>You will save some parameters, I would however expect the speed-up of this to be relatively small as compared to some of the other methods mentioned here.</p>"},{"location":"CS_Electives/Optimization_for_AI/03/#17-use-input-and-batch-normalization","title":"17. Use input and batch normalization","text":"<p>You're probably already doing this but you might want to double-check:</p> <ul> <li>Are you normalizing your input?</li> <li>Are you using batch-normalization?</li> </ul> <p>And here's a reminder of why you probably should.</p>"},{"location":"CS_Electives/Optimization_for_AI/03/#bonus-tip-from-the-comments-use-jit-to-fuse-point-wise-operations","title":"Bonus tip from the comments: Use JIT to fuse point-wise operations.","text":"<p>If you have adjacent point-wise operations you can use PyTorch JIT to combine them into one FusionGroup which can then be launched on a single kernel rather than multiple kernels as would have been done per default. You'll also save some memory reads and writes.</p> <p>Szymon Migacz shows how you can use the <code>@torch.jit.script</code> decorator to fuse the operations in a GELU, for instance:</p> <pre><code>@torch.jit.script\ndef fused_gelu(x):\n    return x * 0.5 * (1.0 + torch.erf(x / 1.41421))\n</code></pre> <p>In this case, fusing the operations leads to a 5x speed-up for the execution of <code>fused_gelu</code> as compared to the unfused version.</p> <p>See also this post for an example of how Torchscript can be used to accelerate an RNN.</p> <p>Hat tip to u/Patient_Atmosphere45 for the suggestion.</p>"},{"location":"CS_Electives/Optimization_for_AI/04/","title":"04","text":""},{"location":"CS_Electives/Optimization_for_AI/04/#gradient-problems","title":"Gradient Problems","text":"<p>FFNN can cope with these problems because they only have a few hidden layers, but RNN struggles.</p> Vanishing (Converging) Exploding (Diverging) CauseWeights multiplied during BPTT are Too small Too large Gradients __ exponentially during back-propagation shrink grow Resultant problemEffect on current output due to past input Too little Too high Solutions Scaling Clipping"},{"location":"CS_Electives/Optimization_for_AI/04/#initial-weights","title":"Initial Weights","text":"<p>We can avoid this by initializing the weights very carefully</p>"},{"location":"CS_Electives/Optimization_for_AI/04/#clipping","title":"Clipping","text":"<p>rescales gradient to size at most \\(\\theta\\).</p> \\[ g \\leftarrow \\min \\left( 1, \\frac{\\theta}{\\vert g \\vert}  \\right) g \\] <p>If the weights are large, the gradients grow exponentially during back-propagation</p>"},{"location":"CS_Electives/Optimization_for_AI/05_Automatic_Differentiation/","title":"Automatic Differentiation","text":""},{"location":"CS_Electives/Optimization_for_AI/05_Automatic_Differentiation/#differentiation-types","title":"Differentiation Types","text":"Error Disadvantage Numerical \\(\\lim \\limits_{\\epsilon \\to 0} \\dfrac{f(x + \\epsilon) - f(x)}{\\epsilon}\\) \\(O(\\epsilon)\\) Less accurate NumericalType 2 \\(\\lim \\limits_{\\epsilon \\to 0} \\dfrac{f(x + \\epsilon) - f(x-\\epsilon)}{2\\epsilon}\\) \\(O(\\epsilon^2)\\) Numerical errorComputationally-expensive Symbolic Derive gradient by sum, product, chain rules TediousComputationally-expensive Backprop Run backward operations the same forward graph Forward mode automatic Output: Computational graphDefine \\(\\dot v_i = \\dfrac{\\partial v_i}{\\partial x_j}\\)where \\(v_i\\) is an intermediate result Computationally-expensive: \\(n\\) forward passes required to get gradient of each input Reverse mode automatic Output: Computational graphDefine adjoint \\(\\bar v_i = \\dfrac{\\partial y}{\\partial v_i}\\)where \\(v_i\\) is an  intermediate result\\(\\overline{v_{k \\to i}} = \\bar v_i \\dfrac{\\partial v_i}{\\partial v_k}\\)"},{"location":"CS_Electives/Optimization_for_AI/05_Automatic_Differentiation/#numerical-gradient-checking","title":"Numerical gradient checking","text":"\\[ \\Delta^T \\nabla_x f(x) = \\dfrac{f(x + \\epsilon \\delta) - f(x - \\epsilon \\delta)}{2 \\epsilon} + O(\\epsilon^2) \\] <p>Pick \\(\\delta\\)\u00a0from unit ball</p>"},{"location":"CS_Electives/Programmable_Electronics/","title":"Programmable Electronics","text":"<p>ASIC: application-specific IC</p> <p>FPGA: Field Programmable Gate Arrays</p>"},{"location":"CS_Electives/Programmable_Electronics/#references","title":"References","text":"<ul> <li> ECE5760 DE2/115 lectures 2011 | Bruce Land</li> <li> Introduction to FPGA | DigiKey</li> <li> Learn FPGA | Invent Box Tutorials</li> <li> Neural Networks on FPGA | Piero Nexus</li> </ul>"},{"location":"CS_Electives/RL/","title":"Reinforcement Learning","text":""},{"location":"CS_Electives/RL/#references","title":"References","text":"<ul> <li> DeepMind x UCL | Deep Learning Lecture Series 2021</li> <li> Deep Reinforcement Learning For Finance 101 | Bam Tungom</li> <li> Reinforcement Learning (HMC CS 181V)\u2014Spring, 2020 | Neil Rhodes</li> <li> Reinforcement Learning | IIT Madras</li> <li> Deep RL | UC Berkeley</li> </ul>"},{"location":"CS_Electives/Robotics/","title":"Robotics","text":""},{"location":"CS_Electives/Robotics/#references","title":"References","text":"<ul> <li> <p> CS287 Advanced Robotics at UC Berkeley Fall 2019</p> </li> <li> <p> Introduction to Robotics and Robots | Paul McWhorter</p> </li> </ul>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/00/","title":"Course Information","text":"<p>Foundations of Sports Analytics</p> <p>Coursera</p> <p>University of Michigan</p> <p>https://www.coursera.org/learn/foundations-sports-analytics</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/01/","title":"Pythagorean Expectation","text":"<p>Expected Win % is proportional to ratio of square of my team's parameter and sum of squares of both teams' parameters. Parameter could be goals,points, etc. Regression analysis ensures right parameter(s).</p> <p>$$ \\text{Expected Win}\\% \\propto \\frac{x<sup>2}{x</sup>2 + y^2} $$ where</p> <ul> <li>x = parameter scored</li> <li>y = parameter conceded</li> </ul> <p>A graph for pythagorean expectation vs win % will show a strong relation. Relation is characterized by the following values</p> <ul> <li>correlation</li> <li>standard error   smaller the better</li> <li>\\(t\\) statistic   greater the better   \\(t = \\frac{ \\text{coefficent} } { \\text{std err} }\\), (coefficient = slope)</li> <li>\\(p \\le 0.05\\) (statistically significant)   smaller the better</li> <li>R-squared   greater the better</li> </ul>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/01/#advantages","title":"Advantages","text":"<p>You might be thinking, why not just use the previous W% to predict future W%. The problem is that W% involves randomness. Meanwhile, PE captures the performance of the team.</p> <p>Imagine a really good team which lost games because of some random events - let\u2019s say opponent goal keepers were just too good; that doesn\u2019t mean that we played bad. Meanwhile PE using shot scored on target/conceded will provide a better picture. Hence, past PE is a better predictor than past W%</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/01/#limitations","title":"Limitations","text":"<p>This isn\u2019t ideal for</p> <ol> <li>small datasets</li> <li>games with smaller innings</li> <li>games like cricket where the chasing team has a target, so it cannot score to the best they can</li> </ol>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/02/","title":"Why Python?","text":"<ul> <li>Open source, poweful, free</li> <li>Good for datascraping</li> <li>has a lot of libraries</li> </ul>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/02/#intro","title":"Intro","text":"<p>I already know the basics of Pandas, so I haven\u2019t explained here.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/02/#variable-types","title":"Variable Types","text":"Qualitative Quantitative Data Type Categorical Numerical Example Gender - M, FCountrySport - Football, BasketballLeague - NFL, NBA Number of goals, Points, Age, Salary Can be discrete/continuous random variables"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/02/#convert-categorical-to-dummy-variable","title":"Convert categorical to dummy variable","text":"<p>One-Hot Encoding</p> <pre><code>dummy = (\n  pd\n  .get_dummies(\n        df,\n    columns = ['WL']\n    )\n  .rename(columns={\n    \"WL_W\": \"Win\"\n  })\n)\n\ndf = (\n  df\n  .concat(\n    [df, dummy['Win']],\n    axis = 1\n  )\n)\n</code></pre>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/02/#summary-statistics","title":"Summary Statistics","text":"<pre><code>flowchart TB\n\nct[Central Tendancy] --&gt;\nMean &amp; Median &amp; Mode\n\nVariation --&gt;\nVariance &amp; sd[Standard Deviation] &amp; cv[Coefficient of Variation]</code></pre>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/02/#coefficient-of-variation","title":"Coefficient of variation","text":"<ul> <li>\\(CV = \\frac{\\sigma}{\\mu}\\)</li> <li>\\(0 \\le CV \\le 1\\)</li> <li>useful for comparing variations of different measurement scale</li> </ul>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/02/#correlation-analyses","title":"Correlation Analyses","text":"<p>Helps understand relationship between 2 variables</p> <p>Correlation \\(\\ne\\) causal relationship.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/02/#covariance","title":"Covariance","text":"<p>Measure of the joint variability of 2 random variables. $$ \\sigma_{xy} = \\text{cov}(x, y) = E\\Bigg[     \\Big( x-E(x) \\Big)     \\Big( y-E(y) \\Big) \\Bigg] $$ The sign of the covariance shows the tendency of the linear relationship between the variables. We do not analyze the magnitude of covariance, as it depends on the unit of measurement.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/02/#correlation-coefficient","title":"Correlation coefficient","text":"<p>Summarizes the relationship between 2 variables. But doesn\u2019t show the exact value change. $$ r = \\frac{ \\sigma_{x y} }{     \\sigma_x \\sigma_y } \\</p> <p>-1 \\le r \\le 1 $$</p> \\(\\sigma_{xy}\\) or \\(r\\) Conclusion 0 linear-relationship non-existentsome other relationship may/may not exist &gt;0 \\(x \\propto y\\) &lt;0 \\(x \\propto \\frac{1}{y}\\)"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/02/#notebooks","title":"Notebooks","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/03/","title":"03","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/04/","title":"Linear Regression","text":"<p>Find the \u2018best-fit\u2019 line to better understand the relationship between 2 variables $$ y = \\textcolor{hotpink}{ \\underbrace{a+bx}\\text{Systematic Component} } + \\textcolor{orange}{ \\underbrace{\\epsilon}\\text{Error Term} } $$ | Term       | Meaning                                |                                                              | | ---------- | -------------------------------------- | ------------------------------------------------------------ | | \\(y\\)        | Output                                 |                                                              | | \\(a\\)        | Vertical Intercept                     | What is the value of \\(y\\), if \\(x=0\\)                           | | \\(b\\)        | Slope                                  | What is the change on \\(y\\), for every unit increase of \\(x\\)    | | \\(X\\)        | Input                                  |                                                              | | \\(\\epsilon\\) | Error                                  | We need \\(E[\\epsilon] = 0\\)                                    |</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/04/#ols","title":"OLS","text":"<p>Ordinary Least Squares</p> <p>We try to find parameters that minimize the square of errors. $$ \\min \\sum_{i=1}^n \\epsilon_i^2 \\ \\epsilon = y-(a+bx) $$</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/04/#r2","title":"\\(R^2\\)","text":"<p>% of variation in \\(Y\\) explained by \\(X\\)</p> <p>It shows the relative contribution of systematic component to values of \\(y\\) $$ 0 \\le R^2 \\le 1 \\ \\text{Good Fit} \\propto R^2 $$</p> <p>Higher the \\(R^2\\), better the fit.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/04/#statistical-significance","title":"Statistical Significance","text":"<p>How confident that \\(x\\) has an impact on \\(y\\). (Doesn\u2019t necessarily mean causal impact)</p> <p>Using a null hypothesis test</p> <ol> <li>we take</li> <li>\\(H_0: b = 0\\)</li> <li>\\(H_1: b \\ne 0\\)</li> <li>\\(t \\text{-statistics} = \\frac{\\hat b}{\\epsilon}\\)</li> <li>Using normal distribution table, find \\(P(Z &gt; t \\text{-statistics})\\)</li> <li>Usually</li> <li>Confidence interval = 95%</li> <li>Level of significance \\(\\alpha = 0.05\\)</li> <li>If \\(p &lt; \\alpha\\), \\(b\\) is statistically significant at \\(\\alpha\\) </li> </ol>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/04/#statsmodes","title":"<code>statsmodes</code>","text":"<pre><code>regression = sm.ols(\n    formula = \"Salary ~ batsman + bowler + batsman*bowler\",\n  data = IPLPlayer,\n  missing = \"drop\"\n).fit()\n\nregression.summary()\n</code></pre> <p><code>batsman*bowler</code> m.eans all-rounder (batsman and bowler)</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/04/#notebooks","title":"Notebooks","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/05/","title":"2 Types of regression analyses","text":"<ol> <li>Predict \\(y\\) using \\(x\\)</li> <li>Forecast \\(y_{t+1}\\) using \\(x_t\\) and \\(y_t\\)</li> </ol>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/05/#suggested-analysis-method","title":"Suggested Analysis Method","text":"<p>Let\u2019s say you\u2019re trying to analyze the correlation impact of \\(x\\) on \\(y\\)</p> Step Leads to ___ of effect of \\(x\\) on \\(y\\) First analyze obvious factors \\(x\\) Over-estimation Include omitted variables and lagged \\(x\\) Under-estimation Include heterogeneous effectsie, the effect of being Manchester United Hopefully accurate estimation"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/05/#lagged-value","title":"Lagged Value","text":"<p>Note: This does not matter for <code>grangercausalitytests</code> library</p> <pre><code>nba[\"wpc_lag\"] = (\n  nba\n  .groupby(\"Team\")\n  [\"wpc\"]\n  .shift(1)\n)\n</code></pre>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/05/#fixed-effect","title":"Fixed Effect","text":"<p>Helps understand the effect of history of a team.</p> <p>ie, apart from other factors, does your position matter that you are Manchester United.</p> <pre><code>regression = smf.ols(\n    formula = \"wpc ~ wpc_lag + relsal + C(Team, Treatment('Everton'))\",\n  data = NBA\n).fit()\n</code></pre>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/05/#notebooks","title":"Notebooks","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/06/","title":"Hot Hand","text":"<p>idea that success rate of scoring is a pattern, and not random</p> <p>originated from basketball</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/06/#reason","title":"Reason","text":"<p>Previous success can change the psychological attitude of the player, and hence change the subsequent success rate</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/06/#research","title":"Research","text":"<p>There is research that supports Hot Hand.</p> <p>But, there is also research suggesting that this is just a \u2018fallacy\u2019.</p> <ul> <li>People have a tendency to try finding patterns within randomness</li> <li>Misconception - law of large numbers apply to small samples</li> </ul> <p>This could probably be because, if a player scores, the opponent would defend them better on the next play.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/06/#thahirs-opinion","title":"Thahir\u2019s Opinion","text":"<p>I believe that the Hot Hand idea is true. As an athlete myself, my confidence goes up when I do make shots, and I end up scoring more subsequently, unless I get over-confident.</p> <p>This is like Classical Economics (people are rational) vs Behavioral Economics (people are not always rational). Technically, player performance should be random, but since we are not mathematical machines, but humans with random complexities, the Hot Hand works.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/06/#conditional-probability","title":"Conditional Probability","text":"\\[ P(A|B) = \\frac{     P(A \\cap B) }{     P(B) } \\]"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/06/#independence","title":"Independence","text":"<p>If \\(A\\) and \\(B\\) are independent</p> <ol> <li>\\(P(A \\cap B) = P(A) \\cdot P(B)\\)</li> <li>\\(P(A|B) = P(A)\\)</li> <li>\\(P(B|A) = P(B)\\)</li> </ol> <p>That is, occurence of A does not affect occurence of B, and vice-versa.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/06/#t-test","title":"T Test","text":"<p>We can do a t test to determine if 2 distributions are different.</p> <pre><code>import scipy.stats as sp\nsp.stats.ttest_ind(\n    Player_Stats[\"conditional_prob\"],\n  Player_Stats[\"average_hit\"],\n)\n</code></pre> <p>The null hypothesis is that they are both are equal/similar. So, if the p value &lt; 0.05, we can say the 2 distributions are different.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/06/#autocorrelation-serial-correlation","title":"Autocorrelation (Serial Correlation)","text":"<p>Linear relationship between adjacent values of the same variable. $$ y_t = a + by_{t-1} $$ eg: Relationship between performance this year and performance last year.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/06/#autocorrelation-coefficent","title":"Autocorrelation Coefficent","text":"\\[ r_a = \\frac{     \\text{cov}(x_t, x_{t-1}) }{     \\sigma_{x_{\\small{t}}} \\sigma_{x_{\\small{t-1}}} } \\] <p>\\(-1 \\le r_a \\le 1\\)</p> <p>\\(r_a\\) is independent of unit of measurement</p> \\(r_a &gt; 0\\) \\(r_a &lt; 0\\) Above average follows Above average Below average Below average follows Below average Above average Nature of graph Smooth Zigzag"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/06/#wls-regression","title":"WLS Regression","text":"<p>Weighted Least Squares</p> <p>Weights the observations proportional to the reciprocal of the error variance of the observation. Helps overcome the problem of non-constant variance.</p> <p>For eg, if some players took 1000 shots, but others only took 10shots.</p> <p>If we are weighting by the number of shots per game, then weight \\(= \\frac{1}{\\text{Shots per Game}}\\)</p> <pre><code>reg = sm.wls(\n    formula = \"error ~ lagerror + player_position\",\n  weight = 1/ShotLog[\"shots_per_game\"],\n  data = ShotLog\n)\n</code></pre>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/06/#notebooks","title":"Notebooks","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/01/","title":"English Premier League (EPL) Pythagorean Predictor","text":"In\u00a0[1]: Copied! <pre># Importing Packages\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Custom\nimport warnings\nwarnings.filterwarnings('ignore')\n%config InlineBackend.figure_formats = ['svg'] # makes everything svg by default\n%matplotlib inline\n</pre> # Importing Packages  import pandas as pd import numpy as np import statsmodels.formula.api as smf import matplotlib.pyplot as plt import seaborn as sns  # Custom import warnings warnings.filterwarnings('ignore') %config InlineBackend.figure_formats = ['svg'] # makes everything svg by default %matplotlib inline In\u00a0[4]: Copied! <pre># Read Data\n\ndataset = pd.read_excel('ds/EPL2017-18.xlsx')\n\ndisplay( dataset.head() )\n</pre> # Read Data  dataset = pd.read_excel('ds/EPL2017-18.xlsx')  display( dataset.head() ) Date HomeTeam AwayTeam FTHG FTAG FTR 0 20170811 Arsenal Leicester 4 3 H 1 20170812 Brighton Man City 0 2 A 2 20170812 Chelsea Burnley 2 3 A 3 20170812 Crystal Palace Huddersfield 0 3 A 4 20170812 Everton Stoke 1 0 H In\u00a0[5]: Copied! <pre># Cleanup\ndataset['count'] = 1\n\ndataset['hwinvalue'] = np.where( dataset['FTR']=='H',1, np.where(dataset['FTR']=='D',.5,0) )\ndataset['awinvalue'] = np.where( dataset['FTR']=='A',1, np.where(dataset['FTR']=='D',.5,0) )\n\nhome1 = dataset[dataset.Date &lt; 20180000].groupby(['HomeTeam'])['count','hwinvalue', 'FTHG','FTAG']\\\n    .sum().reset_index()\nhome1 = home1.rename(columns={'HomeTeam':'Team','count':'MPh','FTHG':'GFh', 'FTAG':'GAh'})\n\naway1 = dataset[dataset.Date &lt; 20180000].groupby(['AwayTeam'])['count','awinvalue', 'FTHG','FTAG']\\\n    .sum().reset_index()\naway1 = away1.rename(columns={'AwayTeam':'Team','count':'MPa','FTHG':'GAa','FTAG':'GFa'})\n# because my goals in away ground will be home goals against for the other team\n\n\nhome2 = dataset[dataset.Date &gt; 20180000].groupby(['HomeTeam'])['count','hwinvalue', 'FTHG','FTAG']\\\n    .sum().reset_index()\nhome2 = home2.rename(columns={'HomeTeam':'Team','count':'MPh','FTHG':'GFh', 'FTAG':'GAh'})\n\naway2 = dataset[dataset.Date &gt; 20180000].groupby(['AwayTeam'])['count','awinvalue', 'FTHG','FTAG']\\\n    .sum().reset_index()\naway2 = away2.rename(columns={'AwayTeam':'Team','count':'MPa','FTHG':'GAa','FTAG':'GFa'})\n# because my goals in away ground will be home goals against for the other team\n\nhalf1 = pd.merge(home1, away1, on=\"Team\")\nhalf2 = pd.merge(home2, away2, on=\"Team\")\n</pre> # Cleanup dataset['count'] = 1  dataset['hwinvalue'] = np.where( dataset['FTR']=='H',1, np.where(dataset['FTR']=='D',.5,0) ) dataset['awinvalue'] = np.where( dataset['FTR']=='A',1, np.where(dataset['FTR']=='D',.5,0) )  home1 = dataset[dataset.Date &lt; 20180000].groupby(['HomeTeam'])['count','hwinvalue', 'FTHG','FTAG']\\     .sum().reset_index() home1 = home1.rename(columns={'HomeTeam':'Team','count':'MPh','FTHG':'GFh', 'FTAG':'GAh'})  away1 = dataset[dataset.Date &lt; 20180000].groupby(['AwayTeam'])['count','awinvalue', 'FTHG','FTAG']\\     .sum().reset_index() away1 = away1.rename(columns={'AwayTeam':'Team','count':'MPa','FTHG':'GAa','FTAG':'GFa'}) # because my goals in away ground will be home goals against for the other team   home2 = dataset[dataset.Date &gt; 20180000].groupby(['HomeTeam'])['count','hwinvalue', 'FTHG','FTAG']\\     .sum().reset_index() home2 = home2.rename(columns={'HomeTeam':'Team','count':'MPh','FTHG':'GFh', 'FTAG':'GAh'})  away2 = dataset[dataset.Date &gt; 20180000].groupby(['AwayTeam'])['count','awinvalue', 'FTHG','FTAG']\\     .sum().reset_index() away2 = away2.rename(columns={'AwayTeam':'Team','count':'MPa','FTHG':'GAa','FTAG':'GFa'}) # because my goals in away ground will be home goals against for the other team  half1 = pd.merge(home1, away1, on=\"Team\") half2 = pd.merge(home2, away2, on=\"Team\") In\u00a0[6]: Copied! <pre># Evaluations\nhalves = [half1, half2]\n\nfor half in halves:\n    half[\"MP\"] = half[\"MPh\"] + half[\"MPa\"]\n    half[\"wValue\"] = half[\"hwinvalue\"] + half[\"awinvalue\"]\n    half[\"GF\"] = half[\"GFh\"] + half[\"GFa\"]\n    half[\"GA\"] = half[\"GAh\"] + half[\"GAa\"]\n\n\nhalf1[\"pyth1\"] = (half1[\"GF\"]**2) / (half1[\"GF\"]**2 + half1[\"GA\"]**2)\nhalf1[\"wpc1\"] = half1[\"wValue\"]/half1[\"MP\"]\n\n\nhalf2[\"pyth2\"] = (half2[\"GF\"]**2) / (half2[\"GF\"]**2 + half2[\"GA\"]**2)\nhalf2[\"wpc2\"] = half2[\"wValue\"]/half2[\"MP\"]\n</pre> # Evaluations halves = [half1, half2]  for half in halves:     half[\"MP\"] = half[\"MPh\"] + half[\"MPa\"]     half[\"wValue\"] = half[\"hwinvalue\"] + half[\"awinvalue\"]     half[\"GF\"] = half[\"GFh\"] + half[\"GFa\"]     half[\"GA\"] = half[\"GAh\"] + half[\"GAa\"]   half1[\"pyth1\"] = (half1[\"GF\"]**2) / (half1[\"GF\"]**2 + half1[\"GA\"]**2) half1[\"wpc1\"] = half1[\"wValue\"]/half1[\"MP\"]   half2[\"pyth2\"] = (half2[\"GF\"]**2) / (half2[\"GF\"]**2 + half2[\"GA\"]**2) half2[\"wpc2\"] = half2[\"wValue\"]/half2[\"MP\"] In\u00a0[7]: Copied! <pre># Cleaned up Dataset\ndropCols = [\"MPh\", \"hwinvalue\", \"GFh\", \"GAh\", \"MPa\", \"awinvalue\", \"GFa\", \"GAa\"]\n\nfor half in halves:\n    display( \n        half.drop(columns = dropCols).head()\n    )\n</pre> # Cleaned up Dataset dropCols = [\"MPh\", \"hwinvalue\", \"GFh\", \"GAh\", \"MPa\", \"awinvalue\", \"GFa\", \"GAa\"]  for half in halves:     display(          half.drop(columns = dropCols).head()     ) Team MP wValue GF GA pyth1 wpc1 0 Arsenal 21 13.5 38 26 0.681132 0.642857 1 Bournemouth 21 7.5 20 32 0.280899 0.357143 2 Brighton 21 8.5 15 25 0.264706 0.404762 3 Burnley 21 12.5 18 17 0.528548 0.595238 4 Chelsea 21 15.5 39 14 0.885847 0.738095 Team MP wValue GF GA pyth2 wpc2 0 Arsenal 17 8.5 36 25 0.674649 0.500000 1 Bournemouth 17 9.0 25 29 0.426330 0.529412 2 Brighton 17 7.0 19 29 0.300333 0.411765 3 Burnley 17 7.5 18 22 0.400990 0.441176 4 Chelsea 17 9.0 23 24 0.478733 0.529412 In\u00a0[15]: Copied! <pre># using half 1 pyth as predictor for half 2 wpc\npredictor = pd.merge(half1, half2, on = \"Team\")\ndisplay(predictor.head())\n</pre> # using half 1 pyth as predictor for half 2 wpc predictor = pd.merge(half1, half2, on = \"Team\") display(predictor.head()) Team MPh_x hwinvalue_x GFh_x GAh_x MPa_x awinvalue_x GAa_x GFa_x MP_x ... awinvalue_y GAa_y GFa_y MP_y wValue_y GF_y GA_y pyth2 wpc2 gap 0 Arsenal 10 8.5 25 10 11 5.0 16 13 21 ... 1.0 15 7 17 8.5 36 25 0.674649 0.500000 6.5 1 Bournemouth 11 4.5 14 17 10 3.0 15 6 21 ... 4.0 16 13 17 9.0 25 29 0.426330 0.529412 1.0 2 Brighton 10 5.5 10 12 11 3.0 13 5 21 ... 1.5 16 5 17 7.0 19 29 0.300333 0.411765 4.0 3 Burnley 10 6.0 7 6 11 6.5 11 11 21 ... 4.0 11 9 17 7.5 18 22 0.400990 0.441176 0.5 4 Chelsea 11 8.5 21 7 10 7.0 7 18 21 ... 4.5 15 14 17 9.0 23 24 0.478733 0.529412 0.0 <p>5 rows \u00d7 31 columns</p> In\u00a0[18]: Copied! <pre>title = \"2nd Half Win% vs 1st Half Win %\"\n\nsns.relplot(x=\"wpc1\", y=\"wpc2\", data = predictor)\nplt.title(title)\nplt.xlim(0, 1), plt.ylim(0, 1)\nplt.savefig(title + \".svg\", dpi=300, bbox_inches = 'tight')\nplt.show()\n</pre> title = \"2nd Half Win% vs 1st Half Win %\"  sns.relplot(x=\"wpc1\", y=\"wpc2\", data = predictor) plt.title(title) plt.xlim(0, 1), plt.ylim(0, 1) plt.savefig(title + \".svg\", dpi=300, bbox_inches = 'tight') plt.show() In\u00a0[20]: Copied! <pre>title = \"2nd Half Win% vs 1st Half Pythagorean Expectation\"\n\n# Plotting\nsns.relplot(x=\"pyth1\", y=\"wpc2\", data = predictor)\nplt.title(title)\nplt.xlim(0, 1), plt.ylim(0, 1)\nplt.savefig(title + \".svg\", dpi=300, bbox_inches = 'tight')\nplt.show()\n</pre> title = \"2nd Half Win% vs 1st Half Pythagorean Expectation\"  # Plotting sns.relplot(x=\"pyth1\", y=\"wpc2\", data = predictor) plt.title(title) plt.xlim(0, 1), plt.ylim(0, 1) plt.savefig(title + \".svg\", dpi=300, bbox_inches = 'tight') plt.show() In\u00a0[17]: Copied! <pre>regression = smf.ols(formula = 'wpc2 ~ wpc1', data=predictor).fit()\nregression.summary()\n</pre> regression = smf.ols(formula = 'wpc2 ~ wpc1', data=predictor).fit() regression.summary() Out[17]: OLS Regression Results Dep. Variable: wpc2   R-squared:             0.572 Model: OLS   Adj. R-squared:        0.549 Method: Least Squares   F-statistic:           24.10 Date: Sat, 09 Apr 2022   Prob (F-statistic): 0.000113 Time: 08:19:08   Log-Likelihood:       18.002 No. Observations:     20   AIC:                  -32.00 Df Residuals:     18   BIC:                  -30.01 Df Model:      1 Covariance Type: nonrobust coef std err t P&gt;|t| [0.025 0.975] Intercept     0.1800     0.069     2.607  0.018     0.035     0.325 wpc1     0.6384     0.130     4.909  0.000     0.365     0.912 Omnibus:  2.990   Durbin-Watson:         1.996 Prob(Omnibus):  0.224   Jarque-Bera (JB):      1.319 Skew:  0.196   Prob(JB):              0.517 Kurtosis:  1.805   Cond. No.               7.05 Notes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.  In\u00a0[11]: Copied! <pre># Regression\n\nregression = smf.ols(formula = 'wpc2 ~ pyth1', data=predictor).fit()\nregression.summary()\n</pre> # Regression  regression = smf.ols(formula = 'wpc2 ~ pyth1', data=predictor).fit() regression.summary() Out[11]: OLS Regression Results Dep. Variable: wpc2   R-squared:             0.633 Model: OLS   Adj. R-squared:        0.613 Method: Least Squares   F-statistic:           31.06 Date: Sat, 09 Apr 2022   Prob (F-statistic): 2.73e-05 Time: 08:16:43   Log-Likelihood:       19.534 No. Observations:     20   AIC:                  -35.07 Df Residuals:     18   BIC:                  -33.08 Df Model:      1 Covariance Type: nonrobust coef std err t P&gt;|t| [0.025 0.975] Intercept     0.2897     0.043     6.690  0.000     0.199     0.381 pyth1     0.4543     0.082     5.573  0.000     0.283     0.626 Omnibus:  4.877   Durbin-Watson:         2.048 Prob(Omnibus):  0.087   Jarque-Bera (JB):      1.521 Skew: -0.033   Prob(JB):              0.467 Kurtosis:  1.650   Cond. No.               4.65 Notes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.  In\u00a0[12]: Copied! <pre># correlation matrix\n\nvalues = predictor[['Team', 'wpc1', 'wpc2', 'pyth1', 'pyth2']]\ndisplay( values.corr() )\n</pre> # correlation matrix  values = predictor[['Team', 'wpc1', 'wpc2', 'pyth1', 'pyth2']] display( values.corr() ) wpc1 wpc2 pyth1 pyth2 wpc1 1.000000 0.756573 0.968204 0.745832 wpc2 0.756573 1.000000 0.795693 0.955986 pyth1 0.968204 0.795693 1.000000 0.795331 pyth2 0.745832 0.955986 0.795331 1.000000 In\u00a0[13]: Copied! <pre># Quiz Questions\n\n\nprint(\n    \"How many EPL games from this season were played in 2018?\"\n    + \"\\n\" +\n    str(dataset[dataset.Date &gt; 20180000].shape[0])\n)\n\nprint(\n    \"Which team scored the highest number of goals while playing at home in the first half of the season?\"\n    + \"\\n\" +\n    half1.sort_values(\"GFh\", ascending=False).iloc[0][0]\n)\n\nprint(\n    \"Which team conceded the highest number of goals while playing away in the first half of the season?\"\n    + \"\\n\" +\n    half1.sort_values(\"GAa\", ascending=False).iloc[0][0]\n)\n\nhalf1['dev'] = abs(half1['wpc1'] - half1['pyth1'])\nprint(\n    \"Which of the following teams had the smallest difference between their win percentage and Pythagorean expectation in the first half of the season?\"\n)\ndisplay( half1.sort_values(\"dev\", ascending=True).head() )\nprint(\"Mancity\")\nprint(\n    \"Which of the following teams had the smallest difference between their win percentage and Pythagorean expectation in the first half of the season?\"\n)\ndisplay( half1.sort_values(\"dev\", ascending=True).head() )\nprint(\"Leicester\")\n\n\nprint(\n    \"Which of the following teams had the highest value for away wins (awinvalue) for in the first half of the season?\"\n)\ndisplay( half1.sort_values(\"awinvalue\", ascending=False).tail() )\n\nhalf2['gap'] = abs(half2['hwinvalue'] - half2['awinvalue'])\nprint(\n    \"Which team had the largest gap between home points won (hwinvalue) and away points won (awinvalue) in the second half the season?\"\n    + \"\\n\" +\n    half2.sort_values(\"gap\", ascending=False).iloc[0][0]\n)\n\nprint(\n    \"What was the correlation between win percentage and the Pythagorean expectation in the first half of the season?\"\n)\ndisplay(\n    round(values.corr().iloc[0, 2], 3)\n)\n\n\nprint(\n    \"What was the correlation between win percentage in the first half of the season and the second half of the season?\"\n)\ndisplay(\n    round(values.corr().iloc[0, 1], 3)\n)\n\nprint(\n    \"What was the correlation between win percentage in the second half of the season and the Pythagorean expectation in the first half of the season?\"\n)\ndisplay(\n    round(values.corr().iloc[1, 2], 3)\n)\n</pre> # Quiz Questions   print(     \"How many EPL games from this season were played in 2018?\"     + \"\\n\" +     str(dataset[dataset.Date &gt; 20180000].shape[0]) )  print(     \"Which team scored the highest number of goals while playing at home in the first half of the season?\"     + \"\\n\" +     half1.sort_values(\"GFh\", ascending=False).iloc[0][0] )  print(     \"Which team conceded the highest number of goals while playing away in the first half of the season?\"     + \"\\n\" +     half1.sort_values(\"GAa\", ascending=False).iloc[0][0] )  half1['dev'] = abs(half1['wpc1'] - half1['pyth1']) print(     \"Which of the following teams had the smallest difference between their win percentage and Pythagorean expectation in the first half of the season?\" ) display( half1.sort_values(\"dev\", ascending=True).head() ) print(\"Mancity\") print(     \"Which of the following teams had the smallest difference between their win percentage and Pythagorean expectation in the first half of the season?\" ) display( half1.sort_values(\"dev\", ascending=True).head() ) print(\"Leicester\")   print(     \"Which of the following teams had the highest value for away wins (awinvalue) for in the first half of the season?\" ) display( half1.sort_values(\"awinvalue\", ascending=False).tail() )  half2['gap'] = abs(half2['hwinvalue'] - half2['awinvalue']) print(     \"Which team had the largest gap between home points won (hwinvalue) and away points won (awinvalue) in the second half the season?\"     + \"\\n\" +     half2.sort_values(\"gap\", ascending=False).iloc[0][0] )  print(     \"What was the correlation between win percentage and the Pythagorean expectation in the first half of the season?\" ) display(     round(values.corr().iloc[0, 2], 3) )   print(     \"What was the correlation between win percentage in the first half of the season and the second half of the season?\" ) display(     round(values.corr().iloc[0, 1], 3) )  print(     \"What was the correlation between win percentage in the second half of the season and the Pythagorean expectation in the first half of the season?\" ) display(     round(values.corr().iloc[1, 2], 3) ) <pre>How many EPL games from this season were played in 2018?\n171\nWhich team scored the highest number of goals while playing at home in the first half of the season?\nMan City\nWhich team conceded the highest number of goals while playing away in the first half of the season?\nStoke\nWhich of the following teams had the smallest difference between their win percentage and Pythagorean expectation in the first half of the season?\n</pre> Team MPh hwinvalue GFh GAh MPa awinvalue GAa GFa MP wValue GF GA pyth1 wpc1 dev 8 Leicester 10 5.0 13 14 11 5.0 18 18 21 10.0 31 32 0.484131 0.476190 0.007941 10 Man City 10 9.5 36 7 11 10.5 5 25 21 20.0 61 12 0.962743 0.952381 0.010362 17 Watford 11 4.5 14 23 10 4.5 14 16 21 9.0 30 37 0.396651 0.428571 0.031921 0 Arsenal 10 8.5 25 10 11 5.0 16 13 21 13.5 38 26 0.681132 0.642857 0.038275 12 Newcastle 11 4.0 9 13 10 3.0 17 10 21 7.0 19 30 0.286281 0.333333 0.047053 <pre>Mancity\nWhich of the following teams had the smallest difference between their win percentage and Pythagorean expectation in the first half of the season?\n</pre> Team MPh hwinvalue GFh GAh MPa awinvalue GAa GFa MP wValue GF GA pyth1 wpc1 dev 8 Leicester 10 5.0 13 14 11 5.0 18 18 21 10.0 31 32 0.484131 0.476190 0.007941 10 Man City 10 9.5 36 7 11 10.5 5 25 21 20.0 61 12 0.962743 0.952381 0.010362 17 Watford 11 4.5 14 23 10 4.5 14 16 21 9.0 30 37 0.396651 0.428571 0.031921 0 Arsenal 10 8.5 25 10 11 5.0 16 13 21 13.5 38 26 0.681132 0.642857 0.038275 12 Newcastle 11 4.0 9 13 10 3.0 17 10 21 7.0 19 30 0.286281 0.333333 0.047053 <pre>Leicester\nWhich of the following teams had the highest value for away wins (awinvalue) for in the first half of the season?\n</pre> Team MPh hwinvalue GFh GAh MPa awinvalue GAa GFa MP wValue GF GA pyth1 wpc1 dev 2 Brighton 10 5.5 10 12 11 3.0 13 5 21 8.5 15 25 0.264706 0.404762 0.140056 19 West Ham 9 4.0 10 14 11 3.0 24 12 20 7.0 22 38 0.251037 0.350000 0.098963 5 Crystal Palace 11 5.0 14 18 10 2.5 14 4 21 7.5 18 32 0.240356 0.357143 0.116787 14 Stoke 10 5.0 13 19 11 2.5 27 10 21 7.5 23 46 0.200000 0.357143 0.157143 18 West Brom 11 4.5 10 15 10 2.5 13 5 21 7.0 15 28 0.222993 0.333333 0.110340 <pre>Which team had the largest gap between home points won (hwinvalue) and away points won (awinvalue) in the second half the season?\nArsenal\nWhat was the correlation between win percentage and the Pythagorean expectation in the first half of the season?\n</pre> <pre>0.968</pre> <pre>What was the correlation between win percentage in the first half of the season and the second half of the season?\n</pre> <pre>0.757</pre> <pre>What was the correlation between win percentage in the second half of the season and the Pythagorean expectation in the first half of the season?\n</pre> <pre>0.796</pre>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/01/#english-premier-league-epl-pythagorean-predictor","title":"English Premier League (EPL) Pythagorean Predictor\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/01/#pythagorean-expectation","title":"Pythagorean Expectation\u00b6","text":"<p>Expected Win% $\\propto\\frac{x^2}{x^2 + y^2}$, where</p> <ul> <li>x = parameter scored</li> <li>y = parameter conceded</li> </ul>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.01/","title":"02.01","text":"<p>In this week, we will use basketball data downloaded from NBA.com to demonstrate how to import data into Python, how to clean up data before conducting any data analyses, as well how to describe and summarize data.</p> In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport numpy as np\n</pre> import pandas as pd import numpy as np In\u00a0[\u00a0]: Copied! <pre>NBA_Teams=pd.read_csv(\"../../Data/Week 2/nba_teams.csv\")\n</pre> NBA_Teams=pd.read_csv(\"../../Data/Week 2/nba_teams.csv\") <p>We can take a quick look at the data we imported by displaying the dataset.</p> In\u00a0[\u00a0]: Copied! <pre>display(NBA_Teams)\n</pre> display(NBA_Teams) <p>This dataset provides some basic information of the NBA teams.</p> <p>For a dataset, each row represents an observation, i.e., a team in this dataset and each column represents a variable which contains information of a characteristics of the observation. A variable can take different values in different situations. The number of observation in a dataset represents the size of our sample and the number of variables represents the richness of information in our dataset.</p> In\u00a0[\u00a0]: Copied! <pre>NBA_Teams.shape\n</pre> NBA_Teams.shape <p>We can see that there are 30 observations (rows) and 8 variables (columns).</p> In\u00a0[\u00a0]: Copied! <pre>NBA_Teams.rename(columns={'Unnamed: 0':'TEAM_NUMBER'}, inplace=True)\n\nNBA_Teams.rename(columns={'ID':'TEAM_ID'}, inplace=True)\ndisplay(NBA_Teams)\n</pre> NBA_Teams.rename(columns={'Unnamed: 0':'TEAM_NUMBER'}, inplace=True)  NBA_Teams.rename(columns={'ID':'TEAM_ID'}, inplace=True) display(NBA_Teams) In\u00a0[\u00a0]: Copied! <pre>#Your Code Here\n</pre> #Your Code Here In\u00a0[\u00a0]: Copied! <pre>NBA_Teams.drop(['TEAM_NUMBER'], axis=1, inplace=True)\ndisplay(NBA_Teams)\n</pre> NBA_Teams.drop(['TEAM_NUMBER'], axis=1, inplace=True) display(NBA_Teams) In\u00a0[\u00a0]: Copied! <pre>Games=pd.read_csv(\"../../Data/Week 2/basketball_games.csv\")\nGames.head()\n</pre> Games=pd.read_csv(\"../../Data/Week 2/basketball_games.csv\") Games.head() <p>Upon importing the game data, we notice that the first five games are not NBA games, instead, they are WNBA games. Indeed, this dataset contains NBA games, WNBA games, NBA 2K (simulation video) games.</p> In\u00a0[\u00a0]: Copied! <pre>Games.drop([0], axis=0, inplace=True)\nGames.head()\n</pre> Games.drop([0], axis=0, inplace=True) Games.head() In\u00a0[\u00a0]: Copied! <pre>Games=Games[Games.TEAM_NAME !=\"Las Vegas Aces\"]\nGames.head()\n</pre> Games=Games[Games.TEAM_NAME !=\"Las Vegas Aces\"] Games.head() In\u00a0[\u00a0]: Copied! <pre>#Your Code Here\n</pre> #Your Code Here In\u00a0[\u00a0]: Copied! <pre>NBA_Games=pd.merge(NBA_Teams, Games, on=['TEAM_ID', 'TEAM_NAME'])\nNBA_Games.head()\n</pre> NBA_Games=pd.merge(NBA_Teams, Games, on=['TEAM_ID', 'TEAM_NAME']) NBA_Games.head() In\u00a0[\u00a0]: Copied! <pre>NBA_Games.columns\n</pre> NBA_Games.columns In\u00a0[\u00a0]: Copied! <pre>NBA_Games.drop(['ABBREVIATION'], axis=1, inplace=True, errors='ignore')\n</pre> NBA_Games.drop(['ABBREVIATION'], axis=1, inplace=True, errors='ignore') In\u00a0[\u00a0]: Copied! <pre>#Your Code Here\n</pre> #Your Code Here <p>The merged dataset is sorted by the criteria we use to merge the datasets. Thus, the NBA_Games dataset is currently sorted by \"TEAM_ID.\" We may be interested to sort the data by other criteria, for example, the date of the game.</p> In\u00a0[\u00a0]: Copied! <pre>NBA_Games.sort_values(by=['GAME_ID'], ascending=[False]).head(20)\n</pre> NBA_Games.sort_values(by=['GAME_ID'], ascending=[False]).head(20) In\u00a0[\u00a0]: Copied! <pre>NBA_Games.info()\n</pre> NBA_Games.info() <p>The total number of rows is 18956, so there are missing values in variable WL, FG_PCT, FG3_PCT, and FT_PCT.</p> In\u00a0[\u00a0]: Copied! <pre>NBA_Games.notnull()\n</pre> NBA_Games.notnull() In\u00a0[\u00a0]: Copied! <pre>NBA_Games=NBA_Games[pd.notnull(NBA_Games[\"FG_PCT\"])]\nNBA_Games.shape\n</pre> NBA_Games=NBA_Games[pd.notnull(NBA_Games[\"FG_PCT\"])] NBA_Games.shape <ul> <li>Second, we can replace the missing values with valid values (Imputation), such as mean and median.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>NBA_Games=NBA_Games.fillna(NBA_Games.mean())\nNBA_Games.info()\n</pre> NBA_Games=NBA_Games.fillna(NBA_Games.mean()) NBA_Games.info() In\u00a0[\u00a0]: Copied! <pre>NBA_Games['GM']=NBA_Games['FGM']+NBA_Games['FG3M']+NBA_Games['FTM']\n</pre> NBA_Games['GM']=NBA_Games['FGM']+NBA_Games['FG3M']+NBA_Games['FTM'] In\u00a0[\u00a0]: Copied! <pre>#Your Code Here\n</pre> #Your Code Here In\u00a0[\u00a0]: Copied! <pre>NBA_Games['RESULT'] = np.where(NBA_Games['PLUS_MINUS']&gt;0, 'W', 'L')\n</pre> NBA_Games['RESULT'] = np.where(NBA_Games['PLUS_MINUS']&gt;0, 'W', 'L') <p>We will now drop this newly created \"RESULT\" variable.</p> In\u00a0[\u00a0]: Copied! <pre>NBA_Games.drop(['RESULT'], axis=1, inplace=True)\n</pre> NBA_Games.drop(['RESULT'], axis=1, inplace=True) In\u00a0[\u00a0]: Copied! <pre>NBA_Games.sort_values(['GAME_ID','WL'], inplace=True)\nNBA_Games[\"POINT_DIFF\"]=NBA_Games.groupby([\"GAME_ID\"])[\"PTS\"].diff()\n</pre> NBA_Games.sort_values(['GAME_ID','WL'], inplace=True) NBA_Games[\"POINT_DIFF\"]=NBA_Games.groupby([\"GAME_ID\"])[\"PTS\"].diff() <p>The \"POINT_DIFF\" variable only has the point difference for the winning team, we need to impute the point difference for the losing team as well.</p> In\u00a0[\u00a0]: Copied! <pre>NBA_Games['POINT_DIFF'] = NBA_Games['POINT_DIFF'].fillna(NBA_Games.groupby('GAME_ID')['POINT_DIFF'].transform('mean'))\n</pre> NBA_Games['POINT_DIFF'] = NBA_Games['POINT_DIFF'].fillna(NBA_Games.groupby('GAME_ID')['POINT_DIFF'].transform('mean')) <ul> <li>We can also drop all observations with missing value in at least one variable using the \"dropna()\" command.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>NBA_Games=NBA_Games.dropna()\nNBA_Games.shape\n</pre> NBA_Games=NBA_Games.dropna() NBA_Games.shape In\u00a0[\u00a0]: Copied! <pre>NBA_Team_Stats=NBA_Games.groupby(['TEAM_ID', 'SEASON_ID'])['PTS','FGM','FGA','FG_PCT','FG3M','FG3A','FG3_PCT','FTM','FTA','FT_PCT','OREB','DREB','REB','AST','STL','BLK','TOV','PF','PLUS_MINUS'].sum()\ndisplay(NBA_Team_Stats)\n</pre> NBA_Team_Stats=NBA_Games.groupby(['TEAM_ID', 'SEASON_ID'])['PTS','FGM','FGA','FG_PCT','FG3M','FG3A','FG3_PCT','FTM','FTA','FT_PCT','OREB','DREB','REB','AST','STL','BLK','TOV','PF','PLUS_MINUS'].sum() display(NBA_Team_Stats) <p>Notice that the newly created dataset has two levels of index, the \"TEAM_ID\" and \"SEASON_ID\"</p> In\u00a0[\u00a0]: Copied! <pre>NBA_Team_Stats=NBA_Team_Stats.reset_index()\ndisplay(NBA_Team_Stats)\n</pre> NBA_Team_Stats=NBA_Team_Stats.reset_index() display(NBA_Team_Stats) In\u00a0[\u00a0]: Copied! <pre>NBA_Game_Count=NBA_Games.groupby(['TEAM_ID','SEASON_ID']).size().reset_index(name='GAME_COUNT')\ndisplay(NBA_Game_Count)\n</pre> NBA_Game_Count=NBA_Games.groupby(['TEAM_ID','SEASON_ID']).size().reset_index(name='GAME_COUNT') display(NBA_Game_Count) In\u00a0[\u00a0]: Copied! <pre>NBA_Games.to_csv(\"../../Data/Week 2/NBA_Games.csv\", index=False)\n</pre> NBA_Games.to_csv(\"../../Data/Week 2/NBA_Games.csv\", index=False) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.01/#importing-data-into-python","title":"Importing data into Python\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.01/#before-we-import-the-dataset-into-jupyter-notebook-we-need-to-first-import-the-python-libraries-that-we-will-use-to-analyze-the-data","title":"Before we import the dataset into Jupyter notebook, we need to first import the Python libraries that we will use to analyze the data\u00b6","text":"<ul> <li>pandas</li> <li>numpy</li> </ul>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.01/#in-our-data-repository-we-have-a-dataset-that-contains-nba-team-information-lets-import-this-dataset-into-the-jupyter-notebook","title":"In our data repository, we have a dataset that contains NBA team information. Let's import this dataset into the Jupyter notebook.\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.01/#we-can-use-the-shape-function-in-python-to-see-how-many-variables-and-observations-in-our-dataset","title":"We can use the \u201cshape\u201d function in Python to see how many variables and observations in our dataset.\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.01/#renaming-variables","title":"Renaming Variables\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.01/#we-can-rename-a-variable-using-the-rename-function-in-python","title":"We can rename a variable using the \u201crename\u201d function in Python.\u00b6","text":"<p>Inplace parameter</p> <ul> <li>True: replace the old variable with a new name;</li> <li>False: create a new variable with the new name.</li> </ul> <p>The first variable is unnamed, let's rename it to be \"TEAM_NUMBER\"; let's also rename \"ID\" to \"TEAM_ID.\"</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.01/#self-test-1","title":"Self Test - 1\u00b6","text":"<ul> <li>Rename \"FULL_NAME\" to \"TEAM_NAME\"</li> </ul>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.01/#dropping-variables-columns","title":"Dropping Variables (columns)\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.01/#to-drop-a-variable-ie-to-delete-a-column-we-can-use-the-drop-command","title":"To drop a variable, i.e., to delete a column, we can use the \u201cdrop\u201d command.\u00b6","text":"<ul> <li>We need to provide the name of the variable;</li> <li>We also need to use the argument \u201caxis=1\u201d which tells Python that we are dropping a column, not a row.</li> </ul> <p>The variable \"TEAM_NUMBER\" has little meaning, let's drop it.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.01/#next-we-will-work-on-game-level-data","title":"Next we will work on game level data.\u00b6","text":"<p>Import the game level dataset from our data repository.</p> <ul> <li>We can display just first five rows of the dataset using the \"head\" command.</li> </ul>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.01/#dropping-observations-rows","title":"Dropping observations (rows)\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.01/#to-drop-an-observation-we-can-use-the-index-number-on-the-left-to-specify-the-row-we-want-to-drop","title":"To drop an observation, we can use the index number on the left to specify the row we want to drop.\u00b6","text":"<ul> <li>The argument axis=0 specifies that we want to drop a row instead of a column.</li> </ul>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.01/#more-often-we-will-drop-observations-based-on-certain-conditions","title":"More often, we will drop observations based on certain conditions.\u00b6","text":"<p>For example, Las Vegas Aces is a women\u2019s basketball team. If we are only going to focus on men\u2019s basketball games, we will drop all the games played by Las Vegas Aces. In this case, we don\u2019t have to use the \u201cdrop\u201d function. We can specify our TEAM_NAME variables to be not equal to \u201cLas Vegas Aces.\u201d</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.01/#self-test-2","title":"Self Test - 2\u00b6","text":"<ul> <li>Drop all the Phoenix Mercury games</li> </ul>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.01/#merging-dataframes","title":"Merging Dataframes\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.01/#we-will-only-focus-on-nba-games-we-could-merge-the-nba_teams-and-games-datasets-to-filter-out-nba-games","title":"We will only focus on NBA games. We could merge the NBA_Teams and Games datasets to filter out NBA games.\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.01/#teams-are-identified-by-the-team_id-so-lets-merge-the-datasets-by-team_id-since-the-variable-team_name-is-also-present-in-both-datasets-we-could-also-include-this-variable-as-a-criteria-to-merge-the-datasets-so-that-in-our-new-dataset-there-is-no-duplicate-variables","title":"Teams are identified by the TEAM_ID. So, let\u2019s merge the datasets by TEAM_ID. Since the variable \u201cTEAM_NAME\u201d is also present in both datasets, we could also include this variable as a criteria to merge the datasets so that in our new dataset, there is no duplicate variables.\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.01/#understanding-and-cleaning-the-merged-dataset","title":"Understanding and cleaning the merged dataset\u00b6","text":"<p>As you can tell, the merged dataset has a lot more variables and Python cannot fit all of them in the screen.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.01/#we-can-obtain-the-list-of-variables-using-the-columns-command-this-provides-us-a-full-list-of-variables-in-our-dataset","title":"We can obtain the list of variables using the \u201ccolumns\u201d command. This provides us a full list of variables in our dataset.\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.01/#data-cleaning","title":"Data Cleaning\u00b6","text":"<p>The variable \u201cABBREVIATION\u201d AND \u201cTEAM_ABBREVIATION\u201d carry the same information and it is not necessary to keep both of them.</p> <ul> <li>Delete \"ABBREVIATION\"</li> </ul>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.01/#self-test-3","title":"Self Test - 3\u00b6","text":"<ul> <li>Find the number of observations and the number of variables in the dataset</li> </ul>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.01/#we-can-do-so-by-using-the-sort_values-option","title":"We can do so by using the \u201csort_values\u201d option.\u00b6","text":"<p>In our dataset, \"GAME_ID\" is created based on the date of the game. We can sort the games by \u201cGAME_ID\u201d and display the 20 most recent games.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.01/#missing-values","title":"Missing Values\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.01/#before-we-move-on-to-doing-any-data-analyses-we-usually-need-to-check-if-there-is-any-missing-value-that-is-the-source-may-have-failed-to-collect-some-information","title":"Before we move on to doing any data analyses, we usually need to check if there is any missing value, that is, the source may have failed to collect some information.\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.01/#we-can-use-the-info-command-which-will-return-the-total-number-of-observations-that-have-real-values-by-looking-at-these-total-numbers-we-can-see-if-there-is-any-variable-with-missing-value","title":"We can use the info() command which will return the total number of observations that have real values. By looking at these total numbers, we can see if there is any variable with missing value.\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.01/#detecting-missing-values","title":"Detecting missing values\u00b6","text":"<p>We can use the isnull() function and the notnull() function to detect where the missing values are.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.01/#handling-missing-values","title":"Handling Missing Values\u00b6","text":"<p>There are two main approaches to handle missing values.</p> <ul> <li>First, we can simply drop the observations with missing value.</li> </ul>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.01/#drop-observations-with-missing-value-in-the-variable-fg_pct","title":"Drop observations with missing value in the variable \"FG_PCT\"\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.01/#we-can-use-the-fillna-command-to-replace-missing-values-with-the-mean-or-the-median-of-the-variable","title":"We can use the fillna() command to replace missing values with the mean or the median of the variable.\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.01/#creating-variables","title":"Creating variables\u00b6","text":"<p>We can create a variable equals to the total number of goals made.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.01/#self-test-4","title":"Self Test - 4\u00b6","text":"<ul> <li>Create a variable called \"GA\" equals to the total number of goals attempted.</li> </ul>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.01/#create-variables-based-on-conditions","title":"Create variables based on conditions\u00b6","text":"<ul> <li>We can create a variable conditional on the value of another variable.</li> </ul> <p>For example, we can create a variable \"RESULT\" that equals to 1 if the team won the game and 0 otherwise. The result of the game can be captured in the points of the team receive, whether it was positive or negative.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.01/#create-a-variable-within-group","title":"Create a variable within group\u00b6","text":"<p>In the dataset, each game has two observations, one represents the statistics of the home team, one represents those of the away team. Both observations have the same GAME_ID. We can create a variable \"POINT_DIFF\" that equals the difference between the points earned by the two teams.</p> <p>We will first sort the data not only by the \"GAME_ID\" but also by the result \"WL\".</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.01/#creating-new-dataframe","title":"Creating new dataframe\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.01/#create-a-new-dataframe-that-aggregates-information-by-group","title":"Create a new dataframe that aggregates information by group\u00b6","text":"<p>Sometimes we may want to work with season level data rather than team level data. We can create a new dataset that includes aggregate information of team statistics in each season.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.01/#if-we-want-to-convert-these-two-indexes-back-as-variables-we-can-use-the-reset_index-command","title":"If we want to convert these two indexes back as variables, we can use the \"reset_index\" command.\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.01/#we-can-create-a-variable-that-equals-to-the-total-number-of-observations-within-a-specified-group-using-the-size-command","title":"We can create a variable that equals to the total number of observations within a specified group using the size() command.\u00b6","text":"<ul> <li>Create a variable that equals to the total number of games played by a team in each season, name this variable \"GAME_COUNT\".</li> </ul>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.01/#saving-data","title":"Saving data\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.01/#we-can-save-a-dataframe-by-exporting-the-edited-dataframe-to-csv-file-using-the-to_csv-command","title":"We can save a dataframe by exporting the edited dataframe to csv file using the \u201cto_csv\u201d command.\u00b6","text":"<ul> <li>Save merged data as a csv file We can use the \"index=False\" command to save the data without adding the index as a column in the csv file</li> </ul>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.02/","title":"02.02","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nNBA_Games=pd.read_csv(\"../../Data/Week 2/NBA_Games.csv\")\nNBA_Games.head()\n</pre> import pandas as pd NBA_Games=pd.read_csv(\"../../Data/Week 2/NBA_Games.csv\") NBA_Games.head() In\u00a0[\u00a0]: Copied! <pre>NBA_Games.dtypes\n</pre> NBA_Games.dtypes <p>In data analysis, we often convert categorical variable into dummy variable, if the observation belongs to the specified category, the dummy variable indicating the category would equal to 1, otherwise it equals to 0.</p> In\u00a0[\u00a0]: Copied! <pre>dummy=pd.get_dummies(NBA_Games, columns=['WL'])\n</pre> dummy=pd.get_dummies(NBA_Games, columns=['WL']) In\u00a0[\u00a0]: Copied! <pre>dummy.columns\n</pre> dummy.columns <p>Notice that two variables are created, WL_L and WL_W. WL_L=1 if the team lost and WL_L=0 if the team won. The original variable WL is deleted.</p> In\u00a0[\u00a0]: Copied! <pre>NBA_Games=pd.concat([NBA_Games, dummy['WL_W']], axis=1)\nNBA_Games.head()\n</pre> NBA_Games=pd.concat([NBA_Games, dummy['WL_W']], axis=1) NBA_Games.head()  In\u00a0[\u00a0]: Copied! <pre>NBA_Games.rename(columns={'WL_W':'WIN'}, inplace=True)\nNBA_Games.head()\n</pre> NBA_Games.rename(columns={'WL_W':'WIN'}, inplace=True) NBA_Games.head() In\u00a0[\u00a0]: Copied! <pre>NBA_Games['GAME_DATE'].dtype\n</pre> NBA_Games['GAME_DATE'].dtype <p>The date variable is originally stored as an object. In this case, each date is treated equally without ordering.</p> In\u00a0[\u00a0]: Copied! <pre>import datetime\nNBA_Games['GAME_DATE']=pd.to_datetime(NBA_Games['GAME_DATE'])\nNBA_Games['GAME_DATE'].head()\n</pre> import datetime NBA_Games['GAME_DATE']=pd.to_datetime(NBA_Games['GAME_DATE']) NBA_Games['GAME_DATE'].head() In\u00a0[\u00a0]: Copied! <pre>NBA_Games.describe()\n</pre> NBA_Games.describe() In\u00a0[\u00a0]: Copied! <pre>NBA_Games.describe(include='all')\n</pre> NBA_Games.describe(include='all') In\u00a0[\u00a0]: Copied! <pre>NBA_Games['PTS'].describe()\n</pre> NBA_Games['PTS'].describe() In\u00a0[\u00a0]: Copied! <pre>NBA_Games['FGM'].mean()\n</pre> NBA_Games['FGM'].mean() <ul> <li>Calculate median of a numerical variable</li> </ul> In\u00a0[\u00a0]: Copied! <pre>NBA_Games['FGM'].median()\n</pre> NBA_Games['FGM'].median() <ul> <li>Calculate standard deviation of a numerical variable</li> </ul> In\u00a0[\u00a0]: Copied! <pre>NBA_Games['FGM'].std()\n</pre> NBA_Games['FGM'].std() In\u00a0[\u00a0]: Copied! <pre>#Your Code Here\n</pre> #Your Code Here In\u00a0[\u00a0]: Copied! <pre>NBA_Games.groupby(['WL']).mean()\n</pre> NBA_Games.groupby(['WL']).mean() <ul> <li>Calculate the mean of a single (points in this example) variable by group.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>NBA_Games.groupby(['WL'])['PTS'].mean()\n</pre> NBA_Games.groupby(['WL'])['PTS'].mean() In\u00a0[\u00a0]: Copied! <pre>NBA_Games['GAME_DATE'].describe()\n</pre> NBA_Games['GAME_DATE'].describe() In\u00a0[\u00a0]: Copied! <pre>NBA_Games.hist(column='PTS')\n</pre> NBA_Games.hist(column='PTS') In\u00a0[\u00a0]: Copied! <pre>NBA_Games.hist(column='PTS', bins=20)\n</pre> NBA_Games.hist(column='PTS', bins=20) In\u00a0[\u00a0]: Copied! <pre>NBA_Games.hist(column='PTS', bins=20, rwidth=0.9)\n</pre> NBA_Games.hist(column='PTS', bins=20, rwidth=0.9) In\u00a0[\u00a0]: Copied! <pre>NBA_Games.to_csv(\"../../Data/Week 2/NBA_Games2.csv\", index=False)\n</pre> NBA_Games.to_csv(\"../../Data/Week 2/NBA_Games2.csv\", index=False) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.02/#import-merged-nba-game-data","title":"Import Merged NBA Game Data\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.02/#explore-the-dataset","title":"Explore the dataset\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.02/#qualitative-categorical-vs-quantitative-numerical-data","title":"Qualitative (Categorical) vs. Quantitative (Numerical) Data\u00b6","text":"<p>-- To assess the variable type in Python, we use the \u201cdtypes\u201d command.</p> <ul> <li>object: qualitative variable -- variables that are not in numerical form</li> <li>int64: quantitative &amp; discrete -- integer</li> <li>float64: quantitative &amp; continuous -- real numbers that may contain decimal points</li> </ul>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.02/#convert-a-categorical-variable-to-a-dummy-variable","title":"Convert a categorical variable to a dummy variable\u00b6","text":"<p>The variable \"WL\" only carries two values, win or lose. We will create dummy variables to capture the categories.</p> <p>We can use the \u201cpd.get_dummies\u201d function to convert a categorical variable to dummy variable. This function will also omit any missing value.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.02/#we-can-attach-the-wl_w-dummy-variable-back-to-our-nba_games-dataset-using-the-pdconcat-function","title":"We can attach the \"WL_W\" dummy variable back to our NBA_Games dataset using the pd.concat function.\u00b6","text":"<ul> <li>axis=1 specifies that we are adding a column to the dataset;</li> <li>axis=0 indicates adding rows</li> </ul>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.02/#rename-wl_w-to-win","title":"Rename \"WL_W\" to \"WIN\"\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.02/#working-with-date-variable","title":"Working with date variable\u00b6","text":"<p>In sports, we often have to work with date and time data.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.02/#we-can-use-the-pd_to_datetime-command-to-convert-the-object-variable-to-a-date-variable","title":"We can use the \u201cpd._to_datetime()\u201d command to convert the object variable to a date variable.\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.02/#descriptive-and-summary-analyses","title":"Descriptive and Summary Analyses\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.02/#summarize-numerical-data","title":"Summarize numerical data\u00b6","text":"<p>We can use the \u201cdescribe()\u201d command to calculate summary statistics. This will return basic summary statistics for all the numerical variables which include the total number of observations (count), the average, standard deviation, min and max, median, and the first and third quartiles of the values of the variable.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.02/#we-can-also-add-non-numerical-variable-into-this-summary-statistics-table-by-adding-the-argument-includeall-you-will-see-that-for-non-numerical-variables-it-provides-the-number-of-distinct-values-in-the-variable-and-the-most-frequently-appeared-value-of-the-variable-as-well-as-its-frequency-for-date-variable-in-addition-to-providing-the-most-frequent-date-appeared-in-the-dataset-it-also-summarizes-the-start-and-end-dates-of-the-dataset","title":"We can also add non-numerical variable into this summary statistics table by adding the argument \u201cinclude=\u2019all\u2019\u201d. You will see that for non-numerical variables, it provides the number of distinct values in the variable and the most frequently appeared value of the variable as well as its frequency. For date variable, in addition to providing the most frequent date appeared in the dataset, it also summarizes the start and end dates of the dataset.\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.02/#we-can-summarize-a-single-variable-by-specifying-the-variable","title":"We can summarize a single variable by specifying the variable.\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.02/#we-can-also-calculate-individual-statistics-by-using-the-mean-median-std","title":"We can also calculate individual statistics by using the mean(), median(), std().\u00b6","text":"<ul> <li>Calculate mean of a numerical variable</li> </ul>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.02/#self-test","title":"Self Test\u00b6","text":"<ol> <li><p>Find the mean of field goals attempted;</p> </li> <li><p>Find the median of 3-point field goals made;</p> </li> <li><p>Find the standard deviation of the number of rebounds</p> </li> </ol>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.02/#we-can-also-calculate-the-summary-statistics-of-a-variable-based-on-another-variable-usually-based-on-a-different-categorical-variable","title":"We can also calculate the summary statistics of a variable based on another variable, usually based on a different categorical variable.\u00b6","text":"<ul> <li>Calculate means by groups using \"groupby\" command.</li> </ul>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.02/#summarize-date-variable","title":"Summarize date variable\u00b6","text":"<ul> <li>We can find some basic statistics of the date variable. The describe() function returns the number of unique value of the date variable, the most frequently appeared date, i.e., the date with most number of games, and the first and the last dates.</li> </ul>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.02/#visualizing-data","title":"Visualizing data\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.02/#histogram","title":"Histogram\u00b6","text":"<p>-- We can visualize the distribution of a variable using a histogram.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.02/#we-can-specify-the-number-of-bins-in-a-histogram-different-numbers-of-bins-may-give-us-slightly-different-graphs","title":"We can specify the number of bins in a histogram; different numbers of bins may give us slightly different graphs.\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.02/#for-visual-appeal-sometimes-it-may-be-helpful-to-add-space-between-bins","title":"For visual appeal, sometimes it may be helpful to add space between bins.\u00b6","text":"<p>For example, we can narrow the bin to 0.9 width.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.02/#save-edited-dataset","title":"Save edited dataset\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.03/","title":"02.03","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nNBA_Games=pd.read_csv(\"../../Data/Week 2/NBA_Games2.csv\")\nNBA_Games.head()\n</pre> import pandas as pd NBA_Games=pd.read_csv(\"../../Data/Week 2/NBA_Games2.csv\") NBA_Games.head() In\u00a0[\u00a0]: Copied! <pre>NBA_Games['FG_PCT'].describe()\n</pre> NBA_Games['FG_PCT'].describe() <ul> <li>Three-point field goals</li> </ul> In\u00a0[\u00a0]: Copied! <pre>NBA_Games['FG3_PCT'].describe()\n</pre> NBA_Games['FG3_PCT'].describe() <p>We can see that the average success rate of 2-point field goals is about 45.27% while the average success rate of 3-point field goals is 35.07%. That means that the overall success rate of 2-point field goals is about 10% higher than the overall success rate of 3-point field goals. The median of 2-point field goal success rate is 45.20%, while the median 3-point field goal success rate is 35.00%. This means half of the teams have 2-point field-goal success rates less than 45% and half of the teams have 3-point field goal success rate of less than 35%.</p> <p>The standard deviation for 2-point field goal success rate is 0.056, while the standard deviation for 3-point field goal success rate is 0.09956. This means that there is a greater variation in 3-point field goals than 2-point field goals.</p> In\u00a0[\u00a0]: Copied! <pre>NBA_Games.hist(column=['FG_PCT','FG3_PCT'], bins=20, sharex=True, sharey=True)\n</pre> NBA_Games.hist(column=['FG_PCT','FG3_PCT'], bins=20, sharex=True, sharey=True) In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\nNBA_Games[['FG_PCT','FG3_PCT']].plot.hist(alpha=0.3, bins=20)\nplt.xlabel('Field Goal Percentage')\nplt.ylabel('Frequency')\nplt.title(\"Distributions of Field Goal Percentages\", fontsize=15)\nplt.savefig('FG_PCT_Distributions.png')\n</pre> import matplotlib.pyplot as plt  NBA_Games[['FG_PCT','FG3_PCT']].plot.hist(alpha=0.3, bins=20) plt.xlabel('Field Goal Percentage') plt.ylabel('Frequency') plt.title(\"Distributions of Field Goal Percentages\", fontsize=15) plt.savefig('FG_PCT_Distributions.png') In\u00a0[\u00a0]: Copied! <pre>NBA_Games.hist(by='WL', column='FG_PCT', color='red', bins=15, sharex=True, sharey=True)\nplt.savefig('FG_PCT_WL.png')\n</pre> NBA_Games.hist(by='WL', column='FG_PCT', color='red', bins=15, sharex=True, sharey=True) plt.savefig('FG_PCT_WL.png') In\u00a0[\u00a0]: Copied! <pre>#Your Code Here\n</pre> #Your Code Here In\u00a0[\u00a0]: Copied! <pre>import datetime\nNBA_Games['GAME_DATE']=pd.to_datetime(NBA_Games['GAME_DATE'])\nNBA_Games['GAME_DATE'].head()\n</pre> import datetime NBA_Games['GAME_DATE']=pd.to_datetime(NBA_Games['GAME_DATE']) NBA_Games['GAME_DATE'].head() In\u00a0[\u00a0]: Copied! <pre>Pistons_Games=NBA_Games[(NBA_Games.NICKNAME == 'Pistons')&amp;(NBA_Games.SEASON_ID==22017)&amp; (NBA_Games.GAME_DATE&gt;='2017-10-17')]\ndisplay(Pistons_Games)\n</pre> Pistons_Games=NBA_Games[(NBA_Games.NICKNAME == 'Pistons')&amp;(NBA_Games.SEASON_ID==22017)&amp; (NBA_Games.GAME_DATE&gt;='2017-10-17')] display(Pistons_Games) In\u00a0[\u00a0]: Copied! <pre>Pistons_Games.plot(x='GAME_DATE', y='PTS')\nplt.savefig('PISTONS_PTS_TIME.png')\n</pre> Pistons_Games.plot(x='GAME_DATE', y='PTS') plt.savefig('PISTONS_PTS_TIME.png') In\u00a0[\u00a0]: Copied! <pre>#Your Code Here\n</pre> #Your Code Here In\u00a0[\u00a0]: Copied! <pre>NBA_Games.plot.scatter(x='AST', y='FGM')\n</pre> NBA_Games.plot.scatter(x='AST', y='FGM') In\u00a0[\u00a0]: Copied! <pre>import seaborn as sns\nsns.regplot(x='AST', y='FGM', data=NBA_Games,  marker='.')\nplt.xlabel('Assists')\nplt.ylabel('Field Goals Made')\nplt.title(\"Relationship between the Numbers of Assists and Field Goals Made\", fontsize=15)\n</pre> import seaborn as sns sns.regplot(x='AST', y='FGM', data=NBA_Games,  marker='.') plt.xlabel('Assists') plt.ylabel('Field Goals Made') plt.title(\"Relationship between the Numbers of Assists and Field Goals Made\", fontsize=15) <p>As we can see from the graph, as the number of assists increase, the number of field goals made also increases. In this case, we say there is a positive relationship between the two variables, or a positive correlation.</p> In\u00a0[\u00a0]: Copied! <pre>NBA_Games['AST'].corr(NBA_Games['FGM'])\n</pre> NBA_Games['AST'].corr(NBA_Games['FGM']) <p>The correlation coefficient between the number of assist and field goal made is 0.70 so there is a positive correlation between the two.</p> In\u00a0[\u00a0]: Copied! <pre>sns.regplot(x='AST', y='FGA', data=NBA_Games,  marker='.')\nplt.xlabel('Assists')\nplt.ylabel('Field Goals Attempted')\nplt.title(\"Relationship between the Numbers of Assists and Field Goals Attempted\", fontsize=15)\n</pre> sns.regplot(x='AST', y='FGA', data=NBA_Games,  marker='.') plt.xlabel('Assists') plt.ylabel('Field Goals Attempted') plt.title(\"Relationship between the Numbers of Assists and Field Goals Attempted\", fontsize=15) In\u00a0[\u00a0]: Copied! <pre>NBA_Games['AST'].corr(NBA_Games['FGA'])\n</pre> NBA_Games['AST'].corr(NBA_Games['FGA']) <p>Both the graph and the correlation coefficient suggest that there is only a slight positive relationship between the two.</p> In\u00a0[\u00a0]: Copied! <pre>sns.lmplot(x='AST', y='FGA', hue='WL', data=NBA_Games)\nplt.xlabel('Assists')\nplt.ylabel('Field Goals Made')\nplt.title(\"Relationship between the Numbers of Assists and Field Goals Made\", fontsize=15)\n</pre> sns.lmplot(x='AST', y='FGA', hue='WL', data=NBA_Games) plt.xlabel('Assists') plt.ylabel('Field Goals Made') plt.title(\"Relationship between the Numbers of Assists and Field Goals Made\", fontsize=15) In\u00a0[\u00a0]: Copied! <pre>NBA_Games.corr(method='pearson')\n</pre> NBA_Games.corr(method='pearson') In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.03/#import-updated-nba-game-data","title":"Import Updated NBA Game Data\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.03/#more-on-summary-statistics","title":"More on Summary Statistics\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.03/#central-tendency-vs-variation","title":"Central Tendency vs. Variation\u00b6","text":"<p>We will compare the success rates of two-point field goals and three-point field goals to demonstrate the difference between central tendency and variation.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.03/#calculate-summary-statistics-for-the-percentages-of-two-point-field-goals-and-three-point-field-goals","title":"Calculate summary statistics for the percentages of two-point field goals and three-point field goals\u00b6","text":"<ul> <li>Two-point field goals</li> </ul>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.03/#compare-the-distribution-of-two-point-field-goal-percentage-and-three-point-field-goal-percentage-using-a-histogram","title":"Compare the distribution of two-point field goal percentage and three-point field goal percentage using a Histogram\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.03/#plot-two-histograms-side-by-side","title":"Plot two histograms side by side\u00b6","text":"<p>The options \"sharex\" and \"sharey\" ask if we want to restrict the same range of x and same range of y for the two histograms</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.03/#plot-two-histograms-in-the-same-graph-in-different-colors","title":"Plot two histograms in the same graph in different colors\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.03/#we-will-first-introduce-a-new-library-matplotlib-that-provides-more-useful-functions-to-make-plots","title":"We will first introduce a new library \"matplotlib\" that provides more useful functions to make plots.\u00b6","text":"<ul> <li>We will use \"plot.hist\" instead of \"hist\" to make this plot</li> <li>The option \"alpha\" specifies transparency, so that the two histograms would not block each other entirely (alpha=0: fully transparent; alpha=1: fully opaque)</li> <li>We can also add a title and axis labels using \"plt.title,\" \"plt.xlabel\" and \"plt.ylabel\" commands</li> <li>We can also export the graph as a png file using the \"plt.savefig\" command</li> </ul>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.03/#histogram-by-the-result-of-the-game-using-the-by-option","title":"Histogram by the result of the game using the \"by\" option\u00b6","text":"<p>We can also change the colors of the graphs using the \"color\" option</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.03/#self-test-1","title":"Self Test - 1\u00b6","text":"<ol> <li>Calculate summary statistics for the three-point field goal percentage by the result of the game</li> <li>Graph a histogram of the three-point field goal percentage by the result of the game and provide interpretation</li> </ol> <ul> <li>Number of bins=10, the two subgraphs should have the same x and y ranges, color is green</li> <li>Export the graph as \"FG3_PCT_Distribution\" in png format</li> </ul>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.03/#create-time-series-graphs","title":"Create time series graphs\u00b6","text":"<p>Let's first change the data type of \"GAME_DATE\" from object to datetime.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.03/#subsetting-a-dataset","title":"Subsetting a dataset\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.03/#the-dataset-we-are-working-with-contains-games-of-different-nba-teams-lets-focus-on-one-team-to-produce-a-time-series-graph","title":"The dataset we are working with contains games of different NBA teams. Let's focus on one team to produce a time series graph.\u00b6","text":"<p>Extract Pistons' game data in the 2017-2018 season.</p> <p>Note that for date variable, we can use the &gt;, =, &lt; operators. When we specify the condition of the date, we need to use \"\"</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.03/#now-we-can-plot-the-points-earned-by-the-pistons-by-time","title":"Now we can plot the points earned by the Pistons by time.\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.03/#self-test-2","title":"Self Test - 2\u00b6","text":"<ol> <li>Graph Toronto Raptors' points in each game throughout the 2018-2019 seaon. (SEASON ID is 22018, and the regular season started on October 16, 2018.)</li> <li>Export the graph as \"RAPTORS_PTS_TIME\" in png format</li> </ol>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.03/#correlation-analysis","title":"Correlation Analysis\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.03/#we-can-first-detect-the-relationship-between-two-variables-in-a-scatterplot","title":"We can first detect the relationship between two variables in a scatterplot.\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.03/#lets-use-the-number-of-assists-and-the-number-of-field-goals-made-as-an-example","title":"Let's use the number of assists and the number of field goals made as an example.\u00b6","text":"<p>We can create a scatter plot using the \"plot.scatter\" function with the number of assists in the horizontal axis and the number of field goals made in the vertical axis.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.03/#we-can-use-the-functions-in-the-seaborn-library-to-graph-the-relationships-between-two-variables","title":"We can use the functions in the \"seaborn\" library to graph the relationships between two variables\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.03/#we-will-use-the-function-regplot-to-graph-the-two-variables-this-function-graphs-a-scatterplot-as-well-as-a-regression-line","title":"We will use the function \"regplot\" to graph the two variables. This function graphs a scatterplot as well as a regression line.\u00b6","text":"<p>We will learn about regression analysis more systematically in week 4</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.03/#correlation-coefficient","title":"Correlation Coefficient\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.03/#we-can-quantify-the-linear-correlation-by-a-correlation-coefficient-a-correlation-coefficient-measures-the-joint-variability-of-two-random-variables-we-can-calculate-correlation-coefficient-using-the-corr-function","title":"We can quantify the linear correlation by a correlation coefficient. A correlation coefficient measures the joint variability of two random variables.  We can calculate correlation coefficient using the \"corr\" function.\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.03/#lets-investigate-the-relationship-between-the-number-of-assists-and-the-number-of-field-goals-attempted","title":"Let's investigate the relationship between the number of assists and the number of field goals attempted.\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.03/#we-can-further-graph-the-scatter-plot-by-group-using-the-hue-option","title":"We can further graph the scatter plot by group using the \"hue\" option.\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.03/#lets-separate-by-the-results-of-the-game-win-or-lose-and-produce-scatter-plots-between-number-of-assists-and-field-goals-made","title":"Let's separate by the results of the game (win or lose), and produce scatter plots between number of assists and field goals made.\u00b6","text":"<p>In this case, we can use lmplot() instead of regplot().</p> <ul> <li>lmplot() combines regplot() and FacetGrid.</li> <li>FacetGrid produces multi-plot grid for plotting conditional relationships. Thus, FacetGrid allows us to separate the dataset into multiple panels based on specified conditions to visualize the relationship between multiple variables.</li> </ul>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/02.03/#we-can-also-find-correlation-coefficients-for-all-the-numerical-variables","title":"We can also find correlation coefficients for all the numerical variables.\u00b6","text":"<p>We will specify the method to be pearson.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/03.01/","title":"Visualizing sports data: basketball","text":"In\u00a0[\u00a0]: Copied! <pre># As usual, we begin by importing the packages we will need\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n</pre> # As usual, we begin by importing the packages we will need  import pandas as pd import numpy as np import matplotlib.pyplot as plt In\u00a0[\u00a0]: Copied! <pre># The data consists of the shot log for the NBA season 2016/17\n\nshot = pd.read_csv(\"../../Data/Week 3/NBA Shotlog_16_17.csv\")\npd.set_option('display.max_columns', 100)\nprint(shot.columns.tolist())\nshot.describe()\n</pre> # The data consists of the shot log for the NBA season 2016/17  shot = pd.read_csv(\"../../Data/Week 3/NBA Shotlog_16_17.csv\") pd.set_option('display.max_columns', 100) print(shot.columns.tolist()) shot.describe() <p>From the print() command we have a good deal of information on each shot, including the name of the player, the type of shot, team names, time in the game and so on. From the .describe() we can see that there are over 210,000 shots in the data although there is a small number of shots with missing coordinates (around 400). Also note that the x-axis location variable (this is the sideline) ranges from 0 to 993, while the y-axis variable (this is the baseline) runs from 0 to 500.</p> <p>We can generate a simple plot as follows:</p> In\u00a0[\u00a0]: Copied! <pre># A simple plot of coordinates\n\nx = shot['location_x']\ny = shot['location_y']\nplt.scatter(x,y, s=.005,c='r', marker= '.')\n</pre> # A simple plot of coordinates  x = shot['location_x'] y = shot['location_y'] plt.scatter(x,y, s=.005,c='r', marker= '.') <p>One point to note about the data is that it does not include free throws - otherwise there would be a deep red concentration at the center of the free throw line.</p> <p>The plot above gives us a very clear picture of the location of shots, notably from under the basket and from the three point line. However, the plot does not take account of the proportions of the court. Moreover, it would be useful to add a grid to identify more clearly the different locations.</p> <p>The dimensions of the court defined by the NBA are 94 feet by 50 feet. We can specify that ratio using figsize = (a,b). Note that we can also scale this to control the size of the plot in our window (we divide both the x and y figsize by 6).</p> In\u00a0[\u00a0]: Copied! <pre># A simple plot of coordinates, scaled for court dimensions and with grid added.\n\nplt.figure(figsize=(94/6,50/6))\nplt.scatter(x,y, s=.1,c='r', marker= '.')\nplt.minorticks_on()\nplt.grid(which='major', linestyle='-', linewidth='.5', color='black')\nplt.grid(which='minor', linestyle=':', linewidth='0.5', color='red')\n</pre> # A simple plot of coordinates, scaled for court dimensions and with grid added.  plt.figure(figsize=(94/6,50/6)) plt.scatter(x,y, s=.1,c='r', marker= '.') plt.minorticks_on() plt.grid(which='major', linestyle='-', linewidth='.5', color='black') plt.grid(which='minor', linestyle=':', linewidth='0.5', color='red') <p>Rather than look at both ends of the court, we can just look at one half of the court by fixing the range of the x axis using plt.xlim, which we set to cover the right hand half court. Note that to maintain the same dimensions we also need to halve the x axis on figsize (to 94/12 rather than 94/6).</p> In\u00a0[\u00a0]: Copied! <pre># The right hand half court\n\nplt.figure(figsize=(94/12,50/6))\nplt.scatter(x,y, s=.1,c='r', marker= '.')\nplt.minorticks_on()\nplt.grid(which='major', linewidth='.5', color='black')\nplt.grid(which='minor', linewidth='.5', color='red')\nplt.xlim(933/2, 933)\n</pre> # The right hand half court  plt.figure(figsize=(94/12,50/6)) plt.scatter(x,y, s=.1,c='r', marker= '.') plt.minorticks_on() plt.grid(which='major', linewidth='.5', color='black') plt.grid(which='minor', linewidth='.5', color='red') plt.xlim(933/2, 933) <p>The last plot just shows us one half of the court. To include both halves but show only the half court, we can convert the coordinates from the left hand half court so that they have the same relative location on the right hand half court (of course, there should be no real difference in performance based on which end of the court the player is attacking, and the two halves are mirror images of each other).</p> <p>We can convert the plot to show shots from both ends on a half court if we adjust the coordinates. The x-axis runs from 0 to 933, so if we want to just show the right hand half court with all shots, we can recode the location_x variables, where x is less the 933/2 (the left had half court) as equal to \"933 - location_x\". This produces a mirror image of the x coordinate in the right hand half of the court. However, since the direction toward the basket is the reverse of the other half court, we need to take the mirror image of the y coordinate for shots in the left had half court - that is \"500 - location_y\".</p> <p>The following code (which works like an \"if\" statement in Excel) creates these mirror images:</p> In\u00a0[\u00a0]: Copied! <pre>shot['halfcourt_x'] =np.where(shot['location_x'] &lt; 933/2, 933 - shot['location_x'],shot['location_x'])\nshot['halfcourt_y'] =np.where(shot['location_x'] &lt; 933/2, 500 - shot['location_y'],shot['location_y'])\nshot.describe()\n</pre> shot['halfcourt_x'] =np.where(shot['location_x'] &lt; 933/2, 933 - shot['location_x'],shot['location_x']) shot['halfcourt_y'] =np.where(shot['location_x'] &lt; 933/2, 500 - shot['location_y'],shot['location_y']) shot.describe() <p>Note that the sideline coordinates now only run between 468 and 933, but that the baseline coordinates still run between 0 and 500, even though all the y locations have been changed. We can now plot the half court data, noting that we now halve the x axis in figsize, to preserve the same proportions as before.</p> In\u00a0[\u00a0]: Copied! <pre># all shots shown on a half court\n\nhx = shot['halfcourt_x']\nhy = shot['halfcourt_y']\nplt.figure(figsize=(94/12,50/6))\nplt.scatter(hx,hy, s=.01,c='r', marker= '.')\nplt.minorticks_on()\nplt.grid(which='major', linestyle='-', linewidth='.5', color='black')\nplt.grid(which='minor', linestyle=':', linewidth='0.5', color='red')\nplt.title(\"Shots\", fontsize = 15)\n</pre> # all shots shown on a half court  hx = shot['halfcourt_x'] hy = shot['halfcourt_y'] plt.figure(figsize=(94/12,50/6)) plt.scatter(hx,hy, s=.01,c='r', marker= '.') plt.minorticks_on() plt.grid(which='major', linestyle='-', linewidth='.5', color='black') plt.grid(which='minor', linestyle=':', linewidth='0.5', color='red') plt.title(\"Shots\", fontsize = 15)  <p>We now breakdown shots into three categories: scored, missed and blocked. To do this we simply create subsets of the shot df based on shot outcome.</p> In\u00a0[\u00a0]: Copied! <pre># Scoring shots\n\nScored = shot[shot.current_shot_outcome == 'SCORED']\nhxs = Scored['halfcourt_x']\nhys = Scored['halfcourt_y']\nplt.figure(figsize=(94/12,50/6))\nplt.scatter(hxs,hys, s=.01,c='g', marker= '.')\nplt.minorticks_on()\nplt.grid(which='major', linestyle='-', linewidth='.5', color='black')\nplt.grid(which='minor', linestyle=':', linewidth='0.5', color='red')\nplt.title(\"Scored\", fontsize = 15)\n</pre> # Scoring shots  Scored = shot[shot.current_shot_outcome == 'SCORED'] hxs = Scored['halfcourt_x'] hys = Scored['halfcourt_y'] plt.figure(figsize=(94/12,50/6)) plt.scatter(hxs,hys, s=.01,c='g', marker= '.') plt.minorticks_on() plt.grid(which='major', linestyle='-', linewidth='.5', color='black') plt.grid(which='minor', linestyle=':', linewidth='0.5', color='red') plt.title(\"Scored\", fontsize = 15)  In\u00a0[\u00a0]: Copied! <pre># Missed Shots\n\nMissed = shot[shot.current_shot_outcome == 'MISSED']\n\nhxm = Missed['halfcourt_x']\nhym = Missed['halfcourt_y']\n\nplt.figure(figsize=(94/12,50/6))\nplt.scatter(hxm,hym, s=.01,c='b', marker= '.')\nplt.minorticks_on()\nplt.grid(which='major', linestyle='-', linewidth='.5', color='black')\nplt.grid(which='minor', linestyle=':', linewidth='0.5', color='red')\nplt.title(\"Missed\", fontsize = 15)\n</pre> # Missed Shots  Missed = shot[shot.current_shot_outcome == 'MISSED']  hxm = Missed['halfcourt_x'] hym = Missed['halfcourt_y']  plt.figure(figsize=(94/12,50/6)) plt.scatter(hxm,hym, s=.01,c='b', marker= '.') plt.minorticks_on() plt.grid(which='major', linestyle='-', linewidth='.5', color='black') plt.grid(which='minor', linestyle=':', linewidth='0.5', color='red') plt.title(\"Missed\", fontsize = 15) In\u00a0[\u00a0]: Copied! <pre># Blocked shots\n\nBlocked = shot[shot.current_shot_outcome == 'BLOCKED']\n\nhxb = Blocked['halfcourt_x']\nhyb = Blocked['halfcourt_y']\n\nplt.figure(figsize=(94/12,50/6))\nplt.scatter(hxb,hyb, s=1,c='m', marker= '.')\nplt.minorticks_on()\nplt.grid(which='major', linestyle='-', linewidth='.5', color='black')\nplt.grid(which='minor', linestyle=':', linewidth='0.5', color='red')\nplt.title(\"Blocked\", fontsize = 15)\n</pre> # Blocked shots  Blocked = shot[shot.current_shot_outcome == 'BLOCKED']  hxb = Blocked['halfcourt_x'] hyb = Blocked['halfcourt_y']  plt.figure(figsize=(94/12,50/6)) plt.scatter(hxb,hyb, s=1,c='m', marker= '.') plt.minorticks_on() plt.grid(which='major', linestyle='-', linewidth='.5', color='black') plt.grid(which='minor', linestyle=':', linewidth='0.5', color='red') plt.title(\"Blocked\", fontsize = 15)  <p>While informative, the location of shots by shot type is not that surprising. There has been a lot of interest in recent years in the rise of the 3-point shot with evidence suggesting that in the past there were more more shots from distance inside the three-point line, and that these have tended to die out since the payoff is much lower than a shot from just outside the three-point. We don't have past data here to make the comparison.</p> <p>Probably the greatest interest lies in comparing individual players. We can do this easily by taking subsets as we did above. First it's useful to generate a list of player names.</p> In\u00a0[\u00a0]: Copied! <pre># Comparing players\n\n# We use a pivot table here to list players by shots\n\nplayersn = shot.groupby('shoot_player')['current_shot_outcome'].describe().reset_index()\nplayersn.sort_values(by = 'count', ascending = False)\n</pre> # Comparing players  # We use a pivot table here to list players by shots  playersn = shot.groupby('shoot_player')['current_shot_outcome'].describe().reset_index() playersn.sort_values(by = 'count', ascending = False) <p>We now compare LeBron James with Steph Curry.</p> In\u00a0[\u00a0]: Copied! <pre># LeBron subset\n\nLeBron = shot[shot['shoot_player']=='LeBron James']\nLeBron\n</pre> # LeBron subset  LeBron = shot[shot['shoot_player']=='LeBron James'] LeBron In\u00a0[\u00a0]: Copied! <pre># LeBron plot\n# Note how the shots can be color coded using an np.where statement. \n\nhxL = LeBron['halfcourt_x']\nhyL = LeBron['halfcourt_y']\ncolors = np.where(LeBron['current_shot_outcome']=='SCORED','r',np.where(LeBron['current_shot_outcome']=='MISSED','b','g'))\nplt.figure(figsize=(94/12,50/6))\nplt.scatter(hxL,hyL, s=10, c= colors, marker= '.')\nplt.grid(True)\nplt.title(\"LeBron James\", fontsize = 15)\n</pre> # LeBron plot # Note how the shots can be color coded using an np.where statement.   hxL = LeBron['halfcourt_x'] hyL = LeBron['halfcourt_y'] colors = np.where(LeBron['current_shot_outcome']=='SCORED','r',np.where(LeBron['current_shot_outcome']=='MISSED','b','g')) plt.figure(figsize=(94/12,50/6)) plt.scatter(hxL,hyL, s=10, c= colors, marker= '.') plt.grid(True) plt.title(\"LeBron James\", fontsize = 15) In\u00a0[\u00a0]: Copied! <pre># Steph Curry plot\n\nCurry = shot[shot['shoot_player']=='Stephen Curry']\nhxC = Curry['halfcourt_x']\nhyC = Curry['halfcourt_y']\ncolors = np.where(Curry['current_shot_outcome']=='SCORED','r',np.where(Curry['current_shot_outcome']=='MISSED','b','g'))\nplt.figure(figsize=(94/12,50/6))\nplt.scatter(hxC,hyC, s=10, c= colors, marker= '.')\nplt.grid(True)\nplt.title(\"Steph Curry\", fontsize = 15)\n</pre> # Steph Curry plot  Curry = shot[shot['shoot_player']=='Stephen Curry'] hxC = Curry['halfcourt_x'] hyC = Curry['halfcourt_y'] colors = np.where(Curry['current_shot_outcome']=='SCORED','r',np.where(Curry['current_shot_outcome']=='MISSED','b','g')) plt.figure(figsize=(94/12,50/6)) plt.scatter(hxC,hyC, s=10, c= colors, marker= '.') plt.grid(True) plt.title(\"Steph Curry\", fontsize = 15) In\u00a0[\u00a0]: Copied! <pre># LeBron James and Steph Curry side by side\n\nf = plt.figure(figsize=(94/6,50/6))\nax = f.add_subplot(121)\ncolors = np.where(LeBron['current_shot_outcome']=='SCORED','r',np.where(LeBron['current_shot_outcome']=='MISSED','b','g'))\nax = plt.scatter(hxL,hyL, s=10, c= colors, marker= '.')\nplt.grid(True)\nplt.title(\"LeBron James\", fontsize = 15)\nax = f.add_subplot(122)\ncolors = np.where(Curry['current_shot_outcome']=='SCORED','r',np.where(Curry['current_shot_outcome']=='MISSED','b','g'))\nax = plt.scatter(hxC,hyC, s=10, c= colors, marker= '.')\nplt.grid(True)\nplt.title(\"Steph Curry\", fontsize = 15)\n</pre> # LeBron James and Steph Curry side by side  f = plt.figure(figsize=(94/6,50/6)) ax = f.add_subplot(121) colors = np.where(LeBron['current_shot_outcome']=='SCORED','r',np.where(LeBron['current_shot_outcome']=='MISSED','b','g')) ax = plt.scatter(hxL,hyL, s=10, c= colors, marker= '.') plt.grid(True) plt.title(\"LeBron James\", fontsize = 15) ax = f.add_subplot(122) colors = np.where(Curry['current_shot_outcome']=='SCORED','r',np.where(Curry['current_shot_outcome']=='MISSED','b','g')) ax = plt.scatter(hxC,hyC, s=10, c= colors, marker= '.') plt.grid(True) plt.title(\"Steph Curry\", fontsize = 15) <p>These simple plots make clear some of the differences between the two players. It is clearly visible that Curry shoots more from the three-point line than LeBron, while LeBron is more active on the paint. Some more subtle differences are also visible. LeBron is more active on the left hand side of the court, while Curry is equally active on both sides. Curry also shoots more thre-pointers from the corners.</p> As a final exercise we zoom in on the data to see what it looks like close up. We can identify a particular location based on our grid. First, we look at shots from the paint. We can define the paint here as x coordinates between 700 and 900, and y coordinates 200 and 300. We first create this subset, then we create the plot. In\u00a0[\u00a0]: Copied! <pre># Define the subset\n\nrect1 = shot[(((shot['location_x']&gt;700) &amp; (shot['location_x']&lt;900)) &amp; \\\n                 ((shot['location_y'] &gt; 200) &amp; (shot['location_y'] &lt; 300)))]\nrect1\n</pre> # Define the subset  rect1 = shot[(((shot['location_x']&gt;700) &amp; (shot['location_x']&lt;900)) &amp; \\                  ((shot['location_y'] &gt; 200) &amp; (shot['location_y'] &lt; 300)))] rect1 In\u00a0[\u00a0]: Copied! <pre># Shots on the paint\n\nxr = rect1['location_x']\nyr = rect1['location_y']\ncolors = np.where(rect1['current_shot_outcome']=='SCORED','r',np.where(rect1['current_shot_outcome']=='MISSED','b','g'))\nplt.figure(figsize=(94/6,50/6))\nplt.scatter(xr,yr, s=50, c= colors, marker= '.')\nplt.grid(True)\n</pre> # Shots on the paint  xr = rect1['location_x'] yr = rect1['location_y'] colors = np.where(rect1['current_shot_outcome']=='SCORED','r',np.where(rect1['current_shot_outcome']=='MISSED','b','g')) plt.figure(figsize=(94/6,50/6)) plt.scatter(xr,yr, s=50, c= colors, marker= '.') plt.grid(True) <p>Note how the locations appear in vertical lines. This reflects the fact that the resolution of the location coding is finite. The difference between adjacent vertical lines on this plot is approximately one inch (2.5cm).</p> <p>But we can still zoom in further. Below we look at the area immediately under the basket:</p> In\u00a0[\u00a0]: Copied! <pre># subset area\n\nrect2 = shot[(((shot['location_x']&gt;850) &amp; (shot['location_x']&lt;875)) &amp; \\\n                 ((shot['location_y'] &gt; 240) &amp; (shot['location_y'] &lt; 260)))]\nrect2\n</pre> # subset area  rect2 = shot[(((shot['location_x']&gt;850) &amp; (shot['location_x']&lt;875)) &amp; \\                  ((shot['location_y'] &gt; 240) &amp; (shot['location_y'] &lt; 260)))] rect2 In\u00a0[\u00a0]: Copied! <pre># Shots under the basket\n\nxq = rect2['location_x']\nyq = rect2['location_y']\ncolors = np.where(rect2['current_shot_outcome']=='SCORED','r',np.where(rect2['current_shot_outcome']=='MISSED','b','g'))\nplt.figure(figsize=(94/6,50/6))\nplt.scatter(xq,yq, s=50, c= colors, marker= '.')\nplt.grid(True)\n</pre> # Shots under the basket  xq = rect2['location_x'] yq = rect2['location_y'] colors = np.where(rect2['current_shot_outcome']=='SCORED','r',np.where(rect2['current_shot_outcome']=='MISSED','b','g')) plt.figure(figsize=(94/6,50/6)) plt.scatter(xq,yq, s=50, c= colors, marker= '.') plt.grid(True) <p>It turns out that this degree of resolution is not very informative. The problem is that the all the points are piled upon each other and there is nothing in between - recalling that the gaps are still approximately one inch. It would only be possble to extract a reliable picture if the data were even more fine grained.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/03.01/#visualizing-sports-data-basketball","title":"Visualizing sports data: basketball\u00b6","text":"<p>In this notebook we're going to look at ways of visualizing performance in basketball. We are going to use the data that we were introduced to last week.</p> <p>Our analysis here is going to focus on the where the ball was thrown from, which is recorded using (x, y) co-ordinates, in the same way that we had the coordinates for the baseball data. This data is  easy to graph and illuminating.</p> <p>We choose three different ways to look at the data</p> <ol> <li>shot outcome: scored, missed or blocked</li> <li>shot types by player</li> <li>narrowing the focus to a particular region of the court</li> </ol>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/03.01/#easy-self-test","title":"Easy self test\u00b6","text":"<p>Experiment with different sized markers. What size do you think gives you the best visual representation of the data? Why?</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/03.01/#more-difficult-self-test","title":"More difficult self test\u00b6","text":"<p>Show all the shots on a halfcourt, but show the data with the baseline on the left, not on the right.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/03.01/#self-test","title":"Self Test\u00b6","text":"<p>Identify two other players in the data and generate a similar comparison for those players.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/03.01/#conclusion","title":"Conclusion\u00b6","text":"<p>Using charts can be incredibly helpful in understanding the pattern of events on the field/court/pitch/ice. Charts can suggest to us logical relationships which may exist, but they do not quantify them. In the next two weeks we turn to regression analysis, which is one of the most common ways to quantify relationships among variables.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/03.02/","title":"Visualizing Sports Data","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n</pre> import pandas as pd import numpy as np import matplotlib.pyplot as plt In\u00a0[\u00a0]: Copied! <pre>IPL2018 = pd.read_excel('../../Data/Week 3/IPL2018_results.xlsx')\npd.set_option('display.max_columns', 50)\ndisplay(IPL2018)\n</pre> IPL2018 = pd.read_excel('../../Data/Week 3/IPL2018_results.xlsx') pd.set_option('display.max_columns', 50) display(IPL2018) <p>We can use the \"print(filename.tolist())\" command to see the names of the variables in our data</p> In\u00a0[\u00a0]: Copied! <pre>print(IPL2018.columns.tolist())\n</pre> print(IPL2018.columns.tolist()) In\u00a0[\u00a0]: Copied! <pre>IPL2018.hist(column='innings1', bins=10)\n</pre> IPL2018.hist(column='innings1', bins=10) In\u00a0[\u00a0]: Copied! <pre># now we generate the graph for innings2\n\nIPL2018.hist(column='innings2', bins=10)\n</pre> # now we generate the graph for innings2  IPL2018.hist(column='innings2', bins=10) <p>Comparing these two graphs it looks as if the scores in innings1 are skewed slightly to the left, and in innings2 slightly to the right. This could tell us something, but we should be careful. The x-axis for innings1 runs from 80 to 240, while for innings2 runs from 60 to 220. To compare, we really should specify that the x-axis has the same range for both sets of data. We do that in the next line:</p> In\u00a0[\u00a0]: Copied! <pre>IPL2018.hist(column='innings1', bins=10)\nplt.xlim((60, 250))\nplt.ylim((0, 20))\nIPL2018.hist(column='innings2', bins=10)\nplt.xlim((60, 250))\nplt.ylim((0, 20))\nplt.plot\n</pre> IPL2018.hist(column='innings1', bins=10) plt.xlim((60, 250)) plt.ylim((0, 20)) IPL2018.hist(column='innings2', bins=10) plt.xlim((60, 250)) plt.ylim((0, 20)) plt.plot <p>We can now see that the two innings have quite similar distributions centered around roughly the same median score. The two main differences are that the scores for innings2 seem truncated around the 200-215 mark. That is probably a result of the run chase effect - teams batting second either reach the required target and stop, or the target set in the first place was so high that the team batting second collapsed with a very low score.</p> In\u00a0[\u00a0]: Copied! <pre># We can show the two distributions on the same histogram.\n# note that in addition to specifying the number of bins, we also specify alpha, which is the degree of transparency.\n\nIPL2018[['innings1','innings2']].plot.hist(alpha=.5,bins=10)\nplt.xlabel('Runs')\nplt.ylabel('Frequency')\nplt.title(\"Runs distribution by innings\", fontsize=15)\nplt.xlim((60, 250))\nplt.ylim((0, 20))\n</pre> # We can show the two distributions on the same histogram. # note that in addition to specifying the number of bins, we also specify alpha, which is the degree of transparency.  IPL2018[['innings1','innings2']].plot.hist(alpha=.5,bins=10) plt.xlabel('Runs') plt.ylabel('Frequency') plt.title(\"Runs distribution by innings\", fontsize=15) plt.xlim((60, 250)) plt.ylim((0, 20)) In\u00a0[\u00a0]: Copied! <pre>IPL2018['winscore']= np.where(IPL2018['innings1']&gt;IPL2018['innings2'],IPL2018['innings1'],IPL2018['innings2'])\nIPL2018['losescore'] = np.where(IPL2018['innings1']&gt;IPL2018['innings2'],IPL2018['innings2'],IPL2018['innings1'])\n</pre> IPL2018['winscore']= np.where(IPL2018['innings1']&gt;IPL2018['innings2'],IPL2018['innings1'],IPL2018['innings2']) IPL2018['losescore'] = np.where(IPL2018['innings1']&gt;IPL2018['innings2'],IPL2018['innings2'],IPL2018['innings1']) In\u00a0[\u00a0]: Copied! <pre># Now we can plot two histograms together\n\nIPL2018[['winscore','losescore']].plot.hist(alpha=.5,bins=10)\nplt.xlabel('Runs')\nplt.ylabel('Frequency')\nplt.title(\"Runs distribution by innings\", fontsize=15)\nplt.xlim((60, 250))\nplt.ylim((0, 20))\n</pre> # Now we can plot two histograms together  IPL2018[['winscore','losescore']].plot.hist(alpha=.5,bins=10) plt.xlabel('Runs') plt.ylabel('Frequency') plt.title(\"Runs distribution by innings\", fontsize=15) plt.xlim((60, 250)) plt.ylim((0, 20)) <p>We can see that the winning score appears like a rightward shift of the losing score- which should not be surprising! For a low score, losing scores must outnumber winning scores, while for high scores, winning scores must outnumber losing scores.</p> In\u00a0[\u00a0]: Copied! <pre>MI_CSK = pd.read_excel('../../Data/Week 3/MIvCSKadj.xlsx')\nprint(MI_CSK.columns.tolist())\nMI_CSK\n</pre> MI_CSK = pd.read_excel('../../Data/Week 3/MIvCSKadj.xlsx') print(MI_CSK.columns.tolist()) MI_CSK <p>Bowling in cricket alternates between each end of the batting strip (also, confusingly, called the \"wicket\"). Thus one bowler delivers six consecutive balls from one end, and then another bowler delivers six consecutive balls from the other end. Each batch of six balls (\"deliveries\") is called an \"over\". Hence a Twenty20 game refers to 20 over, equal to 120 deliveries. In the dataframe \"ball_no\" refers to the ball of the over in question, which is referred to by \"over_no\". \"delivery_no\" refers to its position in the inning as a whole. You can see that, in fact, there were 122 deliveries in the Mumbai innings, while there were 124 deliveries in the Chennai innings, despite only 19.5 overs being completed (Chennai won, i.e. surpassed the Mumbai score, with one ball to spare). This arises because some balls delivered do not count- these could be either \"wides\" - called when the umpire decides that the ball was delivered so far away from the batsman that he had no reasonable chance of hitting it - or \"no balls\", called when the bowler released the ball too close to the batsman (there is a line which the bowler must not overstep).</p> <p>Runs_total_end refers to the number of runs scored by each team by the completion of a given delivery number. So we can now draw a linechart plotting the runs against delivery number for Mumbai, the team which batted first:</p> In\u00a0[\u00a0]: Copied! <pre>plt.plot(MI_CSK['MI_delivery_no'],MI_CSK['MI_runs_total_end'])\n</pre> plt.plot(MI_CSK['MI_delivery_no'],MI_CSK['MI_runs_total_end']) <p>We can see from this chart that Mumbai maintained a steady pace throughout the inning - the score increases more or less linearly with the number of balls.</p> <p>The next thing we want to do is incorporate the fall of wickets into the chart, to see how their batting resources changed as the inning progressed.</p> <p>The \"wicket\" columns for each team tell us if a wicket fell on that delivery ('1') or not ('0'). We can create dfs as subsets of main df, to identify the delivery number and runs scored when the wicket fell.</p> In\u00a0[\u00a0]: Copied! <pre>MIwicket = MI_CSK[MI_CSK['MI_wicket']&gt;0]\nCSKwicket = MI_CSK[MI_CSK['CSK_wicket']&gt;0]\nMIwicket\n</pre> MIwicket = MI_CSK[MI_CSK['MI_wicket']&gt;0] CSKwicket = MI_CSK[MI_CSK['CSK_wicket']&gt;0] MIwicket In\u00a0[\u00a0]: Copied! <pre># We can now plot the fall of wickets alongside the runs total \n# note that we obtain the red dots for wickets by specifying 'ro' - 'r' for red and 'o' for circle dots\n\nplt.plot(MI_CSK['MI_delivery_no'],MI_CSK['MI_runs_total_end'],MIwicket['MI_runs_total_end'], 'ro')\n</pre> # We can now plot the fall of wickets alongside the runs total  # note that we obtain the red dots for wickets by specifying 'ro' - 'r' for red and 'o' for circle dots  plt.plot(MI_CSK['MI_delivery_no'],MI_CSK['MI_runs_total_end'],MIwicket['MI_runs_total_end'], 'ro') <p>We can see from this that Mumbai made a realtively slow start, and lost two wickets early on. After that the innings stabilized, and then acclerated rapidly in the middle, only to slow down again after two more wickets fell. Some acceleration is also visible at the end of the innings.</p> In\u00a0[\u00a0]: Copied! <pre># we now combine the runs scored profile for Mumbai with Chennai's\n\nplt.plot(MI_CSK['CSK_delivery_no'],MI_CSK['MI_runs_total_end'],MI_CSK['CSK_runs_total_end'])\n</pre> # we now combine the runs scored profile for Mumbai with Chennai's  plt.plot(MI_CSK['CSK_delivery_no'],MI_CSK['MI_runs_total_end'],MI_CSK['CSK_runs_total_end']) <p>We can see that Chennai's innings (in orange) progressed rather differently. Initially Chennai were well ahead of the pace set by Mumbai. However, Chennai slowed down considerably after about 25 deliveries, and were well behind Mumbai's scoring rate by mid-innings. Only at the end did Chennai accelerate, and overtake Mumbai's score to win the game.</p> In\u00a0[\u00a0]: Copied! <pre># We now plot the fall of wickets on the chart\n\nplt.plot(MI_CSK['CSK_delivery_no'],MI_CSK['MI_runs_total_end'],MIwicket['MI_runs_total_end'], 'bo')\nplt.plot(MI_CSK['CSK_delivery_no'],MI_CSK['CSK_runs_total_end'],CSKwicket['CSK_runs_total_end'], 'ro')\n</pre> # We now plot the fall of wickets on the chart  plt.plot(MI_CSK['CSK_delivery_no'],MI_CSK['MI_runs_total_end'],MIwicket['MI_runs_total_end'], 'bo') plt.plot(MI_CSK['CSK_delivery_no'],MI_CSK['CSK_runs_total_end'],CSKwicket['CSK_runs_total_end'], 'ro') <p>Now we can see why Chennai fell behind after the initial good start. Unlike Mumbai, Chennai's wickets fell steadily throughout the innings, a fact which tended to slow down the scoring rate. In fact, Chennai had lost eight wickets before its final accleration around the 100 delivery mark. In cricketing terms this was a fairly exceptional performance, for it is unusual for teams that have already lost eight wickets to score many runs, let alone outperform outpace the rest of the innings to this extent.</p> <p>This analysis was for just one game. We now write a program which will allow us to reproduce these profiles for any game in IPL 2018 season, and compare profiles for any pair of games.</p> <p>We now load a dataframe that includes every delivery for the season:</p> In\u00a0[\u00a0]: Copied! <pre>IPLbyb = pd.read_excel('../../Data/Week 3/IPLbyb.xlsx')\nprint(IPLbyb.columns.tolist())\n</pre> IPLbyb = pd.read_excel('../../Data/Week 3/IPLbyb.xlsx') print(IPLbyb.columns.tolist()) <p>We are now going to define two functions: one that will allow to create a comparable chart for any game, and another that will allow us to specify two games to compare.</p> <p>We will encounter functions again, but for Python novices these take a while to become accustomed to. Rather than explain in detail here, we'll just run the functions to show what can be done.</p> In\u00a0[\u00a0]: Copied! <pre># First, the function for plotting the runs and wickets for each team in a game.\n\ndef plot_runs_wickets(IPLbyb, ax):\n    gameno = IPLbyb['gameno'].unique()[0]\n    for inning, data in IPLbyb.groupby('innings_number'):\n        # create separate dataframe for wickets\n        wicket = data[data['wicket'] &gt; 0]\n        # plots line\n        ax.plot(data['delivery_no'],data['runs_total_end'])\n        # plots markers\n        marker = 'bo' if inning == 1 else 'ro'\n        ax.plot(wicket['delivery_no'],wicket['runs_total_end'], marker)\n        # labels\n        ax.set_xlabel('balls')\n        ax.set_ylabel('runs')\n        ax.set_title(f'Game {gameno}')\n    ax.legend(['runs1','wkt1','runs2','wkt2'])\n</pre> # First, the function for plotting the runs and wickets for each team in a game.  def plot_runs_wickets(IPLbyb, ax):     gameno = IPLbyb['gameno'].unique()[0]     for inning, data in IPLbyb.groupby('innings_number'):         # create separate dataframe for wickets         wicket = data[data['wicket'] &gt; 0]         # plots line         ax.plot(data['delivery_no'],data['runs_total_end'])         # plots markers         marker = 'bo' if inning == 1 else 'ro'         ax.plot(wicket['delivery_no'],wicket['runs_total_end'], marker)         # labels         ax.set_xlabel('balls')         ax.set_ylabel('runs')         ax.set_title(f'Game {gameno}')     ax.legend(['runs1','wkt1','runs2','wkt2']) In\u00a0[\u00a0]: Copied! <pre># Second, a function that allows us to plot two or more games at the same time. \n\ndef plot_runs_wickets_multi_game(list_games):\n    n = len(list_games)\n    fig, axs = plt.subplots(n, 1, figsize=(6,15))\n    for i, gameno in enumerate(list_games):\n        game = IPLbyb[IPLbyb['gameno'] == gameno]\n        plot_runs_wickets(game, axs[i] if n &gt; 1 else axs)\n</pre> # Second, a function that allows us to plot two or more games at the same time.   def plot_runs_wickets_multi_game(list_games):     n = len(list_games)     fig, axs = plt.subplots(n, 1, figsize=(6,15))     for i, gameno in enumerate(list_games):         game = IPLbyb[IPLbyb['gameno'] == gameno]         plot_runs_wickets(game, axs[i] if n &gt; 1 else axs) <p>These functions will allow us to produce mutliple charts in order to display games alongside each other. Before we do that, though, we should generate a list of games, so we can decide which game's number refer to which teams. The code below generates a list for us. In our chart we'll see innings1 and innings2 as before. The next three lines of code creates a table which specifies the home team, the away team and and whether the home team batted first, so that the teams 1 and 2 can be identified for any game.</p> In\u00a0[\u00a0]: Copied! <pre># Identify if the home team batted first\n\nIPLbyb['hometeambatsfirst']= np.where((IPLbyb['home team']==IPLbyb['batting_team']) &amp; (IPLbyb['innings_number']==1),'yes','no')\nIPLbyb\n</pre> # Identify if the home team batted first  IPLbyb['hometeambatsfirst']= np.where((IPLbyb['home team']==IPLbyb['batting_team']) &amp; (IPLbyb['innings_number']==1),'yes','no') IPLbyb In\u00a0[\u00a0]: Copied! <pre># drop duplicates so we just have a list of games\n\ngames = IPLbyb.drop_duplicates('gameno')\n</pre> # drop duplicates so we just have a list of games  games = IPLbyb.drop_duplicates('gameno') In\u00a0[\u00a0]: Copied! <pre># generate list of games\n\ngames = games[['gameno','home team','batting_team','bowling_team','hometeambatsfirst']]\ngames['road team'] = games.apply(lambda x: x['batting_team'] if x['home team'] == x['bowling_team'] else x['bowling_team'], axis=1)\nassert (games['home team'] != games['road team']).all()\ngames = games[['gameno','home team','road team','hometeambatsfirst']]\ngames\n</pre> # generate list of games  games = games[['gameno','home team','batting_team','bowling_team','hometeambatsfirst']] games['road team'] = games.apply(lambda x: x['batting_team'] if x['home team'] == x['bowling_team'] else x['bowling_team'], axis=1) assert (games['home team'] != games['road team']).all() games = games[['gameno','home team','road team','hometeambatsfirst']] games <p>You can see from the list above that game 27 was the return game between MI and CSK, with CSK the home team and batting first. So let's compare these two games.</p> In\u00a0[\u00a0]: Copied! <pre># Inside the square brackets we can type in game numbers, separated by a comma, to specify the games we want to compare\n\nplot_runs_wickets_multi_game([1, 27])\n</pre> # Inside the square brackets we can type in game numbers, separated by a comma, to specify the games we want to compare  plot_runs_wickets_multi_game([1, 27]) <p>As with game 1, the team batting second won the game, but this time is was the Mumbai Indians that won. Apart from the middle portion of the game, the scoring rates of each team were much closer, and the main difference was that toward the end of innings 1 (CSK) three wickets fell quickly, which slowed down the scoring rate.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/03.02/#visualizing-sports-data","title":"Visualizing Sports Data\u00b6","text":"<p>Last week we introduced some simple ways to graph data. This week we introduce some more applications of visualization methods in Python. There are entire courses on data visualization methods in Python or R, so here we will only skim the surface. Our object is to show how powerful graphs can be as tools for understanding sports data.</p> <p>As a general rule, any data analysis should start with drawing some graphs. Literally looking at the relationship between variables in your data can help you in a number of different ways, including:</p> <ol> <li>Identifying the correlation between variables</li> <li>Identifying particular ranges of values where the relationship might change</li> <li>Identifying outliers which may not conform to a typical pattern</li> </ol> <p>In essence, all data analysis is about looking for patterns, and using a visual representation is an especially good way for humans to recognize patterns.</p> <p>During this week we will look at data from four different sports: baseball, basketball, hockey, but first, cricket.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/03.02/#cricket","title":"Cricket\u00b6","text":"<p>Cricket is a sport which originated in England several hundred years ago- records of games date back over three hundred years. It is the most popular game in India, Pakistan, Bangladesh and Sri Lanka - home to almost one quarter of the world's population. It is also a popular sport in Australia, New Zealand, South Africa, Zimbabwe, and the islands of the Caribbean.</p> <p>It is a bat and ball game that bears some resemblance to baseball: a ball is thrown by a player (called the bowler in cricket) on one team at a player (called the batsman) on the other team, who attempts to hit it in order to score runs. (Note: although women's cricket is popular and has a long history, the term \"batsman\" is used for men and women alike). A batsman is \"out\" if he hits the ball and it is caught on the fly, and is also \"out\" if the ball hits the \"wicket\", which is a little like the strike-zone in baseball, except that the wicket is a physical object comprising three sticks planted vertically (the \"stumps\") with two smaller sticks resting on top (the \"bails\"). Unlike baseball, a player does not get three strikes before being out- if the wicket is hit and a bail dislodged, the batsman is out.</p> <p>Runs are also simpler in cricket- to complete a run a batsman has to be able to reach the \"crease\" at the \"other end\", which refers to the place from which the bowler delivered the ball, and where there stands another wicket (stumps and bails). At that other end stands another batsman- there are always two at any one time - and that batsman must also run to the opposite crease for the run to be complete. If any fielder can dislodge a bail at the end to which a batsman is running before the batsman reaches the crease then no run is scored and, more importantly, the batsman is out (this, again, is similar to baseball).</p> <p>To score runs the batsman needs to hit the ball to locations which are not close to fielders, to give the batsmen time to complete a run. Depending on where the ball is hit, the batsmen may have time to complete one, two runs, or even three. The game is played on an oval shaped field, and if the ball reaches the designated boundary of the field, four runs are scored, unless the ball crosses the boundary without hitting the ground (like a home run) in which case six runs are awarded.</p> <p>Two important differences between baseball and cricket are that (a) there are fewer outs in a game and (b) many more runs are typically scored. A game of cricket is typically played with one or two innings per team. We are going to look at data from the Indian Premier League, which is played under the \"Twenty20\" (T20) rules, in which each team gets only one inning. So each player may bat only once. There are 11 players on each team, and once ten batsmen are \"out\", the innings is over.</p> <p>The higher rate of scoring runs arises because (i) the bat in cricket is larger, and hence hitting the ball is easier (the ball is of similar size, and the distance between bowler and batsman is roughly the same as between pitcher and batter in baseball- the speed of delivery is tyically a bit faster in baseball) (ii) there is no obligation to run when the ball is hit, and there is no rule of three strikes and out - so that a player may stand and bat without attmempting to score runs.</p> <p>T20 cricket was invented in the early 2000s, and is a game played at a fast pace over short period of time (contrary to the popular perception of cricket). In a T20 game each side faces a maximum of 120 balls, and the game is usually completed within three hours (if ten batsmen are out before 120 balls have been bowled, then the innings is over).</p> <p>The team with the highest runs score wins. Statistically speaking, one other critical difference from baseball is that if the team batting second wins, there is no way of knowing how may runs that team could have scored in a completed inning- it is sufficient to outscore the team batting first for the game to be over. You will recall from week 1 that the Pythagorean expectation model fitted the cricket data poorly - and this may be in part due to the fact that the maximum potential score of the team in such cases is unknown.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/03.02/#the-indian-premier-league","title":"The Indian Premier League\u00b6","text":"<p>The Indian Premier League (IPL) was founded in 2008 and is the most popular cricket competition in world, and the value of braodcasting rights is estimated to be over $1 billion. The league currently consists of eight franchises based in Indian cities, and the league typically plays over seven weeks in April and May, with every team playing every other team home and away, followed by playoffs involving the top four teams. The season consists of 60 games in total.</p> <p>Most of the world's best players are recruited for the IPL. Each season there is a player auction, and teams are subject to a salary cap.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/03.02/#the-run-chase","title":"The Run Chase\u00b6","text":"<p>The order in which teams bat is decided by the toss of a coin at the start of the game. Teams generally elect to field first (over 80% of the time in our data). Batting with the knowledge of the total you need to reach, having seen the batting conditions (the weather can play a significant role) is a big advantage. There is a fundamental trade-off between batting aggressively, (allowing you to score runs quickly) and batting conservatively (which means that \"outs\" are less likely). The ten wickets (outs) allowed for each team essentially represent resources at the batting team's commands. However, as in baseball, the bowlers tend to be less skilled in batting, and the better batsmen typically play at the top of the innings.</p> <p>Batting is typically hardest at the beginning of an innings, because the bowlers are freshest and the ball is new (unlike baseball, the same ball is used throughout a whole inning). However, there are also fielding restrictions, known as powerplays, which make it easier to make big hits safely, especially during the first ten overs of the game.</p> <p>From all this, it should be clear that managing the pace of an innings is a crucial team skill in cricket. We are going to use the IPL data to compare visually the run/wicket profiles of teams in each game.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/03.02/#plan-for-this-session","title":"Plan for this session\u00b6","text":"<p>Runs scored determine the outcome of the game, so we will begin by looking at runs totals for each game from the IPL 2018 season and generating charts to show variations in runs scored depending on (a) which team batted first and (b) which team won the game.</p> <p>We then turn to a particular game - first game of the 2018 season between the Mumbai Indians and the Chennai Super Kings, and compare the progress of each team in terms of runs scored and wickets lost.</p> <p>Finally, we write a program to compare teams in any game.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/03.02/#starting","title":"Starting\u00b6","text":"<p>As ever we begin by importing the packages we need, and loading up the data.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/03.02/#the-distribution-of-runs-scored","title":"The distribution of runs scored\u00b6","text":"<p>The variables we are interested in are the runs scored by each team, which are listed in \"innings1\" and \"innings2\". A histogram will show us the variation of runs scored. We specify the number of 'bins' - we use 10 bins here, which divides the data between the highest and lowest scores into ten equals ranges of 20 runs. The vertical axis then tells us the percentage of scores in innings1 in each data range.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/03.02/#self-test","title":"Self test\u00b6","text":"<p>Compare the charts you produce when you specify (a) 5 bins and (b) 20 bins. Which do you think is better representation of the data and why?</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/03.02/#self-test","title":"Self test\u00b6","text":"<p>Try experimenting with different values of alpha to see how this changes the the visual effectiveness of the histogram.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/03.02/#the-distribution-of-runs-scored-by-winning-and-losing-teams","title":"The distribution of runs scored by winning and losing teams\u00b6","text":"<p>Having looked at the distributions comparing the team batting first and second, now let's compare the histograms for the teams that win and the teams that lose.</p> <p>First, define winning and losing teams - which is derived by comparing the number of runs scored.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/03.02/#run-accumulation-and-wickets","title":"Run Accumulation and Wickets\u00b6","text":"<p>We now look at a particular game, the opening game of the 2018 season between the Mumbai Indians (the reigning champions) and Chennai Super Kings, played in Mumbai on April 7, 2018. The file we now load contains a record of the game, ball by ball.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/03.02/#self-test","title":"Self test\u00b6","text":"<p>Try using 'ro' as the marker for the runs scored. Based on this you can consider other types of colors and/or markers for each variable. You can find options for markers in matplotlib here: https://matplotlib.org/3.1.1/api/markers_api.html and for colors here: https://matplotlib.org/3.1.0/gallery/color/named_colors.html</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/03.02/#self-test","title":"Self test\u00b6","text":"<p>Pick another pair of games between the same teams and compare scoring rates.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/03.02/#conclusion","title":"Conclusion\u00b6","text":"<p>In this notebook we have shown how a particular kind of visualization can help to understand the performance of teams in cricket.</p> <p>Looking at these charts, you start to think about ways to explain outcomes. For example, it seems clear that the rate at which runs are scored is a function of the number of wickets that have fallen, and that the fall of a wicket generally slows the run rate. Likewise, winning teams seem to enjoy periods of rapid acceleration. These observations should lead you to formulate stories (hypotheses) which can be tested more formally. In that sense, visualizing the data is a stepping stone to a more rigorous statistical analysis.</p> <p>The Nobel prize winning economist Sir John Hicks once said that he was never convinced of a theory until he could formalize it in an equation, write it down in plain English and draw a picture. Much the same thing is true for data analytics. However, whereas Hicks worked in an age where only limited amounts of data could be analyzed, and so often the picture was an afterthought, we now have the opportunity to start our analysis by drawing pictures which can sometimes show us clearly what the story is to be told.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/03.03/","title":"Visualizing sports data: baseball","text":"In\u00a0[\u00a0]: Copied! <pre># Import the packages we need\n# matplotlib will enable us to produce all our dataplots in this session.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n</pre> # Import the packages we need # matplotlib will enable us to produce all our dataplots in this session.  import pandas as pd import matplotlib.pyplot as plt In\u00a0[\u00a0]: Copied! <pre># Read in MLBAM Data for 2018\n\nMLBAM18 = pd.read_csv(\"../../Data/Week 3/MLBAM18.csv\")\nMLBAM18.drop(['Unnamed: 0'], axis=1, inplace=True)\npd.set_option('display.max_columns', 100)\ndisplay(MLBAM18)\n</pre> # Read in MLBAM Data for 2018  MLBAM18 = pd.read_csv(\"../../Data/Week 3/MLBAM18.csv\") MLBAM18.drop(['Unnamed: 0'], axis=1, inplace=True) pd.set_option('display.max_columns', 100) display(MLBAM18) In\u00a0[\u00a0]: Copied! <pre># The dataframe contains a lot of variables - most of which we won't need for this exercise\n\nprint(MLBAM18.columns.tolist())\n</pre> # The dataframe contains a lot of variables - most of which we won't need for this exercise  print(MLBAM18.columns.tolist()) <p>So we now restrict the data to a manageable set of variables. Note that the data contains co-ordinates 'x' and 'y' as well as 'our.x' and 'our.y' The difference here is the point of view: 'x' and'y' are looking toward the batter from the bleachers, 'our.x' and 'our.y' are looking from behind the batter. We'll adopt the more conventional view by using 'our.x' and 'our.y'.</p> In\u00a0[\u00a0]: Copied! <pre># Limiting the set of variables\n\nMLBmap = MLBAM18[['gameId','home_team','away_team','stadium','inning', 'batterId', 'batterName',\\\n                  'pitcherId', 'pitcherName','event','timestamp','stand', 'throws','x','y','our.x','our.y']]\nMLBmap\n</pre> # Limiting the set of variables  MLBmap = MLBAM18[['gameId','home_team','away_team','stadium','inning', 'batterId', 'batterName',\\                   'pitcherId', 'pitcherName','event','timestamp','stand', 'throws','x','y','our.x','our.y']] MLBmap <p>We focus a good deal on different events, so it is useful to list them before we go any further. We can do this using .unique():</p> In\u00a0[\u00a0]: Copied! <pre>MLBmap['event'].unique()\n</pre> MLBmap['event'].unique() In\u00a0[\u00a0]: Copied! <pre>plt.scatter(MLBmap['our.x'],MLBmap['our.y'], s=.001,c='r', marker= '.')\n</pre> plt.scatter(MLBmap['our.x'],MLBmap['our.y'], s=.001,c='r', marker= '.') <p>Although this is a scatter plot, we are able to use very small dots and as a result we can see the pattern of where the ball lands more frequently on the baseball diamond. Note that the pattern reveals very clearly the shape of the baseball field, even though the results are drawn from multiple ballparks which have slightly different dimensions. The most intense colors are around the infield bases and the pitcher's mound (at the bottom of the screen). In the outfield, the deepest colors are around the locations where outfielders are typically stationed. Note the space between these outfield locations and the infield has realtively few dots, given that balls which might land there have usually been stopped by an infielder.</p> In\u00a0[\u00a0]: Copied! <pre># 2. Plot of singles\n\nSingle = MLBmap[MLBmap.event == 'Single']\nplt.scatter(Single['our.x'],Single['our.y'], s=.02,c='darkorange', marker= '.')\n</pre> # 2. Plot of singles  Single = MLBmap[MLBmap.event == 'Single'] plt.scatter(Single['our.x'],Single['our.y'], s=.02,c='darkorange', marker= '.') <p>From the more intensely colored areas, it's clear that singles are scored in two types of situations: (i) when the ball gets over the infield and reaches one of the outfielders (note that there are few singles scored when the ball goes beyond the outfielders because such a hit will likely produce more than a single) and (ii) when the ball is hit along the left foul line, typically between the third baseman and pitcher. Such a hit is usually produced by a \"bunt\", where the batter intends to just touch the ball and let it roll, and use his speed to reach first base before a fielder can recover the ball.</p> In\u00a0[\u00a0]: Copied! <pre># 3. Plot of doubles\n\nDouble = MLBmap[MLBmap.event == 'Double']\nplt.scatter(Double['our.x'],Double['our.y'], s=1,c='dodgerblue', marker= '.')\n</pre> # 3. Plot of doubles  Double = MLBmap[MLBmap.event == 'Double'] plt.scatter(Double['our.x'],Double['our.y'], s=1,c='dodgerblue', marker= '.') <p>We see quite a different pattern with doubles. A big hit to the infield hardly ever results in a double, and outfield hits are typically hit along the foul lines, over the heads of the outfielders. The spaces within sprinting distance of the outfielders are largely empty, since doubles are usually produced by balls hit into the air and are caught if the outfielders are close enough.</p> <p>Note that since these events are rarer than singles, we use a larger market size to make sure the distribution is clearly visible.</p> In\u00a0[\u00a0]: Copied! <pre># 4. Plot of triples\n\nTriple = MLBmap[MLBmap.event == 'Triple']\nplt.scatter(Triple['our.x'],Triple['our.y'], s=1,c='g', marker= '.')\n</pre> # 4. Plot of triples  Triple = MLBmap[MLBmap.event == 'Triple'] plt.scatter(Triple['our.x'],Triple['our.y'], s=1,c='g', marker= '.') <p>Triples are much rarer even than doubles. Mostly these are created by hits along the right foul line or over the heads of the outfielders.</p> In\u00a0[\u00a0]: Copied! <pre># 5. plot of home runs\n\nHomer = MLBmap[MLBmap.event == 'Home Run']\nplt.scatter(Homer['our.x'],Homer['our.y'], s=.20,c='m', marker= '.')\n#ax.set(xlim=(-300,300), ylim=(0,450))\nplt.ylim((0,500))\n</pre> # 5. plot of home runs  Homer = MLBmap[MLBmap.event == 'Home Run'] plt.scatter(Homer['our.x'],Homer['our.y'], s=.20,c='m', marker= '.') #ax.set(xlim=(-300,300), ylim=(0,450)) plt.ylim((0,500)) <p>By definition, a home run is hit out of the park. This plot therefore not only illustrates the boundaries of the ballparks and the distance the ball is hit, but also, to some extent, the differing dimensions of ballparks. Some of the dots on the inside of the semi-circle would not reach beyond the outfield of some ballparks.</p> <p>We can now plot the four scatter diagrams together, to make comparison easier, we can set a common scale for the vertical axis.</p> In\u00a0[\u00a0]: Copied! <pre># 6. the four plots: Singles, doubles, triples and home runs\n\nf = plt.figure(figsize=(15,3))\nax = f.add_subplot(141)\nax=plt.scatter(Single['our.x'],Single['our.y'], s=.01,c='darkorange', marker= '.')\nplt.ylim((0,500))\nax = f.add_subplot(142)\nax=plt.scatter(Double['our.x'],Double['our.y'], s=.1,c='dodgerblue', marker= '.')\nplt.ylim((0,500))\nax = f.add_subplot(143)\nax = plt.scatter(Triple['our.x'],Triple['our.y'], s=1,c='g', marker= '.')\nplt.ylim((0,500))\nax = f.add_subplot(144)\nax = plt.scatter(Homer['our.x'],Homer['our.y'], s=.5,c='m', marker= '.')\nplt.ylim((0,500))\n</pre> # 6. the four plots: Singles, doubles, triples and home runs  f = plt.figure(figsize=(15,3)) ax = f.add_subplot(141) ax=plt.scatter(Single['our.x'],Single['our.y'], s=.01,c='darkorange', marker= '.') plt.ylim((0,500)) ax = f.add_subplot(142) ax=plt.scatter(Double['our.x'],Double['our.y'], s=.1,c='dodgerblue', marker= '.') plt.ylim((0,500)) ax = f.add_subplot(143) ax = plt.scatter(Triple['our.x'],Triple['our.y'], s=1,c='g', marker= '.') plt.ylim((0,500)) ax = f.add_subplot(144) ax = plt.scatter(Homer['our.x'],Homer['our.y'], s=.5,c='m', marker= '.') plt.ylim((0,500)) In\u00a0[\u00a0]: Copied! <pre>#7. A single plot - the four types of hit on one plot \n\nax=plt.scatter(Single['our.x'],Single['our.y'], s=.01,c='darkorange', marker= '.')\nax=plt.scatter(Double['our.x'],Double['our.y'], s=.1,c='dodgerblue', marker= '.')\nax = plt.scatter(Triple['our.x'],Triple['our.y'], s=1,c='g', marker= '.')\nax = plt.scatter(Homer['our.x'],Homer['our.y'], s=.5,c='m', marker= '.')\n</pre> #7. A single plot - the four types of hit on one plot   ax=plt.scatter(Single['our.x'],Single['our.y'], s=.01,c='darkorange', marker= '.') ax=plt.scatter(Double['our.x'],Double['our.y'], s=.1,c='dodgerblue', marker= '.') ax = plt.scatter(Triple['our.x'],Triple['our.y'], s=1,c='g', marker= '.') ax = plt.scatter(Homer['our.x'],Homer['our.y'], s=.5,c='m', marker= '.') <p>Another way to use the scatter diagram is to compare at-bats which result in a hit, and at-bats that result in an out. First we generate the two scatter plots separately, then we create them alongside each other.</p> In\u00a0[\u00a0]: Copied! <pre># Outs\nOuts = MLBmap[(MLBmap.event == 'Groundout')|(MLBmap.event == 'Flyout')| (MLBmap.event == 'Pop Out')|\n             (MLBmap.event == 'Forceout')|(MLBmap.event == 'Lineout')| (MLBmap.event == 'Grounded Into DP')]\nplt.scatter(Outs['our.x'],Outs['our.y'], s=.01,c='r', marker= '.')\n</pre> # Outs Outs = MLBmap[(MLBmap.event == 'Groundout')|(MLBmap.event == 'Flyout')| (MLBmap.event == 'Pop Out')|              (MLBmap.event == 'Forceout')|(MLBmap.event == 'Lineout')| (MLBmap.event == 'Grounded Into DP')] plt.scatter(Outs['our.x'],Outs['our.y'], s=.01,c='r', marker= '.') In\u00a0[\u00a0]: Copied! <pre># Hits\nHits = MLBmap[(MLBmap.event == 'Single')|(MLBmap.event == 'Double')| (MLBmap.event == 'Triple')|\n             (MLBmap.event == 'Home Run')]\nplt.scatter(Hits['our.x'],Hits['our.y'], s=.01,c='b', marker= '.')\n</pre> # Hits Hits = MLBmap[(MLBmap.event == 'Single')|(MLBmap.event == 'Double')| (MLBmap.event == 'Triple')|              (MLBmap.event == 'Home Run')] plt.scatter(Hits['our.x'],Hits['our.y'], s=.01,c='b', marker= '.') In\u00a0[\u00a0]: Copied! <pre># Hits vs. Outs\nf = plt.figure(figsize=(15,3))\nax = f.add_subplot(131)\nax=plt.scatter(Outs['our.x'],Outs['our.y'], s=.01,c='r', marker= '.')\nax2 = f.add_subplot(132)\nax2 = plt.scatter(Hits['our.x'],Hits['our.y'], s=.01,c='b', marker= '.')\nax3 = f.add_subplot(133)\nax3=plt.scatter(Outs['our.x'],Outs['our.y'], s=.01,c='r', marker= '.')\nax3=plt.scatter(Hits['our.x'],Hits['our.y'], s=.01,c='b', marker= '.')\n</pre> # Hits vs. Outs f = plt.figure(figsize=(15,3)) ax = f.add_subplot(131) ax=plt.scatter(Outs['our.x'],Outs['our.y'], s=.01,c='r', marker= '.') ax2 = f.add_subplot(132) ax2 = plt.scatter(Hits['our.x'],Hits['our.y'], s=.01,c='b', marker= '.') ax3 = f.add_subplot(133) ax3=plt.scatter(Outs['our.x'],Outs['our.y'], s=.01,c='r', marker= '.') ax3=plt.scatter(Hits['our.x'],Hits['our.y'], s=.01,c='b', marker= '.') <p>What is striking about this comparison is that the locations of hits and outs are largely complementary. For example, the densest region of hits is precisely that region between the infield and the outfield where outs are relatively sparse. It is also clear hits which reach the fence (typically home runs) have no corresponding outs, while outs which involve the batter being caught in foul territory have no corresponding hits.</p> <p>We now turn to comparing stadiums. Ballparks do not have identical dimensions, and so we can look to see if the pattern of co-ordinates is different. First, it is useful to create a list of stadiums.</p> In\u00a0[\u00a0]: Copied! <pre># 7. Summary list of stadiums\n\nstadiums = MLBmap.groupby('stadium')['gameId'].count().reset_index()\nstadiums\n</pre> # 7. Summary list of stadiums  stadiums = MLBmap.groupby('stadium')['gameId'].count().reset_index() stadiums <p>Tropicana Field is said to be the smallest ballpark and Dodger Stadium the largest ballpark - so let's compare the heatmaps.</p> In\u00a0[\u00a0]: Copied! <pre># 8. Tropicana Field\n\nTrop = MLBmap[MLBmap.stadium == 'Tropicana Field']\nplt.scatter(Trop['our.x'],Trop['our.y'], s=1,c='r', marker= '.')\n</pre> # 8. Tropicana Field  Trop = MLBmap[MLBmap.stadium == 'Tropicana Field'] plt.scatter(Trop['our.x'],Trop['our.y'], s=1,c='r', marker= '.') In\u00a0[\u00a0]: Copied! <pre># 9. Dodger Stadium \n\nDodge = MLBmap[MLBmap.stadium == 'Dodger Stadium']\nplt.scatter(Dodge['our.x'],Dodge['our.y'], s=1,c='dodgerblue', marker= '.')\n</pre> # 9. Dodger Stadium   Dodge = MLBmap[MLBmap.stadium == 'Dodger Stadium'] plt.scatter(Dodge['our.x'],Dodge['our.y'], s=1,c='dodgerblue', marker= '.') In\u00a0[\u00a0]: Copied! <pre># 10. Tropicana Field and Dodger Stadium\n\n# In fact the heatmaps don't look so different.\n\n\nf = plt.figure(figsize=(15,3))\nax = f.add_subplot(131)\nax=plt.scatter(Trop['our.x'],Trop['our.y'], s=.5,c='r', marker= '.')\nax2 = f.add_subplot(132)\nax2=plt.scatter(Dodge['our.x'],Dodge['our.y'], s=.5,c='dodgerblue', marker= '.')\nax3 = f.add_subplot(133)\nax3 = plt.scatter(Trop['our.x'],Trop['our.y'], s=.5,c='r', marker= '.')\nax3 = plt.scatter(Dodge['our.x'],Dodge['our.y'], s=.5,c='dodgerblue', marker= '.')\n</pre> # 10. Tropicana Field and Dodger Stadium  # In fact the heatmaps don't look so different.   f = plt.figure(figsize=(15,3)) ax = f.add_subplot(131) ax=plt.scatter(Trop['our.x'],Trop['our.y'], s=.5,c='r', marker= '.') ax2 = f.add_subplot(132) ax2=plt.scatter(Dodge['our.x'],Dodge['our.y'], s=.5,c='dodgerblue', marker= '.') ax3 = f.add_subplot(133) ax3 = plt.scatter(Trop['our.x'],Trop['our.y'], s=.5,c='r', marker= '.') ax3 = plt.scatter(Dodge['our.x'],Dodge['our.y'], s=.5,c='dodgerblue', marker= '.') <p>In this case there does not appear to be a significant difference between the distributions.</p> <p>Another use for this technique is to compare where the players hit. Batters are identified in the data as lefties or righties. First we list all the players, and then choose a righty - Justin Turner, with a lefty Nick Maskakis. As you can see below, they each ranked among the players with the largest numbers of at-bat in 2018.</p> In\u00a0[\u00a0]: Copied! <pre># 11. Comparing players\n\n# We use a pivot table here to list players by at bats\n\nplayersn = MLBmap.groupby('batterId')['batterName'].describe().reset_index()\nplayersn.sort_values(by = 'count', ascending = False)\n</pre> # 11. Comparing players  # We use a pivot table here to list players by at bats  playersn = MLBmap.groupby('batterId')['batterName'].describe().reset_index() playersn.sort_values(by = 'count', ascending = False) In\u00a0[\u00a0]: Copied! <pre># 12. Compare a righty (R) to a lefty (L)\n\n# Turner (R)\n\nb607208 = MLBmap[MLBmap.batterId == 607208]\nplt.scatter(b607208['our.x'],b607208['our.y'], s=10,c='r', marker= '.')\n</pre> # 12. Compare a righty (R) to a lefty (L)  # Turner (R)  b607208 = MLBmap[MLBmap.batterId == 607208] plt.scatter(b607208['our.x'],b607208['our.y'], s=10,c='r', marker= '.') In\u00a0[\u00a0]: Copied! <pre># 13. Markakis (L)\n\nb455976 = MLBmap[MLBmap.batterId == 455976]\nplt.scatter(b455976['our.x'],b455976['our.y'], s=10,c='b', marker= '.')\n</pre> # 13. Markakis (L)  b455976 = MLBmap[MLBmap.batterId == 455976] plt.scatter(b455976['our.x'],b455976['our.y'], s=10,c='b', marker= '.') In\u00a0[\u00a0]: Copied! <pre># 14. Turner and Markakis together\n\nplt.scatter(b607208['our.x'],b607208['our.y'], s=10,c='r', marker= '.')\nplt.scatter(b455976['our.x'],b455976['our.y'], s=10,c='b', marker= '.')\n</pre> # 14. Turner and Markakis together  plt.scatter(b607208['our.x'],b607208['our.y'], s=10,c='r', marker= '.') plt.scatter(b455976['our.x'],b455976['our.y'], s=10,c='b', marker= '.')  In\u00a0[\u00a0]: Copied! <pre># 15. Turner and Markakis in three plots\n\nf = plt.figure(figsize=(15,3))\nax = f.add_subplot(131)\nax=plt.scatter(b607208['our.x'],b607208['our.y'], s=10,c='darkorange', marker= '.')\nax2 = f.add_subplot(132)\nax2=plt.scatter(b455976['our.x'],b455976['our.y'], s=5,c='b', marker= '.')\nax3 = f.add_subplot(133)\nax3 = plt.scatter(b607208['our.x'],b607208['our.y'], s=10,c='darkorange', marker= '.')\nax3 = plt.scatter(b455976['our.x'],b455976['our.y'], s=5,c='b', marker= '.')\n</pre> # 15. Turner and Markakis in three plots  f = plt.figure(figsize=(15,3)) ax = f.add_subplot(131) ax=plt.scatter(b607208['our.x'],b607208['our.y'], s=10,c='darkorange', marker= '.') ax2 = f.add_subplot(132) ax2=plt.scatter(b455976['our.x'],b455976['our.y'], s=5,c='b', marker= '.') ax3 = f.add_subplot(133) ax3 = plt.scatter(b607208['our.x'],b607208['our.y'], s=10,c='darkorange', marker= '.') ax3 = plt.scatter(b455976['our.x'],b455976['our.y'], s=5,c='b', marker= '.') <p>This series of charts makes quite clear the difference between the lefty and the righty. Both are \"opposite field hitters\" - the ball tends to travel toward the side of the field they are facing - rather than being \"pull hitters\". While both hit roughly equally into centerfield, it's noticeable that they each hit further into the opposite field.</p> <p>We now compare all lefties with all righties:</p> In\u00a0[\u00a0]: Copied! <pre># 16. Overall heatmap for lefties and righties\n\n# Lefties\n\nLeft = MLBmap[MLBmap.stand == 'L']\nplt.scatter(Left['our.x'],Left['our.y'], s=.01,c='r', marker= '.')\n</pre> # 16. Overall heatmap for lefties and righties  # Lefties  Left = MLBmap[MLBmap.stand == 'L'] plt.scatter(Left['our.x'],Left['our.y'], s=.01,c='r', marker= '.')  In\u00a0[\u00a0]: Copied! <pre># 17. Righties\n\nRight = MLBmap[MLBmap.stand == 'R']\nplt.scatter(Right['our.x'],Right['our.y'], s=.01,c='b', marker= '.')\n</pre> # 17. Righties  Right = MLBmap[MLBmap.stand == 'R'] plt.scatter(Right['our.x'],Right['our.y'], s=.01,c='b', marker= '.') In\u00a0[\u00a0]: Copied! <pre># 18. Lefties and Righties in three plots\n\nf = plt.figure(figsize=(15,3))\nax = f.add_subplot(131)\nax=plt.scatter(Left['our.x'],Left['our.y'], s=.01,c='r', marker= '.')\nax2 = f.add_subplot(132)\nax2=plt.scatter(Right['our.x'],Right['our.y'], s=.01,c='b', marker= '.')\nax3 = f.add_subplot(133)\nax3 = plt.scatter(Left['our.x'],Left['our.y'], s=.01,c='r', marker= '.')\nax3 = plt.scatter(Right['our.x'],Right['our.y'], s=.01,c='b', marker= '.')\n</pre> # 18. Lefties and Righties in three plots  f = plt.figure(figsize=(15,3)) ax = f.add_subplot(131) ax=plt.scatter(Left['our.x'],Left['our.y'], s=.01,c='r', marker= '.') ax2 = f.add_subplot(132) ax2=plt.scatter(Right['our.x'],Right['our.y'], s=.01,c='b', marker= '.') ax3 = f.add_subplot(133) ax3 = plt.scatter(Left['our.x'],Left['our.y'], s=.01,c='r', marker= '.') ax3 = plt.scatter(Right['our.x'],Right['our.y'], s=.01,c='b', marker= '.') <p>What's striking about this comparison is that, in aggregate, both lefties and righties are pull hitters, and are able to hit the ball further when pull hitting.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/03.03/#visualizing-sports-data-baseball","title":"Visualizing sports data: baseball\u00b6","text":"<p>In this notebook we're going to look at ways of visualizing performance in baseball. Our data was obtained from MLBAM (Major League Baseball Advanced Media) which make available online play-by-play statistics that can be used for data analytics. The data is produced annually, and in any season there are around 200,000 individual events in Major League Baseball. This data will prove extemely useful in the next course: Moneyball and Beyond.</p> <p>Our analysis here is going to focus on where the ball was hit, which is recorded using (x, y) co-ordinates. This data is  easy to graph and is illuminating.</p> <p>We're going to look at five different ways to visualize the data:</p> <ol> <li>Locations of hits broken down by singles, doubles, triples and home runs (for those not familiar with baseball this refers to base reached by the batter following a hit- first, second or third base, or, in the case of a home run, all run bases to score a run).</li> <li>Locations by hits (where the batter successfully reaches a base) versus outs (where the runner is caught or thrown out).</li> <li>Locations by handedness - to show the difference between lefties and righties.</li> <li>Locations by individual batters- to show how batters differ.</li> <li>Locations by stadium</li> </ol> <p>Graphs which show the distribution of locations are sometimes called \"heatmaps\" - because they show not only location, but the frequency with which a particular location appears. At a later stage we will look at the code for generating a more advanced heatmap, but in this notebook we're going to create very simple ones.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/03.03/#1-a-plot-of-coordinates","title":"1. A plot of coordinates\u00b6","text":"<p>This is a basic scatter plot. Note the of 's = ' to control the size of the markers, 'c = ' to control the color and 'marker =' to control the size of the marker. For an index of colors and marker styles you can visit these websites: https://matplotlib.org/3.1.1/api/markers_api.html and https://matplotlib.org/3.1.0/gallery/color/named_colors.html</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/03.03/#self-test","title":"Self test\u00b6","text":"<p>Use 'x' and 'y' coordinate values to generate the view from the other end of the field.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/03.03/#plotting-hits","title":"Plotting hits\u00b6","text":"<p>A 'hit' in baseball is is when the batter succeeds in getting on base by hitting the ball. A hit might enable the batter to reach first base (a single), second base (a double), third base (a triple) or to round all the bases (a home run). We now map each of these events for the 2018 season.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/03.03/#self-test","title":"Self test\u00b6","text":"<p>Experiment with different sizes, different markers and different colors to find the best effects.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/03.03/#self-test","title":"Self Test\u00b6","text":"<p>Now look at the category of fielding errors (\"Field Error\"). Draw a chart of their location. What do you notice?</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/03.03/#self-test","title":"Self Test\u00b6","text":"<p>Choose a lefty and righty with more average looking statistics - how do their heatmaps compare?</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/03.03/#conclusion","title":"Conclusion\u00b6","text":"<p>We have shown how simply drawing charts can provide considerable insight into performance statistics of all kinds. Charts allow us to see patterns that we might not otherwise notice. Before moving on to more complex methods of analysis, we examine some plots for basketball data.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/04.01/","title":"04.01","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n</pre> import pandas as pd import numpy as np import matplotlib.pyplot as plt In\u00a0[\u00a0]: Copied! <pre>NHL_Team_Stats=pd.read_csv(\"../../Data/Week 4/NHL_Team_Stats.csv\")\nNHL_Team_R_Stats=pd.read_csv(\"../../Data/Week 4/NHL_Team_R_Stats.csv\")\nNHL_Team_Stats.head()\n</pre> NHL_Team_Stats=pd.read_csv(\"../../Data/Week 4/NHL_Team_Stats.csv\") NHL_Team_R_Stats=pd.read_csv(\"../../Data/Week 4/NHL_Team_R_Stats.csv\") NHL_Team_Stats.head() In\u00a0[\u00a0]: Copied! <pre>import statsmodels.formula.api as sm\n</pre> import statsmodels.formula.api as sm <p>At the end of the assignment in week 2, we observed that there is a linear relationship between total goals for and winning percentage. Let\u2019s run a regression where winning percentage is the dependent variable and total goals for is the explanatory variable.</p> <ul> <li>We can use the command \u201cols()\u201d to indicate an ordinary least squared regression.</li> <li>The \u201cfit()\u201d function would allow us to obtain the estimated coefficient of our regression model.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>reg1 = sm.ols(formula = 'win_pct ~ goals_for', data= NHL_Team_R_Stats).fit()\n</pre> reg1 = sm.ols(formula = 'win_pct ~ goals_for', data= NHL_Team_R_Stats).fit() <p>After we run a regression, we can use the \u201csummary()\u201d command to obtain a number of statistics from our regression model.</p> In\u00a0[\u00a0]: Copied! <pre>print(reg1.summary())\n</pre> print(reg1.summary()) In\u00a0[\u00a0]: Copied! <pre>import seaborn as sns\nsns.lmplot(x='goals_against', y='win_pct',  data=NHL_Team_R_Stats)\nplt.xlabel('Total Goals against')\nplt.ylabel('Winning Percentage')\nplt.title(\"Relationship between Goals against and Winning Percentage\", fontsize=20)\n</pre> import seaborn as sns sns.lmplot(x='goals_against', y='win_pct',  data=NHL_Team_R_Stats) plt.xlabel('Total Goals against') plt.ylabel('Winning Percentage') plt.title(\"Relationship between Goals against and Winning Percentage\", fontsize=20) <ul> <li>Calculate the correlation coefficient between goals against and winning percentage</li> </ul> In\u00a0[\u00a0]: Copied! <pre>NHL_Team_R_Stats['goals_against'].corr(NHL_Team_R_Stats['win_pct'])\n</pre> NHL_Team_R_Stats['goals_against'].corr(NHL_Team_R_Stats['win_pct']) <ul> <li>Run a simple linear regression to find NHL team winning percentage as a function of total goals against</li> </ul> In\u00a0[\u00a0]: Copied! <pre>reg2 = sm.ols(formula = 'win_pct ~ goals_against', data= NHL_Team_R_Stats).fit()\nprint(reg2.summary())\n</pre> reg2 = sm.ols(formula = 'win_pct ~ goals_against', data= NHL_Team_R_Stats).fit() print(reg2.summary()) In\u00a0[\u00a0]: Copied! <pre>#Your Code Here\n</pre> #Your Code Here In\u00a0[\u00a0]: Copied! <pre>reg4 = sm.ols(formula = 'win_pct ~ avg_gf+avg_ga', data= NHL_Team_R_Stats).fit()\nprint(reg4.summary())\n</pre> reg4 = sm.ols(formula = 'win_pct ~ avg_gf+avg_ga', data= NHL_Team_R_Stats).fit() print(reg4.summary()) <p>Interpret the coefficients</p> <ul> <li>Average goals for: for the same average number of goals against, scoring one more goal per game will increase the winning percentage by 0.1909 (19.09%)</li> <li>Average goals against: having the same average number of goals for per game, conceding one more goal per game will decrease the winning percentage by 0.1874 (18.74%)</li> </ul> In\u00a0[\u00a0]: Copied! <pre>NHL_Team_Stats['type']=NHL_Team_Stats['type'].astype(object)\n</pre> NHL_Team_Stats['type']=NHL_Team_Stats['type'].astype(object) <p>Now we can run a regression where winning percentage is a function of average goals for and the type of competition.</p> In\u00a0[\u00a0]: Copied! <pre>reg5 = sm.ols(formula = 'win_pct ~ avg_gf+type', data= NHL_Team_Stats).fit()\nprint(reg5.summary())\n</pre> reg5 = sm.ols(formula = 'win_pct ~ avg_gf+type', data= NHL_Team_Stats).fit() print(reg5.summary()) <p>A dummy variable = 1 if type= 3 (playoff) and = 0 if type = 2 (regular season) is included in the regression.</p> <p>Interpretation: with the same average goals for per game, the winning percentage in the playoff games is 0.0160 (1.6%) lower than the winning percentage in the regular season games.</p> In\u00a0[\u00a0]: Copied! <pre>#Your Code Here\n</pre> #Your Code Here In\u00a0[\u00a0]: Copied! <pre>reg7 = sm.ols(formula = 'win_pct ~ avg_gf+type+avg_gf*type', data= NHL_Team_Stats).fit()\nprint(reg7.summary())\n</pre> reg7 = sm.ols(formula = 'win_pct ~ avg_gf+type+avg_gf*type', data= NHL_Team_Stats).fit() print(reg7.summary()) <p>Interpretations</p> <ul> <li>For regular season games (type =2), scoring one more goal per game can increase the winning percentage by 0.2365 (23.65%);</li> <li>For the playoff games (type =3), scoring one more goal per game will increase the winning percentage by 0.2365-0.0802=0.1563 (15.63%).</li> </ul> In\u00a0[\u00a0]: Copied! <pre>#Your Code Here\n</pre> #Your Code Here <ol> <li>Create a scatter plot to show the relationship between Pythagorean winning percentage and the actual winning percentage</li> </ol> In\u00a0[\u00a0]: Copied! <pre>#Your Code Here\n</pre> #Your Code Here <ol> <li>Run a linear regression (reg8) where winning percentage is the dependent variable and Pythagorean winning percentage is the explanatory variable.</li> <li>Interpret the estimate on the Pythagorean winning percentage and the goodness of fit of the regression model.</li> </ol> In\u00a0[\u00a0]: Copied! <pre>#Your Code Here\n</pre> #Your Code Here <ol> <li>Create a scatter plot to show the relationship between winning percentage and Pythagorean winning percentage, seperate the data points by the type of competition.</li> </ol> In\u00a0[\u00a0]: Copied! <pre>#Your Code Here\n</pre> #Your Code Here <ol> <li>Run a regression (reg9) where winning percentage is the dependent variable and Pythagorean winning percentage is the explanatory variable, controlling for the different competitions.</li> <li>Interpret the estimate on the Pythagorean winning percentage and the goodness of fit of the regression model.</li> </ol> In\u00a0[\u00a0]: Copied! <pre>#Your Code Here\n</pre> #Your Code Here <ol> <li>Run a regression (reg10) where winning percentage is the dependent variable and Pythagorean winning percentage, competition, and the interaction between competition and Pythagorean are the explanatory variables</li> <li>Interpret the estimate on the Pythagorean winning percentage and the goodness of fit of the regression model</li> </ol> In\u00a0[\u00a0]: Copied! <pre>#Your Code Here\n</pre> #Your Code Here <ol> <li>Discussion question: how well does Pythagorean winning percentage predicts the actual winning percentage based on our data?</li> </ol>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/04.01/#import-useful-libraries","title":"Import useful libraries\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/04.01/#in-this-section-we-will-use-the-nhl_team_stats-dataset-we-compiled-and-cleaned-up-in-the-assignment-for-week-2","title":"In this section, we will use the NHL_Team_Stats dataset we compiled and cleaned up in the assignment for Week 2.\u00b6","text":"<p>Import both NHL_Team_Stats and NHL_Team_R_Stats data from Week 2 assignment into Python.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/04.01/#regression-analyses-in-python","title":"Regression analyses in Python\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/04.01/#to-run-regressions-in-python-we-will-introduce-a-new-library-statsmodels-which-is-a-python-module-that-provides-classes-and-functions-for-the-estimation-of-many-different-statistical-models-as-well-as-for-conducting-statistical-tests-and-statistical-data-exploration","title":"To run regressions in Python, we will introduce a new library, \u201cstatsmodels,\u201d which is a Python module that provides classes and functions for the estimation of many different statistical models, as well as for conducting statistical tests, and statistical data exploration.\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/04.01/#interpreting-results","title":"Interpreting results\u00b6","text":"<p>From the result table, we can see that the dependent variable is winning percentage (\"win_pct\") and there are 181 observations in this regression. The independent variable is the number of goals for the team (\"goals_for\"). An intercept is also included in the regression.</p> <p>The estimated coefficient on goals_for is 0.003. This means that an additional goal scored by the team will increase the team's winning percentage of 0.003. The estimate on the intercept is -0.1781. This means that without scoring any goal, the winning percentage for the team would be -0.1781. As we know, the winning percentage cannot be negative. The reason we get a negative estimate on the intercept is because in our sample, there is not a single game where a team scored zero goals.</p> <ul> <li>t-statistics and p-value</li> </ul> <p>t-statistics is defined as the estimated coefficient divided by its standard error. If the estimated coefficient is large compared to its standard error, then it is likely to be different than zero.</p> <p>p-value is defined as the probability of obtaining a result as extreme as the result actually observed, in this case, the t-statistics we have in the regression analysis. Comparing the t-statistics with the student t distribution, if 95% of the t distribution is closer to the mean than the t-statistics, we will have p-value of 0.05, which is also referred to a 5% significance level. A p-value no more than 0.05 (5%) is generally accepted in rejecting the null hypothesis. We say that the estimated coefficient is statistically significant at the 5% level.</p> <p>In this regression, the p-value of the goals_for variable is 0.000 which suggests that the estimate is statistically significant at the 1% level.</p> <ul> <li>R-squared</li> </ul> <p>R-squared measures the goodness of fit of the model. The R-squared of a regression is the fraction of the variation in the dependent variable that is accounted for by the independent variables. R-squared is always between 0 and 1. The larger the R-squared, the more variation is accounted for by the regression model.</p> <p>In this regression, the R-squared is 0.591 which means that approximately 59.1% of the variation of the winning percentage is accounted for by the model.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/04.01/#lets-explore-the-relationship-between-goals_against-and-winning-percentage-in-the-regular-season","title":"Let's explore the relationship between goals_against and winning percentage in the regular season.\u00b6","text":"<ul> <li>Create a scatter plot to depict the relationship between total goals against and winning percentage without seperating the data by competition</li> </ul>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/04.01/#self-test-1","title":"Self Test - 1\u00b6","text":"<ol> <li>Use the regular season data, create a scatterplot and a regression line to demonstrate the relationship between average goals for per game and winning percentage.</li> <li>Run a linear regression where winning percentage is the dependent variable and average goals for is the explanatory variable</li> <li>Interpret the coefficient on the average goals for, is this estimate statistically significant?</li> <li>How well does this regression do in fitting the data?</li> </ol>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/04.01/#multiple-regression-more-than-one-explanatory-variables","title":"Multiple Regression -  more than one explanatory variables.\u00b6","text":"<p>Often times, the outcome variable of interest is affected by multiple factors. We can specify a regression equation where the outcome is function of more than one explanatory variables.</p> <p>Let's run a linear regression where winning percentage is a function of both average number of goals for per game and average number of goals against per game.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/04.01/#regression-with-categorical-variables","title":"Regression with categorical variables\u00b6","text":"<p>In the above regressions, we focus on using quantitative variables as explanatory variables. We could also include categorical variables as explanatory variables in the regression as well.</p> <p>Essentially, when we incorporate a categorical variable, we first transform it into dummy variable(s) that carry value of either 0 or 1. We then use the dummy variable(s) into our regression.</p> <p>Let's consider the dataset that includes both regular season and playoff. In this dataset, the variable \"type\" captures whether a game is a regular season game or playoff game. type=2 means it is regular season competition while type=3 means it is a playoff game.</p> <p>We will first convert variable \"type\" into categorical variable.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/04.01/#self-test-2","title":"Self Test - 2\u00b6","text":"<ol> <li><p>Run a regression where winning percentage is a function of average goals for, average goals against, and control for the different competitions.</p> </li> <li><p>Interpret the coefficients.</p> </li> </ol>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/04.01/#regression-with-an-interaction-term","title":"Regression with an interaction term\u00b6","text":"<p>What if the impact of an independent variable depends on the value of another variable? We can use interaction terms to allow for different impact of a variable based on one or more levels of another categorical variable.</p> <p>Let's consider the possibility that the average goals for may have different impact on winning percentage depending on the type of the game. We can run a regression of winning percentage on the average goals for, the type of the game, as well as the interaction between average goals for and type.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/04.01/#self-test-3","title":"Self Test - 3\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/04.01/#perform-a-similar-exercise-to-find-the-relationship-between-the-actual-winning-percentage-and-pythagorean-winning-percentage","title":"Perform a similar exercise to find the relationship between the actual winning percentage and pythagorean winning percentage\u00b6","text":"<ol> <li>In the NHL_Team_Stats data, create the pythagorean winning percentage=goals_for^2/(goals_for^2+goals_against^2), call this new variable \"pyth_pct\" (In Python, ** is the operator for exponentiation. For example, the square of x would be x**2 in Python.)</li> </ol>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/04.02/","title":"Week 4.1 - Introduction to Regression Analysis","text":"In\u00a0[1]: Copied! <pre>#Prerequisite Code\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nNHL_Team_Stats=pd.read_csv(\"../../Data/Week 4/NHL_Team_Stats.csv\")\nNHL_Team_R_Stats=pd.read_csv(\"../../Data/Week 4/NHL_Team_R_Stats.csv\")\nNHL_Team_Stats.head()\nimport statsmodels.formula.api as sm\nreg1 = sm.ols(formula = 'win_pct ~ goals_for', data= NHL_Team_R_Stats).fit()\nimport seaborn as sns\nsns.lmplot(x='goals_against', y='win_pct',  data=NHL_Team_R_Stats)\nplt.xlabel('Total Goals against')\nplt.ylabel('Winning Percentage')\nplt.title(\"Relationship between Goals against and Winning Percentage\", fontsize=20)\nNHL_Team_R_Stats['goals_against'].corr(NHL_Team_R_Stats['win_pct'])\nreg2 = sm.ols(formula = 'win_pct ~ goals_against', data= NHL_Team_R_Stats).fit()\nprint(reg2.summary())\n</pre> #Prerequisite Code import pandas as pd import numpy as np import matplotlib.pyplot as plt NHL_Team_Stats=pd.read_csv(\"../../Data/Week 4/NHL_Team_Stats.csv\") NHL_Team_R_Stats=pd.read_csv(\"../../Data/Week 4/NHL_Team_R_Stats.csv\") NHL_Team_Stats.head() import statsmodels.formula.api as sm reg1 = sm.ols(formula = 'win_pct ~ goals_for', data= NHL_Team_R_Stats).fit() import seaborn as sns sns.lmplot(x='goals_against', y='win_pct',  data=NHL_Team_R_Stats) plt.xlabel('Total Goals against') plt.ylabel('Winning Percentage') plt.title(\"Relationship between Goals against and Winning Percentage\", fontsize=20) NHL_Team_R_Stats['goals_against'].corr(NHL_Team_R_Stats['win_pct']) reg2 = sm.ols(formula = 'win_pct ~ goals_against', data= NHL_Team_R_Stats).fit() print(reg2.summary()) <pre>                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                win_pct   R-squared:                       0.554\nModel:                            OLS   Adj. R-squared:                  0.552\nMethod:                 Least Squares   F-statistic:                     222.6\nDate:                Thu, 02 Sep 2021   Prob (F-statistic):           3.09e-33\nTime:                        20:20:15   Log-Likelihood:                 246.15\nNo. Observations:                 181   AIC:                            -488.3\nDf Residuals:                     179   BIC:                            -481.9\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept         1.1651      0.045     25.839      0.000       1.076       1.254\ngoals_against    -0.0029      0.000    -14.920      0.000      -0.003      -0.003\n==============================================================================\nOmnibus:                        0.581   Durbin-Watson:                   1.647\nProb(Omnibus):                  0.748   Jarque-Bera (JB):                0.688\nSkew:                          -0.125   Prob(JB):                        0.709\nKurtosis:                       2.830   Cond. No.                     2.23e+03\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.23e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n</pre> In\u00a0[2]: Copied! <pre>sns.lmplot(x='avg_gf', y='win_pct',  data=NHL_Team_R_Stats)\nplt.xlabel('Average Goals for per Game')\nplt.ylabel('Winning Percentage')\nplt.title(\"Relationship between Average Goals for and Winning Percentage\", fontsize=20)\n</pre> sns.lmplot(x='avg_gf', y='win_pct',  data=NHL_Team_R_Stats) plt.xlabel('Average Goals for per Game') plt.ylabel('Winning Percentage') plt.title(\"Relationship between Average Goals for and Winning Percentage\", fontsize=20) Out[2]: <pre>Text(0.5, 1.0, 'Relationship between Average Goals for and Winning Percentage')</pre> In\u00a0[3]: Copied! <pre>reg3 = sm.ols(formula = 'win_pct ~ avg_gf', data= NHL_Team_R_Stats).fit()\nprint(reg3.summary())\n</pre> reg3 = sm.ols(formula = 'win_pct ~ avg_gf', data= NHL_Team_R_Stats).fit() print(reg3.summary()) <pre>                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                win_pct   R-squared:                       0.573\nModel:                            OLS   Adj. R-squared:                  0.571\nMethod:                 Least Squares   F-statistic:                     240.1\nDate:                Thu, 02 Sep 2021   Prob (F-statistic):           6.66e-35\nTime:                        20:20:16   Log-Likelihood:                 250.01\nNo. Observations:                 181   AIC:                            -496.0\nDf Residuals:                     179   BIC:                            -489.6\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -0.1804      0.044     -4.111      0.000      -0.267      -0.094\navg_gf         0.2378      0.015     15.496      0.000       0.208       0.268\n==============================================================================\nOmnibus:                        1.181   Durbin-Watson:                   1.710\nProb(Omnibus):                  0.554   Jarque-Bera (JB):                1.272\nSkew:                          -0.149   Prob(JB):                        0.529\nKurtosis:                       2.718   Cond. No.                         31.0\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n</pre> In\u00a0[4]: Copied! <pre>#Prerequisite Code\nNHL_Team_Stats['type']=NHL_Team_Stats['type'].astype(object)\nreg5 = sm.ols(formula = 'win_pct ~ avg_gf+type', data= NHL_Team_Stats).fit()\nprint(reg5.summary())\n</pre> #Prerequisite Code NHL_Team_Stats['type']=NHL_Team_Stats['type'].astype(object) reg5 = sm.ols(formula = 'win_pct ~ avg_gf+type', data= NHL_Team_Stats).fit() print(reg5.summary()) <pre>                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                win_pct   R-squared:                       0.426\nModel:                            OLS   Adj. R-squared:                  0.423\nMethod:                 Least Squares   F-statistic:                     136.0\nDate:                Thu, 02 Sep 2021   Prob (F-statistic):           6.88e-45\nTime:                        20:20:16   Log-Likelihood:                 320.28\nNo. Observations:                 369   AIC:                            -634.6\nDf Residuals:                     366   BIC:                            -622.8\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -0.0197      0.035     -0.558      0.577      -0.089       0.050\ntype[T.3]     -0.0160      0.012     -1.344      0.180      -0.039       0.007\navg_gf         0.1818      0.012     14.914      0.000       0.158       0.206\n==============================================================================\nOmnibus:                       63.422   Durbin-Watson:                   1.880\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              156.332\nSkew:                          -0.843   Prob(JB):                     1.13e-34\nKurtosis:                       5.707   Cond. No.                         20.9\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n</pre> In\u00a0[5]: Copied! <pre>reg6 = sm.ols(formula = 'win_pct ~ avg_gf+avg_ga+competition_name', data= NHL_Team_Stats).fit()\nprint(reg6.summary())\n</pre> reg6 = sm.ols(formula = 'win_pct ~ avg_gf+avg_ga+competition_name', data= NHL_Team_Stats).fit() print(reg6.summary()) <pre>                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                win_pct   R-squared:                       0.782\nModel:                            OLS   Adj. R-squared:                  0.771\nMethod:                 Least Squares   F-statistic:                     73.95\nDate:                Thu, 02 Sep 2021   Prob (F-statistic):          9.02e-105\nTime:                        20:20:17   Log-Likelihood:                 498.58\nNo. Observations:                 369   AIC:                            -961.2\nDf Residuals:                     351   BIC:                            -890.8\nDf Model:                          17                                         \nCovariance Type:            nonrobust                                         \n===============================================================================================================\n                                                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------------------------------------\nIntercept                                       0.4011      0.034     11.688      0.000       0.334       0.469\ncompetition_name[T.2010 NHL Regular Season]     0.0263      0.020      1.322      0.187      -0.013       0.066\ncompetition_name[T.2011 NHL Playoff]            0.0134      0.023      0.584      0.560      -0.032       0.059\ncompetition_name[T.2011 NHL Regular Season]     0.0304      0.020      1.526      0.128      -0.009       0.070\ncompetition_name[T.2012 NHL Playoff]            0.0321      0.023      1.403      0.161      -0.013       0.077\ncompetition_name[T.2012 NHL Regular Season]     0.0322      0.020      1.617      0.107      -0.007       0.071\ncompetition_name[T.2013 NHL Playoff]            0.0123      0.023      0.540      0.590      -0.032       0.057\ncompetition_name[T.2013 NHL Regular Season]     0.0309      0.020      1.550      0.122      -0.008       0.070\ncompetition_name[T.2014 NHL Playoff]           -0.0030      0.023     -0.133      0.894      -0.048       0.042\ncompetition_name[T.2014 NHL Regular Season]     0.0298      0.020      1.494      0.136      -0.009       0.069\ncompetition_name[T.2015 NHL Playoff]            0.0203      0.023      0.890      0.374      -0.025       0.065\ncompetition_name[T.2015 NHL Regular Season]     0.0301      0.020      1.511      0.132      -0.009       0.069\ncompetition_name[T.2016 NHL Playoff]           -0.0138      0.023     -0.600      0.549      -0.059       0.031\ncompetition_name[T.2016 NHL Regular Season]     0.0285      0.020      1.429      0.154      -0.011       0.068\ncompetition_name[T.2017 NHL Playoff]            0.0228      0.023      1.002      0.317      -0.022       0.068\ncompetition_name[T.2017 NHL Regular Season]     0.0226      0.020      1.135      0.257      -0.017       0.062\navg_gf                                          0.1928      0.008     24.391      0.000       0.177       0.208\navg_ga                                         -0.1693      0.007    -23.064      0.000      -0.184      -0.155\n==============================================================================\nOmnibus:                       49.542   Durbin-Watson:                   2.086\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              169.228\nSkew:                          -0.551   Prob(JB):                     1.79e-37\nKurtosis:                       6.129   Cond. No.                         81.5\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n</pre> In\u00a0[6]: Copied! <pre>#Prerequisite Code\nreg7 = sm.ols(formula = 'win_pct ~ avg_gf+type+avg_gf*type', data= NHL_Team_Stats).fit()\nprint(reg7.summary())\n</pre> #Prerequisite Code reg7 = sm.ols(formula = 'win_pct ~ avg_gf+type+avg_gf*type', data= NHL_Team_Stats).fit() print(reg7.summary()) <pre>                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                win_pct   R-squared:                       0.441\nModel:                            OLS   Adj. R-squared:                  0.436\nMethod:                 Least Squares   F-statistic:                     96.00\nDate:                Thu, 02 Sep 2021   Prob (F-statistic):           8.01e-46\nTime:                        20:20:17   Log-Likelihood:                 325.08\nNo. Observations:                 369   AIC:                            -642.2\nDf Residuals:                     365   BIC:                            -626.5\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n====================================================================================\n                       coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------------\nIntercept           -0.1748      0.061     -2.868      0.004      -0.295      -0.055\ntype[T.3]            0.2029      0.072      2.835      0.005       0.062       0.344\navg_gf               0.2365      0.021     11.081      0.000       0.194       0.278\navg_gf:type[T.3]    -0.0802      0.026     -3.102      0.002      -0.131      -0.029\n==============================================================================\nOmnibus:                       59.042   Durbin-Watson:                   1.801\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              145.437\nSkew:                          -0.787   Prob(JB):                     2.62e-32\nKurtosis:                       5.643   Cond. No.                         56.6\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n</pre> In\u00a0[7]: Copied! <pre>NHL_Team_Stats['pyth_pct']=NHL_Team_Stats['goals_for']**2/(NHL_Team_Stats['goals_for']**2+NHL_Team_Stats['goals_against']**2)\n</pre> NHL_Team_Stats['pyth_pct']=NHL_Team_Stats['goals_for']**2/(NHL_Team_Stats['goals_for']**2+NHL_Team_Stats['goals_against']**2) <ol> <li>Create a scatter plot to show the relationship between Pythagorean winning percentage and the actual winning percentage</li> </ol> In\u00a0[8]: Copied! <pre>sns.lmplot(x='pyth_pct', y='win_pct',  data=NHL_Team_Stats)\nplt.xlabel('Pythagorean Winning Percentage')\nplt.ylabel('Winning Percentage')\nplt.title(\"Relationship between Pythagorean Winning Percentage and Winning Percentage\", fontsize=20)\n</pre> sns.lmplot(x='pyth_pct', y='win_pct',  data=NHL_Team_Stats) plt.xlabel('Pythagorean Winning Percentage') plt.ylabel('Winning Percentage') plt.title(\"Relationship between Pythagorean Winning Percentage and Winning Percentage\", fontsize=20) Out[8]: <pre>Text(0.5, 1.0, 'Relationship between Pythagorean Winning Percentage and Winning Percentage')</pre> <ol> <li>Run a linear regression (reg8) where winning percentage is the dependent variable and Pythagorean winning percentage is the explanatory variable.</li> <li>Interpret the estimate on the Pythagorean winning percentage and the goodness of fit of the regression model.</li> </ol> In\u00a0[9]: Copied! <pre>reg8 = sm.ols(formula = 'win_pct ~ pyth_pct', data= NHL_Team_Stats).fit()\nprint(reg8.summary())\n</pre> reg8 = sm.ols(formula = 'win_pct ~ pyth_pct', data= NHL_Team_Stats).fit() print(reg8.summary()) <pre>                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                win_pct   R-squared:                       0.779\nModel:                            OLS   Adj. R-squared:                  0.779\nMethod:                 Least Squares   F-statistic:                     1297.\nDate:                Thu, 02 Sep 2021   Prob (F-statistic):          1.65e-122\nTime:                        20:20:18   Log-Likelihood:                 496.63\nNo. Observations:                 369   AIC:                            -989.3\nDf Residuals:                     367   BIC:                            -981.4\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -0.0447      0.015     -3.052      0.002      -0.074      -0.016\npyth_pct       1.0673      0.030     36.011      0.000       1.009       1.126\n==============================================================================\nOmnibus:                       86.393   Durbin-Watson:                   2.032\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              317.333\nSkew:                          -0.989   Prob(JB):                     1.24e-69\nKurtosis:                       7.090   Cond. No.                         11.1\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n</pre> <ol> <li>Create a scatter plot to show the relationship between winning percentage and Pythagorean winning percentage, seperate the data points by the type of competition.</li> </ol> In\u00a0[10]: Copied! <pre>sns.lmplot(x='pyth_pct', y='win_pct', hue='competition_name',  data=NHL_Team_Stats)\nplt.xlabel('Pythagorean Winning Percentage')\nplt.ylabel('Winning Percentage')\nplt.title(\"Relationship between Pythagorean Winning Percentage and Winning Percentage\", fontsize=20)\n</pre> sns.lmplot(x='pyth_pct', y='win_pct', hue='competition_name',  data=NHL_Team_Stats) plt.xlabel('Pythagorean Winning Percentage') plt.ylabel('Winning Percentage') plt.title(\"Relationship between Pythagorean Winning Percentage and Winning Percentage\", fontsize=20) Out[10]: <pre>Text(0.5, 1.0, 'Relationship between Pythagorean Winning Percentage and Winning Percentage')</pre> <ol> <li>Run a regression (reg9) where winning percentage is the dependent variable and Pythagorean winning percentage is the explanatory variable, controlling for the different competitions.</li> <li>Interpret the estimate on the Pythagorean winning percentage and the goodness of fit of the regression model.</li> </ol> In\u00a0[11]: Copied! <pre>reg9 = sm.ols(formula = 'win_pct ~ pyth_pct+competition_name', data= NHL_Team_Stats).fit()\nprint(reg9.summary())\n</pre> reg9 = sm.ols(formula = 'win_pct ~ pyth_pct+competition_name', data= NHL_Team_Stats).fit() print(reg9.summary()) <pre>                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                win_pct   R-squared:                       0.789\nModel:                            OLS   Adj. R-squared:                  0.780\nMethod:                 Least Squares   F-statistic:                     82.48\nDate:                Thu, 02 Sep 2021   Prob (F-statistic):          1.94e-108\nTime:                        20:20:23   Log-Likelihood:                 505.19\nNo. Observations:                 369   AIC:                            -976.4\nDf Residuals:                     352   BIC:                            -909.9\nDf Model:                          16                                         \nCovariance Type:            nonrobust                                         \n===============================================================================================================\n                                                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------------------------------------\nIntercept                                      -0.0607      0.021     -2.870      0.004      -0.102      -0.019\ncompetition_name[T.2010 NHL Regular Season]     0.0305      0.020      1.563      0.119      -0.008       0.069\ncompetition_name[T.2011 NHL Playoff]            0.0147      0.022      0.659      0.510      -0.029       0.058\ncompetition_name[T.2011 NHL Regular Season]     0.0359      0.020      1.838      0.067      -0.003       0.074\ncompetition_name[T.2012 NHL Playoff]            0.0251      0.022      1.125      0.261      -0.019       0.069\ncompetition_name[T.2012 NHL Regular Season]     0.0348      0.020      1.782      0.076      -0.004       0.073\ncompetition_name[T.2013 NHL Playoff]            0.0158      0.022      0.711      0.478      -0.028       0.060\ncompetition_name[T.2013 NHL Regular Season]     0.0347      0.020      1.778      0.076      -0.004       0.073\ncompetition_name[T.2014 NHL Playoff]           -0.0015      0.022     -0.068      0.946      -0.045       0.042\ncompetition_name[T.2014 NHL Regular Season]     0.0331      0.020      1.693      0.091      -0.005       0.072\ncompetition_name[T.2015 NHL Playoff]            0.0215      0.022      0.966      0.335      -0.022       0.065\ncompetition_name[T.2015 NHL Regular Season]     0.0330      0.020      1.689      0.092      -0.005       0.071\ncompetition_name[T.2016 NHL Playoff]           -0.0156      0.022     -0.699      0.485      -0.059       0.028\ncompetition_name[T.2016 NHL Regular Season]     0.0330      0.020      1.691      0.092      -0.005       0.071\ncompetition_name[T.2017 NHL Playoff]            0.0251      0.022      1.122      0.263      -0.019       0.069\ncompetition_name[T.2017 NHL Regular Season]     0.0316      0.019      1.626      0.105      -0.007       0.070\npyth_pct                                        1.0477      0.030     34.383      0.000       0.988       1.108\n==============================================================================\nOmnibus:                       46.828   Durbin-Watson:                   2.117\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              190.341\nSkew:                          -0.449   Prob(JB):                     4.66e-42\nKurtosis:                       6.402   Cond. No.                         22.4\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n</pre> <ol> <li>Run a regression (reg10) where winning percentage is the dependent variable and Pythagorean winning percentage, competition, and the interaction between competition and Pythagorean are the explanatory variables</li> <li>Interpret the estimate on the Pythagorean winning percentage and the goodness of fit of the regression model</li> </ol> In\u00a0[12]: Copied! <pre>reg10 = sm.ols(formula = 'win_pct ~ pyth_pct+competition_name+pyth_pct*competition_name', data= NHL_Team_Stats).fit()\nprint(reg10.summary())\n</pre> reg10 = sm.ols(formula = 'win_pct ~ pyth_pct+competition_name+pyth_pct*competition_name', data= NHL_Team_Stats).fit() print(reg10.summary()) <pre>                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                win_pct   R-squared:                       0.806\nModel:                            OLS   Adj. R-squared:                  0.788\nMethod:                 Least Squares   F-statistic:                     45.21\nDate:                Thu, 02 Sep 2021   Prob (F-statistic):          4.32e-101\nTime:                        20:20:23   Log-Likelihood:                 520.45\nNo. Observations:                 369   AIC:                            -976.9\nDf Residuals:                     337   BIC:                            -851.8\nDf Model:                          31                                         \nCovariance Type:            nonrobust                                         \n========================================================================================================================\n                                                           coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------------------------------------------------\nIntercept                                                0.0078      0.053      0.147      0.884      -0.097       0.113\ncompetition_name[T.2010 NHL Regular Season]              0.0025      0.091      0.027      0.978      -0.176       0.181\ncompetition_name[T.2011 NHL Playoff]                    -0.1367      0.081     -1.682      0.093      -0.296       0.023\ncompetition_name[T.2011 NHL Regular Season]              0.0249      0.077      0.321      0.748      -0.128       0.177\ncompetition_name[T.2012 NHL Playoff]                    -0.0185      0.070     -0.264      0.792      -0.157       0.120\ncompetition_name[T.2012 NHL Regular Season]             -0.0918      0.086     -1.065      0.287      -0.261       0.078\ncompetition_name[T.2013 NHL Playoff]                    -0.1095      0.082     -1.331      0.184      -0.271       0.052\ncompetition_name[T.2013 NHL Regular Season]             -0.0564      0.085     -0.665      0.507      -0.223       0.111\ncompetition_name[T.2014 NHL Playoff]                    -0.2583      0.091     -2.833      0.005      -0.438      -0.079\ncompetition_name[T.2014 NHL Regular Season]             -0.0426      0.083     -0.516      0.606      -0.205       0.120\ncompetition_name[T.2015 NHL Playoff]                     0.1065      0.071      1.491      0.137      -0.034       0.247\ncompetition_name[T.2015 NHL Regular Season]             -0.0887      0.103     -0.859      0.391      -0.292       0.114\ncompetition_name[T.2016 NHL Playoff]                    -0.1098      0.071     -1.537      0.125      -0.250       0.031\ncompetition_name[T.2016 NHL Regular Season]             -0.0254      0.084     -0.303      0.762      -0.191       0.140\ncompetition_name[T.2017 NHL Playoff]                    -0.0485      0.067     -0.729      0.467      -0.179       0.082\ncompetition_name[T.2017 NHL Regular Season]             -0.1513      0.089     -1.695      0.091      -0.327       0.024\npyth_pct                                                 0.9000      0.110      8.175      0.000       0.683       1.117\npyth_pct:competition_name[T.2010 NHL Regular Season]     0.0669      0.182      0.367      0.714      -0.292       0.425\npyth_pct:competition_name[T.2011 NHL Playoff]            0.3296      0.170      1.933      0.054      -0.006       0.665\npyth_pct:competition_name[T.2011 NHL Regular Season]     0.0328      0.156      0.211      0.833      -0.273       0.339\npyth_pct:competition_name[T.2012 NHL Playoff]            0.0908      0.148      0.615      0.539      -0.200       0.381\npyth_pct:competition_name[T.2012 NHL Regular Season]     0.2638      0.173      1.526      0.128      -0.076       0.604\npyth_pct:competition_name[T.2013 NHL Playoff]            0.2732      0.174      1.575      0.116      -0.068       0.615\npyth_pct:competition_name[T.2013 NHL Regular Season]     0.1929      0.170      1.132      0.258      -0.142       0.528\npyth_pct:competition_name[T.2014 NHL Playoff]            0.5547      0.191      2.902      0.004       0.179       0.931\npyth_pct:competition_name[T.2014 NHL Regular Season]     0.1619      0.166      0.978      0.329      -0.164       0.488\npyth_pct:competition_name[T.2015 NHL Playoff]           -0.1966      0.149     -1.317      0.189      -0.490       0.097\npyth_pct:competition_name[T.2015 NHL Regular Season]     0.2540      0.207      1.227      0.221      -0.153       0.661\npyth_pct:competition_name[T.2016 NHL Playoff]            0.2059      0.150      1.374      0.170      -0.089       0.501\npyth_pct:competition_name[T.2016 NHL Regular Season]     0.1276      0.169      0.756      0.450      -0.205       0.460\npyth_pct:competition_name[T.2017 NHL Playoff]            0.1597      0.141      1.133      0.258      -0.118       0.437\npyth_pct:competition_name[T.2017 NHL Regular Season]     0.3767      0.179      2.100      0.036       0.024       0.730\n==============================================================================\nOmnibus:                       56.671   Durbin-Watson:                   2.196\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              209.699\nSkew:                          -0.618   Prob(JB):                     2.91e-46\nKurtosis:                       6.480   Cond. No.                         181.\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n</pre> <ol> <li>Discussion question: how well does Pythagorean winning percentage predicts the actual winning percentage based on our data?</li> </ol>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/04.02/#week-41-introduction-to-regression-analysis","title":"Week 4.1 - Introduction to Regression Analysis\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/04.02/#self-test-1-solution","title":"Self Test - 1 Solution\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/04.02/#self-test-2-solution","title":"Self Test - 2 Solution\u00b6","text":"<ol> <li><p>Run a regression where winning percentage is a function of average goals for, average goals against, and control for the different competitions.</p> </li> <li><p>Interpret the coefficients.</p> </li> </ol>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/04.02/#self-test-3-solution","title":"Self Test - 3 Solution\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/04.02/#perform-a-similar-exercise-to-find-the-relationship-between-the-actual-winning-percentage-and-pythagorean-winning-percentage","title":"Perform a similar exercise to find the relationship between the actual winning percentage and pythagorean winning percentage\u00b6","text":"<ol> <li>In the NHL_Team_Stats data, create the pythagorean winning percentage=goals_for^2/(goals_for^2+goals_against^2), call this new variable \"pyth_pct\" (In Python, ** is the operator for exponentiation. For example, the square of x would be x**2 in Python.)</li> </ol>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/04.03/","title":"Regression Analyses with Cricket Data","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.formula.api as sm\n</pre> import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import statsmodels.formula.api as sm In\u00a0[\u00a0]: Copied! <pre>IPLPlayer=pd.read_csv(\"../../Data/Week 4/IPL18Player.csv\")\nIPLPlayer.head()\n</pre> IPLPlayer=pd.read_csv(\"../../Data/Week 4/IPL18Player.csv\") IPLPlayer.head() In\u00a0[\u00a0]: Copied! <pre>IPLPlayer.shape\n</pre> IPLPlayer.shape In\u00a0[\u00a0]: Copied! <pre>IPLPlayer.info()\n</pre> IPLPlayer.info() <p>There are missing values in the salary variable. We will drop observations with missing values.</p> In\u00a0[\u00a0]: Copied! <pre>IPLPlayer=IPLPlayer.dropna()\nIPLPlayer.shape\n</pre> IPLPlayer=IPLPlayer.dropna() IPLPlayer.shape In\u00a0[\u00a0]: Copied! <pre>IPLPlayer['batsman']=np.where(IPLPlayer['innings']&gt; 0, 1, 0)\nIPLPlayer['batsman'].describe()\n</pre> IPLPlayer['batsman']=np.where(IPLPlayer['innings']&gt; 0, 1, 0) IPLPlayer['batsman'].describe() <ul> <li>Create a variable to indicate bowler.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>IPLPlayer['bowler']=np.where(IPLPlayer['matches_bowled']&gt; 0, 1, 0)\nIPLPlayer['bowler'].describe()\n</pre> IPLPlayer['bowler']=np.where(IPLPlayer['matches_bowled']&gt; 0, 1, 0) IPLPlayer['bowler'].describe() <p>The last type of player that is not captured by either batsman or bowler is wicket keeper. In the dataset, the variable \"matches_keeper\" indicates the number of matches that a player is a wicket keeper.</p> In\u00a0[\u00a0]: Copied! <pre>IPLPlayer['outs']=np.where(IPLPlayer['batsman']==1, IPLPlayer['innings']-IPLPlayer['not_outs'], 0)\nIPLPlayer['outs'].describe()\n</pre> IPLPlayer['outs']=np.where(IPLPlayer['batsman']==1, IPLPlayer['innings']-IPLPlayer['not_outs'], 0) IPLPlayer['outs'].describe() <p>Create batting average, batting strke rate, bowling average, and bowling strike rate variables. Add 1 to the number of outs, balls faced, andn wickets taken in calculating these variables.</p> In\u00a0[\u00a0]: Copied! <pre>IPLPlayer['batting_average']=IPLPlayer['runs']/(IPLPlayer['outs']+1)\nIPLPlayer['batting_strike']=IPLPlayer['runs']/((IPLPlayer['balls_faced']+1))*100\nIPLPlayer['bowling_average']=IPLPlayer['runs_conceded']/(IPLPlayer['wickets']+1)\nIPLPlayer['bowling_strike']=IPLPlayer['balls_bowled']/(IPLPlayer['wickets']+1)\n</pre> IPLPlayer['batting_average']=IPLPlayer['runs']/(IPLPlayer['outs']+1) IPLPlayer['batting_strike']=IPLPlayer['runs']/((IPLPlayer['balls_faced']+1))*100 IPLPlayer['bowling_average']=IPLPlayer['runs_conceded']/(IPLPlayer['wickets']+1) IPLPlayer['bowling_strike']=IPLPlayer['balls_bowled']/(IPLPlayer['wickets']+1) In\u00a0[\u00a0]: Copied! <pre>IPLPlayer['batting_average'].describe()\n</pre> IPLPlayer['batting_average'].describe() In\u00a0[\u00a0]: Copied! <pre>IPLPlayer['batting_strike'].describe()\n</pre> IPLPlayer['batting_strike'].describe() In\u00a0[\u00a0]: Copied! <pre>IPLPlayer['bowling_average'].describe()\n</pre> IPLPlayer['bowling_average'].describe() In\u00a0[\u00a0]: Copied! <pre>IPLPlayer['bowling_strike'].describe()\n</pre> IPLPlayer['bowling_strike'].describe() In\u00a0[\u00a0]: Copied! <pre>reg_IPL1=sm.ols(formula = 'Salary ~ batsman+ bowler+ batsman*bowler', data= IPLPlayer, missing=\"drop\").fit()\nprint(reg_IPL1.summary())\n</pre> reg_IPL1=sm.ols(formula = 'Salary ~ batsman+ bowler+ batsman*bowler', data= IPLPlayer, missing=\"drop\").fit() print(reg_IPL1.summary()) In\u00a0[\u00a0]: Copied! <pre>reg_IPL2=sm.ols(formula = 'Salary ~ runs', data= IPLPlayer).fit()\nprint(reg_IPL2.summary())\n</pre> reg_IPL2=sm.ols(formula = 'Salary ~ runs', data= IPLPlayer).fit() print(reg_IPL2.summary()) In\u00a0[\u00a0]: Copied! <pre>reg_IPL3=sm.ols(formula = 'Salary ~ runs+not_outs', data= IPLPlayer).fit()\nprint(reg_IPL3.summary())\n</pre> reg_IPL3=sm.ols(formula = 'Salary ~ runs+not_outs', data= IPLPlayer).fit() print(reg_IPL3.summary()) In\u00a0[\u00a0]: Copied! <pre>reg_IPL4=sm.ols(formula = 'Salary ~ runs+not_outs+balls_faced', data= IPLPlayer).fit()\nprint(reg_IPL4.summary())\n</pre> reg_IPL4=sm.ols(formula = 'Salary ~ runs+not_outs+balls_faced', data= IPLPlayer).fit() print(reg_IPL4.summary()) <p>In the next regressions, we will use the modified batting average and batting strike variables to measure player performance.</p> In\u00a0[\u00a0]: Copied! <pre>reg_IPL5=sm.ols(formula = 'Salary ~ batting_average', data= IPLPlayer).fit()\nprint(reg_IPL5.summary())\n</pre> reg_IPL5=sm.ols(formula = 'Salary ~ batting_average', data= IPLPlayer).fit() print(reg_IPL5.summary()) In\u00a0[\u00a0]: Copied! <pre>reg_IPL6=sm.ols(formula = 'Salary ~ batting_average+batting_strike', data= IPLPlayer).fit()\nprint(reg_IPL6.summary())\n</pre> reg_IPL6=sm.ols(formula = 'Salary ~ batting_average+batting_strike', data= IPLPlayer).fit() print(reg_IPL6.summary()) In\u00a0[\u00a0]: Copied! <pre>reg_IPL7=sm.ols(formula = 'Salary ~ runs_conceded', data= IPLPlayer).fit()\nprint(reg_IPL7.summary())\n</pre> reg_IPL7=sm.ols(formula = 'Salary ~ runs_conceded', data= IPLPlayer).fit() print(reg_IPL7.summary()) In\u00a0[\u00a0]: Copied! <pre>reg_IPL8=sm.ols(formula = 'Salary ~ runs_conceded+balls_bowled', data= IPLPlayer).fit()\nprint(reg_IPL8.summary())\n</pre> reg_IPL8=sm.ols(formula = 'Salary ~ runs_conceded+balls_bowled', data= IPLPlayer).fit() print(reg_IPL8.summary()) In\u00a0[\u00a0]: Copied! <pre>reg_IPL9=sm.ols(formula = 'Salary ~ runs_conceded+balls_bowled+wickets', data= IPLPlayer).fit()\nprint(reg_IPL9.summary())\n</pre> reg_IPL9=sm.ols(formula = 'Salary ~ runs_conceded+balls_bowled+wickets', data= IPLPlayer).fit() print(reg_IPL9.summary()) <p>In the next regression, we will use the modified bowling average and bowling strike variables to measure player performance.</p> In\u00a0[\u00a0]: Copied! <pre>reg_IPL10=sm.ols(formula = 'Salary ~ bowling_average+bowling_strike', data= IPLPlayer).fit()\nprint(reg_IPL10.summary())\n</pre> reg_IPL10=sm.ols(formula = 'Salary ~ bowling_average+bowling_strike', data= IPLPlayer).fit() print(reg_IPL10.summary()) In\u00a0[\u00a0]: Copied! <pre>reg_IPL11=sm.ols(formula = 'Salary ~ runs+not_outs+balls_faced+runs_conceded+balls_bowled+wickets', data= IPLPlayer).fit()\nprint(reg_IPL11.summary())\n</pre> reg_IPL11=sm.ols(formula = 'Salary ~ runs+not_outs+balls_faced+runs_conceded+balls_bowled+wickets', data= IPLPlayer).fit() print(reg_IPL11.summary()) <p>We will also use the modified batting average, batting strike, bowling average, and bowling strike variables to measure the player performance.</p> In\u00a0[\u00a0]: Copied! <pre>reg_IPL12=sm.ols(formula = 'Salary ~ batting_average+batting_strike+bowling_average+bowling_strike', data= IPLPlayer).fit()\nprint(reg_IPL12.summary())\n</pre> reg_IPL12=sm.ols(formula = 'Salary ~ batting_average+batting_strike+bowling_average+bowling_strike', data= IPLPlayer).fit() print(reg_IPL12.summary()) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/04.03/#regression-analyses-with-cricket-data","title":"Regression Analyses with Cricket Data\u00b6","text":"<p>In week 1, we took a brief look at the cricket match of statistics of the Indian Premier league in 2018 (IPL2018teams dataset). In this week, we will look at the player level statistics. In particular, we are interested in whether the player performance impact their salaries.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/04.03/#import-useful-libraries","title":"Import useful libraries\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/04.03/#import-cricket-data","title":"Import cricket data\u00b6","text":"<p>In our data repository, there is a data set \u201cIPL18Player.csv\u201d which contains performance statistics as well as salary information of cricket players in the Indian Premier League in 2018.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/04.03/#data-exploration-and-preparation","title":"Data Exploration and Preparation\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/04.03/#missing-values","title":"Missing Values\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/04.03/#create-useful-variables","title":"Create useful variables\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/04.03/#create-dummy-variables-to-indicate-the-role-of-the-players","title":"Create dummy variables to indicate the role of the players.\u00b6","text":"<ul> <li>Create a variable to indicate whether a player had played as a batsman.</li> </ul> <p>The variable \"innings\" indicates how many innings a player had batted in.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/04.03/#performance-measures","title":"Performance Measures\u00b6","text":"<ol> <li>batting average = runs / the numbers of outs</li> <li>batting strike rate = (runs * 100) / balls faced</li> <li>bowling average = runs conceded / wicket taken</li> <li>bowling strike rate = number of balls bowled / wicket taken</li> </ol> <p>Notice that if a batsman has scored runs but not been dismissed, his batting average is technically infinite. Similarly, if a player did not face any ball, his batting strike would be infinite and if a player did not lose any wicket, his bowling average or bowling strike would be infinite.</p> <p>We will not be able to run a regression when our variables have some infinite values.</p> <p>There are two alternatives we will consider to deal with this issue.</p> <ol> <li>Add 1 to the number of outs, balls faced, andn wickets taken in calculating the above variables.</li> <li>Instead of creating the above measures, we can simply include total runs, total number of outs, and balls faced to measure a batsman's performance, and include runs conceded, number of balls bowled, and wickets taken to measure a bowler's performance.</li> </ol>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/04.03/#regression-analyses","title":"Regression Analyses\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/04.03/#first-lets-run-a-regression-of-the-salary-on-the-type-of-player-batsman-bowler-and-all-rounder","title":"First let's run a regression of the salary on the type of player, batsman, bowler, and all-rounder.\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/04.03/#next-we-will-first-focus-on-performance-of-batsman","title":"Next we will first focus on performance of batsman.\u00b6","text":"<p>We will first simply use the total number of runs, number of not outs, and number of balls faced to measure players\u2019 performance.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/04.03/#we-will-now-turn-to-bowlers-performance","title":"We will now turn to bowlers' performance.\u00b6","text":"<p>Again, we will first use number of runs conceded, number of balls bowled, and number of wickets taken to measure bowlers' performance.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/04.03/#lastly-we-will-incorporate-performance-measures-of-both-batsman-and-bowler-in-the-same-regression","title":"Lastly, we will incorporate performance measures of both batsman and bowler in the same regression.\u00b6","text":"<p>We will first use the original variables, total number of runs, number of not outs, number of balls faced, number of runs conceded, number of balls bowled, and number of wickets in the regression.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/04.03/#self-test","title":"Self Test\u00b6","text":"<ul> <li>Run a regression of salary as a function of the interaction of batsman and runs and the interaction of bowler and wickets taken.</li> <li>Interpret your regression results.</li> </ul>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/05.01/","title":"The Salary-Performance Relationship in the English Premier League","text":"In\u00a0[\u00a0]: Copied! <pre># As usual, we begin by loading the packages we will need\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.formula.api as smf\n</pre> # As usual, we begin by loading the packages we will need  import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import statsmodels.formula.api as smf In\u00a0[\u00a0]: Copied! <pre># Now we load the data\n\nEPL=pd.read_excel(\"../../Data/Week 5/EPL pay and performance.xlsx\")\n</pre> # Now we load the data  EPL=pd.read_excel(\"../../Data/Week 5/EPL pay and performance.xlsx\") <p>We use .describe() to look at the summary statistics for the data. From this we can see that we have 380 observations, for teams running from 1997 to 2015 (19 seasons). Our two main variables of interest are win percentage and team salaries. We also include a dummy variable for whether the team had been promoted that season. We can also use .info() to summarize the dataframe.</p> In\u00a0[\u00a0]: Copied! <pre>EPL.describe()\n</pre> EPL.describe() In\u00a0[\u00a0]: Copied! <pre>EPL.info()\n</pre> EPL.info() <p>We use .groupby to sum salaries. Note the way we use .reset_index and .rename(columns ={}) to make the data manageable.</p> In\u00a0[\u00a0]: Copied! <pre>Sumsal = EPL.groupby(['Season_ending'])['salaries'].sum().reset_index().rename(columns={'salaries':'allsal'})\nSumsal\n</pre> Sumsal = EPL.groupby(['Season_ending'])['salaries'].sum().reset_index().rename(columns={'salaries':'allsal'}) Sumsal <p>For numbers that have many digits, Python will typically use scientific notation to represent them. If you would rather see the data in standard decimal format you can use \"pd.options.display.float_format = \". In the line below we show the salary data with no decimal places- '{:.0f}' - if you wanted the data to one decimal place you would write '{:.1f}' - this way you can format the data to as many decimal places as you prefer. Another option, if you would prefer to see the numbers in a format that is easier to read from the screen-in this case would be to divide allsal by 1,000,000.</p> In\u00a0[\u00a0]: Copied! <pre>pd.options.display.float_format = '{:.0f}'.format\nSumsal\n</pre> pd.options.display.float_format = '{:.0f}'.format Sumsal <p>As with the NBA, the sharp upward trend in total salaries is clearly visible. allsal increased from \u00a3220 million in 1997 to \u00a32031 million in 2015. In each season we want to compare team spending relative to the average of that season.</p> <p>To do this we now merge the aggregate salaries back in to the main dataframe and then divide the team's salary bill in each year by allsal in that year.</p> In\u00a0[\u00a0]: Copied! <pre>EPL = pd.merge(EPL, Sumsal, on=['Season_ending'], how='left')\ndisplay(EPL)\n</pre> EPL = pd.merge(EPL, Sumsal, on=['Season_ending'], how='left') display(EPL) In\u00a0[\u00a0]: Copied! <pre># Here we create the variable 'relsal' for the EPL \n\nEPL['relsal']= EPL['salaries']/EPL['allsal']\n</pre> # Here we create the variable 'relsal' for the EPL   EPL['relsal']= EPL['salaries']/EPL['allsal'] <p>Before running a regression, we use sns.reglot() to look at the relationship between salaries and win percentage on a chart.</p> In\u00a0[\u00a0]: Copied! <pre>sns.regplot(x=\"relsal\", y=\"Position\", data = EPL, ci=False)\n</pre> sns.regplot(x=\"relsal\", y=\"Position\", data = EPL, ci=False) <p>The chart shows that there is a negative relationship between league position and relsal. This is because a lower numerical value of league position means a better performance (e.g. 9 is better than 10 and 1 is better than 2). Higher wage spending relative to other teams generates a higher league position. To avoid confusion, we can reverse the relationship, so that higher spending (on the x axis) leads to a higher position on the y axis. We do this simply by defining 'mpos\" as 'position' multiplied by -1. This changes nothing about the underlying logic of the relationship.</p> In\u00a0[\u00a0]: Copied! <pre>EPL['mpos'] = -EPL['Position']\n</pre> EPL['mpos'] = -EPL['Position']  In\u00a0[\u00a0]: Copied! <pre>sns.regplot(x=\"relsal\", y=\"mpos\", data = EPL, ci=False)\n</pre> sns.regplot(x=\"relsal\", y=\"mpos\", data = EPL, ci=False) <p>One thing you might notice about the data is that there appears to be a certain amount of curvature, with many dots (each dot represents a single team in a single year) located around the lower values on the x axis, and a smaller number of clubs strung out with high values on the x and y axes. This is a common feature of many types of data. In our regression, we estimate a linear relationship. Hence, it is better if we can first linearize our data, which we can often achieve by taking logarithms. We do that next.</p> In\u00a0[\u00a0]: Copied! <pre>EPL['lnpos']= -np.log(EPL['Position'])\n</pre> EPL['lnpos']= -np.log(EPL['Position']) In\u00a0[\u00a0]: Copied! <pre>sns.regplot(x=\"relsal\", y=\"lnpos\", data = EPL, ci=False)\n</pre> sns.regplot(x=\"relsal\", y=\"lnpos\", data = EPL, ci=False) <p>We now run the simple regression of league position on salaries:</p> In\u00a0[\u00a0]: Copied! <pre>possal1_lm = smf.ols(formula = 'lnpos ~ relsal ', data=EPL).fit()\nprint(possal1_lm.summary())\n</pre> possal1_lm = smf.ols(formula = 'lnpos ~ relsal ', data=EPL).fit() print(possal1_lm.summary()) <p>As with the NBA data, the relsal variable is statistically significant. However, it is also noticeable that its impact is much larger, in that it accounts for much more of the variation in league position - the R-squared is 0.657, meaning that almost two thirds of the variation can be explained by salaries alone (recall the figure was 17% for the NBA).</p> <p>Why is that relsal is so much more powerful in terms of explaining the variation in player salaries for the EPL than it was for the NBA? The answer lies in what was discussed at the beginning of this notebook - there are fewer restrictions on the operation of the market, there is much greater inequality between the teams, and this reveals itself in the fact that salaries are a much better explanatory variable for team performance.</p> <p>We now consider other factors, to see if omitted variable bias might have caused us to under- or over- estimate the impact of player salaries.</p> <p>The first factor we consider is one that is specific to the promotion and relegation system. During the period in question, three teams were promoted to the EPL in each year (replacing three teams that had been relegated). Do promoted teams start with a disadvantage relative to other teams? We can test for this by adding a dummy variable which is equal to one if the team in question was promoted to the EPL in that season, and otherwise equals zero. We run the regression again with the promotion dummy variable included:</p> In\u00a0[\u00a0]: Copied! <pre>possal2_lm = smf.ols(formula = 'lnpos ~ relsal + promoted_last_season', data=EPL).fit()\nprint(possal2_lm.summary())\n</pre> possal2_lm = smf.ols(formula = 'lnpos ~ relsal + promoted_last_season', data=EPL).fit() print(possal2_lm.summary()) <p>The coefficient on promotion is statistically insignificant. This might come as a surprise - it might seem obvious that promoted teams are at a disadvantage, but there are other factors at play. If a promoted team spends money on players, then they appear to be in same position as everyone else. However, promoted teams may not have as much cash to spend, and hence they do experience a disadvantage, but that is channeled entirely through the effect of relsal. Promoted teams are often smaller than the established teams, but they often enjoy a boost in popularity from fans who are excited by the team's improved status. There can be positive and negative factors associated with promotion, and these can cancel each other out.</p> <p>Note that the addition of the promotion variable hardly changed the estimated coefficient of relsal.</p> <p>Given that the promotion effect is statistically insignificant, we now drop it from the regression analysis.</p> <p>We now consider, as we did with the NBA, the impact of lagged dependent variable- league position in the previous season. As before, we do this by first sorting the df by teams and by season, and then use .shift(1) to create the lag of league position.</p> In\u00a0[\u00a0]: Copied! <pre>EPL.sort_values(by=['Club','Season_ending'], ascending=True)\nEPL\n</pre> EPL.sort_values(by=['Club','Season_ending'], ascending=True) EPL In\u00a0[\u00a0]: Copied! <pre>pd.set_option('display.max_rows', 400)\nEPL\n</pre> pd.set_option('display.max_rows', 400) EPL In\u00a0[\u00a0]: Copied! <pre>EPL['lnpos_lag'] = EPL.groupby('Club')['lnpos'].shift(1)\nEPL\n</pre> EPL['lnpos_lag'] = EPL.groupby('Club')['lnpos'].shift(1) EPL <p>If you scroll through the df you will see that, as with the NBA data, we have missing values (NaN) for the first season (1997), since the values for the previous season are not in the data. But also you will see that there are missing values for some teams in other seasons. These are for clubs which were promoted in that season, and hence they had no lagged value for their EPL position.</p> <p>This means we will lose some observations when we run the regressions. It's always worse to have fewer observations, but on the other hand it's always better to include potential omitted variables. There is a trade-off here between reducing the size of our dataset and including all relevant variables. Here the problem is not too serious, since we lose 42 observations and still have 333 in our dataset, whereas we expect that omitting the lagged dependent variable would lead to significant bias in the estimate of relsal.</p> In\u00a0[\u00a0]: Copied! <pre>possal3_lm = smf.ols(formula = 'lnpos ~lnpos_lag + relsal', data=EPL).fit()\nprint(possal3_lm.summary())\n</pre> possal3_lm = smf.ols(formula = 'lnpos ~lnpos_lag + relsal', data=EPL).fit() print(possal3_lm.summary()) <p>As expected, the lagged dependent variable has a a large and statistically significant effect on league position. As far as our estimate of relsal is concerned, we can see that our estimate has fallen from 23.9 to 14.7, suggesting that the omission of the lagged dependent variable led to a significant upward bias in our estimate of relsal.</p> <p>Finally, as we did with the NBA, we consider the possible effects of heterogeneity by adding fixed effects into our regression, recall that we do this simply by adding C(Club) to our regression formula:</p> In\u00a0[\u00a0]: Copied! <pre>possal4_lm = smf.ols(formula = \"lnpos ~ lnpos_lag + relsal +C(Club)\", data=EPL).fit()\nprint(possal4_lm.summary())\n</pre> possal4_lm = smf.ols(formula = \"lnpos ~ lnpos_lag + relsal +C(Club)\", data=EPL).fit() print(possal4_lm.summary()) <p>Remember that our main interest is in the effect of relsal. Adding the fixed effects has reduced the value of the coefficient a little further - to 10.98 - again suggesting that our original specification suffered from omitted variable bias, which biased our estimate of relsal upwards.</p> <p>Before asking what the value of the coefficient of relsal means for league position, we should stop to consider the fixed effects. As can be seen from the regression output, almost all are negative and statistically significant. The reason for this is that when estimating the fixed effects there must always be a reference group- so that the fixed effect measures performance relative to the reference group. By default Python uses the first name on the list as the reference group, and since our clubs are listed alphabetically, the reference group is Arsenal. Now, over the period 1997-2015 Arsenal was one of the most consistently successful teams, which explains why most of the coefficients are negative. Most teams were performing worse than Arsenal, even after taking account of wage spending via relsal.</p> <p>In this case, it might make more sense to evaluate the fixed effects relative to a mid-table team. We can choose the reference group, but first let's list the average league performance of the teams, to see which club would be a good candidate for the reference group. We use .groupby() to calculate average leagues position by club:</p> In\u00a0[\u00a0]: Copied! <pre>Avpos = EPL.groupby(['Club'])['Position'].mean()\nAvpos\n</pre> Avpos = EPL.groupby(['Club'])['Position'].mean() Avpos <p>\"Mid-table\" means an average league position of 10 or 11. There are a few we could choose from, but one of the most consistent over the period was Everton, so we use them.</p> <p>To specify the reference group we expand the C() statement to define the \"treatment\" group reference. Hence we write C(Club, Treatment('Everton')):</p> In\u00a0[\u00a0]: Copied! <pre>possal5_lm = smf.ols(formula = \"lnpos ~ lnpos_lag + relsal +C(Club, Treatment('Everton'))\", data=EPL).fit()\nprint(possal5_lm.summary())\n</pre> possal5_lm = smf.ols(formula = \"lnpos ~ lnpos_lag + relsal +C(Club, Treatment('Everton'))\", data=EPL).fit() print(possal5_lm.summary()) <p>We now see that only four clubs have statistically significant coefficients. Two of these are Manchester United and Arsenal, the two dominant clubs over the period. This implies that these clubs, which spent more money than the others on players, still managed to extract better than average performance from these players. This fact is likely related to the two iconic managers of these clubs, Sir Alex Ferguson and Arsene Wenger.</p> <p>Notice that changing the reference group does not change the coefficient on relsal or on the lagged dependent variable. The R-squared of the regression, or any other diagnostic statistic. The only thing that changes are the coefficients of the fixed effects themselves, and also the coefficient of the constant.</p> <p>Finally, we consider how changes in relsal affect league positions, given our estimated coefficient of just under 11. Ignoring the fixed effects and the lagged dependent variable, minus the log of league position can be expressed as a function of the constant plus the relsal coefficient times the value of relsal, i.e. -lnpos = -2.1 + 11 relsal. Because we have expressed league position as a logarithm, the impact on league position will differ for different values of relsal. From the charts above we can see that relsal varies roughly between 0.02 (2%) and 0.14 (14%).</p> <p>Let's consider three values of relsal: .02, .07 and .14. What league positions are implied by these values? To convert -lnpos back into position we have to multiply by -1 and then take the exponent. To take an exponent using numpy you just type np.exp() with the expression in parentheses. If we do that to the right hand side of the equation then we have our answer.</p> In\u00a0[\u00a0]: Copied! <pre>print(np.exp(2.1- 11*.02))\nprint(np.exp(2.1- 11*.08))\nprint(np.exp(2.1- 11*.14))\n</pre> print(np.exp(2.1- 11*.02)) print(np.exp(2.1- 11*.08)) print(np.exp(2.1- 11*.14))  <p>It is not surprising to see that the highest spending level implies a very high league position - somewhere between first (1) and second (2). It is more suprising to see that a level of spending somewhere around the mean (0.08) implies a position between 3rd and 4th, while even lowest spending (.02) implies a league position between 6th and 7th. The explanation for this lies with the role of the lagged dependent variable. This tends to emphasize the role of past performance in contributing to current performance. Teams that are able to spend consistently can more easily achieve a high league position than teams which attempt to do so by a short term infusion of spending.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/05.01/#the-salary-performance-relationship-in-the-english-premier-league","title":"The Salary-Performance Relationship in the English Premier League\u00b6","text":"<p>Last week we looked at the salary-performance relationship in the NBA. The rules governing the operation of the English Premier League are very different. Unlike the NBA, there is no salary cap, nor is there a draft system, roster limits, and the revenue sharing mechanisms used in the NBA and other North American major leagues are much more limited.</p> <p>Another important difference, which is not unconnnected, is the system of promotion and relegation. This requires that the worst performing teams in the league (measured by league position) are automatically relegated the following season to play in the next tier down, to be replaced by the best performing teams from that lower tier. This system is the norm in the world of soccer, and is perhaps the main reason that teams do not agree to restraints such as salary caps. There is a high degree of revenue inequality in soccer, and richer clubs are unwilling to share with the poorer ones, for fear that this might cause them to be relegated.</p> <p>This week we are going to follow the same procedure as we did for the NBA. We will look at the impact of salaries (relative to the average for the season) on team performance (measured this time by league position), and then see how the addition of potential omitted variables - (the lagged depedent variable and fixed effects) impact the estimates.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/05.01/#self-test","title":"Self Test\u00b6","text":"<p>Try running the regression again using mpos instead of lnpos as the y variable. What differences do you see when comparing the two regressions?</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/05.01/#self-test","title":"Self Test\u00b6","text":"<p>Calculate the fixed effects using Sunderland as the reference team. What changes do you see in the estimates?</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/05.01/#self-test","title":"Self Test\u00b6","text":"<p>Calculate the expected position of (a) Arsenal and (b) West Ham United, using the same relsal values as above (i.e. when relsal is .02, .08 and .14) but now including the fixed effects for the two clubs.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/05.01/#conclusion","title":"Conclusion\u00b6","text":"<p>While we have repeated the analysis that we conducted for the NBA almost exactly, our results have been quite different, reflecting the different organizational structure of the soccer in England (and in other soccer leagues outside North America). The main result of our analysis is that salary spending varies much more than it does in the NBA, and has a much larger impact on outcomes, even after we allow for possible omitted variables and heterogeneity. Next week we will look at Major League Baseball (MLB).</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/05.02/","title":"Salaries and Performance in the National Hockey League (NHL)","text":"In\u00a0[\u00a0]: Copied! <pre># As usual, we begin by loading the packages we will need\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.formula.api as smf\n</pre> # As usual, we begin by loading the packages we will need  import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import statsmodels.formula.api as smf In\u00a0[\u00a0]: Copied! <pre># Now we load the data\n\nHockey=pd.read_excel(\"../../Data/Week 5/NHL pay and performance.xlsx\")\n</pre> # Now we load the data  Hockey=pd.read_excel(\"../../Data/Week 5/NHL pay and performance.xlsx\") In\u00a0[\u00a0]: Copied! <pre>Hockey.describe()\n</pre> Hockey.describe() In\u00a0[\u00a0]: Copied! <pre>Hockey.info()\n</pre> Hockey.info() <p>We can see that we have 301 observations in total covering the seasons 2009 to 2018. This is somewhat more than we has in the NBA case, but much less than the MLB case. We can now look at the changes in total salary spending across the seasons:</p> In\u00a0[\u00a0]: Copied! <pre>Sumsal = Hockey.groupby(['season'])['salaries'].sum().reset_index().rename(columns={'salaries':'allsal'})\nSumsal\n</pre> Sumsal = Hockey.groupby(['season'])['salaries'].sum().reset_index().rename(columns={'salaries':'allsal'}) Sumsal <p>Salary inflation has not been as dramatic in the NHL as in other leagues we have looked at, but they have still increased by more than one third in a decade, which is very unlikely to be caused by improving player quality on average. As with the other leagues, the main driver of increasing salaries has been increasing team revenues and the capacity of the players to bargain for higher wages.</p> <p>As before, we use pd.merge() to add the aggregate salaries for each season to our original dataframe:</p> In\u00a0[\u00a0]: Copied! <pre>Hockey = pd.merge(Hockey, Sumsal, on=['season'], how='left')\ndisplay(Hockey)\n</pre> Hockey = pd.merge(Hockey, Sumsal, on=['season'], how='left') display(Hockey) <p>We can now create a variable which we call 'relsal', which measures the share of a team's salary spend in the total spending of all teams in that season:</p> In\u00a0[\u00a0]: Copied! <pre>Hockey['relsal']= Hockey['salaries']/Hockey['allsal']\n</pre> Hockey['relsal']= Hockey['salaries']/Hockey['allsal'] <p>Before running a regression, it makes sense to look at the relationship between salaries and win percentage on a chart. To do this we use sns.reglot(). Since our argument is that higher relative salaries mean better players which in turns leads to more wins, we put relsal on the x axis and wpc on the y axis.</p> In\u00a0[\u00a0]: Copied! <pre>sns.regplot(x=\"relsal\", y=\"wpc\", data = Hockey, ci=False)\n</pre> sns.regplot(x=\"relsal\", y=\"wpc\", data = Hockey, ci=False) <p>Note that the values of relsal on the x axis tend to vary between around 0.02 and 0.045, while wpc on the y axis tends to vary between around .3 and .7. The variation in relsal is much more like the NBA, while the variation in wpc is more like MLB.</p> <p>As with all the other leagues it is clear from the data that there is a positive correlation between relsal and wpc, as shown by the regression line which regplot adds to the scatter diagram. We now run a regression using smf.ols() in order to derive the coefficients of the regression and other diagnostic statistics.</p> In\u00a0[\u00a0]: Copied! <pre>wpcsal1_lm = smf.ols(formula = 'wpc ~ relsal', data=Hockey).fit()\nprint(wpcsal1_lm.summary())\n</pre> wpcsal1_lm = smf.ols(formula = 'wpc ~ relsal', data=Hockey).fit() print(wpcsal1_lm.summary()) <p>For the NHL we find that the coefficient of relsal is quite similar to that of the NBA. In fact, the regression looks quite similar in terms of the intercept (this is the value of win percentage if relsal were zero) and the R-squared.</p> <p>Let's see how the addition of the lagged dependent variable changes our relsal estimate.</p> In\u00a0[\u00a0]: Copied! <pre># first we sort the values\n\nHockey.sort_values(by=['Team','season'], ascending=True)\n</pre> # first we sort the values  Hockey.sort_values(by=['Team','season'], ascending=True) In\u00a0[\u00a0]: Copied! <pre># this will allow us to inspect all rows in the data\n\npd.set_option('display.max_rows', 400)\nHockey\n</pre> # this will allow us to inspect all rows in the data  pd.set_option('display.max_rows', 400) Hockey In\u00a0[\u00a0]: Copied! <pre># now we create the lagged dependend variable\n\nHockey['wpc_lag'] = Hockey.groupby('Team')['wpc'].shift(1)\nHockey\n</pre> # now we create the lagged dependend variable  Hockey['wpc_lag'] = Hockey.groupby('Team')['wpc'].shift(1) Hockey <p>We now run our regression again, but adding wpc_lag into the regression equation:</p> In\u00a0[\u00a0]: Copied! <pre>wpcsal2_lm = smf.ols(formula = 'wpc ~wpc_lag + relsal', data=Hockey).fit()\nprint(wpcsal2_lm.summary())\n</pre> wpcsal2_lm = smf.ols(formula = 'wpc ~wpc_lag + relsal', data=Hockey).fit() print(wpcsal2_lm.summary()) <p>Once again adding the lagged dependent variable is justified both in terms of the statistical significance of the variable and the addition to R-squared (whether adjusted or not). However, the impact on our main variable of interest, relsal, is relatively small. Its value has fallen from 10.8 to 8.4, which while it does suggest that there was some omitted variable bias, it is not as great as in the NBA case, while in the MLB case the coefficient of relsal was much smaller to begin with.</p> <p>Let's now see what changes if we include fixed effects:</p> In\u00a0[\u00a0]: Copied! <pre>wpcsal3_lm = smf.ols(formula = 'wpc ~wpc_lag + relsal +C(Team)', data=Hockey).fit()\nprint(wpcsal3_lm.summary())\n</pre> wpcsal3_lm = smf.ols(formula = 'wpc ~wpc_lag + relsal +C(Team)', data=Hockey).fit() print(wpcsal3_lm.summary()) <p>Here we find ten fixed effects that are statistically significant. The fixed effects add considerably to the R-squared of the regression, and only marginally reduce the value of relsal. However, the most striking impact of the fixed effects is to reduce the value of of the lagged dependent variable to the point where it is statistically insignificant. This is in contrast to what we found in all three of the other other leagues. Because it is statistically insignificant in this version, and since we want to keep the fixed effects, we can drop the lagged dependent variable, which we do in this regression:</p> In\u00a0[\u00a0]: Copied! <pre>wpcsal4_lm = smf.ols(formula = \"wpc ~  relsal +C(Team)\", data=Hockey).fit()\nprint(wpcsal4_lm.summary())\n</pre> wpcsal4_lm = smf.ols(formula = \"wpc ~  relsal +C(Team)\", data=Hockey).fit() print(wpcsal4_lm.summary()) <p>This model has a very simple interpretation: wpc = 0.256 + 8.76 x relsal + fixed effects. If we ignore fixed effects, we can identify the expected win percentage for low, average and high relative spending:</p> In\u00a0[\u00a0]: Copied! <pre>print(0.256 + 8.76*0.02)\nprint(0.256 + 8.76*0.0325)\nprint(0.256 + 8.76*0.045)\n</pre> print(0.256 + 8.76*0.02) print(0.256 + 8.76*0.0325) print(0.256 + 8.76*0.045) <p>Looking at this, we can see that the numbers are slightly skewed- the performance levels are higher than one might expect at each level, and this most likely reflects the impacts of the fixed effects. It's clear that most of the fixed effects are negative, and this would bring down teams to a lower level of performance. It suggests that those teams that are able to dominate, are capable of doing so (or at least were capable of doing so during this era) because of factors other than wage spending.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/05.02/#salaries-and-performance-in-the-national-hockey-league-nhl","title":"Salaries and Performance in the National Hockey League (NHL)\u00b6","text":"<p>By looking at third league modeled on the North American system we can get a better understanding of the three variables we have used to explain win perecentage: salaries, lagged win percentage, and fixed effects.</p> <p>We follow the same steps as we did for both those leagues.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/05.02/#self-test","title":"Self Test\u00b6","text":"<p>Re-run the regplot with smaller dots so that there is no overlap.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/05.02/#self-test","title":"Self Test\u00b6","text":"<p>Create a subset of the data which includes only the 2018 season and run the regression of wpc on relsal. How do the results compare to the results above?</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/05.02/#self-test","title":"Self Test\u00b6","text":"<p>Based on the fixed effects regression, calculate the win percentage of:</p> <p>(a) The Calgary Flames assuming the value of relsal for the team is 0.03 (b) The Edmonton Oilers assuming the value of relsal for the team is 0.04 (c) The Montreal Canadiens assuming the value of relsal for the team is 0.05</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/05.02/#conclusion","title":"Conclusion\u00b6","text":"<p>This week we have looked at four different leagues and used salary data to assess the impact of wage spending on team performance. In every case we found it had a significant impact, but that impact varied depending on the league. The league system also mattered, as we saw when contrasting the cases of the EPL with the NBA, MLB and NHL.</p> <p>We also introduced two issues which should always be considered when running regressions: omitted variable bias and heterogeneity.</p> <p>Finally, we should mention one issue which arises in the context of this type of exercise. The data we study here is \"observational\", meaning that we collect the data based on what actually happened, during events over which we had no control. This raises the question, \"How would outcomes have been different if some particular variable had had a different value?\" The regression coefficients produced some answers for us, but how can we be sure that there was not some other factor which we have omitted, which was what really mattered? We can't be sure.</p> <p>Scientists in laboratories typically don't have this problem - they use \"experimental data\" which they create in a controlled environment, so that they can control all observable factors. You can use that kind of data to measure the aerobic capacity of an athlete but, since you can't directly control the game, you can never use it to analyze game outcomes.</p> <p>Some experimental scientists would go so far as to say that we can infer nothing from observational data. This is the logic of a phrase you may have encountered: \"correlation is not causation\". We have observed correlations in the data using regression analysis, but that does not prove that the links were causal (you could go so far as to argue that win percentage causes salaries to increase, and not vice versa).</p> <p>We think that is too pessimistic a view. Observational data is certainly more challenging to work with, but it is possible for us to gain insight into the underlying relationships through careful study. It is important to be aware of the pitfalls, and it is important to focus on the logical coherence of the analysis rather than just running regressions. Another way to say this is that one should always have in mind a theory that one is trying to test, and be willing to discard that theory if the data renders it untenable. With careful thought and attention to details, it is possible to generate results which can enhance our understanding.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/05.03/","title":"Salaries and Performance in Major League Baseball","text":"In\u00a0[1]: Copied! <pre>%%capture\n# As usual, we begin by loading the packages we will need\n# The following statements are to resolve version mismatches that exist \n# with this current image, if your R-squared values differ from what is\n# expected, then these lines should fix it.\n\nimport sys\n!{sys.executable} -m pip uninstall statsmodels --yes\n!{sys.executable} -m pip uninstall numpy --yes\n!{sys.executable} -m pip install numpy==1.16.5\n!{sys.executable} -m pip install statsmodels==0.10.1\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.formula.api as smf\n</pre> %%capture # As usual, we begin by loading the packages we will need # The following statements are to resolve version mismatches that exist  # with this current image, if your R-squared values differ from what is # expected, then these lines should fix it.  import sys !{sys.executable} -m pip uninstall statsmodels --yes !{sys.executable} -m pip uninstall numpy --yes !{sys.executable} -m pip install numpy==1.16.5 !{sys.executable} -m pip install statsmodels==0.10.1  import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import statsmodels.formula.api as smf In\u00a0[2]: Copied! <pre># Now we load the data\n\nMLB=pd.read_excel(\"../../Data/Week 5/MLB pay and performance.xlsx\")\n</pre> # Now we load the data  MLB=pd.read_excel(\"../../Data/Week 5/MLB pay and performance.xlsx\") In\u00a0[3]: Copied! <pre>MLB.describe()\n</pre> MLB.describe() Out[3]: season salaries wpc G W count 918.000000 9.180000e+02 918.000000 918.000000 918.000000 mean 2000.978214 6.004263e+07 0.499844 159.932462 79.943355 std 9.135570 4.330992e+07 0.068669 8.675654 11.840224 min 1985.000000 8.800000e+05 0.265432 112.000000 43.000000 25% 1993.000000 2.543571e+07 0.450617 162.000000 71.250000 50% 2001.000000 5.053732e+07 0.500000 162.000000 80.000000 75% 2009.000000 8.441608e+07 0.549383 162.000000 89.000000 max 2016.000000 2.319789e+08 0.716049 164.000000 116.000000 In\u00a0[4]: Copied! <pre>MLB.info()\n</pre> MLB.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 918 entries, 0 to 917\nData columns (total 7 columns):\nseason      918 non-null int64\nTeam        918 non-null object\nlgID        918 non-null object\nsalaries    918 non-null int64\nwpc         918 non-null float64\nG           918 non-null int64\nW           918 non-null int64\ndtypes: float64(1), int64(4), object(2)\nmemory usage: 50.3+ KB\n</pre> <p>We can see that we have 918 observations in total covering the seasons 1985 to 2016. This data covers even more years than our NBA or EPL data, and therefore we would expect the effect of salary inflation to be even greater. We can see that when we measure the total expenditure on salaries by season:</p> In\u00a0[5]: Copied! <pre>Sumsal = MLB.groupby(['season'])['salaries'].sum().reset_index().rename(columns={'salaries':'allsal'})\nSumsal\n</pre> Sumsal = MLB.groupby(['season'])['salaries'].sum().reset_index().rename(columns={'salaries':'allsal'}) Sumsal Out[5]: season allsal 0 1985 261964696 1 1986 307854518 2 1987 272575375 3 1988 300452424 4 1989 359995711 5 1990 443881193 6 1991 613048418 7 1992 805543323 8 1993 901740134 9 1994 927836287 10 1995 951469367 11 1996 956983550 12 1997 1127285885 13 1998 1278282871 14 1999 1494228750 15 2000 1666135102 16 2001 1960663313 17 2002 2024077522 18 2003 2128262128 19 2004 2070665943 20 2005 2188713398 21 2006 2321472617 22 2007 2476688987 23 2008 2684858670 24 2009 2664726994 25 2010 2721359865 26 2011 2784505291 27 2012 2932741192 28 2013 3034525648 29 2014 3192317623 30 2015 3514142569 31 2016 3750137392 <p>In 1985, the total salaries paid out by MLB teams amounted to \\$262 million and by 2016 this had risen to $3750 million. As with the NBA and EPL, this does not reflect improvements in player quality, but rather the growth of revenues of MLB and the capacity of players to bargain for a significant share of these revenues.</p> <p>We now merge these totals into our original dataset.</p> In\u00a0[6]: Copied! <pre>MLB = pd.merge(MLB, Sumsal, on=['season'], how='left')\ndisplay(MLB)\n</pre> MLB = pd.merge(MLB, Sumsal, on=['season'], how='left') display(MLB) season Team lgID salaries wpc G W allsal 0 1997 ANA AL 31135472 0.518519 162 84 1127285885 1 1998 ANA AL 41281000 0.524691 162 85 1278282871 2 1999 ANA AL 55388166 0.432099 162 70 1494228750 3 2000 ANA AL 51464167 0.506173 162 82 1666135102 4 2001 ANA AL 47535167 0.462963 162 75 1960663313 5 2002 ANA AL 61721667 0.611111 162 99 2024077522 6 2003 ANA AL 79031667 0.475309 162 77 2128262128 7 2004 ANA AL 100534667 0.567901 162 92 2070665943 8 1998 ARI NL 32347000 0.401235 162 65 1278282871 9 1999 ARI NL 68703999 0.617284 162 100 1494228750 10 2000 ARI NL 81027833 0.524691 162 85 1666135102 11 2001 ARI NL 85082999 0.567901 162 92 1960663313 12 2002 ARI NL 102819999 0.604938 162 98 2024077522 13 2003 ARI NL 80657000 0.518519 162 84 2128262128 14 2004 ARI NL 69780750 0.314815 162 51 2070665943 15 2005 ARI NL 62329166 0.475309 162 77 2188713398 16 2006 ARI NL 59684226 0.469136 162 76 2321472617 17 2007 ARI NL 52067546 0.555556 162 90 2476688987 18 2008 ARI NL 66202712 0.506173 162 82 2684858670 19 2009 ARI NL 73115666 0.432099 162 70 2664726994 20 2010 ARI NL 60718166 0.401235 162 65 2721359865 21 2011 ARI NL 53639833 0.580247 162 94 2784505291 22 2012 ARI NL 73804833 0.500000 162 81 2932741192 23 2013 ARI NL 90132000 0.500000 162 81 3034525648 24 2014 ARI NL 97861500 0.395062 162 64 3192317623 25 2015 ARI NL 61834000 0.487654 162 79 3514142569 26 2016 ARI NL 87439063 0.425926 162 69 3750137392 27 1985 ATL NL 14807000 0.407407 162 66 261964696 28 1986 ATL NL 17102786 0.447205 161 72 307854518 29 1987 ATL NL 16544560 0.428571 161 69 272575375 ... ... ... ... ... ... ... ... ... 888 1999 TOR AL 45444333 0.518519 162 84 1494228750 889 2000 TOR AL 44838332 0.512346 162 83 1666135102 890 2001 TOR AL 76895999 0.493827 162 80 1960663313 891 2002 TOR AL 76864333 0.481481 162 78 2024077522 892 2003 TOR AL 51269000 0.530864 162 86 2128262128 893 2004 TOR AL 50017000 0.416149 161 67 2070665943 894 2005 TOR AL 45719500 0.493827 162 80 2188713398 895 2006 TOR AL 71365000 0.537037 162 87 2321472617 896 2007 TOR AL 81942800 0.512346 162 83 2476688987 897 2008 TOR AL 97793900 0.530864 162 86 2684858670 898 2009 TOR AL 80538300 0.462963 162 75 2664726994 899 2010 TOR AL 62234000 0.524691 162 85 2721359865 900 2011 TOR AL 62567800 0.500000 162 81 2784505291 901 2012 TOR AL 75009200 0.450617 162 73 2932741192 902 2013 TOR AL 126288100 0.456790 162 74 3034525648 903 2014 TOR AL 109920100 0.512346 162 83 3192317623 904 2015 TOR AL 112992400 0.574074 162 93 3514142569 905 2016 TOR AL 138701700 0.549383 162 89 3750137392 906 2005 WAS NL 48581500 0.500000 162 81 2188713398 907 2006 WAS NL 63143000 0.438272 162 71 2321472617 908 2007 WAS NL 36947500 0.450617 162 73 2476688987 909 2008 WAS NL 54961000 0.366460 161 59 2684858670 910 2009 WAS NL 59928000 0.364198 162 59 2664726994 911 2010 WAS NL 61400000 0.425926 162 69 2721359865 912 2011 WAS NL 63856928 0.496894 161 80 2784505291 913 2012 WAS NL 80855143 0.604938 162 98 2932741192 914 2013 WAS NL 113703270 0.530864 162 86 3034525648 915 2014 WAS NL 131983680 0.592593 162 96 3192317623 916 2015 WAS NL 155587472 0.512346 162 83 3514142569 917 2016 WAS NL 141652646 0.586420 162 95 3750137392 <p>918 rows \u00d7 8 columns</p> In\u00a0[7]: Copied! <pre># we now create the relsal variable for MLB\n\nMLB['relsal']= MLB['salaries']/MLB['allsal']\n</pre> # we now create the relsal variable for MLB  MLB['relsal']= MLB['salaries']/MLB['allsal'] In\u00a0[8]: Copied! <pre># This command enables us to inspect all rows in the data, not just a subset\n\npd.set_option('display.max_rows', None)\nMLB\n</pre> # This command enables us to inspect all rows in the data, not just a subset  pd.set_option('display.max_rows', None) MLB Out[8]: season Team lgID salaries wpc G W allsal relsal 0 1997 ANA AL 31135472 0.518519 162 84 1127285885 0.027620 1 1998 ANA AL 41281000 0.524691 162 85 1278282871 0.032294 2 1999 ANA AL 55388166 0.432099 162 70 1494228750 0.037068 3 2000 ANA AL 51464167 0.506173 162 82 1666135102 0.030888 4 2001 ANA AL 47535167 0.462963 162 75 1960663313 0.024244 5 2002 ANA AL 61721667 0.611111 162 99 2024077522 0.030494 6 2003 ANA AL 79031667 0.475309 162 77 2128262128 0.037134 7 2004 ANA AL 100534667 0.567901 162 92 2070665943 0.048552 8 1998 ARI NL 32347000 0.401235 162 65 1278282871 0.025305 9 1999 ARI NL 68703999 0.617284 162 100 1494228750 0.045980 10 2000 ARI NL 81027833 0.524691 162 85 1666135102 0.048632 11 2001 ARI NL 85082999 0.567901 162 92 1960663313 0.043395 12 2002 ARI NL 102819999 0.604938 162 98 2024077522 0.050798 13 2003 ARI NL 80657000 0.518519 162 84 2128262128 0.037898 14 2004 ARI NL 69780750 0.314815 162 51 2070665943 0.033700 15 2005 ARI NL 62329166 0.475309 162 77 2188713398 0.028478 16 2006 ARI NL 59684226 0.469136 162 76 2321472617 0.025710 17 2007 ARI NL 52067546 0.555556 162 90 2476688987 0.021023 18 2008 ARI NL 66202712 0.506173 162 82 2684858670 0.024658 19 2009 ARI NL 73115666 0.432099 162 70 2664726994 0.027438 20 2010 ARI NL 60718166 0.401235 162 65 2721359865 0.022312 21 2011 ARI NL 53639833 0.580247 162 94 2784505291 0.019264 22 2012 ARI NL 73804833 0.500000 162 81 2932741192 0.025166 23 2013 ARI NL 90132000 0.500000 162 81 3034525648 0.029702 24 2014 ARI NL 97861500 0.395062 162 64 3192317623 0.030655 25 2015 ARI NL 61834000 0.487654 162 79 3514142569 0.017596 26 2016 ARI NL 87439063 0.425926 162 69 3750137392 0.023316 27 1985 ATL NL 14807000 0.407407 162 66 261964696 0.056523 28 1986 ATL NL 17102786 0.447205 161 72 307854518 0.055555 29 1987 ATL NL 16544560 0.428571 161 69 272575375 0.060697 30 1988 ATL NL 12728174 0.337500 160 54 300452424 0.042363 31 1989 ATL NL 11112334 0.391304 161 63 359995711 0.030868 32 1990 ATL NL 14555501 0.401235 162 65 443881193 0.032791 33 1991 ATL NL 18403500 0.580247 162 94 613048418 0.030020 34 1992 ATL NL 34625333 0.604938 162 98 805543323 0.042984 35 1993 ATL NL 41641417 0.641975 162 104 901740134 0.046179 36 1994 ATL NL 49383513 0.596491 114 68 927836287 0.053224 37 1995 ATL NL 47235445 0.625000 144 90 951469367 0.049645 38 1996 ATL NL 49698500 0.592593 162 96 956983550 0.051932 39 1997 ATL NL 52278500 0.623457 162 101 1127285885 0.046376 40 1998 ATL NL 61186000 0.654321 162 106 1278282871 0.047866 41 1999 ATL NL 73140000 0.635802 162 103 1494228750 0.048948 42 2000 ATL NL 84537836 0.586420 162 95 1666135102 0.050739 43 2001 ATL NL 91936166 0.543210 162 88 1960663313 0.046890 44 2002 ATL NL 92870367 0.627329 161 101 2024077522 0.045883 45 2003 ATL NL 106243667 0.623457 162 101 2128262128 0.049920 46 2004 ATL NL 90182500 0.592593 162 96 2070665943 0.043552 47 2005 ATL NL 86457302 0.555556 162 90 2188713398 0.039501 48 2006 ATL NL 90156876 0.487654 162 79 2321472617 0.038836 49 2007 ATL NL 87290833 0.518519 162 84 2476688987 0.035245 50 2008 ATL NL 102365683 0.444444 162 72 2684858670 0.038127 51 2009 ATL NL 96726166 0.530864 162 86 2664726994 0.036299 52 2010 ATL NL 84423666 0.561728 162 91 2721359865 0.031023 53 2011 ATL NL 87002692 0.549383 162 89 2784505291 0.031245 54 2012 ATL NL 82829942 0.580247 162 94 2932741192 0.028243 55 2013 ATL NL 87871525 0.592593 162 96 3034525648 0.028957 56 2014 ATL NL 97609000 0.487654 162 79 3192317623 0.030576 57 2015 ATL NL 71781250 0.413580 162 67 3514142569 0.020426 58 2016 ATL NL 68498291 0.422360 161 68 3750137392 0.018266 59 1985 BAL AL 11560712 0.515528 161 83 261964696 0.044131 60 1986 BAL AL 13001258 0.450617 162 73 307854518 0.042232 61 1987 BAL AL 13900273 0.413580 162 67 272575375 0.050996 62 1988 BAL AL 13532075 0.335404 161 54 300452424 0.045039 63 1989 BAL AL 8275167 0.537037 162 87 359995711 0.022987 64 1990 BAL AL 9680084 0.472050 161 76 443881193 0.021808 65 1991 BAL AL 17519000 0.413580 162 67 613048418 0.028577 66 1992 BAL AL 23780667 0.549383 162 89 805543323 0.029521 67 1993 BAL AL 29096500 0.524691 162 85 901740134 0.032267 68 1994 BAL AL 38849769 0.562500 112 63 927836287 0.041871 69 1995 BAL AL 43942521 0.493056 144 71 951469367 0.046184 70 1996 BAL AL 54490315 0.539877 163 88 956983550 0.056940 71 1997 BAL AL 58516400 0.604938 162 98 1127285885 0.051909 72 1998 BAL AL 72355634 0.487654 162 79 1278282871 0.056604 73 1999 BAL AL 80605863 0.481481 162 78 1494228750 0.053945 74 2000 BAL AL 81447435 0.456790 162 74 1666135102 0.048884 75 2001 BAL AL 67599540 0.388889 162 63 1960663313 0.034478 76 2002 BAL AL 60493487 0.413580 162 67 2024077522 0.029887 77 2003 BAL AL 73877500 0.435583 163 71 2128262128 0.034713 78 2004 BAL AL 51623333 0.481481 162 78 2070665943 0.024931 79 2005 BAL AL 73914333 0.456790 162 74 2188713398 0.033771 80 2006 BAL AL 72585582 0.432099 162 70 2321472617 0.031267 81 2007 BAL AL 93174808 0.425926 162 69 2476688987 0.037621 82 2008 BAL AL 67196246 0.422360 161 68 2684858670 0.025028 83 2009 BAL AL 67101666 0.395062 162 64 2664726994 0.025181 84 2010 BAL AL 81612500 0.407407 162 66 2721359865 0.029990 85 2011 BAL AL 85304038 0.425926 162 69 2784505291 0.030635 86 2012 BAL AL 77353999 0.574074 162 93 2932741192 0.026376 87 2013 BAL AL 84393333 0.524691 162 85 3034525648 0.027811 88 2014 BAL AL 103416000 0.592593 162 96 3192317623 0.032395 89 2015 BAL AL 115044833 0.500000 162 81 3514142569 0.032738 90 2016 BAL AL 161863456 0.549383 162 89 3750137392 0.043162 91 1985 BOS AL 10897560 0.496933 163 81 261964696 0.041599 92 1986 BOS AL 14402239 0.590062 161 95 307854518 0.046783 93 1987 BOS AL 10144167 0.481481 162 78 272575375 0.037216 94 1988 BOS AL 13896092 0.549383 162 89 300452424 0.046251 95 1989 BOS AL 17481748 0.512346 162 83 359995711 0.048561 96 1990 BOS AL 20558333 0.543210 162 88 443881193 0.046315 97 1991 BOS AL 35167500 0.518519 162 84 613048418 0.057365 98 1992 BOS AL 43610584 0.450617 162 73 805543323 0.054138 99 1993 BOS AL 37120583 0.493827 162 80 901740134 0.041165 100 1994 BOS AL 37859084 0.469565 115 54 927836287 0.040804 101 1995 BOS AL 32455518 0.597222 144 86 951469367 0.034111 102 1996 BOS AL 42393500 0.524691 162 85 956983550 0.044299 103 1997 BOS AL 43558750 0.481481 162 78 1127285885 0.038640 104 1998 BOS AL 56757000 0.567901 162 92 1278282871 0.044401 105 1999 BOS AL 63497500 0.580247 162 94 1494228750 0.042495 106 2000 BOS AL 77940333 0.524691 162 85 1666135102 0.046779 107 2001 BOS AL 110035833 0.509317 161 82 1960663313 0.056122 108 2002 BOS AL 108366060 0.574074 162 93 2024077522 0.053538 109 2003 BOS AL 99946500 0.586420 162 95 2128262128 0.046962 110 2004 BOS AL 127298500 0.604938 162 98 2070665943 0.061477 111 2005 BOS AL 123505125 0.586420 162 95 2188713398 0.056428 112 2006 BOS AL 120099824 0.530864 162 86 2321472617 0.051734 113 2007 BOS AL 143026214 0.592593 162 96 2476688987 0.057749 114 2008 BOS AL 133390035 0.586420 162 95 2684858670 0.049682 115 2009 BOS AL 121345999 0.586420 162 95 2664726994 0.045538 116 2010 BOS AL 162447333 0.549383 162 89 2721359865 0.059693 117 2011 BOS AL 161762475 0.555556 162 90 2784505291 0.058094 118 2012 BOS AL 173186617 0.425926 162 69 2932741192 0.059053 119 2013 BOS AL 151530000 0.598765 162 97 3034525648 0.049935 120 2014 BOS AL 139019929 0.438272 162 71 3192317623 0.043548 121 2015 BOS AL 181103400 0.481481 162 78 3514142569 0.051536 122 2016 BOS AL 188545761 0.574074 162 93 3750137392 0.050277 123 1985 CAL AL 14427894 0.555556 162 90 261964696 0.055076 124 1986 CAL AL 14427258 0.567901 162 92 307854518 0.046864 125 1987 CAL AL 12843499 0.462963 162 75 272575375 0.047119 126 1988 CAL AL 11947388 0.462963 162 75 300452424 0.039765 127 1989 CAL AL 15097833 0.561728 162 91 359995711 0.041939 128 1990 CAL AL 21720000 0.493827 162 80 443881193 0.048932 129 1991 CAL AL 33060001 0.500000 162 81 613048418 0.053927 130 1992 CAL AL 34749334 0.444444 162 72 805543323 0.043138 131 1993 CAL AL 28588334 0.438272 162 71 901740134 0.031704 132 1994 CAL AL 25156218 0.408696 115 47 927836287 0.027113 133 1995 CAL AL 31223171 0.537931 145 78 951469367 0.032816 134 1996 CAL AL 28738000 0.434783 161 70 956983550 0.030030 135 1985 CHA AL 9846178 0.521472 163 85 261964696 0.037586 136 1986 CHA AL 10418819 0.444444 162 72 307854518 0.033843 137 1987 CHA AL 10641843 0.475309 162 77 272575375 0.039042 138 1988 CHA AL 6390000 0.440994 161 71 300452424 0.021268 139 1989 CHA AL 7265410 0.428571 161 69 359995711 0.020182 140 1990 CHA AL 9491500 0.580247 162 94 443881193 0.021383 141 1991 CHA AL 16919667 0.537037 162 87 613048418 0.027599 142 1992 CHA AL 30160833 0.530864 162 86 805543323 0.037442 143 1993 CHA AL 39696166 0.580247 162 94 901740134 0.044022 144 1994 CHA AL 39183836 0.592920 113 67 927836287 0.042231 145 1995 CHA AL 46961282 0.468966 145 68 951469367 0.049357 146 1996 CHA AL 45139500 0.524691 162 85 956983550 0.047169 147 1997 CHA AL 57740000 0.496894 161 80 1127285885 0.051220 148 1998 CHA AL 38335000 0.490798 163 80 1278282871 0.029989 149 1999 CHA AL 25620000 0.462963 162 75 1494228750 0.017146 150 2000 CHA AL 31133500 0.586420 162 95 1666135102 0.018686 151 2001 CHA AL 65653667 0.512346 162 83 1960663313 0.033485 152 2002 CHA AL 57052833 0.500000 162 81 2024077522 0.028187 153 2003 CHA AL 51010000 0.530864 162 86 2128262128 0.023968 154 2004 CHA AL 65212500 0.512346 162 83 2070665943 0.031493 155 2005 CHA AL 75178000 0.611111 162 99 2188713398 0.034348 156 2006 CHA AL 102750667 0.555556 162 90 2321472617 0.044261 157 2007 CHA AL 108671833 0.444444 162 72 2476688987 0.043878 158 2008 CHA AL 121189332 0.546012 163 89 2684858670 0.045138 159 2009 CHA AL 96068500 0.487654 162 79 2664726994 0.036052 160 2010 CHA AL 105530000 0.543210 162 88 2721359865 0.038778 161 2011 CHA AL 127789000 0.487654 162 79 2784505291 0.045893 162 2012 CHA AL 96919500 0.524691 162 85 2932741192 0.033047 163 2013 CHA AL 120065277 0.388889 162 63 3034525648 0.039566 164 2014 CHA AL 81830500 0.450617 162 73 3192317623 0.025634 165 2015 CHA AL 112373700 0.469136 162 76 3514142569 0.031978 166 2016 CHA AL 112998667 0.481481 162 78 3750137392 0.030132 167 1985 CHN NL 12702917 0.475309 162 77 261964696 0.048491 168 1986 CHN NL 17208165 0.437500 160 70 307854518 0.055897 169 1987 CHN NL 14307999 0.472050 161 76 272575375 0.052492 170 1988 CHN NL 13119198 0.472393 163 77 300452424 0.043665 171 1989 CHN NL 10668000 0.574074 162 93 359995711 0.029634 172 1990 CHN NL 13624000 0.475309 162 77 443881193 0.030693 173 1991 CHN NL 23175667 0.481250 160 77 613048418 0.037804 174 1992 CHN NL 29829686 0.481481 162 78 805543323 0.037031 175 1993 CHN NL 39386666 0.515337 163 84 901740134 0.043679 176 1994 CHN NL 36287333 0.433628 113 49 927836287 0.039110 177 1995 CHN NL 29505834 0.506944 144 73 951469367 0.031011 178 1996 CHN NL 33081000 0.469136 162 76 956983550 0.034568 179 1997 CHN NL 42155333 0.419753 162 68 1127285885 0.037395 180 1998 CHN NL 50838000 0.552147 163 90 1278282871 0.039771 181 1999 CHN NL 62343000 0.413580 162 67 1494228750 0.041723 182 2000 CHN NL 60539333 0.401235 162 65 1666135102 0.036335 183 2001 CHN NL 64715833 0.543210 162 88 1960663313 0.033007 184 2002 CHN NL 75690833 0.413580 162 67 2024077522 0.037395 185 2003 CHN NL 79868333 0.543210 162 88 2128262128 0.037527 186 2004 CHN NL 90560000 0.549383 162 89 2070665943 0.043735 187 2005 CHN NL 87032933 0.487654 162 79 2188713398 0.039764 188 2006 CHN NL 94424499 0.407407 162 66 2321472617 0.040674 189 2007 CHN NL 99670332 0.524691 162 85 2476688987 0.040243 190 2008 CHN NL 118345833 0.602484 161 97 2684858670 0.044079 191 2009 CHN NL 134809000 0.515528 161 83 2664726994 0.050590 192 2010 CHN NL 146609000 0.462963 162 75 2721359865 0.053873 193 2011 CHN NL 125047329 0.438272 162 71 2784505291 0.044908 194 2012 CHN NL 88197033 0.376543 162 61 2932741192 0.030073 195 2013 CHN NL 100567726 0.407407 162 66 3034525648 0.033141 196 2014 CHN NL 65522500 0.450617 162 73 3192317623 0.020525 197 2015 CHN NL 115879310 0.598765 162 97 3514142569 0.032975 198 2016 CHN NL 154067668 0.635802 162 103 3750137392 0.041083 199 1985 CIN NL 8359917 0.549383 162 89 261964696 0.031912 200 1986 CIN NL 11906388 0.530864 162 86 307854518 0.038675 201 1987 CIN NL 9281500 0.518519 162 84 272575375 0.034051 202 1988 CIN NL 8888409 0.540373 161 87 300452424 0.029583 203 1989 CIN NL 11072000 0.462963 162 75 359995711 0.030756 204 1990 CIN NL 14370000 0.561728 162 91 443881193 0.032374 205 1991 CIN NL 26305333 0.456790 162 74 613048418 0.042909 206 1992 CIN NL 35931499 0.555556 162 90 805543323 0.044605 207 1993 CIN NL 44879666 0.450617 162 73 901740134 0.049770 208 1994 CIN NL 40961833 0.573913 115 66 927836287 0.044148 209 1995 CIN NL 43144670 0.590278 144 85 951469367 0.045345 210 1996 CIN NL 42526334 0.500000 162 81 956983550 0.044438 211 1997 CIN NL 49768000 0.469136 162 76 1127285885 0.044149 212 1998 CIN NL 23005000 0.475309 162 77 1278282871 0.017997 213 1999 CIN NL 33962761 0.588957 163 96 1494228750 0.022729 214 2000 CIN NL 46867200 0.521472 163 85 1666135102 0.028129 215 2001 CIN NL 48986000 0.407407 162 66 1960663313 0.024984 216 2002 CIN NL 45050390 0.481481 162 78 2024077522 0.022257 217 2003 CIN NL 59355667 0.425926 162 69 2128262128 0.027889 218 2004 CIN NL 46615250 0.469136 162 76 2070665943 0.022512 219 2005 CIN NL 61892583 0.447853 163 73 2188713398 0.028278 220 2006 CIN NL 60909519 0.493827 162 80 2321472617 0.026237 221 2007 CIN NL 68524980 0.444444 162 72 2476688987 0.027668 222 2008 CIN NL 74117695 0.456790 162 74 2684858670 0.027606 223 2009 CIN NL 73558500 0.481481 162 78 2664726994 0.027605 224 2010 CIN NL 71761542 0.561728 162 91 2721359865 0.026370 225 2011 CIN NL 75947134 0.487654 162 79 2784505291 0.027275 226 2012 CIN NL 82203616 0.598765 162 97 2932741192 0.028030 227 2013 CIN NL 106404462 0.555556 162 90 3034525648 0.035065 228 2014 CIN NL 108217500 0.469136 162 76 3192317623 0.033899 229 2015 CIN NL 113072286 0.395062 162 64 3514142569 0.032176 230 2016 CIN NL 88940059 0.419753 162 68 3750137392 0.023716 231 1985 CLE AL 6551666 0.370370 162 60 261964696 0.025010 232 1986 CLE AL 7809500 0.515337 163 84 307854518 0.025368 233 1987 CLE AL 8513750 0.376543 162 61 272575375 0.031234 234 1988 CLE AL 8936500 0.481481 162 78 300452424 0.029743 235 1989 CLE AL 9094500 0.450617 162 73 359995711 0.025263 236 1990 CLE AL 14487000 0.475309 162 77 443881193 0.032637 237 1991 CLE AL 17635000 0.351852 162 57 613048418 0.028766 238 1992 CLE AL 9373044 0.469136 162 76 805543323 0.011636 239 1993 CLE AL 18561000 0.469136 162 76 901740134 0.020584 240 1994 CLE AL 30490500 0.584071 113 66 927836287 0.032862 241 1995 CLE AL 37937835 0.694444 144 100 951469367 0.039873 242 1996 CLE AL 48107360 0.614907 161 99 956983550 0.050270 243 1997 CLE AL 56802460 0.534161 161 86 1127285885 0.050389 244 1998 CLE AL 60800166 0.549383 162 89 1278282871 0.047564 245 1999 CLE AL 72978462 0.598765 162 97 1494228750 0.048840 246 2000 CLE AL 75880771 0.555556 162 90 1666135102 0.045543 247 2001 CLE AL 93152001 0.561728 162 91 1960663313 0.047510 248 2002 CLE AL 78909449 0.456790 162 74 2024077522 0.038985 249 2003 CLE AL 48584834 0.419753 162 68 2128262128 0.022828 250 2004 CLE AL 34319300 0.493827 162 80 2070665943 0.016574 251 2005 CLE AL 41502500 0.574074 162 93 2188713398 0.018962 252 2006 CLE AL 56031500 0.481481 162 78 2321472617 0.024136 253 2007 CLE AL 61673267 0.592593 162 96 2476688987 0.024901 254 2008 CLE AL 78970066 0.500000 162 81 2684858670 0.029413 255 2009 CLE AL 81579166 0.401235 162 65 2664726994 0.030614 256 2010 CLE AL 61203966 0.425926 162 69 2721359865 0.022490 257 2011 CLE AL 48776566 0.493827 162 80 2784505291 0.017517 258 2012 CLE AL 78430300 0.419753 162 68 2932741192 0.026743 259 2013 CLE AL 75771800 0.567901 162 92 3034525648 0.024970 260 2014 CLE AL 82151899 0.524691 162 85 3192317623 0.025734 261 2015 CLE AL 87663766 0.503106 161 81 3514142569 0.024946 262 2016 CLE AL 74311900 0.583851 161 94 3750137392 0.019816 263 1993 COL NL 10353500 0.413580 162 67 901740134 0.011482 264 1994 COL NL 23887333 0.452991 117 53 927836287 0.025745 265 1995 COL NL 34154717 0.534722 144 77 951469367 0.035897 266 1996 COL NL 40179823 0.512346 162 83 956983550 0.041986 267 1997 COL NL 43559667 0.512346 162 83 1127285885 0.038641 268 1998 COL NL 50484648 0.475309 162 77 1278282871 0.039494 269 1999 COL NL 61935837 0.444444 162 72 1494228750 0.041450 270 2000 COL NL 61111190 0.506173 162 82 1666135102 0.036678 271 2001 COL NL 71541334 0.450617 162 73 1960663313 0.036488 272 2002 COL NL 56851043 0.450617 162 73 2024077522 0.028087 273 2003 COL NL 67179667 0.456790 162 74 2128262128 0.031566 274 2004 COL NL 65445167 0.419753 162 68 2070665943 0.031606 275 2005 COL NL 47839000 0.413580 162 67 2188713398 0.021857 276 2006 COL NL 41233000 0.469136 162 76 2321472617 0.017762 277 2007 COL NL 54041000 0.552147 163 90 2476688987 0.021820 278 2008 COL NL 68655500 0.456790 162 74 2684858670 0.025571 279 2009 COL NL 75201000 0.567901 162 92 2664726994 0.028221 280 2010 COL NL 84227000 0.512346 162 83 2721359865 0.030950 281 2011 COL NL 88148071 0.450617 162 73 2784505291 0.031657 282 2012 COL NL 78069571 0.395062 162 64 2932741192 0.026620 283 2013 COL NL 74409071 0.456790 162 74 3034525648 0.024521 284 2014 COL NL 95403500 0.407407 162 66 3192317623 0.029885 285 2015 COL NL 95688600 0.419753 162 68 3514142569 0.027230 286 2016 COL NL 112645071 0.462963 162 75 3750137392 0.030038 287 1985 DET AL 10348143 0.521739 161 84 261964696 0.039502 288 1986 DET AL 12335714 0.537037 162 87 307854518 0.040070 289 1987 DET AL 12122881 0.604938 162 98 272575375 0.044475 290 1988 DET AL 12869571 0.543210 162 88 300452424 0.042834 291 1989 DET AL 15146404 0.364198 162 59 359995711 0.042074 292 1990 DET AL 17593238 0.487654 162 79 443881193 0.039635 293 1991 DET AL 23838333 0.518519 162 84 613048418 0.038885 294 1992 DET AL 27322834 0.462963 162 75 805543323 0.033919 295 1993 DET AL 38150165 0.524691 162 85 901740134 0.042307 296 1994 DET AL 41446501 0.460870 115 53 927836287 0.044670 297 1995 DET AL 37044168 0.416667 144 60 951469367 0.038934 298 1996 DET AL 23438000 0.327160 162 53 956983550 0.024492 299 1997 DET AL 17272000 0.487654 162 79 1127285885 0.015322 300 1998 DET AL 24065000 0.401235 162 65 1278282871 0.018826 301 1999 DET AL 36489666 0.428571 161 69 1494228750 0.024420 302 2000 DET AL 58265167 0.487654 162 79 1666135102 0.034970 303 2001 DET AL 53416167 0.407407 162 66 1960663313 0.027244 304 2002 DET AL 55048000 0.341615 161 55 2024077522 0.027197 305 2003 DET AL 49168000 0.265432 162 43 2128262128 0.023102 306 2004 DET AL 46832000 0.444444 162 72 2070665943 0.022617 307 2005 DET AL 69092000 0.438272 162 71 2188713398 0.031567 308 2006 DET AL 82612866 0.586420 162 95 2321472617 0.035586 309 2007 DET AL 94800369 0.543210 162 88 2476688987 0.038277 310 2008 DET AL 137685196 0.456790 162 74 2684858670 0.051282 311 2009 DET AL 115085145 0.527607 163 86 2664726994 0.043188 312 2010 DET AL 122864928 0.500000 162 81 2721359865 0.045148 313 2011 DET AL 105700231 0.586420 162 95 2784505291 0.037960 314 2012 DET AL 132300000 0.543210 162 88 2932741192 0.045111 315 2013 DET AL 145989500 0.574074 162 93 3034525648 0.048109 316 2014 DET AL 152855500 0.555556 162 90 3192317623 0.047882 317 2015 DET AL 172284750 0.459627 161 74 3514142569 0.049026 318 2016 DET AL 194876481 0.534161 161 86 3750137392 0.051965 319 1993 FLO NL 19330545 0.395062 162 64 901740134 0.021437 320 1994 FLO NL 21633000 0.443478 115 51 927836287 0.023316 321 1995 FLO NL 24515781 0.468531 143 67 951469367 0.025766 322 1996 FLO NL 31022500 0.493827 162 80 956983550 0.032417 323 1997 FLO NL 48692500 0.567901 162 92 1127285885 0.043194 324 1998 FLO NL 41322667 0.333333 162 54 1278282871 0.032327 325 1999 FLO NL 21085000 0.395062 162 64 1494228750 0.014111 326 2000 FLO NL 19872000 0.490683 161 79 1666135102 0.011927 327 2001 FLO NL 35762500 0.469136 162 76 1960663313 0.018240 328 2002 FLO NL 41979917 0.487654 162 79 2024077522 0.020740 329 2003 FLO NL 49450000 0.561728 162 91 2128262128 0.023235 330 2004 FLO NL 42143042 0.512346 162 83 2070665943 0.020352 331 2005 FLO NL 60408834 0.512346 162 83 2188713398 0.027600 332 2006 FLO NL 14671500 0.481481 162 78 2321472617 0.006320 333 2007 FLO NL 30507000 0.438272 162 71 2476688987 0.012318 334 2008 FLO NL 21811500 0.521739 161 84 2684858670 0.008124 335 2009 FLO NL 36834000 0.537037 162 87 2664726994 0.013823 336 2010 FLO NL 57029719 0.493827 162 80 2721359865 0.020956 337 2011 FLO NL 56944000 0.444444 162 72 2784505291 0.020450 338 1985 HOU NL 9993051 0.512346 162 83 261964696 0.038147 339 1986 HOU NL 9873276 0.592593 162 96 307854518 0.032071 340 1987 HOU NL 12608371 0.469136 162 76 272575375 0.046256 341 1988 HOU NL 12286167 0.506173 162 82 300452424 0.040892 342 1989 HOU NL 15029500 0.530864 162 86 359995711 0.041749 343 1990 HOU NL 18330000 0.462963 162 75 443881193 0.041295 344 1991 HOU NL 12852500 0.401235 162 65 613048418 0.020965 345 1992 HOU NL 15407500 0.500000 162 81 805543323 0.019127 346 1993 HOU NL 30210500 0.524691 162 85 901740134 0.033502 347 1994 HOU NL 33126000 0.573913 115 66 927836287 0.035702 348 1995 HOU NL 34169834 0.527778 144 76 951469367 0.035913 349 1996 HOU NL 28487000 0.506173 162 82 956983550 0.029767 350 1997 HOU NL 34777500 0.518519 162 84 1127285885 0.030851 351 1998 HOU NL 42374000 0.629630 162 102 1278282871 0.033149 352 1999 HOU NL 54914000 0.598765 162 97 1494228750 0.036751 353 2000 HOU NL 51289111 0.444444 162 72 1666135102 0.030783 354 2001 HOU NL 60612667 0.574074 162 93 1960663313 0.030914 355 2002 HOU NL 63448417 0.518519 162 84 2024077522 0.031347 356 2003 HOU NL 71040000 0.537037 162 87 2128262128 0.033379 357 2004 HOU NL 75397000 0.567901 162 92 2070665943 0.036412 358 2005 HOU NL 76779000 0.546012 163 89 2188713398 0.035080 359 2006 HOU NL 88694435 0.506173 162 82 2321472617 0.038206 360 2007 HOU NL 87759000 0.450617 162 73 2476688987 0.035434 361 2008 HOU NL 88930414 0.534161 161 86 2684858670 0.033123 362 2009 HOU NL 102996414 0.456790 162 74 2664726994 0.038652 363 2010 HOU NL 92355500 0.469136 162 76 2721359865 0.033937 364 2011 HOU NL 70694000 0.345679 162 56 2784505291 0.025388 365 2012 HOU NL 60651000 0.339506 162 55 2932741192 0.020681 366 2013 HOU AL 17890700 0.314815 162 51 3034525648 0.005896 367 2014 HOU AL 35116300 0.432099 162 70 3192317623 0.011000 368 2015 HOU AL 72256200 0.530864 162 86 3514142569 0.020562 369 2016 HOU AL 94893700 0.518519 162 84 3750137392 0.025304 370 1985 KCA AL 9321179 0.561728 162 91 261964696 0.035582 371 1986 KCA AL 13043698 0.469136 162 76 307854518 0.042370 372 1987 KCA AL 11828056 0.512346 162 83 272575375 0.043394 373 1988 KCA AL 14556562 0.521739 161 84 300452424 0.048449 374 1989 KCA AL 18683568 0.567901 162 92 359995711 0.051899 375 1990 KCA AL 23361084 0.465839 161 75 443881193 0.052629 376 1991 KCA AL 26319834 0.506173 162 82 613048418 0.042933 377 1992 KCA AL 33893834 0.444444 162 72 805543323 0.042076 378 1993 KCA AL 41346167 0.518519 162 84 901740134 0.045852 379 1994 KCA AL 40541334 0.556522 115 64 927836287 0.043694 380 1995 KCA AL 29532834 0.486111 144 70 951469367 0.031039 381 1996 KCA AL 20281250 0.465839 161 75 956983550 0.021193 382 1997 KCA AL 34655000 0.416149 161 67 1127285885 0.030742 383 1998 KCA AL 36862500 0.447205 161 72 1278282871 0.028838 384 1999 KCA AL 26225000 0.397516 161 64 1494228750 0.017551 385 2000 KCA AL 23433000 0.475309 162 77 1666135102 0.014064 386 2001 KCA AL 35422500 0.401235 162 65 1960663313 0.018067 387 2002 KCA AL 47257000 0.382716 162 62 2024077522 0.023347 388 2003 KCA AL 40518000 0.512346 162 83 2128262128 0.019038 389 2004 KCA AL 47609000 0.358025 162 58 2070665943 0.022992 390 2005 KCA AL 36881000 0.345679 162 56 2188713398 0.016851 391 2006 KCA AL 47294000 0.382716 162 62 2321472617 0.020372 392 2007 KCA AL 67116500 0.425926 162 69 2476688987 0.027099 393 2008 KCA AL 58245500 0.462963 162 75 2684858670 0.021694 394 2009 KCA AL 70519333 0.401235 162 65 2664726994 0.026464 395 2010 KCA AL 71405210 0.413580 162 67 2721359865 0.026239 396 2011 KCA AL 35712000 0.438272 162 71 2784505291 0.012825 397 2012 KCA AL 60916225 0.444444 162 72 2932741192 0.020771 398 2013 KCA AL 80091725 0.530864 162 86 3034525648 0.026393 399 2014 KCA AL 74594075 0.549383 162 89 3192317623 0.023367 400 2015 KCA AL 112107025 0.586420 162 95 3514142569 0.031902 401 2016 KCA AL 131487125 0.500000 162 81 3750137392 0.035062 402 2005 LAA AL 94867822 0.586420 162 95 2188713398 0.043344 403 2006 LAA AL 103472000 0.549383 162 89 2321472617 0.044572 404 2007 LAA AL 109251333 0.580247 162 94 2476688987 0.044112 405 2008 LAA AL 119216333 0.617284 162 100 2684858670 0.044403 406 2009 LAA AL 113709000 0.598765 162 97 2664726994 0.042672 407 2010 LAA AL 104963866 0.493827 162 80 2721359865 0.038570 408 2011 LAA AL 138543166 0.530864 162 86 2784505291 0.049755 409 2012 LAA AL 154485166 0.549383 162 89 2932741192 0.052676 410 2013 LAA AL 124174750 0.481481 162 78 3034525648 0.040921 411 2014 LAA AL 121988250 0.604938 162 98 3192317623 0.038213 412 2015 LAA AL 120005415 0.524691 162 85 3514142569 0.034149 413 2016 LAA AL 137251333 0.456790 162 74 3750137392 0.036599 414 1985 LAN NL 10967917 0.586420 162 95 261964696 0.041868 415 1986 LAN NL 14913776 0.450617 162 73 307854518 0.048444 416 1987 LAN NL 13675403 0.450617 162 73 272575375 0.050171 417 1988 LAN NL 16850515 0.580247 162 94 300452424 0.056084 418 1989 LAN NL 21071562 0.481250 160 77 359995711 0.058533 419 1990 LAN NL 21318704 0.530864 162 86 443881193 0.048028 420 1991 LAN NL 32790664 0.574074 162 93 613048418 0.053488 421 1992 LAN NL 44788166 0.388889 162 63 805543323 0.055600 422 1993 LAN NL 39331999 0.500000 162 81 901740134 0.043618 423 1994 LAN NL 38000001 0.508772 114 58 927836287 0.040956 424 1995 LAN NL 39273201 0.541667 144 78 951469367 0.041276 425 1996 LAN NL 35355000 0.555556 162 90 956983550 0.036944 426 1997 LAN NL 45380304 0.543210 162 88 1127285885 0.040256 427 1998 LAN NL 48820000 0.512346 162 83 1278282871 0.038192 428 1999 LAN NL 80862453 0.475309 162 77 1494228750 0.054117 429 2000 LAN NL 87924286 0.530864 162 86 1666135102 0.052771 430 2001 LAN NL 109105953 0.530864 162 86 1960663313 0.055647 431 2002 LAN NL 94850953 0.567901 162 92 2024077522 0.046861 432 2003 LAN NL 105572620 0.524691 162 85 2128262128 0.049605 433 2004 LAN NL 92902001 0.574074 162 93 2070665943 0.044866 434 2005 LAN NL 83039000 0.438272 162 71 2188713398 0.037940 435 2006 LAN NL 98447187 0.543210 162 88 2321472617 0.042407 436 2007 LAN NL 108454524 0.506173 162 82 2476688987 0.043790 437 2008 LAN NL 118588536 0.518519 162 84 2684858670 0.044169 438 2009 LAN NL 100414592 0.586420 162 95 2664726994 0.037683 439 2010 LAN NL 95358016 0.493827 162 80 2721359865 0.035041 440 2011 LAN NL 104188999 0.509317 161 82 2784505291 0.037417 441 2012 LAN NL 95143575 0.530864 162 86 2932741192 0.032442 442 2013 LAN NL 223362196 0.567901 162 92 3034525648 0.073607 443 2014 LAN NL 217014600 0.580247 162 94 3192317623 0.067980 444 2015 LAN NL 215792000 0.567901 162 92 3514142569 0.061407 445 2016 LAN NL 221288380 0.561728 162 91 3750137392 0.059008 446 2012 MIA NL 118078000 0.425926 162 69 2932741192 0.040262 447 2013 MIA NL 33601900 0.382716 162 62 3034525648 0.011073 448 2014 MIA NL 41836900 0.475309 162 77 3192317623 0.013105 449 2015 MIA NL 68056500 0.438272 162 71 3514142569 0.019366 450 2016 MIA NL 77314202 0.490683 161 79 3750137392 0.020616 451 1998 MIL NL 33914904 0.456790 162 74 1278282871 0.026532 452 1999 MIL NL 43377395 0.459627 161 74 1494228750 0.029030 453 2000 MIL NL 36505333 0.447853 163 73 1666135102 0.021910 454 2001 MIL NL 43886833 0.419753 162 68 1960663313 0.022384 455 2002 MIL NL 50287833 0.345679 162 56 2024077522 0.024845 456 2003 MIL NL 40627000 0.419753 162 68 2128262128 0.019089 457 2004 MIL NL 27528500 0.416149 161 67 2070665943 0.013295 458 2005 MIL NL 39934833 0.500000 162 81 2188713398 0.018246 459 2006 MIL NL 57568333 0.462963 162 75 2321472617 0.024798 460 2007 MIL NL 70986500 0.512346 162 83 2476688987 0.028662 461 2008 MIL NL 80937499 0.555556 162 90 2684858670 0.030146 462 2009 MIL NL 80182502 0.493827 162 80 2664726994 0.030090 463 2010 MIL NL 81108278 0.475309 162 77 2721359865 0.029804 464 2011 MIL NL 85497333 0.592593 162 96 2784505291 0.030705 465 2012 MIL NL 97653944 0.512346 162 83 2932741192 0.033298 466 2013 MIL NL 76947033 0.456790 162 74 3034525648 0.025357 467 2014 MIL NL 101217000 0.506173 162 82 3192317623 0.031706 468 2015 MIL NL 100850000 0.419753 162 68 3514142569 0.028698 469 2016 MIL NL 68775237 0.450617 162 73 3750137392 0.018339 470 1985 MIN AL 5764821 0.475309 162 77 261964696 0.022006 471 1986 MIN AL 8748167 0.438272 162 71 307854518 0.028417 472 1987 MIN AL 6397500 0.524691 162 85 272575375 0.023471 473 1988 MIN AL 12462666 0.561728 162 91 300452424 0.041480 474 1989 MIN AL 15531666 0.493827 162 80 359995711 0.043144 475 1990 MIN AL 14602000 0.456790 162 74 443881193 0.032896 476 1991 MIN AL 23361833 0.586420 162 95 613048418 0.038108 477 1992 MIN AL 28027834 0.555556 162 90 805543323 0.034794 478 1993 MIN AL 28217933 0.438272 162 71 901740134 0.031293 479 1994 MIN AL 28438500 0.469027 113 53 927836287 0.030650 480 1995 MIN AL 25410500 0.388889 144 56 951469367 0.026707 481 1996 MIN AL 23117000 0.481481 162 78 956983550 0.024156 482 1997 MIN AL 34072500 0.419753 162 68 1127285885 0.030225 483 1998 MIN AL 27927500 0.432099 162 70 1278282871 0.021848 484 1999 MIN AL 21257500 0.391304 161 63 1494228750 0.014226 485 2000 MIN AL 16519500 0.425926 162 69 1666135102 0.009915 486 2001 MIN AL 24130000 0.524691 162 85 1960663313 0.012307 487 2002 MIN AL 40425000 0.583851 161 94 2024077522 0.019972 488 2003 MIN AL 55505000 0.555556 162 90 2128262128 0.026080 489 2004 MIN AL 53585000 0.567901 162 92 2070665943 0.025878 490 2005 MIN AL 56186000 0.512346 162 83 2188713398 0.025671 491 2006 MIN AL 63396006 0.592593 162 96 2321472617 0.027309 492 2007 MIN AL 71439500 0.487654 162 79 2476688987 0.028845 493 2008 MIN AL 56932766 0.539877 163 88 2684858670 0.021205 494 2009 MIN AL 65299266 0.533742 163 87 2664726994 0.024505 495 2010 MIN AL 97559166 0.580247 162 94 2721359865 0.035849 496 2011 MIN AL 112737000 0.388889 162 63 2784505291 0.040487 497 2012 MIN AL 94085000 0.407407 162 66 2932741192 0.032081 498 2013 MIN AL 75337500 0.407407 162 66 3034525648 0.024827 499 2014 MIN AL 83762500 0.432099 162 70 3192317623 0.026239 500 2015 MIN AL 107755000 0.512346 162 83 3514142569 0.030663 501 2016 MIN AL 102583200 0.364198 162 59 3750137392 0.027355 502 1985 ML4 AL 11284107 0.440994 161 71 261964696 0.043075 503 1986 ML4 AL 9943642 0.478261 161 77 307854518 0.032300 504 1987 ML4 AL 7293224 0.561728 162 91 272575375 0.026757 505 1988 ML4 AL 8402000 0.537037 162 87 300452424 0.027964 506 1989 ML4 AL 11533000 0.500000 162 81 359995711 0.032036 507 1990 ML4 AL 19719167 0.456790 162 74 443881193 0.044424 508 1991 ML4 AL 23115500 0.512346 162 83 613048418 0.037706 509 1992 ML4 AL 31013667 0.567901 162 92 805543323 0.038500 510 1993 ML4 AL 23806834 0.425926 162 69 901740134 0.026401 511 1994 ML4 AL 24350500 0.460870 115 53 927836287 0.026244 512 1995 ML4 AL 17798825 0.451389 144 65 951469367 0.018707 513 1996 ML4 AL 21730000 0.493827 162 80 956983550 0.022707 514 1997 ML4 AL 23655338 0.484472 161 78 1127285885 0.020984 515 1985 MON NL 9470166 0.521739 161 84 261964696 0.036151 516 1986 MON NL 11103600 0.484472 161 78 307854518 0.036068 517 1987 MON NL 6942052 0.561728 162 91 272575375 0.025468 518 1988 MON NL 9603333 0.496933 163 81 300452424 0.031963 519 1989 MON NL 13807389 0.500000 162 81 359995711 0.038354 520 1990 MON NL 16586388 0.524691 162 85 443881193 0.037367 521 1991 MON NL 10732333 0.440994 161 71 613048418 0.017507 522 1992 MON NL 15822334 0.537037 162 87 805543323 0.019642 523 1993 MON NL 18899333 0.576687 163 94 901740134 0.020959 524 1994 MON NL 19098000 0.649123 114 74 927836287 0.020583 525 1995 MON NL 12364000 0.458333 144 66 951469367 0.012995 526 1996 MON NL 16264500 0.543210 162 88 956983550 0.016996 527 1997 MON NL 19295500 0.481481 162 78 1127285885 0.017117 528 1998 MON NL 10641500 0.401235 162 65 1278282871 0.008325 529 1999 MON NL 17903000 0.419753 162 68 1494228750 0.011981 530 2000 MON NL 32994333 0.413580 162 67 1666135102 0.019803 531 2001 MON NL 35159500 0.419753 162 68 1960663313 0.017932 532 2002 MON NL 38670500 0.512346 162 83 2024077522 0.019105 533 2003 MON NL 51948500 0.512346 162 83 2128262128 0.024409 534 2004 MON NL 40897500 0.413580 162 67 2070665943 0.019751 535 1985 NYA AL 14238204 0.602484 161 97 261964696 0.054352 536 1986 NYA AL 18494253 0.555556 162 90 307854518 0.060075 537 1987 NYA AL 17099714 0.549383 162 89 272575375 0.062734 538 1988 NYA AL 19441152 0.527950 161 85 300452424 0.064706 539 1989 NYA AL 17114375 0.459627 161 74 359995711 0.047540 540 1990 NYA AL 20912318 0.413580 162 67 443881193 0.047112 541 1991 NYA AL 27344168 0.438272 162 71 613048418 0.044604 542 1992 NYA AL 37543334 0.469136 162 76 805543323 0.046606 543 1993 NYA AL 42624900 0.543210 162 88 901740134 0.047270 544 1994 NYA AL 45731334 0.619469 113 70 927836287 0.049288 545 1995 NYA AL 48874851 0.544828 145 79 951469367 0.051368 546 1996 NYA AL 54191792 0.567901 162 92 956983550 0.056628 547 1997 NYA AL 62241545 0.592593 162 96 1127285885 0.055214 548 1998 NYA AL 66806867 0.703704 162 114 1278282871 0.052263 549 1999 NYA AL 86734359 0.604938 162 98 1494228750 0.058046 550 2000 NYA AL 92338260 0.540373 161 87 1666135102 0.055421 551 2001 NYA AL 112287143 0.590062 161 95 1960663313 0.057270 552 2002 NYA AL 125928583 0.639752 161 103 2024077522 0.062215 553 2003 NYA AL 152749814 0.619632 163 101 2128262128 0.071772 554 2004 NYA AL 184193950 0.623457 162 101 2070665943 0.088954 555 2005 NYA AL 208306817 0.586420 162 95 2188713398 0.095173 556 2006 NYA AL 194663079 0.598765 162 97 2321472617 0.083853 557 2007 NYA AL 189259045 0.580247 162 94 2476688987 0.076416 558 2008 NYA AL 207896789 0.549383 162 89 2684858670 0.077433 559 2009 NYA AL 201449189 0.635802 162 103 2664726994 0.075598 560 2010 NYA AL 206333389 0.586420 162 95 2721359865 0.075820 561 2011 NYA AL 202275028 0.598765 162 97 2784505291 0.072643 562 2012 NYA AL 196522289 0.586420 162 95 2932741192 0.067010 563 2013 NYA AL 231978886 0.524691 162 85 3034525648 0.076447 564 2014 NYA AL 197543907 0.518519 162 84 3192317623 0.061881 565 2015 NYA AL 212751957 0.537037 162 87 3514142569 0.060542 566 2016 NYA AL 222997792 0.518519 162 84 3750137392 0.059464 567 1985 NYN NL 10834762 0.604938 162 98 261964696 0.041360 568 1986 NYN NL 15393714 0.666667 162 108 307854518 0.050003 569 1987 NYN NL 13846714 0.567901 162 92 272575375 0.050800 570 1988 NYN NL 15269314 0.625000 160 100 300452424 0.050821 571 1989 NYN NL 19885071 0.537037 162 87 359995711 0.055237 572 1990 NYN NL 21722834 0.561728 162 91 443881193 0.048938 573 1991 NYN NL 32590001 0.478261 161 77 613048418 0.053161 574 1992 NYN NL 44602002 0.444444 162 72 805543323 0.055369 575 1993 NYN NL 39043667 0.364198 162 59 901740134 0.043298 576 1994 NYN NL 30956583 0.486726 113 55 927836287 0.033364 577 1995 NYN NL 27674992 0.479167 144 69 951469367 0.029087 578 1996 NYN NL 24479500 0.438272 162 71 956983550 0.025580 579 1997 NYN NL 39800400 0.543210 162 88 1127285885 0.035306 580 1998 NYN NL 52077999 0.543210 162 88 1278282871 0.040741 581 1999 NYN NL 65092092 0.595092 163 97 1494228750 0.043562 582 2000 NYN NL 79509776 0.580247 162 94 1666135102 0.047721 583 2001 NYN NL 93174428 0.506173 162 82 1960663313 0.047522 584 2002 NYN NL 94633593 0.465839 161 75 2024077522 0.046754 585 2003 NYN NL 116876429 0.409938 161 66 2128262128 0.054916 586 2004 NYN NL 96660970 0.438272 162 71 2070665943 0.046681 587 2005 NYN NL 101305821 0.512346 162 83 2188713398 0.046286 588 2006 NYN NL 101084963 0.598765 162 97 2321472617 0.043543 589 2007 NYN NL 115231663 0.543210 162 88 2476688987 0.046526 590 2008 NYN NL 137793376 0.549383 162 89 2684858670 0.051322 591 2009 NYN NL 149373987 0.432099 162 70 2664726994 0.056056 592 2010 NYN NL 134422942 0.487654 162 79 2721359865 0.049396 593 2011 NYN NL 118847309 0.475309 162 77 2784505291 0.042682 594 2012 NYN NL 93353983 0.456790 162 74 2932741192 0.031832 595 2013 NYN NL 49448346 0.456790 162 74 3034525648 0.016295 596 2014 NYN NL 85556990 0.487654 162 79 3192317623 0.026801 597 2015 NYN NL 96766683 0.555556 162 90 3514142569 0.027536 598 2016 NYN NL 133889129 0.537037 162 87 3750137392 0.035702 599 1985 OAK AL 9058606 0.475309 162 77 261964696 0.034579 600 1986 OAK AL 9779421 0.469136 162 76 307854518 0.031766 601 1987 OAK AL 11680839 0.500000 162 81 272575375 0.042854 602 1988 OAK AL 9690000 0.641975 162 104 300452424 0.032251 603 1989 OAK AL 15613070 0.611111 162 99 359995711 0.043370 604 1990 OAK AL 19887501 0.635802 162 103 443881193 0.044804 605 1991 OAK AL 36999167 0.518519 162 84 613048418 0.060353 606 1992 OAK AL 41035000 0.592593 162 96 805543323 0.050941 607 1993 OAK AL 37812333 0.419753 162 68 901740134 0.041933 608 1994 OAK AL 34172500 0.447368 114 51 927836287 0.036830 609 1995 OAK AL 37739225 0.465278 144 67 951469367 0.039664 610 1996 OAK AL 21243000 0.481481 162 78 956983550 0.022198 611 1997 OAK AL 24018500 0.401235 162 65 1127285885 0.021306 612 1998 OAK AL 21303000 0.456790 162 74 1278282871 0.016665 613 1999 OAK AL 24431833 0.537037 162 87 1494228750 0.016351 614 2000 OAK AL 31971333 0.565217 161 91 1666135102 0.019189 615 2001 OAK AL 33810750 0.629630 162 102 1960663313 0.017245 616 2002 OAK AL 40004167 0.635802 162 103 2024077522 0.019764 617 2003 OAK AL 50260834 0.592593 162 96 2128262128 0.023616 618 2004 OAK AL 59425667 0.561728 162 91 2070665943 0.028699 619 2005 OAK AL 55425762 0.543210 162 88 2188713398 0.025323 620 2006 OAK AL 62243079 0.574074 162 93 2321472617 0.026812 621 2007 OAK AL 79366940 0.469136 162 76 2476688987 0.032046 622 2008 OAK AL 47967126 0.465839 161 75 2684858670 0.017866 623 2009 OAK AL 61910000 0.462963 162 75 2664726994 0.023233 624 2010 OAK AL 55254900 0.500000 162 81 2721359865 0.020304 625 2011 OAK AL 66536500 0.456790 162 74 2784505291 0.023895 626 2012 OAK AL 55372500 0.580247 162 94 2932741192 0.018881 627 2013 OAK AL 60132500 0.592593 162 96 3034525648 0.019816 628 2014 OAK AL 72408400 0.543210 162 88 3192317623 0.022682 629 2015 OAK AL 79053501 0.419753 162 68 3514142569 0.022496 630 2016 OAK AL 86806234 0.425926 162 69 3750137392 0.023147 631 1985 PHI NL 10124966 0.462963 162 75 261964696 0.038650 632 1986 PHI NL 11590166 0.534161 161 86 307854518 0.037648 633 1987 PHI NL 11514233 0.493827 162 80 272575375 0.042242 634 1988 PHI NL 13838000 0.401235 162 65 300452424 0.046057 635 1989 PHI NL 10604000 0.411043 163 67 359995711 0.029456 636 1990 PHI NL 13173667 0.475309 162 77 443881193 0.029678 637 1991 PHI NL 22487332 0.481481 162 78 613048418 0.036681 638 1992 PHI NL 24383834 0.432099 162 70 805543323 0.030270 639 1993 PHI NL 28538334 0.598765 162 97 901740134 0.031648 640 1994 PHI NL 31599000 0.469565 115 54 927836287 0.034057 641 1995 PHI NL 30555945 0.479167 144 69 951469367 0.032114 642 1996 PHI NL 34314500 0.413580 162 67 956983550 0.035857 643 1997 PHI NL 36656500 0.419753 162 68 1127285885 0.032517 644 1998 PHI NL 36297500 0.462963 162 75 1278282871 0.028396 645 1999 PHI NL 31692500 0.475309 162 77 1494228750 0.021210 646 2000 PHI NL 47308000 0.401235 162 65 1666135102 0.028394 647 2001 PHI NL 41663833 0.530864 162 86 1960663313 0.021250 648 2002 PHI NL 57954999 0.496894 161 80 2024077522 0.028633 649 2003 PHI NL 70780000 0.530864 162 86 2128262128 0.033257 650 2004 PHI NL 92919167 0.530864 162 86 2070665943 0.044874 651 2005 PHI NL 95522000 0.543210 162 88 2188713398 0.043643 652 2006 PHI NL 88273333 0.524691 162 85 2321472617 0.038025 653 2007 PHI NL 89428213 0.549383 162 89 2476688987 0.036108 654 2008 PHI NL 97879880 0.567901 162 92 2684858670 0.036456 655 2009 PHI NL 113004046 0.574074 162 93 2664726994 0.042407 656 2010 PHI NL 141928379 0.598765 162 97 2721359865 0.052153 657 2011 PHI NL 172976379 0.629630 162 102 2784505291 0.062121 658 2012 PHI NL 174538938 0.500000 162 81 2932741192 0.059514 659 2013 PHI NL 169863189 0.450617 162 73 3034525648 0.055977 660 2014 PHI NL 180944967 0.450617 162 73 3192317623 0.056681 661 2015 PHI NL 111693000 0.388889 162 63 3514142569 0.031784 662 2016 PHI NL 58980000 0.438272 162 71 3750137392 0.015727 663 1985 PIT NL 9227500 0.354037 161 57 261964696 0.035224 664 1986 PIT NL 10843500 0.395062 162 64 307854518 0.035223 665 1987 PIT NL 7652000 0.493827 162 80 272575375 0.028073 666 1988 PIT NL 5998500 0.531250 160 85 300452424 0.019965 667 1989 PIT NL 12737500 0.451220 164 74 359995711 0.035382 668 1990 PIT NL 15556000 0.586420 162 95 443881193 0.035045 669 1991 PIT NL 23634667 0.604938 162 98 613048418 0.038553 670 1992 PIT NL 33944167 0.592593 162 96 805543323 0.042138 671 1993 PIT NL 24822467 0.462963 162 75 901740134 0.027527 672 1994 PIT NL 24217250 0.464912 114 53 927836287 0.026101 673 1995 PIT NL 18355345 0.402778 144 58 951469367 0.019292 674 1996 PIT NL 23017500 0.450617 162 73 956983550 0.024052 675 1997 PIT NL 10771667 0.487654 162 79 1127285885 0.009555 676 1998 PIT NL 15065000 0.423313 163 69 1278282871 0.011785 677 1999 PIT NL 24697666 0.484472 161 78 1494228750 0.016529 678 2000 PIT NL 28928334 0.425926 162 69 1666135102 0.017363 679 2001 PIT NL 57760833 0.382716 162 62 1960663313 0.029460 680 2002 PIT NL 42323599 0.447205 161 72 2024077522 0.020910 681 2003 PIT NL 54812429 0.462963 162 75 2128262128 0.025755 682 2004 PIT NL 32227929 0.447205 161 72 2070665943 0.015564 683 2005 PIT NL 38133000 0.413580 162 67 2188713398 0.017423 684 2006 PIT NL 46717750 0.413580 162 67 2321472617 0.020124 685 2007 PIT NL 38537833 0.419753 162 68 2476688987 0.015560 686 2008 PIT NL 48689783 0.413580 162 67 2684858670 0.018135 687 2009 PIT NL 48693000 0.385093 161 62 2664726994 0.018273 688 2010 PIT NL 34943000 0.351852 162 57 2721359865 0.012840 689 2011 PIT NL 45047000 0.444444 162 72 2784505291 0.016178 690 2012 PIT NL 62951999 0.487654 162 79 2932741192 0.021465 691 2013 PIT NL 77062000 0.580247 162 94 3034525648 0.025395 692 2014 PIT NL 77178000 0.543210 162 88 3192317623 0.024176 693 2015 PIT NL 88892499 0.604938 162 98 3514142569 0.025296 694 2016 PIT NL 103778833 0.481481 162 78 3750137392 0.027673 695 1985 SDN NL 11036583 0.512346 162 83 261964696 0.042130 696 1986 SDN NL 11380693 0.456790 162 74 307854518 0.036968 697 1987 SDN NL 11065796 0.401235 162 65 272575375 0.040597 698 1988 SDN NL 9561002 0.515528 161 83 300452424 0.031822 699 1989 SDN NL 14195000 0.549383 162 89 359995711 0.039431 700 1990 SDN NL 17588334 0.462963 162 75 443881193 0.039624 701 1991 SDN NL 22150001 0.518519 162 84 613048418 0.036131 702 1992 SDN NL 26854167 0.506173 162 82 805543323 0.033337 703 1993 SDN NL 25511333 0.376543 162 61 901740134 0.028291 704 1994 SDN NL 14916333 0.401709 117 47 927836287 0.016076 705 1995 SDN NL 26382334 0.486111 144 70 951469367 0.027728 706 1996 SDN NL 28348172 0.561728 162 91 956983550 0.029622 707 1997 SDN NL 37363672 0.469136 162 76 1127285885 0.033145 708 1998 SDN NL 46861500 0.604938 162 98 1278282871 0.036660 709 1999 SDN NL 49768179 0.456790 162 74 1494228750 0.033307 710 2000 SDN NL 54821000 0.469136 162 76 1666135102 0.032903 711 2001 SDN NL 39182833 0.487654 162 79 1960663313 0.019984 712 2002 SDN NL 41425000 0.407407 162 66 2024077522 0.020466 713 2003 SDN NL 45210000 0.395062 162 64 2128262128 0.021243 714 2004 SDN NL 55384833 0.537037 162 87 2070665943 0.026747 715 2005 SDN NL 63290833 0.506173 162 82 2188713398 0.028917 716 2006 SDN NL 69896141 0.543210 162 88 2321472617 0.030109 717 2007 SDN NL 58110567 0.546012 163 89 2476688987 0.023463 718 2008 SDN NL 73677616 0.388889 162 63 2684858670 0.027442 719 2009 SDN NL 43333700 0.462963 162 75 2664726994 0.016262 720 2010 SDN NL 37799300 0.555556 162 90 2721359865 0.013890 721 2011 SDN NL 45869140 0.438272 162 71 2784505291 0.016473 722 2012 SDN NL 55244700 0.469136 162 76 2932741192 0.018837 723 2013 SDN NL 65585500 0.469136 162 76 3034525648 0.021613 724 2014 SDN NL 75685700 0.475309 162 77 3192317623 0.023709 725 2015 SDN NL 118441300 0.456790 162 74 3514142569 0.033704 726 2016 SDN NL 101424814 0.419753 162 68 3750137392 0.027046 727 1985 SEA AL 4613000 0.456790 162 74 261964696 0.017609 728 1986 SEA AL 5958309 0.413580 162 67 307854518 0.019354 729 1987 SEA AL 2263500 0.481481 162 78 272575375 0.008304 730 1988 SEA AL 7342450 0.422360 161 68 300452424 0.024438 731 1989 SEA AL 9779500 0.450617 162 73 359995711 0.027166 732 1990 SEA AL 12553667 0.475309 162 77 443881193 0.028282 733 1991 SEA AL 15691833 0.512346 162 83 613048418 0.025596 734 1992 SEA AL 23179833 0.395062 162 64 805543323 0.028775 735 1993 SEA AL 32696333 0.506173 162 82 901740134 0.036259 736 1994 SEA AL 29228500 0.437500 112 49 927836287 0.031502 737 1995 SEA AL 36481311 0.544828 145 79 951469367 0.038342 738 1996 SEA AL 41328501 0.527950 161 85 956983550 0.043186 739 1997 SEA AL 41540661 0.555556 162 90 1127285885 0.036850 740 1998 SEA AL 54087036 0.472050 161 76 1278282871 0.042312 741 1999 SEA AL 54125003 0.487654 162 79 1494228750 0.036223 742 2000 SEA AL 58915000 0.561728 162 91 1666135102 0.035360 743 2001 SEA AL 74720834 0.716049 162 116 1960663313 0.038110 744 2002 SEA AL 80282668 0.574074 162 93 2024077522 0.039664 745 2003 SEA AL 86959167 0.574074 162 93 2128262128 0.040859 746 2004 SEA AL 81515834 0.388889 162 63 2070665943 0.039367 747 2005 SEA AL 87754334 0.425926 162 69 2188713398 0.040094 748 2006 SEA AL 87959833 0.481481 162 78 2321472617 0.037890 749 2007 SEA AL 106460833 0.543210 162 88 2476688987 0.042985 750 2008 SEA AL 117666482 0.376543 162 61 2684858670 0.043826 751 2009 SEA AL 98904166 0.524691 162 85 2664726994 0.037116 752 2010 SEA AL 86510000 0.376543 162 61 2721359865 0.031789 753 2011 SEA AL 86110600 0.413580 162 67 2784505291 0.030925 754 2012 SEA AL 81978100 0.462963 162 75 2932741192 0.027953 755 2013 SEA AL 74005043 0.438272 162 71 3034525648 0.024388 756 2014 SEA AL 92531100 0.537037 162 87 3192317623 0.028986 757 2015 SEA AL 122208700 0.469136 162 76 3514142569 0.034776 758 2016 SEA AL 135683339 0.530864 162 86 3750137392 0.036181 759 1985 SFN NL 8221714 0.382716 162 62 261964696 0.031385 760 1986 SFN NL 8947000 0.512346 162 83 307854518 0.029062 761 1987 SFN NL 7290000 0.555556 162 90 272575375 0.026745 762 1988 SFN NL 12380000 0.512346 162 83 300452424 0.041205 763 1989 SFN NL 14962834 0.567901 162 92 359995711 0.041564 764 1990 SFN NL 19335333 0.524691 162 85 443881193 0.043560 765 1991 SFN NL 30967666 0.462963 162 75 613048418 0.050514 766 1992 SFN NL 33163168 0.444444 162 72 805543323 0.041169 767 1993 SFN NL 35050000 0.635802 162 103 901740134 0.038869 768 1994 SFN NL 42638666 0.478261 115 55 927836287 0.045955 769 1995 SFN NL 36462777 0.465278 144 67 951469367 0.038323 770 1996 SFN NL 37144725 0.419753 162 68 956983550 0.038814 771 1997 SFN NL 35592378 0.555556 162 90 1127285885 0.031574 772 1998 SFN NL 42565834 0.546012 163 89 1278282871 0.033299 773 1999 SFN NL 46595057 0.530864 162 86 1494228750 0.031183 774 2000 SFN NL 53737826 0.598765 162 97 1666135102 0.032253 775 2001 SFN NL 63280167 0.555556 162 90 1960663313 0.032275 776 2002 SFN NL 78299835 0.586420 162 95 2024077522 0.038684 777 2003 SFN NL 82852167 0.621118 161 100 2128262128 0.038929 778 2004 SFN NL 82019166 0.561728 162 91 2070665943 0.039610 779 2005 SFN NL 90199500 0.462963 162 75 2188713398 0.041211 780 2006 SFN NL 90056419 0.472050 161 76 2321472617 0.038793 781 2007 SFN NL 90219056 0.438272 162 71 2476688987 0.036427 782 2008 SFN NL 76594500 0.444444 162 72 2684858670 0.028528 783 2009 SFN NL 83026450 0.543210 162 88 2664726994 0.031158 784 2010 SFN NL 98641333 0.567901 162 92 2721359865 0.036247 785 2011 SFN NL 118198333 0.530864 162 86 2784505291 0.042449 786 2012 SFN NL 117620683 0.580247 162 94 2932741192 0.040106 787 2013 SFN NL 140180334 0.469136 162 76 3034525648 0.046195 788 2014 SFN NL 163510167 0.543210 162 88 3192317623 0.051220 789 2015 SFN NL 164701500 0.518519 162 84 3514142569 0.046868 790 2016 SFN NL 172253778 0.537037 162 87 3750137392 0.045933 791 1985 SLN NL 11817083 0.623457 162 101 261964696 0.045109 792 1986 SLN NL 9875010 0.490683 161 79 307854518 0.032077 793 1987 SLN NL 11758000 0.586420 162 95 272575375 0.043137 794 1988 SLN NL 12880000 0.469136 162 76 300452424 0.042869 795 1989 SLN NL 16078833 0.524390 164 86 359995711 0.044664 796 1990 SLN NL 20523334 0.432099 162 70 443881193 0.046236 797 1991 SLN NL 21860001 0.518519 162 84 613048418 0.035658 798 1992 SLN NL 27583836 0.512346 162 83 805543323 0.034243 799 1993 SLN NL 23367334 0.537037 162 87 901740134 0.025914 800 1994 SLN NL 29275601 0.460870 115 53 927836287 0.031553 801 1995 SLN NL 37101000 0.433566 143 62 951469367 0.038993 802 1996 SLN NL 40269667 0.543210 162 88 956983550 0.042080 803 1997 SLN NL 45456667 0.450617 162 73 1127285885 0.040324 804 1998 SLN NL 54672521 0.509202 163 83 1278282871 0.042770 805 1999 SLN NL 49778195 0.465839 161 75 1494228750 0.033314 806 2000 SLN NL 61453863 0.586420 162 95 1666135102 0.036884 807 2001 SLN NL 78538333 0.574074 162 93 1960663313 0.040057 808 2002 SLN NL 74660875 0.598765 162 97 2024077522 0.036886 809 2003 SLN NL 83786666 0.524691 162 85 2128262128 0.039369 810 2004 SLN NL 83228333 0.648148 162 105 2070665943 0.040194 811 2005 SLN NL 92106833 0.617284 162 100 2188713398 0.042083 812 2006 SLN NL 88891371 0.515528 161 83 2321472617 0.038291 813 2007 SLN NL 90286823 0.481481 162 78 2476688987 0.036455 814 2008 SLN NL 99624449 0.530864 162 86 2684858670 0.037106 815 2009 SLN NL 88528409 0.561728 162 91 2664726994 0.033222 816 2010 SLN NL 93540751 0.530864 162 86 2721359865 0.034373 817 2011 SLN NL 105433572 0.555556 162 90 2784505291 0.037864 818 2012 SLN NL 110300862 0.543210 162 88 2932741192 0.037610 819 2013 SLN NL 92260110 0.598765 162 97 3034525648 0.030403 820 2014 SLN NL 120693000 0.555556 162 90 3192317623 0.037807 821 2015 SLN NL 119241500 0.617284 162 100 3514142569 0.033932 822 2016 SLN NL 143053500 0.530864 162 86 3750137392 0.038146 823 1998 TBA AL 27280000 0.388889 162 63 1278282871 0.021341 824 1999 TBA AL 38870000 0.425926 162 69 1494228750 0.026013 825 2000 TBA AL 62765129 0.428571 161 69 1666135102 0.037671 826 2001 TBA AL 56980000 0.382716 162 62 1960663313 0.029062 827 2002 TBA AL 34380000 0.341615 161 55 2024077522 0.016986 828 2003 TBA AL 19630000 0.388889 162 63 2128262128 0.009223 829 2004 TBA AL 29556667 0.434783 161 70 2070665943 0.014274 830 2005 TBA AL 29679067 0.413580 162 67 2188713398 0.013560 831 2006 TBA AL 34917967 0.376543 162 61 2321472617 0.015041 832 2007 TBA AL 24123500 0.407407 162 66 2476688987 0.009740 833 2008 TBA AL 43820597 0.598765 162 97 2684858670 0.016321 834 2009 TBA AL 63313034 0.518519 162 84 2664726994 0.023760 835 2010 TBA AL 71923471 0.592593 162 96 2721359865 0.026429 836 2011 TBA AL 41053571 0.561728 162 91 2784505291 0.014744 837 2012 TBA AL 64173500 0.555556 162 90 2932741192 0.021882 838 2013 TBA AL 52955272 0.564417 163 92 3034525648 0.017451 839 2014 TBA AL 72689100 0.475309 162 77 3192317623 0.022770 840 2015 TBA AL 64521233 0.493827 162 80 3514142569 0.018360 841 2016 TBA AL 57097310 0.419753 162 68 3750137392 0.015225 842 1985 TEX AL 7676500 0.385093 161 62 261964696 0.029304 843 1986 TEX AL 6743119 0.537037 162 87 307854518 0.021904 844 1987 TEX AL 880000 0.462963 162 75 272575375 0.003228 845 1988 TEX AL 5342131 0.434783 161 70 300452424 0.017780 846 1989 TEX AL 11893781 0.512346 162 83 359995711 0.033039 847 1990 TEX AL 14874372 0.512346 162 83 443881193 0.033510 848 1991 TEX AL 18224500 0.524691 162 85 613048418 0.029728 849 1992 TEX AL 30128167 0.475309 162 77 805543323 0.037401 850 1993 TEX AL 36376959 0.530864 162 86 901740134 0.040341 851 1994 TEX AL 32973597 0.456140 114 52 927836287 0.035538 852 1995 TEX AL 34581451 0.513889 144 74 951469367 0.036345 853 1996 TEX AL 39041528 0.552147 163 90 956983550 0.040796 854 1997 TEX AL 53448838 0.475309 162 77 1127285885 0.047414 855 1998 TEX AL 56572095 0.543210 162 88 1278282871 0.044256 856 1999 TEX AL 76709931 0.586420 162 95 1494228750 0.051337 857 2000 TEX AL 70795921 0.438272 162 71 1666135102 0.042491 858 2001 TEX AL 88633500 0.450617 162 73 1960663313 0.045206 859 2002 TEX AL 105526122 0.444444 162 72 2024077522 0.052135 860 2003 TEX AL 103491667 0.438272 162 71 2128262128 0.048627 861 2004 TEX AL 55050417 0.549383 162 89 2070665943 0.026586 862 2005 TEX AL 55849000 0.487654 162 79 2188713398 0.025517 863 2006 TEX AL 68228662 0.493827 162 80 2321472617 0.029390 864 2007 TEX AL 68318675 0.462963 162 75 2476688987 0.027585 865 2008 TEX AL 67712326 0.487654 162 79 2684858670 0.025220 866 2009 TEX AL 68178798 0.537037 162 87 2664726994 0.025586 867 2010 TEX AL 55250544 0.555556 162 90 2721359865 0.020303 868 2011 TEX AL 92299264 0.592593 162 96 2784505291 0.033147 869 2012 TEX AL 120510974 0.574074 162 93 2932741192 0.041092 870 2013 TEX AL 112522600 0.558282 163 91 3034525648 0.037081 871 2014 TEX AL 112255059 0.413580 162 67 3192317623 0.035164 872 2015 TEX AL 143742789 0.543210 162 88 3514142569 0.040904 873 2016 TEX AL 176038723 0.586420 162 95 3750137392 0.046942 874 1985 TOR AL 8812550 0.614907 161 99 261964696 0.033640 875 1986 TOR AL 12611047 0.527607 163 86 307854518 0.040964 876 1987 TOR AL 10479501 0.592593 162 96 272575375 0.038446 877 1988 TOR AL 12241225 0.537037 162 87 300452424 0.040743 878 1989 TOR AL 16261666 0.549383 162 89 359995711 0.045172 879 1990 TOR AL 17756834 0.530864 162 86 443881193 0.040004 880 1991 TOR AL 19902417 0.561728 162 91 613048418 0.032465 881 1992 TOR AL 44788666 0.592593 162 96 805543323 0.055601 882 1993 TOR AL 47279166 0.586420 162 95 901740134 0.052431 883 1994 TOR AL 43433668 0.478261 115 55 927836287 0.046812 884 1995 TOR AL 50590000 0.388889 144 56 951469367 0.053170 885 1996 TOR AL 29555083 0.456790 162 74 956983550 0.030884 886 1997 TOR AL 47079833 0.469136 162 76 1127285885 0.041764 887 1998 TOR AL 51376000 0.539877 163 88 1278282871 0.040191 888 1999 TOR AL 45444333 0.518519 162 84 1494228750 0.030413 889 2000 TOR AL 44838332 0.512346 162 83 1666135102 0.026912 890 2001 TOR AL 76895999 0.493827 162 80 1960663313 0.039219 891 2002 TOR AL 76864333 0.481481 162 78 2024077522 0.037975 892 2003 TOR AL 51269000 0.530864 162 86 2128262128 0.024090 893 2004 TOR AL 50017000 0.416149 161 67 2070665943 0.024155 894 2005 TOR AL 45719500 0.493827 162 80 2188713398 0.020889 895 2006 TOR AL 71365000 0.537037 162 87 2321472617 0.030741 896 2007 TOR AL 81942800 0.512346 162 83 2476688987 0.033086 897 2008 TOR AL 97793900 0.530864 162 86 2684858670 0.036424 898 2009 TOR AL 80538300 0.462963 162 75 2664726994 0.030224 899 2010 TOR AL 62234000 0.524691 162 85 2721359865 0.022869 900 2011 TOR AL 62567800 0.500000 162 81 2784505291 0.022470 901 2012 TOR AL 75009200 0.450617 162 73 2932741192 0.025576 902 2013 TOR AL 126288100 0.456790 162 74 3034525648 0.041617 903 2014 TOR AL 109920100 0.512346 162 83 3192317623 0.034433 904 2015 TOR AL 112992400 0.574074 162 93 3514142569 0.032154 905 2016 TOR AL 138701700 0.549383 162 89 3750137392 0.036986 906 2005 WAS NL 48581500 0.500000 162 81 2188713398 0.022196 907 2006 WAS NL 63143000 0.438272 162 71 2321472617 0.027200 908 2007 WAS NL 36947500 0.450617 162 73 2476688987 0.014918 909 2008 WAS NL 54961000 0.366460 161 59 2684858670 0.020471 910 2009 WAS NL 59928000 0.364198 162 59 2664726994 0.022489 911 2010 WAS NL 61400000 0.425926 162 69 2721359865 0.022562 912 2011 WAS NL 63856928 0.496894 161 80 2784505291 0.022933 913 2012 WAS NL 80855143 0.604938 162 98 2932741192 0.027570 914 2013 WAS NL 113703270 0.530864 162 86 3034525648 0.037470 915 2014 WAS NL 131983680 0.592593 162 96 3192317623 0.041344 916 2015 WAS NL 155587472 0.512346 162 83 3514142569 0.044275 917 2016 WAS NL 141652646 0.586420 162 95 3750137392 0.037773 <p>Before running a regression, we use sns.reglot() to look at the relationship between salaries and win percentage on a chart.</p> In\u00a0[9]: Copied! <pre>sns.regplot(x=\"relsal\", y=\"wpc\", data = MLB, ci=False)\n</pre> sns.regplot(x=\"relsal\", y=\"wpc\", data = MLB, ci=False) Out[9]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f6aaff0c0f0&gt;</pre> <p>The chart shows a positive relationship between win percentage and relsal.</p> <p>The size of the dots, which each represent a single team in a single season, is too large for the scatter to be clearly visible. We can change the size of the dots in regplot using the command \"scatter_kws={'s':3}\".</p> In\u00a0[10]: Copied! <pre>sns.regplot(x=\"relsal\", y=\"wpc\", data = MLB, scatter_kws={'s':3}, ci=False)\n</pre> sns.regplot(x=\"relsal\", y=\"wpc\", data = MLB, scatter_kws={'s':3}, ci=False) Out[10]: <pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f6a9fe2b5c0&gt;</pre> <p>While there are some outliers, the relsal variable on the x axis for most teams lies between 0.01 (1%) and a little over .06 (6%). Win percentage on the y axis for most teams lies between 0.33 and 0.66.</p> <p>We now run a regression using smf.ols() in order to derive the coefficients of the regression and other diagnostic statistics.</p> In\u00a0[11]: Copied! <pre>wpcsal1_lm = smf.ols(formula = 'wpc ~ relsal', data=MLB).fit()\nprint(wpcsal1_lm.summary())\n</pre> wpcsal1_lm = smf.ols(formula = 'wpc ~ relsal', data=MLB).fit() print(wpcsal1_lm.summary()) <pre>                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    wpc   R-squared:                       0.134\nModel:                            OLS   Adj. R-squared:                  0.133\nMethod:                 Least Squares   F-statistic:                     141.5\nDate:                Fri, 11 Jun 2021   Prob (F-statistic):           1.94e-30\nTime:                        16:58:37   Log-Likelihood:                 1222.7\nNo. Observations:                 918   AIC:                            -2441.\nDf Residuals:                     916   BIC:                            -2432.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.4301      0.006     68.965      0.000       0.418       0.442\nrelsal         2.0021      0.168     11.894      0.000       1.672       2.332\n==============================================================================\nOmnibus:                        1.963   Durbin-Watson:                   1.316\nProb(Omnibus):                  0.375   Jarque-Bera (JB):                1.883\nSkew:                          -0.050   Prob(JB):                        0.390\nKurtosis:                       2.802   Cond. No.                         79.9\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n</pre> <p>As with the NBA, we find that the coefficient on relsal is highly significant, but the size of our initial estimate is much smaller- recall that for the NBA the value was 11.3 - nearly six times larger than the coefficient for relsal in MLB. As an initial evaluation we can conclude that the amount of money to outperform your rivals is higher for MLB than than the NBA. Note also that the R-squared (0.134) is a little smaller than the one we found for the NBA (0.172), but not by that much. This suggests that win percentage can buy you success as reliably as it can in the NBA, it's just that you need to spend a mot more (relative to your rivals).</p> <p>Let's now see if the addition of the lagged dependent variable changes our relsal estimate.</p> In\u00a0[12]: Copied! <pre># first we sort the values\n\nMLB.sort_values(by=['Team','season'], ascending=True)\n</pre> # first we sort the values  MLB.sort_values(by=['Team','season'], ascending=True) Out[12]: season Team lgID salaries wpc G W allsal relsal 0 1997 ANA AL 31135472 0.518519 162 84 1127285885 0.027620 1 1998 ANA AL 41281000 0.524691 162 85 1278282871 0.032294 2 1999 ANA AL 55388166 0.432099 162 70 1494228750 0.037068 3 2000 ANA AL 51464167 0.506173 162 82 1666135102 0.030888 4 2001 ANA AL 47535167 0.462963 162 75 1960663313 0.024244 5 2002 ANA AL 61721667 0.611111 162 99 2024077522 0.030494 6 2003 ANA AL 79031667 0.475309 162 77 2128262128 0.037134 7 2004 ANA AL 100534667 0.567901 162 92 2070665943 0.048552 8 1998 ARI NL 32347000 0.401235 162 65 1278282871 0.025305 9 1999 ARI NL 68703999 0.617284 162 100 1494228750 0.045980 10 2000 ARI NL 81027833 0.524691 162 85 1666135102 0.048632 11 2001 ARI NL 85082999 0.567901 162 92 1960663313 0.043395 12 2002 ARI NL 102819999 0.604938 162 98 2024077522 0.050798 13 2003 ARI NL 80657000 0.518519 162 84 2128262128 0.037898 14 2004 ARI NL 69780750 0.314815 162 51 2070665943 0.033700 15 2005 ARI NL 62329166 0.475309 162 77 2188713398 0.028478 16 2006 ARI NL 59684226 0.469136 162 76 2321472617 0.025710 17 2007 ARI NL 52067546 0.555556 162 90 2476688987 0.021023 18 2008 ARI NL 66202712 0.506173 162 82 2684858670 0.024658 19 2009 ARI NL 73115666 0.432099 162 70 2664726994 0.027438 20 2010 ARI NL 60718166 0.401235 162 65 2721359865 0.022312 21 2011 ARI NL 53639833 0.580247 162 94 2784505291 0.019264 22 2012 ARI NL 73804833 0.500000 162 81 2932741192 0.025166 23 2013 ARI NL 90132000 0.500000 162 81 3034525648 0.029702 24 2014 ARI NL 97861500 0.395062 162 64 3192317623 0.030655 25 2015 ARI NL 61834000 0.487654 162 79 3514142569 0.017596 26 2016 ARI NL 87439063 0.425926 162 69 3750137392 0.023316 27 1985 ATL NL 14807000 0.407407 162 66 261964696 0.056523 28 1986 ATL NL 17102786 0.447205 161 72 307854518 0.055555 29 1987 ATL NL 16544560 0.428571 161 69 272575375 0.060697 30 1988 ATL NL 12728174 0.337500 160 54 300452424 0.042363 31 1989 ATL NL 11112334 0.391304 161 63 359995711 0.030868 32 1990 ATL NL 14555501 0.401235 162 65 443881193 0.032791 33 1991 ATL NL 18403500 0.580247 162 94 613048418 0.030020 34 1992 ATL NL 34625333 0.604938 162 98 805543323 0.042984 35 1993 ATL NL 41641417 0.641975 162 104 901740134 0.046179 36 1994 ATL NL 49383513 0.596491 114 68 927836287 0.053224 37 1995 ATL NL 47235445 0.625000 144 90 951469367 0.049645 38 1996 ATL NL 49698500 0.592593 162 96 956983550 0.051932 39 1997 ATL NL 52278500 0.623457 162 101 1127285885 0.046376 40 1998 ATL NL 61186000 0.654321 162 106 1278282871 0.047866 41 1999 ATL NL 73140000 0.635802 162 103 1494228750 0.048948 42 2000 ATL NL 84537836 0.586420 162 95 1666135102 0.050739 43 2001 ATL NL 91936166 0.543210 162 88 1960663313 0.046890 44 2002 ATL NL 92870367 0.627329 161 101 2024077522 0.045883 45 2003 ATL NL 106243667 0.623457 162 101 2128262128 0.049920 46 2004 ATL NL 90182500 0.592593 162 96 2070665943 0.043552 47 2005 ATL NL 86457302 0.555556 162 90 2188713398 0.039501 48 2006 ATL NL 90156876 0.487654 162 79 2321472617 0.038836 49 2007 ATL NL 87290833 0.518519 162 84 2476688987 0.035245 50 2008 ATL NL 102365683 0.444444 162 72 2684858670 0.038127 51 2009 ATL NL 96726166 0.530864 162 86 2664726994 0.036299 52 2010 ATL NL 84423666 0.561728 162 91 2721359865 0.031023 53 2011 ATL NL 87002692 0.549383 162 89 2784505291 0.031245 54 2012 ATL NL 82829942 0.580247 162 94 2932741192 0.028243 55 2013 ATL NL 87871525 0.592593 162 96 3034525648 0.028957 56 2014 ATL NL 97609000 0.487654 162 79 3192317623 0.030576 57 2015 ATL NL 71781250 0.413580 162 67 3514142569 0.020426 58 2016 ATL NL 68498291 0.422360 161 68 3750137392 0.018266 59 1985 BAL AL 11560712 0.515528 161 83 261964696 0.044131 60 1986 BAL AL 13001258 0.450617 162 73 307854518 0.042232 61 1987 BAL AL 13900273 0.413580 162 67 272575375 0.050996 62 1988 BAL AL 13532075 0.335404 161 54 300452424 0.045039 63 1989 BAL AL 8275167 0.537037 162 87 359995711 0.022987 64 1990 BAL AL 9680084 0.472050 161 76 443881193 0.021808 65 1991 BAL AL 17519000 0.413580 162 67 613048418 0.028577 66 1992 BAL AL 23780667 0.549383 162 89 805543323 0.029521 67 1993 BAL AL 29096500 0.524691 162 85 901740134 0.032267 68 1994 BAL AL 38849769 0.562500 112 63 927836287 0.041871 69 1995 BAL AL 43942521 0.493056 144 71 951469367 0.046184 70 1996 BAL AL 54490315 0.539877 163 88 956983550 0.056940 71 1997 BAL AL 58516400 0.604938 162 98 1127285885 0.051909 72 1998 BAL AL 72355634 0.487654 162 79 1278282871 0.056604 73 1999 BAL AL 80605863 0.481481 162 78 1494228750 0.053945 74 2000 BAL AL 81447435 0.456790 162 74 1666135102 0.048884 75 2001 BAL AL 67599540 0.388889 162 63 1960663313 0.034478 76 2002 BAL AL 60493487 0.413580 162 67 2024077522 0.029887 77 2003 BAL AL 73877500 0.435583 163 71 2128262128 0.034713 78 2004 BAL AL 51623333 0.481481 162 78 2070665943 0.024931 79 2005 BAL AL 73914333 0.456790 162 74 2188713398 0.033771 80 2006 BAL AL 72585582 0.432099 162 70 2321472617 0.031267 81 2007 BAL AL 93174808 0.425926 162 69 2476688987 0.037621 82 2008 BAL AL 67196246 0.422360 161 68 2684858670 0.025028 83 2009 BAL AL 67101666 0.395062 162 64 2664726994 0.025181 84 2010 BAL AL 81612500 0.407407 162 66 2721359865 0.029990 85 2011 BAL AL 85304038 0.425926 162 69 2784505291 0.030635 86 2012 BAL AL 77353999 0.574074 162 93 2932741192 0.026376 87 2013 BAL AL 84393333 0.524691 162 85 3034525648 0.027811 88 2014 BAL AL 103416000 0.592593 162 96 3192317623 0.032395 89 2015 BAL AL 115044833 0.500000 162 81 3514142569 0.032738 90 2016 BAL AL 161863456 0.549383 162 89 3750137392 0.043162 91 1985 BOS AL 10897560 0.496933 163 81 261964696 0.041599 92 1986 BOS AL 14402239 0.590062 161 95 307854518 0.046783 93 1987 BOS AL 10144167 0.481481 162 78 272575375 0.037216 94 1988 BOS AL 13896092 0.549383 162 89 300452424 0.046251 95 1989 BOS AL 17481748 0.512346 162 83 359995711 0.048561 96 1990 BOS AL 20558333 0.543210 162 88 443881193 0.046315 97 1991 BOS AL 35167500 0.518519 162 84 613048418 0.057365 98 1992 BOS AL 43610584 0.450617 162 73 805543323 0.054138 99 1993 BOS AL 37120583 0.493827 162 80 901740134 0.041165 100 1994 BOS AL 37859084 0.469565 115 54 927836287 0.040804 101 1995 BOS AL 32455518 0.597222 144 86 951469367 0.034111 102 1996 BOS AL 42393500 0.524691 162 85 956983550 0.044299 103 1997 BOS AL 43558750 0.481481 162 78 1127285885 0.038640 104 1998 BOS AL 56757000 0.567901 162 92 1278282871 0.044401 105 1999 BOS AL 63497500 0.580247 162 94 1494228750 0.042495 106 2000 BOS AL 77940333 0.524691 162 85 1666135102 0.046779 107 2001 BOS AL 110035833 0.509317 161 82 1960663313 0.056122 108 2002 BOS AL 108366060 0.574074 162 93 2024077522 0.053538 109 2003 BOS AL 99946500 0.586420 162 95 2128262128 0.046962 110 2004 BOS AL 127298500 0.604938 162 98 2070665943 0.061477 111 2005 BOS AL 123505125 0.586420 162 95 2188713398 0.056428 112 2006 BOS AL 120099824 0.530864 162 86 2321472617 0.051734 113 2007 BOS AL 143026214 0.592593 162 96 2476688987 0.057749 114 2008 BOS AL 133390035 0.586420 162 95 2684858670 0.049682 115 2009 BOS AL 121345999 0.586420 162 95 2664726994 0.045538 116 2010 BOS AL 162447333 0.549383 162 89 2721359865 0.059693 117 2011 BOS AL 161762475 0.555556 162 90 2784505291 0.058094 118 2012 BOS AL 173186617 0.425926 162 69 2932741192 0.059053 119 2013 BOS AL 151530000 0.598765 162 97 3034525648 0.049935 120 2014 BOS AL 139019929 0.438272 162 71 3192317623 0.043548 121 2015 BOS AL 181103400 0.481481 162 78 3514142569 0.051536 122 2016 BOS AL 188545761 0.574074 162 93 3750137392 0.050277 123 1985 CAL AL 14427894 0.555556 162 90 261964696 0.055076 124 1986 CAL AL 14427258 0.567901 162 92 307854518 0.046864 125 1987 CAL AL 12843499 0.462963 162 75 272575375 0.047119 126 1988 CAL AL 11947388 0.462963 162 75 300452424 0.039765 127 1989 CAL AL 15097833 0.561728 162 91 359995711 0.041939 128 1990 CAL AL 21720000 0.493827 162 80 443881193 0.048932 129 1991 CAL AL 33060001 0.500000 162 81 613048418 0.053927 130 1992 CAL AL 34749334 0.444444 162 72 805543323 0.043138 131 1993 CAL AL 28588334 0.438272 162 71 901740134 0.031704 132 1994 CAL AL 25156218 0.408696 115 47 927836287 0.027113 133 1995 CAL AL 31223171 0.537931 145 78 951469367 0.032816 134 1996 CAL AL 28738000 0.434783 161 70 956983550 0.030030 135 1985 CHA AL 9846178 0.521472 163 85 261964696 0.037586 136 1986 CHA AL 10418819 0.444444 162 72 307854518 0.033843 137 1987 CHA AL 10641843 0.475309 162 77 272575375 0.039042 138 1988 CHA AL 6390000 0.440994 161 71 300452424 0.021268 139 1989 CHA AL 7265410 0.428571 161 69 359995711 0.020182 140 1990 CHA AL 9491500 0.580247 162 94 443881193 0.021383 141 1991 CHA AL 16919667 0.537037 162 87 613048418 0.027599 142 1992 CHA AL 30160833 0.530864 162 86 805543323 0.037442 143 1993 CHA AL 39696166 0.580247 162 94 901740134 0.044022 144 1994 CHA AL 39183836 0.592920 113 67 927836287 0.042231 145 1995 CHA AL 46961282 0.468966 145 68 951469367 0.049357 146 1996 CHA AL 45139500 0.524691 162 85 956983550 0.047169 147 1997 CHA AL 57740000 0.496894 161 80 1127285885 0.051220 148 1998 CHA AL 38335000 0.490798 163 80 1278282871 0.029989 149 1999 CHA AL 25620000 0.462963 162 75 1494228750 0.017146 150 2000 CHA AL 31133500 0.586420 162 95 1666135102 0.018686 151 2001 CHA AL 65653667 0.512346 162 83 1960663313 0.033485 152 2002 CHA AL 57052833 0.500000 162 81 2024077522 0.028187 153 2003 CHA AL 51010000 0.530864 162 86 2128262128 0.023968 154 2004 CHA AL 65212500 0.512346 162 83 2070665943 0.031493 155 2005 CHA AL 75178000 0.611111 162 99 2188713398 0.034348 156 2006 CHA AL 102750667 0.555556 162 90 2321472617 0.044261 157 2007 CHA AL 108671833 0.444444 162 72 2476688987 0.043878 158 2008 CHA AL 121189332 0.546012 163 89 2684858670 0.045138 159 2009 CHA AL 96068500 0.487654 162 79 2664726994 0.036052 160 2010 CHA AL 105530000 0.543210 162 88 2721359865 0.038778 161 2011 CHA AL 127789000 0.487654 162 79 2784505291 0.045893 162 2012 CHA AL 96919500 0.524691 162 85 2932741192 0.033047 163 2013 CHA AL 120065277 0.388889 162 63 3034525648 0.039566 164 2014 CHA AL 81830500 0.450617 162 73 3192317623 0.025634 165 2015 CHA AL 112373700 0.469136 162 76 3514142569 0.031978 166 2016 CHA AL 112998667 0.481481 162 78 3750137392 0.030132 167 1985 CHN NL 12702917 0.475309 162 77 261964696 0.048491 168 1986 CHN NL 17208165 0.437500 160 70 307854518 0.055897 169 1987 CHN NL 14307999 0.472050 161 76 272575375 0.052492 170 1988 CHN NL 13119198 0.472393 163 77 300452424 0.043665 171 1989 CHN NL 10668000 0.574074 162 93 359995711 0.029634 172 1990 CHN NL 13624000 0.475309 162 77 443881193 0.030693 173 1991 CHN NL 23175667 0.481250 160 77 613048418 0.037804 174 1992 CHN NL 29829686 0.481481 162 78 805543323 0.037031 175 1993 CHN NL 39386666 0.515337 163 84 901740134 0.043679 176 1994 CHN NL 36287333 0.433628 113 49 927836287 0.039110 177 1995 CHN NL 29505834 0.506944 144 73 951469367 0.031011 178 1996 CHN NL 33081000 0.469136 162 76 956983550 0.034568 179 1997 CHN NL 42155333 0.419753 162 68 1127285885 0.037395 180 1998 CHN NL 50838000 0.552147 163 90 1278282871 0.039771 181 1999 CHN NL 62343000 0.413580 162 67 1494228750 0.041723 182 2000 CHN NL 60539333 0.401235 162 65 1666135102 0.036335 183 2001 CHN NL 64715833 0.543210 162 88 1960663313 0.033007 184 2002 CHN NL 75690833 0.413580 162 67 2024077522 0.037395 185 2003 CHN NL 79868333 0.543210 162 88 2128262128 0.037527 186 2004 CHN NL 90560000 0.549383 162 89 2070665943 0.043735 187 2005 CHN NL 87032933 0.487654 162 79 2188713398 0.039764 188 2006 CHN NL 94424499 0.407407 162 66 2321472617 0.040674 189 2007 CHN NL 99670332 0.524691 162 85 2476688987 0.040243 190 2008 CHN NL 118345833 0.602484 161 97 2684858670 0.044079 191 2009 CHN NL 134809000 0.515528 161 83 2664726994 0.050590 192 2010 CHN NL 146609000 0.462963 162 75 2721359865 0.053873 193 2011 CHN NL 125047329 0.438272 162 71 2784505291 0.044908 194 2012 CHN NL 88197033 0.376543 162 61 2932741192 0.030073 195 2013 CHN NL 100567726 0.407407 162 66 3034525648 0.033141 196 2014 CHN NL 65522500 0.450617 162 73 3192317623 0.020525 197 2015 CHN NL 115879310 0.598765 162 97 3514142569 0.032975 198 2016 CHN NL 154067668 0.635802 162 103 3750137392 0.041083 199 1985 CIN NL 8359917 0.549383 162 89 261964696 0.031912 200 1986 CIN NL 11906388 0.530864 162 86 307854518 0.038675 201 1987 CIN NL 9281500 0.518519 162 84 272575375 0.034051 202 1988 CIN NL 8888409 0.540373 161 87 300452424 0.029583 203 1989 CIN NL 11072000 0.462963 162 75 359995711 0.030756 204 1990 CIN NL 14370000 0.561728 162 91 443881193 0.032374 205 1991 CIN NL 26305333 0.456790 162 74 613048418 0.042909 206 1992 CIN NL 35931499 0.555556 162 90 805543323 0.044605 207 1993 CIN NL 44879666 0.450617 162 73 901740134 0.049770 208 1994 CIN NL 40961833 0.573913 115 66 927836287 0.044148 209 1995 CIN NL 43144670 0.590278 144 85 951469367 0.045345 210 1996 CIN NL 42526334 0.500000 162 81 956983550 0.044438 211 1997 CIN NL 49768000 0.469136 162 76 1127285885 0.044149 212 1998 CIN NL 23005000 0.475309 162 77 1278282871 0.017997 213 1999 CIN NL 33962761 0.588957 163 96 1494228750 0.022729 214 2000 CIN NL 46867200 0.521472 163 85 1666135102 0.028129 215 2001 CIN NL 48986000 0.407407 162 66 1960663313 0.024984 216 2002 CIN NL 45050390 0.481481 162 78 2024077522 0.022257 217 2003 CIN NL 59355667 0.425926 162 69 2128262128 0.027889 218 2004 CIN NL 46615250 0.469136 162 76 2070665943 0.022512 219 2005 CIN NL 61892583 0.447853 163 73 2188713398 0.028278 220 2006 CIN NL 60909519 0.493827 162 80 2321472617 0.026237 221 2007 CIN NL 68524980 0.444444 162 72 2476688987 0.027668 222 2008 CIN NL 74117695 0.456790 162 74 2684858670 0.027606 223 2009 CIN NL 73558500 0.481481 162 78 2664726994 0.027605 224 2010 CIN NL 71761542 0.561728 162 91 2721359865 0.026370 225 2011 CIN NL 75947134 0.487654 162 79 2784505291 0.027275 226 2012 CIN NL 82203616 0.598765 162 97 2932741192 0.028030 227 2013 CIN NL 106404462 0.555556 162 90 3034525648 0.035065 228 2014 CIN NL 108217500 0.469136 162 76 3192317623 0.033899 229 2015 CIN NL 113072286 0.395062 162 64 3514142569 0.032176 230 2016 CIN NL 88940059 0.419753 162 68 3750137392 0.023716 231 1985 CLE AL 6551666 0.370370 162 60 261964696 0.025010 232 1986 CLE AL 7809500 0.515337 163 84 307854518 0.025368 233 1987 CLE AL 8513750 0.376543 162 61 272575375 0.031234 234 1988 CLE AL 8936500 0.481481 162 78 300452424 0.029743 235 1989 CLE AL 9094500 0.450617 162 73 359995711 0.025263 236 1990 CLE AL 14487000 0.475309 162 77 443881193 0.032637 237 1991 CLE AL 17635000 0.351852 162 57 613048418 0.028766 238 1992 CLE AL 9373044 0.469136 162 76 805543323 0.011636 239 1993 CLE AL 18561000 0.469136 162 76 901740134 0.020584 240 1994 CLE AL 30490500 0.584071 113 66 927836287 0.032862 241 1995 CLE AL 37937835 0.694444 144 100 951469367 0.039873 242 1996 CLE AL 48107360 0.614907 161 99 956983550 0.050270 243 1997 CLE AL 56802460 0.534161 161 86 1127285885 0.050389 244 1998 CLE AL 60800166 0.549383 162 89 1278282871 0.047564 245 1999 CLE AL 72978462 0.598765 162 97 1494228750 0.048840 246 2000 CLE AL 75880771 0.555556 162 90 1666135102 0.045543 247 2001 CLE AL 93152001 0.561728 162 91 1960663313 0.047510 248 2002 CLE AL 78909449 0.456790 162 74 2024077522 0.038985 249 2003 CLE AL 48584834 0.419753 162 68 2128262128 0.022828 250 2004 CLE AL 34319300 0.493827 162 80 2070665943 0.016574 251 2005 CLE AL 41502500 0.574074 162 93 2188713398 0.018962 252 2006 CLE AL 56031500 0.481481 162 78 2321472617 0.024136 253 2007 CLE AL 61673267 0.592593 162 96 2476688987 0.024901 254 2008 CLE AL 78970066 0.500000 162 81 2684858670 0.029413 255 2009 CLE AL 81579166 0.401235 162 65 2664726994 0.030614 256 2010 CLE AL 61203966 0.425926 162 69 2721359865 0.022490 257 2011 CLE AL 48776566 0.493827 162 80 2784505291 0.017517 258 2012 CLE AL 78430300 0.419753 162 68 2932741192 0.026743 259 2013 CLE AL 75771800 0.567901 162 92 3034525648 0.024970 260 2014 CLE AL 82151899 0.524691 162 85 3192317623 0.025734 261 2015 CLE AL 87663766 0.503106 161 81 3514142569 0.024946 262 2016 CLE AL 74311900 0.583851 161 94 3750137392 0.019816 263 1993 COL NL 10353500 0.413580 162 67 901740134 0.011482 264 1994 COL NL 23887333 0.452991 117 53 927836287 0.025745 265 1995 COL NL 34154717 0.534722 144 77 951469367 0.035897 266 1996 COL NL 40179823 0.512346 162 83 956983550 0.041986 267 1997 COL NL 43559667 0.512346 162 83 1127285885 0.038641 268 1998 COL NL 50484648 0.475309 162 77 1278282871 0.039494 269 1999 COL NL 61935837 0.444444 162 72 1494228750 0.041450 270 2000 COL NL 61111190 0.506173 162 82 1666135102 0.036678 271 2001 COL NL 71541334 0.450617 162 73 1960663313 0.036488 272 2002 COL NL 56851043 0.450617 162 73 2024077522 0.028087 273 2003 COL NL 67179667 0.456790 162 74 2128262128 0.031566 274 2004 COL NL 65445167 0.419753 162 68 2070665943 0.031606 275 2005 COL NL 47839000 0.413580 162 67 2188713398 0.021857 276 2006 COL NL 41233000 0.469136 162 76 2321472617 0.017762 277 2007 COL NL 54041000 0.552147 163 90 2476688987 0.021820 278 2008 COL NL 68655500 0.456790 162 74 2684858670 0.025571 279 2009 COL NL 75201000 0.567901 162 92 2664726994 0.028221 280 2010 COL NL 84227000 0.512346 162 83 2721359865 0.030950 281 2011 COL NL 88148071 0.450617 162 73 2784505291 0.031657 282 2012 COL NL 78069571 0.395062 162 64 2932741192 0.026620 283 2013 COL NL 74409071 0.456790 162 74 3034525648 0.024521 284 2014 COL NL 95403500 0.407407 162 66 3192317623 0.029885 285 2015 COL NL 95688600 0.419753 162 68 3514142569 0.027230 286 2016 COL NL 112645071 0.462963 162 75 3750137392 0.030038 287 1985 DET AL 10348143 0.521739 161 84 261964696 0.039502 288 1986 DET AL 12335714 0.537037 162 87 307854518 0.040070 289 1987 DET AL 12122881 0.604938 162 98 272575375 0.044475 290 1988 DET AL 12869571 0.543210 162 88 300452424 0.042834 291 1989 DET AL 15146404 0.364198 162 59 359995711 0.042074 292 1990 DET AL 17593238 0.487654 162 79 443881193 0.039635 293 1991 DET AL 23838333 0.518519 162 84 613048418 0.038885 294 1992 DET AL 27322834 0.462963 162 75 805543323 0.033919 295 1993 DET AL 38150165 0.524691 162 85 901740134 0.042307 296 1994 DET AL 41446501 0.460870 115 53 927836287 0.044670 297 1995 DET AL 37044168 0.416667 144 60 951469367 0.038934 298 1996 DET AL 23438000 0.327160 162 53 956983550 0.024492 299 1997 DET AL 17272000 0.487654 162 79 1127285885 0.015322 300 1998 DET AL 24065000 0.401235 162 65 1278282871 0.018826 301 1999 DET AL 36489666 0.428571 161 69 1494228750 0.024420 302 2000 DET AL 58265167 0.487654 162 79 1666135102 0.034970 303 2001 DET AL 53416167 0.407407 162 66 1960663313 0.027244 304 2002 DET AL 55048000 0.341615 161 55 2024077522 0.027197 305 2003 DET AL 49168000 0.265432 162 43 2128262128 0.023102 306 2004 DET AL 46832000 0.444444 162 72 2070665943 0.022617 307 2005 DET AL 69092000 0.438272 162 71 2188713398 0.031567 308 2006 DET AL 82612866 0.586420 162 95 2321472617 0.035586 309 2007 DET AL 94800369 0.543210 162 88 2476688987 0.038277 310 2008 DET AL 137685196 0.456790 162 74 2684858670 0.051282 311 2009 DET AL 115085145 0.527607 163 86 2664726994 0.043188 312 2010 DET AL 122864928 0.500000 162 81 2721359865 0.045148 313 2011 DET AL 105700231 0.586420 162 95 2784505291 0.037960 314 2012 DET AL 132300000 0.543210 162 88 2932741192 0.045111 315 2013 DET AL 145989500 0.574074 162 93 3034525648 0.048109 316 2014 DET AL 152855500 0.555556 162 90 3192317623 0.047882 317 2015 DET AL 172284750 0.459627 161 74 3514142569 0.049026 318 2016 DET AL 194876481 0.534161 161 86 3750137392 0.051965 319 1993 FLO NL 19330545 0.395062 162 64 901740134 0.021437 320 1994 FLO NL 21633000 0.443478 115 51 927836287 0.023316 321 1995 FLO NL 24515781 0.468531 143 67 951469367 0.025766 322 1996 FLO NL 31022500 0.493827 162 80 956983550 0.032417 323 1997 FLO NL 48692500 0.567901 162 92 1127285885 0.043194 324 1998 FLO NL 41322667 0.333333 162 54 1278282871 0.032327 325 1999 FLO NL 21085000 0.395062 162 64 1494228750 0.014111 326 2000 FLO NL 19872000 0.490683 161 79 1666135102 0.011927 327 2001 FLO NL 35762500 0.469136 162 76 1960663313 0.018240 328 2002 FLO NL 41979917 0.487654 162 79 2024077522 0.020740 329 2003 FLO NL 49450000 0.561728 162 91 2128262128 0.023235 330 2004 FLO NL 42143042 0.512346 162 83 2070665943 0.020352 331 2005 FLO NL 60408834 0.512346 162 83 2188713398 0.027600 332 2006 FLO NL 14671500 0.481481 162 78 2321472617 0.006320 333 2007 FLO NL 30507000 0.438272 162 71 2476688987 0.012318 334 2008 FLO NL 21811500 0.521739 161 84 2684858670 0.008124 335 2009 FLO NL 36834000 0.537037 162 87 2664726994 0.013823 336 2010 FLO NL 57029719 0.493827 162 80 2721359865 0.020956 337 2011 FLO NL 56944000 0.444444 162 72 2784505291 0.020450 338 1985 HOU NL 9993051 0.512346 162 83 261964696 0.038147 339 1986 HOU NL 9873276 0.592593 162 96 307854518 0.032071 340 1987 HOU NL 12608371 0.469136 162 76 272575375 0.046256 341 1988 HOU NL 12286167 0.506173 162 82 300452424 0.040892 342 1989 HOU NL 15029500 0.530864 162 86 359995711 0.041749 343 1990 HOU NL 18330000 0.462963 162 75 443881193 0.041295 344 1991 HOU NL 12852500 0.401235 162 65 613048418 0.020965 345 1992 HOU NL 15407500 0.500000 162 81 805543323 0.019127 346 1993 HOU NL 30210500 0.524691 162 85 901740134 0.033502 347 1994 HOU NL 33126000 0.573913 115 66 927836287 0.035702 348 1995 HOU NL 34169834 0.527778 144 76 951469367 0.035913 349 1996 HOU NL 28487000 0.506173 162 82 956983550 0.029767 350 1997 HOU NL 34777500 0.518519 162 84 1127285885 0.030851 351 1998 HOU NL 42374000 0.629630 162 102 1278282871 0.033149 352 1999 HOU NL 54914000 0.598765 162 97 1494228750 0.036751 353 2000 HOU NL 51289111 0.444444 162 72 1666135102 0.030783 354 2001 HOU NL 60612667 0.574074 162 93 1960663313 0.030914 355 2002 HOU NL 63448417 0.518519 162 84 2024077522 0.031347 356 2003 HOU NL 71040000 0.537037 162 87 2128262128 0.033379 357 2004 HOU NL 75397000 0.567901 162 92 2070665943 0.036412 358 2005 HOU NL 76779000 0.546012 163 89 2188713398 0.035080 359 2006 HOU NL 88694435 0.506173 162 82 2321472617 0.038206 360 2007 HOU NL 87759000 0.450617 162 73 2476688987 0.035434 361 2008 HOU NL 88930414 0.534161 161 86 2684858670 0.033123 362 2009 HOU NL 102996414 0.456790 162 74 2664726994 0.038652 363 2010 HOU NL 92355500 0.469136 162 76 2721359865 0.033937 364 2011 HOU NL 70694000 0.345679 162 56 2784505291 0.025388 365 2012 HOU NL 60651000 0.339506 162 55 2932741192 0.020681 366 2013 HOU AL 17890700 0.314815 162 51 3034525648 0.005896 367 2014 HOU AL 35116300 0.432099 162 70 3192317623 0.011000 368 2015 HOU AL 72256200 0.530864 162 86 3514142569 0.020562 369 2016 HOU AL 94893700 0.518519 162 84 3750137392 0.025304 370 1985 KCA AL 9321179 0.561728 162 91 261964696 0.035582 371 1986 KCA AL 13043698 0.469136 162 76 307854518 0.042370 372 1987 KCA AL 11828056 0.512346 162 83 272575375 0.043394 373 1988 KCA AL 14556562 0.521739 161 84 300452424 0.048449 374 1989 KCA AL 18683568 0.567901 162 92 359995711 0.051899 375 1990 KCA AL 23361084 0.465839 161 75 443881193 0.052629 376 1991 KCA AL 26319834 0.506173 162 82 613048418 0.042933 377 1992 KCA AL 33893834 0.444444 162 72 805543323 0.042076 378 1993 KCA AL 41346167 0.518519 162 84 901740134 0.045852 379 1994 KCA AL 40541334 0.556522 115 64 927836287 0.043694 380 1995 KCA AL 29532834 0.486111 144 70 951469367 0.031039 381 1996 KCA AL 20281250 0.465839 161 75 956983550 0.021193 382 1997 KCA AL 34655000 0.416149 161 67 1127285885 0.030742 383 1998 KCA AL 36862500 0.447205 161 72 1278282871 0.028838 384 1999 KCA AL 26225000 0.397516 161 64 1494228750 0.017551 385 2000 KCA AL 23433000 0.475309 162 77 1666135102 0.014064 386 2001 KCA AL 35422500 0.401235 162 65 1960663313 0.018067 387 2002 KCA AL 47257000 0.382716 162 62 2024077522 0.023347 388 2003 KCA AL 40518000 0.512346 162 83 2128262128 0.019038 389 2004 KCA AL 47609000 0.358025 162 58 2070665943 0.022992 390 2005 KCA AL 36881000 0.345679 162 56 2188713398 0.016851 391 2006 KCA AL 47294000 0.382716 162 62 2321472617 0.020372 392 2007 KCA AL 67116500 0.425926 162 69 2476688987 0.027099 393 2008 KCA AL 58245500 0.462963 162 75 2684858670 0.021694 394 2009 KCA AL 70519333 0.401235 162 65 2664726994 0.026464 395 2010 KCA AL 71405210 0.413580 162 67 2721359865 0.026239 396 2011 KCA AL 35712000 0.438272 162 71 2784505291 0.012825 397 2012 KCA AL 60916225 0.444444 162 72 2932741192 0.020771 398 2013 KCA AL 80091725 0.530864 162 86 3034525648 0.026393 399 2014 KCA AL 74594075 0.549383 162 89 3192317623 0.023367 400 2015 KCA AL 112107025 0.586420 162 95 3514142569 0.031902 401 2016 KCA AL 131487125 0.500000 162 81 3750137392 0.035062 402 2005 LAA AL 94867822 0.586420 162 95 2188713398 0.043344 403 2006 LAA AL 103472000 0.549383 162 89 2321472617 0.044572 404 2007 LAA AL 109251333 0.580247 162 94 2476688987 0.044112 405 2008 LAA AL 119216333 0.617284 162 100 2684858670 0.044403 406 2009 LAA AL 113709000 0.598765 162 97 2664726994 0.042672 407 2010 LAA AL 104963866 0.493827 162 80 2721359865 0.038570 408 2011 LAA AL 138543166 0.530864 162 86 2784505291 0.049755 409 2012 LAA AL 154485166 0.549383 162 89 2932741192 0.052676 410 2013 LAA AL 124174750 0.481481 162 78 3034525648 0.040921 411 2014 LAA AL 121988250 0.604938 162 98 3192317623 0.038213 412 2015 LAA AL 120005415 0.524691 162 85 3514142569 0.034149 413 2016 LAA AL 137251333 0.456790 162 74 3750137392 0.036599 414 1985 LAN NL 10967917 0.586420 162 95 261964696 0.041868 415 1986 LAN NL 14913776 0.450617 162 73 307854518 0.048444 416 1987 LAN NL 13675403 0.450617 162 73 272575375 0.050171 417 1988 LAN NL 16850515 0.580247 162 94 300452424 0.056084 418 1989 LAN NL 21071562 0.481250 160 77 359995711 0.058533 419 1990 LAN NL 21318704 0.530864 162 86 443881193 0.048028 420 1991 LAN NL 32790664 0.574074 162 93 613048418 0.053488 421 1992 LAN NL 44788166 0.388889 162 63 805543323 0.055600 422 1993 LAN NL 39331999 0.500000 162 81 901740134 0.043618 423 1994 LAN NL 38000001 0.508772 114 58 927836287 0.040956 424 1995 LAN NL 39273201 0.541667 144 78 951469367 0.041276 425 1996 LAN NL 35355000 0.555556 162 90 956983550 0.036944 426 1997 LAN NL 45380304 0.543210 162 88 1127285885 0.040256 427 1998 LAN NL 48820000 0.512346 162 83 1278282871 0.038192 428 1999 LAN NL 80862453 0.475309 162 77 1494228750 0.054117 429 2000 LAN NL 87924286 0.530864 162 86 1666135102 0.052771 430 2001 LAN NL 109105953 0.530864 162 86 1960663313 0.055647 431 2002 LAN NL 94850953 0.567901 162 92 2024077522 0.046861 432 2003 LAN NL 105572620 0.524691 162 85 2128262128 0.049605 433 2004 LAN NL 92902001 0.574074 162 93 2070665943 0.044866 434 2005 LAN NL 83039000 0.438272 162 71 2188713398 0.037940 435 2006 LAN NL 98447187 0.543210 162 88 2321472617 0.042407 436 2007 LAN NL 108454524 0.506173 162 82 2476688987 0.043790 437 2008 LAN NL 118588536 0.518519 162 84 2684858670 0.044169 438 2009 LAN NL 100414592 0.586420 162 95 2664726994 0.037683 439 2010 LAN NL 95358016 0.493827 162 80 2721359865 0.035041 440 2011 LAN NL 104188999 0.509317 161 82 2784505291 0.037417 441 2012 LAN NL 95143575 0.530864 162 86 2932741192 0.032442 442 2013 LAN NL 223362196 0.567901 162 92 3034525648 0.073607 443 2014 LAN NL 217014600 0.580247 162 94 3192317623 0.067980 444 2015 LAN NL 215792000 0.567901 162 92 3514142569 0.061407 445 2016 LAN NL 221288380 0.561728 162 91 3750137392 0.059008 446 2012 MIA NL 118078000 0.425926 162 69 2932741192 0.040262 447 2013 MIA NL 33601900 0.382716 162 62 3034525648 0.011073 448 2014 MIA NL 41836900 0.475309 162 77 3192317623 0.013105 449 2015 MIA NL 68056500 0.438272 162 71 3514142569 0.019366 450 2016 MIA NL 77314202 0.490683 161 79 3750137392 0.020616 451 1998 MIL NL 33914904 0.456790 162 74 1278282871 0.026532 452 1999 MIL NL 43377395 0.459627 161 74 1494228750 0.029030 453 2000 MIL NL 36505333 0.447853 163 73 1666135102 0.021910 454 2001 MIL NL 43886833 0.419753 162 68 1960663313 0.022384 455 2002 MIL NL 50287833 0.345679 162 56 2024077522 0.024845 456 2003 MIL NL 40627000 0.419753 162 68 2128262128 0.019089 457 2004 MIL NL 27528500 0.416149 161 67 2070665943 0.013295 458 2005 MIL NL 39934833 0.500000 162 81 2188713398 0.018246 459 2006 MIL NL 57568333 0.462963 162 75 2321472617 0.024798 460 2007 MIL NL 70986500 0.512346 162 83 2476688987 0.028662 461 2008 MIL NL 80937499 0.555556 162 90 2684858670 0.030146 462 2009 MIL NL 80182502 0.493827 162 80 2664726994 0.030090 463 2010 MIL NL 81108278 0.475309 162 77 2721359865 0.029804 464 2011 MIL NL 85497333 0.592593 162 96 2784505291 0.030705 465 2012 MIL NL 97653944 0.512346 162 83 2932741192 0.033298 466 2013 MIL NL 76947033 0.456790 162 74 3034525648 0.025357 467 2014 MIL NL 101217000 0.506173 162 82 3192317623 0.031706 468 2015 MIL NL 100850000 0.419753 162 68 3514142569 0.028698 469 2016 MIL NL 68775237 0.450617 162 73 3750137392 0.018339 470 1985 MIN AL 5764821 0.475309 162 77 261964696 0.022006 471 1986 MIN AL 8748167 0.438272 162 71 307854518 0.028417 472 1987 MIN AL 6397500 0.524691 162 85 272575375 0.023471 473 1988 MIN AL 12462666 0.561728 162 91 300452424 0.041480 474 1989 MIN AL 15531666 0.493827 162 80 359995711 0.043144 475 1990 MIN AL 14602000 0.456790 162 74 443881193 0.032896 476 1991 MIN AL 23361833 0.586420 162 95 613048418 0.038108 477 1992 MIN AL 28027834 0.555556 162 90 805543323 0.034794 478 1993 MIN AL 28217933 0.438272 162 71 901740134 0.031293 479 1994 MIN AL 28438500 0.469027 113 53 927836287 0.030650 480 1995 MIN AL 25410500 0.388889 144 56 951469367 0.026707 481 1996 MIN AL 23117000 0.481481 162 78 956983550 0.024156 482 1997 MIN AL 34072500 0.419753 162 68 1127285885 0.030225 483 1998 MIN AL 27927500 0.432099 162 70 1278282871 0.021848 484 1999 MIN AL 21257500 0.391304 161 63 1494228750 0.014226 485 2000 MIN AL 16519500 0.425926 162 69 1666135102 0.009915 486 2001 MIN AL 24130000 0.524691 162 85 1960663313 0.012307 487 2002 MIN AL 40425000 0.583851 161 94 2024077522 0.019972 488 2003 MIN AL 55505000 0.555556 162 90 2128262128 0.026080 489 2004 MIN AL 53585000 0.567901 162 92 2070665943 0.025878 490 2005 MIN AL 56186000 0.512346 162 83 2188713398 0.025671 491 2006 MIN AL 63396006 0.592593 162 96 2321472617 0.027309 492 2007 MIN AL 71439500 0.487654 162 79 2476688987 0.028845 493 2008 MIN AL 56932766 0.539877 163 88 2684858670 0.021205 494 2009 MIN AL 65299266 0.533742 163 87 2664726994 0.024505 495 2010 MIN AL 97559166 0.580247 162 94 2721359865 0.035849 496 2011 MIN AL 112737000 0.388889 162 63 2784505291 0.040487 497 2012 MIN AL 94085000 0.407407 162 66 2932741192 0.032081 498 2013 MIN AL 75337500 0.407407 162 66 3034525648 0.024827 499 2014 MIN AL 83762500 0.432099 162 70 3192317623 0.026239 500 2015 MIN AL 107755000 0.512346 162 83 3514142569 0.030663 501 2016 MIN AL 102583200 0.364198 162 59 3750137392 0.027355 502 1985 ML4 AL 11284107 0.440994 161 71 261964696 0.043075 503 1986 ML4 AL 9943642 0.478261 161 77 307854518 0.032300 504 1987 ML4 AL 7293224 0.561728 162 91 272575375 0.026757 505 1988 ML4 AL 8402000 0.537037 162 87 300452424 0.027964 506 1989 ML4 AL 11533000 0.500000 162 81 359995711 0.032036 507 1990 ML4 AL 19719167 0.456790 162 74 443881193 0.044424 508 1991 ML4 AL 23115500 0.512346 162 83 613048418 0.037706 509 1992 ML4 AL 31013667 0.567901 162 92 805543323 0.038500 510 1993 ML4 AL 23806834 0.425926 162 69 901740134 0.026401 511 1994 ML4 AL 24350500 0.460870 115 53 927836287 0.026244 512 1995 ML4 AL 17798825 0.451389 144 65 951469367 0.018707 513 1996 ML4 AL 21730000 0.493827 162 80 956983550 0.022707 514 1997 ML4 AL 23655338 0.484472 161 78 1127285885 0.020984 515 1985 MON NL 9470166 0.521739 161 84 261964696 0.036151 516 1986 MON NL 11103600 0.484472 161 78 307854518 0.036068 517 1987 MON NL 6942052 0.561728 162 91 272575375 0.025468 518 1988 MON NL 9603333 0.496933 163 81 300452424 0.031963 519 1989 MON NL 13807389 0.500000 162 81 359995711 0.038354 520 1990 MON NL 16586388 0.524691 162 85 443881193 0.037367 521 1991 MON NL 10732333 0.440994 161 71 613048418 0.017507 522 1992 MON NL 15822334 0.537037 162 87 805543323 0.019642 523 1993 MON NL 18899333 0.576687 163 94 901740134 0.020959 524 1994 MON NL 19098000 0.649123 114 74 927836287 0.020583 525 1995 MON NL 12364000 0.458333 144 66 951469367 0.012995 526 1996 MON NL 16264500 0.543210 162 88 956983550 0.016996 527 1997 MON NL 19295500 0.481481 162 78 1127285885 0.017117 528 1998 MON NL 10641500 0.401235 162 65 1278282871 0.008325 529 1999 MON NL 17903000 0.419753 162 68 1494228750 0.011981 530 2000 MON NL 32994333 0.413580 162 67 1666135102 0.019803 531 2001 MON NL 35159500 0.419753 162 68 1960663313 0.017932 532 2002 MON NL 38670500 0.512346 162 83 2024077522 0.019105 533 2003 MON NL 51948500 0.512346 162 83 2128262128 0.024409 534 2004 MON NL 40897500 0.413580 162 67 2070665943 0.019751 535 1985 NYA AL 14238204 0.602484 161 97 261964696 0.054352 536 1986 NYA AL 18494253 0.555556 162 90 307854518 0.060075 537 1987 NYA AL 17099714 0.549383 162 89 272575375 0.062734 538 1988 NYA AL 19441152 0.527950 161 85 300452424 0.064706 539 1989 NYA AL 17114375 0.459627 161 74 359995711 0.047540 540 1990 NYA AL 20912318 0.413580 162 67 443881193 0.047112 541 1991 NYA AL 27344168 0.438272 162 71 613048418 0.044604 542 1992 NYA AL 37543334 0.469136 162 76 805543323 0.046606 543 1993 NYA AL 42624900 0.543210 162 88 901740134 0.047270 544 1994 NYA AL 45731334 0.619469 113 70 927836287 0.049288 545 1995 NYA AL 48874851 0.544828 145 79 951469367 0.051368 546 1996 NYA AL 54191792 0.567901 162 92 956983550 0.056628 547 1997 NYA AL 62241545 0.592593 162 96 1127285885 0.055214 548 1998 NYA AL 66806867 0.703704 162 114 1278282871 0.052263 549 1999 NYA AL 86734359 0.604938 162 98 1494228750 0.058046 550 2000 NYA AL 92338260 0.540373 161 87 1666135102 0.055421 551 2001 NYA AL 112287143 0.590062 161 95 1960663313 0.057270 552 2002 NYA AL 125928583 0.639752 161 103 2024077522 0.062215 553 2003 NYA AL 152749814 0.619632 163 101 2128262128 0.071772 554 2004 NYA AL 184193950 0.623457 162 101 2070665943 0.088954 555 2005 NYA AL 208306817 0.586420 162 95 2188713398 0.095173 556 2006 NYA AL 194663079 0.598765 162 97 2321472617 0.083853 557 2007 NYA AL 189259045 0.580247 162 94 2476688987 0.076416 558 2008 NYA AL 207896789 0.549383 162 89 2684858670 0.077433 559 2009 NYA AL 201449189 0.635802 162 103 2664726994 0.075598 560 2010 NYA AL 206333389 0.586420 162 95 2721359865 0.075820 561 2011 NYA AL 202275028 0.598765 162 97 2784505291 0.072643 562 2012 NYA AL 196522289 0.586420 162 95 2932741192 0.067010 563 2013 NYA AL 231978886 0.524691 162 85 3034525648 0.076447 564 2014 NYA AL 197543907 0.518519 162 84 3192317623 0.061881 565 2015 NYA AL 212751957 0.537037 162 87 3514142569 0.060542 566 2016 NYA AL 222997792 0.518519 162 84 3750137392 0.059464 567 1985 NYN NL 10834762 0.604938 162 98 261964696 0.041360 568 1986 NYN NL 15393714 0.666667 162 108 307854518 0.050003 569 1987 NYN NL 13846714 0.567901 162 92 272575375 0.050800 570 1988 NYN NL 15269314 0.625000 160 100 300452424 0.050821 571 1989 NYN NL 19885071 0.537037 162 87 359995711 0.055237 572 1990 NYN NL 21722834 0.561728 162 91 443881193 0.048938 573 1991 NYN NL 32590001 0.478261 161 77 613048418 0.053161 574 1992 NYN NL 44602002 0.444444 162 72 805543323 0.055369 575 1993 NYN NL 39043667 0.364198 162 59 901740134 0.043298 576 1994 NYN NL 30956583 0.486726 113 55 927836287 0.033364 577 1995 NYN NL 27674992 0.479167 144 69 951469367 0.029087 578 1996 NYN NL 24479500 0.438272 162 71 956983550 0.025580 579 1997 NYN NL 39800400 0.543210 162 88 1127285885 0.035306 580 1998 NYN NL 52077999 0.543210 162 88 1278282871 0.040741 581 1999 NYN NL 65092092 0.595092 163 97 1494228750 0.043562 582 2000 NYN NL 79509776 0.580247 162 94 1666135102 0.047721 583 2001 NYN NL 93174428 0.506173 162 82 1960663313 0.047522 584 2002 NYN NL 94633593 0.465839 161 75 2024077522 0.046754 585 2003 NYN NL 116876429 0.409938 161 66 2128262128 0.054916 586 2004 NYN NL 96660970 0.438272 162 71 2070665943 0.046681 587 2005 NYN NL 101305821 0.512346 162 83 2188713398 0.046286 588 2006 NYN NL 101084963 0.598765 162 97 2321472617 0.043543 589 2007 NYN NL 115231663 0.543210 162 88 2476688987 0.046526 590 2008 NYN NL 137793376 0.549383 162 89 2684858670 0.051322 591 2009 NYN NL 149373987 0.432099 162 70 2664726994 0.056056 592 2010 NYN NL 134422942 0.487654 162 79 2721359865 0.049396 593 2011 NYN NL 118847309 0.475309 162 77 2784505291 0.042682 594 2012 NYN NL 93353983 0.456790 162 74 2932741192 0.031832 595 2013 NYN NL 49448346 0.456790 162 74 3034525648 0.016295 596 2014 NYN NL 85556990 0.487654 162 79 3192317623 0.026801 597 2015 NYN NL 96766683 0.555556 162 90 3514142569 0.027536 598 2016 NYN NL 133889129 0.537037 162 87 3750137392 0.035702 599 1985 OAK AL 9058606 0.475309 162 77 261964696 0.034579 600 1986 OAK AL 9779421 0.469136 162 76 307854518 0.031766 601 1987 OAK AL 11680839 0.500000 162 81 272575375 0.042854 602 1988 OAK AL 9690000 0.641975 162 104 300452424 0.032251 603 1989 OAK AL 15613070 0.611111 162 99 359995711 0.043370 604 1990 OAK AL 19887501 0.635802 162 103 443881193 0.044804 605 1991 OAK AL 36999167 0.518519 162 84 613048418 0.060353 606 1992 OAK AL 41035000 0.592593 162 96 805543323 0.050941 607 1993 OAK AL 37812333 0.419753 162 68 901740134 0.041933 608 1994 OAK AL 34172500 0.447368 114 51 927836287 0.036830 609 1995 OAK AL 37739225 0.465278 144 67 951469367 0.039664 610 1996 OAK AL 21243000 0.481481 162 78 956983550 0.022198 611 1997 OAK AL 24018500 0.401235 162 65 1127285885 0.021306 612 1998 OAK AL 21303000 0.456790 162 74 1278282871 0.016665 613 1999 OAK AL 24431833 0.537037 162 87 1494228750 0.016351 614 2000 OAK AL 31971333 0.565217 161 91 1666135102 0.019189 615 2001 OAK AL 33810750 0.629630 162 102 1960663313 0.017245 616 2002 OAK AL 40004167 0.635802 162 103 2024077522 0.019764 617 2003 OAK AL 50260834 0.592593 162 96 2128262128 0.023616 618 2004 OAK AL 59425667 0.561728 162 91 2070665943 0.028699 619 2005 OAK AL 55425762 0.543210 162 88 2188713398 0.025323 620 2006 OAK AL 62243079 0.574074 162 93 2321472617 0.026812 621 2007 OAK AL 79366940 0.469136 162 76 2476688987 0.032046 622 2008 OAK AL 47967126 0.465839 161 75 2684858670 0.017866 623 2009 OAK AL 61910000 0.462963 162 75 2664726994 0.023233 624 2010 OAK AL 55254900 0.500000 162 81 2721359865 0.020304 625 2011 OAK AL 66536500 0.456790 162 74 2784505291 0.023895 626 2012 OAK AL 55372500 0.580247 162 94 2932741192 0.018881 627 2013 OAK AL 60132500 0.592593 162 96 3034525648 0.019816 628 2014 OAK AL 72408400 0.543210 162 88 3192317623 0.022682 629 2015 OAK AL 79053501 0.419753 162 68 3514142569 0.022496 630 2016 OAK AL 86806234 0.425926 162 69 3750137392 0.023147 631 1985 PHI NL 10124966 0.462963 162 75 261964696 0.038650 632 1986 PHI NL 11590166 0.534161 161 86 307854518 0.037648 633 1987 PHI NL 11514233 0.493827 162 80 272575375 0.042242 634 1988 PHI NL 13838000 0.401235 162 65 300452424 0.046057 635 1989 PHI NL 10604000 0.411043 163 67 359995711 0.029456 636 1990 PHI NL 13173667 0.475309 162 77 443881193 0.029678 637 1991 PHI NL 22487332 0.481481 162 78 613048418 0.036681 638 1992 PHI NL 24383834 0.432099 162 70 805543323 0.030270 639 1993 PHI NL 28538334 0.598765 162 97 901740134 0.031648 640 1994 PHI NL 31599000 0.469565 115 54 927836287 0.034057 641 1995 PHI NL 30555945 0.479167 144 69 951469367 0.032114 642 1996 PHI NL 34314500 0.413580 162 67 956983550 0.035857 643 1997 PHI NL 36656500 0.419753 162 68 1127285885 0.032517 644 1998 PHI NL 36297500 0.462963 162 75 1278282871 0.028396 645 1999 PHI NL 31692500 0.475309 162 77 1494228750 0.021210 646 2000 PHI NL 47308000 0.401235 162 65 1666135102 0.028394 647 2001 PHI NL 41663833 0.530864 162 86 1960663313 0.021250 648 2002 PHI NL 57954999 0.496894 161 80 2024077522 0.028633 649 2003 PHI NL 70780000 0.530864 162 86 2128262128 0.033257 650 2004 PHI NL 92919167 0.530864 162 86 2070665943 0.044874 651 2005 PHI NL 95522000 0.543210 162 88 2188713398 0.043643 652 2006 PHI NL 88273333 0.524691 162 85 2321472617 0.038025 653 2007 PHI NL 89428213 0.549383 162 89 2476688987 0.036108 654 2008 PHI NL 97879880 0.567901 162 92 2684858670 0.036456 655 2009 PHI NL 113004046 0.574074 162 93 2664726994 0.042407 656 2010 PHI NL 141928379 0.598765 162 97 2721359865 0.052153 657 2011 PHI NL 172976379 0.629630 162 102 2784505291 0.062121 658 2012 PHI NL 174538938 0.500000 162 81 2932741192 0.059514 659 2013 PHI NL 169863189 0.450617 162 73 3034525648 0.055977 660 2014 PHI NL 180944967 0.450617 162 73 3192317623 0.056681 661 2015 PHI NL 111693000 0.388889 162 63 3514142569 0.031784 662 2016 PHI NL 58980000 0.438272 162 71 3750137392 0.015727 663 1985 PIT NL 9227500 0.354037 161 57 261964696 0.035224 664 1986 PIT NL 10843500 0.395062 162 64 307854518 0.035223 665 1987 PIT NL 7652000 0.493827 162 80 272575375 0.028073 666 1988 PIT NL 5998500 0.531250 160 85 300452424 0.019965 667 1989 PIT NL 12737500 0.451220 164 74 359995711 0.035382 668 1990 PIT NL 15556000 0.586420 162 95 443881193 0.035045 669 1991 PIT NL 23634667 0.604938 162 98 613048418 0.038553 670 1992 PIT NL 33944167 0.592593 162 96 805543323 0.042138 671 1993 PIT NL 24822467 0.462963 162 75 901740134 0.027527 672 1994 PIT NL 24217250 0.464912 114 53 927836287 0.026101 673 1995 PIT NL 18355345 0.402778 144 58 951469367 0.019292 674 1996 PIT NL 23017500 0.450617 162 73 956983550 0.024052 675 1997 PIT NL 10771667 0.487654 162 79 1127285885 0.009555 676 1998 PIT NL 15065000 0.423313 163 69 1278282871 0.011785 677 1999 PIT NL 24697666 0.484472 161 78 1494228750 0.016529 678 2000 PIT NL 28928334 0.425926 162 69 1666135102 0.017363 679 2001 PIT NL 57760833 0.382716 162 62 1960663313 0.029460 680 2002 PIT NL 42323599 0.447205 161 72 2024077522 0.020910 681 2003 PIT NL 54812429 0.462963 162 75 2128262128 0.025755 682 2004 PIT NL 32227929 0.447205 161 72 2070665943 0.015564 683 2005 PIT NL 38133000 0.413580 162 67 2188713398 0.017423 684 2006 PIT NL 46717750 0.413580 162 67 2321472617 0.020124 685 2007 PIT NL 38537833 0.419753 162 68 2476688987 0.015560 686 2008 PIT NL 48689783 0.413580 162 67 2684858670 0.018135 687 2009 PIT NL 48693000 0.385093 161 62 2664726994 0.018273 688 2010 PIT NL 34943000 0.351852 162 57 2721359865 0.012840 689 2011 PIT NL 45047000 0.444444 162 72 2784505291 0.016178 690 2012 PIT NL 62951999 0.487654 162 79 2932741192 0.021465 691 2013 PIT NL 77062000 0.580247 162 94 3034525648 0.025395 692 2014 PIT NL 77178000 0.543210 162 88 3192317623 0.024176 693 2015 PIT NL 88892499 0.604938 162 98 3514142569 0.025296 694 2016 PIT NL 103778833 0.481481 162 78 3750137392 0.027673 695 1985 SDN NL 11036583 0.512346 162 83 261964696 0.042130 696 1986 SDN NL 11380693 0.456790 162 74 307854518 0.036968 697 1987 SDN NL 11065796 0.401235 162 65 272575375 0.040597 698 1988 SDN NL 9561002 0.515528 161 83 300452424 0.031822 699 1989 SDN NL 14195000 0.549383 162 89 359995711 0.039431 700 1990 SDN NL 17588334 0.462963 162 75 443881193 0.039624 701 1991 SDN NL 22150001 0.518519 162 84 613048418 0.036131 702 1992 SDN NL 26854167 0.506173 162 82 805543323 0.033337 703 1993 SDN NL 25511333 0.376543 162 61 901740134 0.028291 704 1994 SDN NL 14916333 0.401709 117 47 927836287 0.016076 705 1995 SDN NL 26382334 0.486111 144 70 951469367 0.027728 706 1996 SDN NL 28348172 0.561728 162 91 956983550 0.029622 707 1997 SDN NL 37363672 0.469136 162 76 1127285885 0.033145 708 1998 SDN NL 46861500 0.604938 162 98 1278282871 0.036660 709 1999 SDN NL 49768179 0.456790 162 74 1494228750 0.033307 710 2000 SDN NL 54821000 0.469136 162 76 1666135102 0.032903 711 2001 SDN NL 39182833 0.487654 162 79 1960663313 0.019984 712 2002 SDN NL 41425000 0.407407 162 66 2024077522 0.020466 713 2003 SDN NL 45210000 0.395062 162 64 2128262128 0.021243 714 2004 SDN NL 55384833 0.537037 162 87 2070665943 0.026747 715 2005 SDN NL 63290833 0.506173 162 82 2188713398 0.028917 716 2006 SDN NL 69896141 0.543210 162 88 2321472617 0.030109 717 2007 SDN NL 58110567 0.546012 163 89 2476688987 0.023463 718 2008 SDN NL 73677616 0.388889 162 63 2684858670 0.027442 719 2009 SDN NL 43333700 0.462963 162 75 2664726994 0.016262 720 2010 SDN NL 37799300 0.555556 162 90 2721359865 0.013890 721 2011 SDN NL 45869140 0.438272 162 71 2784505291 0.016473 722 2012 SDN NL 55244700 0.469136 162 76 2932741192 0.018837 723 2013 SDN NL 65585500 0.469136 162 76 3034525648 0.021613 724 2014 SDN NL 75685700 0.475309 162 77 3192317623 0.023709 725 2015 SDN NL 118441300 0.456790 162 74 3514142569 0.033704 726 2016 SDN NL 101424814 0.419753 162 68 3750137392 0.027046 727 1985 SEA AL 4613000 0.456790 162 74 261964696 0.017609 728 1986 SEA AL 5958309 0.413580 162 67 307854518 0.019354 729 1987 SEA AL 2263500 0.481481 162 78 272575375 0.008304 730 1988 SEA AL 7342450 0.422360 161 68 300452424 0.024438 731 1989 SEA AL 9779500 0.450617 162 73 359995711 0.027166 732 1990 SEA AL 12553667 0.475309 162 77 443881193 0.028282 733 1991 SEA AL 15691833 0.512346 162 83 613048418 0.025596 734 1992 SEA AL 23179833 0.395062 162 64 805543323 0.028775 735 1993 SEA AL 32696333 0.506173 162 82 901740134 0.036259 736 1994 SEA AL 29228500 0.437500 112 49 927836287 0.031502 737 1995 SEA AL 36481311 0.544828 145 79 951469367 0.038342 738 1996 SEA AL 41328501 0.527950 161 85 956983550 0.043186 739 1997 SEA AL 41540661 0.555556 162 90 1127285885 0.036850 740 1998 SEA AL 54087036 0.472050 161 76 1278282871 0.042312 741 1999 SEA AL 54125003 0.487654 162 79 1494228750 0.036223 742 2000 SEA AL 58915000 0.561728 162 91 1666135102 0.035360 743 2001 SEA AL 74720834 0.716049 162 116 1960663313 0.038110 744 2002 SEA AL 80282668 0.574074 162 93 2024077522 0.039664 745 2003 SEA AL 86959167 0.574074 162 93 2128262128 0.040859 746 2004 SEA AL 81515834 0.388889 162 63 2070665943 0.039367 747 2005 SEA AL 87754334 0.425926 162 69 2188713398 0.040094 748 2006 SEA AL 87959833 0.481481 162 78 2321472617 0.037890 749 2007 SEA AL 106460833 0.543210 162 88 2476688987 0.042985 750 2008 SEA AL 117666482 0.376543 162 61 2684858670 0.043826 751 2009 SEA AL 98904166 0.524691 162 85 2664726994 0.037116 752 2010 SEA AL 86510000 0.376543 162 61 2721359865 0.031789 753 2011 SEA AL 86110600 0.413580 162 67 2784505291 0.030925 754 2012 SEA AL 81978100 0.462963 162 75 2932741192 0.027953 755 2013 SEA AL 74005043 0.438272 162 71 3034525648 0.024388 756 2014 SEA AL 92531100 0.537037 162 87 3192317623 0.028986 757 2015 SEA AL 122208700 0.469136 162 76 3514142569 0.034776 758 2016 SEA AL 135683339 0.530864 162 86 3750137392 0.036181 759 1985 SFN NL 8221714 0.382716 162 62 261964696 0.031385 760 1986 SFN NL 8947000 0.512346 162 83 307854518 0.029062 761 1987 SFN NL 7290000 0.555556 162 90 272575375 0.026745 762 1988 SFN NL 12380000 0.512346 162 83 300452424 0.041205 763 1989 SFN NL 14962834 0.567901 162 92 359995711 0.041564 764 1990 SFN NL 19335333 0.524691 162 85 443881193 0.043560 765 1991 SFN NL 30967666 0.462963 162 75 613048418 0.050514 766 1992 SFN NL 33163168 0.444444 162 72 805543323 0.041169 767 1993 SFN NL 35050000 0.635802 162 103 901740134 0.038869 768 1994 SFN NL 42638666 0.478261 115 55 927836287 0.045955 769 1995 SFN NL 36462777 0.465278 144 67 951469367 0.038323 770 1996 SFN NL 37144725 0.419753 162 68 956983550 0.038814 771 1997 SFN NL 35592378 0.555556 162 90 1127285885 0.031574 772 1998 SFN NL 42565834 0.546012 163 89 1278282871 0.033299 773 1999 SFN NL 46595057 0.530864 162 86 1494228750 0.031183 774 2000 SFN NL 53737826 0.598765 162 97 1666135102 0.032253 775 2001 SFN NL 63280167 0.555556 162 90 1960663313 0.032275 776 2002 SFN NL 78299835 0.586420 162 95 2024077522 0.038684 777 2003 SFN NL 82852167 0.621118 161 100 2128262128 0.038929 778 2004 SFN NL 82019166 0.561728 162 91 2070665943 0.039610 779 2005 SFN NL 90199500 0.462963 162 75 2188713398 0.041211 780 2006 SFN NL 90056419 0.472050 161 76 2321472617 0.038793 781 2007 SFN NL 90219056 0.438272 162 71 2476688987 0.036427 782 2008 SFN NL 76594500 0.444444 162 72 2684858670 0.028528 783 2009 SFN NL 83026450 0.543210 162 88 2664726994 0.031158 784 2010 SFN NL 98641333 0.567901 162 92 2721359865 0.036247 785 2011 SFN NL 118198333 0.530864 162 86 2784505291 0.042449 786 2012 SFN NL 117620683 0.580247 162 94 2932741192 0.040106 787 2013 SFN NL 140180334 0.469136 162 76 3034525648 0.046195 788 2014 SFN NL 163510167 0.543210 162 88 3192317623 0.051220 789 2015 SFN NL 164701500 0.518519 162 84 3514142569 0.046868 790 2016 SFN NL 172253778 0.537037 162 87 3750137392 0.045933 791 1985 SLN NL 11817083 0.623457 162 101 261964696 0.045109 792 1986 SLN NL 9875010 0.490683 161 79 307854518 0.032077 793 1987 SLN NL 11758000 0.586420 162 95 272575375 0.043137 794 1988 SLN NL 12880000 0.469136 162 76 300452424 0.042869 795 1989 SLN NL 16078833 0.524390 164 86 359995711 0.044664 796 1990 SLN NL 20523334 0.432099 162 70 443881193 0.046236 797 1991 SLN NL 21860001 0.518519 162 84 613048418 0.035658 798 1992 SLN NL 27583836 0.512346 162 83 805543323 0.034243 799 1993 SLN NL 23367334 0.537037 162 87 901740134 0.025914 800 1994 SLN NL 29275601 0.460870 115 53 927836287 0.031553 801 1995 SLN NL 37101000 0.433566 143 62 951469367 0.038993 802 1996 SLN NL 40269667 0.543210 162 88 956983550 0.042080 803 1997 SLN NL 45456667 0.450617 162 73 1127285885 0.040324 804 1998 SLN NL 54672521 0.509202 163 83 1278282871 0.042770 805 1999 SLN NL 49778195 0.465839 161 75 1494228750 0.033314 806 2000 SLN NL 61453863 0.586420 162 95 1666135102 0.036884 807 2001 SLN NL 78538333 0.574074 162 93 1960663313 0.040057 808 2002 SLN NL 74660875 0.598765 162 97 2024077522 0.036886 809 2003 SLN NL 83786666 0.524691 162 85 2128262128 0.039369 810 2004 SLN NL 83228333 0.648148 162 105 2070665943 0.040194 811 2005 SLN NL 92106833 0.617284 162 100 2188713398 0.042083 812 2006 SLN NL 88891371 0.515528 161 83 2321472617 0.038291 813 2007 SLN NL 90286823 0.481481 162 78 2476688987 0.036455 814 2008 SLN NL 99624449 0.530864 162 86 2684858670 0.037106 815 2009 SLN NL 88528409 0.561728 162 91 2664726994 0.033222 816 2010 SLN NL 93540751 0.530864 162 86 2721359865 0.034373 817 2011 SLN NL 105433572 0.555556 162 90 2784505291 0.037864 818 2012 SLN NL 110300862 0.543210 162 88 2932741192 0.037610 819 2013 SLN NL 92260110 0.598765 162 97 3034525648 0.030403 820 2014 SLN NL 120693000 0.555556 162 90 3192317623 0.037807 821 2015 SLN NL 119241500 0.617284 162 100 3514142569 0.033932 822 2016 SLN NL 143053500 0.530864 162 86 3750137392 0.038146 823 1998 TBA AL 27280000 0.388889 162 63 1278282871 0.021341 824 1999 TBA AL 38870000 0.425926 162 69 1494228750 0.026013 825 2000 TBA AL 62765129 0.428571 161 69 1666135102 0.037671 826 2001 TBA AL 56980000 0.382716 162 62 1960663313 0.029062 827 2002 TBA AL 34380000 0.341615 161 55 2024077522 0.016986 828 2003 TBA AL 19630000 0.388889 162 63 2128262128 0.009223 829 2004 TBA AL 29556667 0.434783 161 70 2070665943 0.014274 830 2005 TBA AL 29679067 0.413580 162 67 2188713398 0.013560 831 2006 TBA AL 34917967 0.376543 162 61 2321472617 0.015041 832 2007 TBA AL 24123500 0.407407 162 66 2476688987 0.009740 833 2008 TBA AL 43820597 0.598765 162 97 2684858670 0.016321 834 2009 TBA AL 63313034 0.518519 162 84 2664726994 0.023760 835 2010 TBA AL 71923471 0.592593 162 96 2721359865 0.026429 836 2011 TBA AL 41053571 0.561728 162 91 2784505291 0.014744 837 2012 TBA AL 64173500 0.555556 162 90 2932741192 0.021882 838 2013 TBA AL 52955272 0.564417 163 92 3034525648 0.017451 839 2014 TBA AL 72689100 0.475309 162 77 3192317623 0.022770 840 2015 TBA AL 64521233 0.493827 162 80 3514142569 0.018360 841 2016 TBA AL 57097310 0.419753 162 68 3750137392 0.015225 842 1985 TEX AL 7676500 0.385093 161 62 261964696 0.029304 843 1986 TEX AL 6743119 0.537037 162 87 307854518 0.021904 844 1987 TEX AL 880000 0.462963 162 75 272575375 0.003228 845 1988 TEX AL 5342131 0.434783 161 70 300452424 0.017780 846 1989 TEX AL 11893781 0.512346 162 83 359995711 0.033039 847 1990 TEX AL 14874372 0.512346 162 83 443881193 0.033510 848 1991 TEX AL 18224500 0.524691 162 85 613048418 0.029728 849 1992 TEX AL 30128167 0.475309 162 77 805543323 0.037401 850 1993 TEX AL 36376959 0.530864 162 86 901740134 0.040341 851 1994 TEX AL 32973597 0.456140 114 52 927836287 0.035538 852 1995 TEX AL 34581451 0.513889 144 74 951469367 0.036345 853 1996 TEX AL 39041528 0.552147 163 90 956983550 0.040796 854 1997 TEX AL 53448838 0.475309 162 77 1127285885 0.047414 855 1998 TEX AL 56572095 0.543210 162 88 1278282871 0.044256 856 1999 TEX AL 76709931 0.586420 162 95 1494228750 0.051337 857 2000 TEX AL 70795921 0.438272 162 71 1666135102 0.042491 858 2001 TEX AL 88633500 0.450617 162 73 1960663313 0.045206 859 2002 TEX AL 105526122 0.444444 162 72 2024077522 0.052135 860 2003 TEX AL 103491667 0.438272 162 71 2128262128 0.048627 861 2004 TEX AL 55050417 0.549383 162 89 2070665943 0.026586 862 2005 TEX AL 55849000 0.487654 162 79 2188713398 0.025517 863 2006 TEX AL 68228662 0.493827 162 80 2321472617 0.029390 864 2007 TEX AL 68318675 0.462963 162 75 2476688987 0.027585 865 2008 TEX AL 67712326 0.487654 162 79 2684858670 0.025220 866 2009 TEX AL 68178798 0.537037 162 87 2664726994 0.025586 867 2010 TEX AL 55250544 0.555556 162 90 2721359865 0.020303 868 2011 TEX AL 92299264 0.592593 162 96 2784505291 0.033147 869 2012 TEX AL 120510974 0.574074 162 93 2932741192 0.041092 870 2013 TEX AL 112522600 0.558282 163 91 3034525648 0.037081 871 2014 TEX AL 112255059 0.413580 162 67 3192317623 0.035164 872 2015 TEX AL 143742789 0.543210 162 88 3514142569 0.040904 873 2016 TEX AL 176038723 0.586420 162 95 3750137392 0.046942 874 1985 TOR AL 8812550 0.614907 161 99 261964696 0.033640 875 1986 TOR AL 12611047 0.527607 163 86 307854518 0.040964 876 1987 TOR AL 10479501 0.592593 162 96 272575375 0.038446 877 1988 TOR AL 12241225 0.537037 162 87 300452424 0.040743 878 1989 TOR AL 16261666 0.549383 162 89 359995711 0.045172 879 1990 TOR AL 17756834 0.530864 162 86 443881193 0.040004 880 1991 TOR AL 19902417 0.561728 162 91 613048418 0.032465 881 1992 TOR AL 44788666 0.592593 162 96 805543323 0.055601 882 1993 TOR AL 47279166 0.586420 162 95 901740134 0.052431 883 1994 TOR AL 43433668 0.478261 115 55 927836287 0.046812 884 1995 TOR AL 50590000 0.388889 144 56 951469367 0.053170 885 1996 TOR AL 29555083 0.456790 162 74 956983550 0.030884 886 1997 TOR AL 47079833 0.469136 162 76 1127285885 0.041764 887 1998 TOR AL 51376000 0.539877 163 88 1278282871 0.040191 888 1999 TOR AL 45444333 0.518519 162 84 1494228750 0.030413 889 2000 TOR AL 44838332 0.512346 162 83 1666135102 0.026912 890 2001 TOR AL 76895999 0.493827 162 80 1960663313 0.039219 891 2002 TOR AL 76864333 0.481481 162 78 2024077522 0.037975 892 2003 TOR AL 51269000 0.530864 162 86 2128262128 0.024090 893 2004 TOR AL 50017000 0.416149 161 67 2070665943 0.024155 894 2005 TOR AL 45719500 0.493827 162 80 2188713398 0.020889 895 2006 TOR AL 71365000 0.537037 162 87 2321472617 0.030741 896 2007 TOR AL 81942800 0.512346 162 83 2476688987 0.033086 897 2008 TOR AL 97793900 0.530864 162 86 2684858670 0.036424 898 2009 TOR AL 80538300 0.462963 162 75 2664726994 0.030224 899 2010 TOR AL 62234000 0.524691 162 85 2721359865 0.022869 900 2011 TOR AL 62567800 0.500000 162 81 2784505291 0.022470 901 2012 TOR AL 75009200 0.450617 162 73 2932741192 0.025576 902 2013 TOR AL 126288100 0.456790 162 74 3034525648 0.041617 903 2014 TOR AL 109920100 0.512346 162 83 3192317623 0.034433 904 2015 TOR AL 112992400 0.574074 162 93 3514142569 0.032154 905 2016 TOR AL 138701700 0.549383 162 89 3750137392 0.036986 906 2005 WAS NL 48581500 0.500000 162 81 2188713398 0.022196 907 2006 WAS NL 63143000 0.438272 162 71 2321472617 0.027200 908 2007 WAS NL 36947500 0.450617 162 73 2476688987 0.014918 909 2008 WAS NL 54961000 0.366460 161 59 2684858670 0.020471 910 2009 WAS NL 59928000 0.364198 162 59 2664726994 0.022489 911 2010 WAS NL 61400000 0.425926 162 69 2721359865 0.022562 912 2011 WAS NL 63856928 0.496894 161 80 2784505291 0.022933 913 2012 WAS NL 80855143 0.604938 162 98 2932741192 0.027570 914 2013 WAS NL 113703270 0.530864 162 86 3034525648 0.037470 915 2014 WAS NL 131983680 0.592593 162 96 3192317623 0.041344 916 2015 WAS NL 155587472 0.512346 162 83 3514142569 0.044275 917 2016 WAS NL 141652646 0.586420 162 95 3750137392 0.037773 In\u00a0[13]: Copied! <pre># this will allow us to inspect all rows in the data\n\npd.set_option('display.max_rows', 1000)\nMLB\n</pre> # this will allow us to inspect all rows in the data  pd.set_option('display.max_rows', 1000) MLB Out[13]: season Team lgID salaries wpc G W allsal relsal 0 1997 ANA AL 31135472 0.518519 162 84 1127285885 0.027620 1 1998 ANA AL 41281000 0.524691 162 85 1278282871 0.032294 2 1999 ANA AL 55388166 0.432099 162 70 1494228750 0.037068 3 2000 ANA AL 51464167 0.506173 162 82 1666135102 0.030888 4 2001 ANA AL 47535167 0.462963 162 75 1960663313 0.024244 5 2002 ANA AL 61721667 0.611111 162 99 2024077522 0.030494 6 2003 ANA AL 79031667 0.475309 162 77 2128262128 0.037134 7 2004 ANA AL 100534667 0.567901 162 92 2070665943 0.048552 8 1998 ARI NL 32347000 0.401235 162 65 1278282871 0.025305 9 1999 ARI NL 68703999 0.617284 162 100 1494228750 0.045980 10 2000 ARI NL 81027833 0.524691 162 85 1666135102 0.048632 11 2001 ARI NL 85082999 0.567901 162 92 1960663313 0.043395 12 2002 ARI NL 102819999 0.604938 162 98 2024077522 0.050798 13 2003 ARI NL 80657000 0.518519 162 84 2128262128 0.037898 14 2004 ARI NL 69780750 0.314815 162 51 2070665943 0.033700 15 2005 ARI NL 62329166 0.475309 162 77 2188713398 0.028478 16 2006 ARI NL 59684226 0.469136 162 76 2321472617 0.025710 17 2007 ARI NL 52067546 0.555556 162 90 2476688987 0.021023 18 2008 ARI NL 66202712 0.506173 162 82 2684858670 0.024658 19 2009 ARI NL 73115666 0.432099 162 70 2664726994 0.027438 20 2010 ARI NL 60718166 0.401235 162 65 2721359865 0.022312 21 2011 ARI NL 53639833 0.580247 162 94 2784505291 0.019264 22 2012 ARI NL 73804833 0.500000 162 81 2932741192 0.025166 23 2013 ARI NL 90132000 0.500000 162 81 3034525648 0.029702 24 2014 ARI NL 97861500 0.395062 162 64 3192317623 0.030655 25 2015 ARI NL 61834000 0.487654 162 79 3514142569 0.017596 26 2016 ARI NL 87439063 0.425926 162 69 3750137392 0.023316 27 1985 ATL NL 14807000 0.407407 162 66 261964696 0.056523 28 1986 ATL NL 17102786 0.447205 161 72 307854518 0.055555 29 1987 ATL NL 16544560 0.428571 161 69 272575375 0.060697 30 1988 ATL NL 12728174 0.337500 160 54 300452424 0.042363 31 1989 ATL NL 11112334 0.391304 161 63 359995711 0.030868 32 1990 ATL NL 14555501 0.401235 162 65 443881193 0.032791 33 1991 ATL NL 18403500 0.580247 162 94 613048418 0.030020 34 1992 ATL NL 34625333 0.604938 162 98 805543323 0.042984 35 1993 ATL NL 41641417 0.641975 162 104 901740134 0.046179 36 1994 ATL NL 49383513 0.596491 114 68 927836287 0.053224 37 1995 ATL NL 47235445 0.625000 144 90 951469367 0.049645 38 1996 ATL NL 49698500 0.592593 162 96 956983550 0.051932 39 1997 ATL NL 52278500 0.623457 162 101 1127285885 0.046376 40 1998 ATL NL 61186000 0.654321 162 106 1278282871 0.047866 41 1999 ATL NL 73140000 0.635802 162 103 1494228750 0.048948 42 2000 ATL NL 84537836 0.586420 162 95 1666135102 0.050739 43 2001 ATL NL 91936166 0.543210 162 88 1960663313 0.046890 44 2002 ATL NL 92870367 0.627329 161 101 2024077522 0.045883 45 2003 ATL NL 106243667 0.623457 162 101 2128262128 0.049920 46 2004 ATL NL 90182500 0.592593 162 96 2070665943 0.043552 47 2005 ATL NL 86457302 0.555556 162 90 2188713398 0.039501 48 2006 ATL NL 90156876 0.487654 162 79 2321472617 0.038836 49 2007 ATL NL 87290833 0.518519 162 84 2476688987 0.035245 50 2008 ATL NL 102365683 0.444444 162 72 2684858670 0.038127 51 2009 ATL NL 96726166 0.530864 162 86 2664726994 0.036299 52 2010 ATL NL 84423666 0.561728 162 91 2721359865 0.031023 53 2011 ATL NL 87002692 0.549383 162 89 2784505291 0.031245 54 2012 ATL NL 82829942 0.580247 162 94 2932741192 0.028243 55 2013 ATL NL 87871525 0.592593 162 96 3034525648 0.028957 56 2014 ATL NL 97609000 0.487654 162 79 3192317623 0.030576 57 2015 ATL NL 71781250 0.413580 162 67 3514142569 0.020426 58 2016 ATL NL 68498291 0.422360 161 68 3750137392 0.018266 59 1985 BAL AL 11560712 0.515528 161 83 261964696 0.044131 60 1986 BAL AL 13001258 0.450617 162 73 307854518 0.042232 61 1987 BAL AL 13900273 0.413580 162 67 272575375 0.050996 62 1988 BAL AL 13532075 0.335404 161 54 300452424 0.045039 63 1989 BAL AL 8275167 0.537037 162 87 359995711 0.022987 64 1990 BAL AL 9680084 0.472050 161 76 443881193 0.021808 65 1991 BAL AL 17519000 0.413580 162 67 613048418 0.028577 66 1992 BAL AL 23780667 0.549383 162 89 805543323 0.029521 67 1993 BAL AL 29096500 0.524691 162 85 901740134 0.032267 68 1994 BAL AL 38849769 0.562500 112 63 927836287 0.041871 69 1995 BAL AL 43942521 0.493056 144 71 951469367 0.046184 70 1996 BAL AL 54490315 0.539877 163 88 956983550 0.056940 71 1997 BAL AL 58516400 0.604938 162 98 1127285885 0.051909 72 1998 BAL AL 72355634 0.487654 162 79 1278282871 0.056604 73 1999 BAL AL 80605863 0.481481 162 78 1494228750 0.053945 74 2000 BAL AL 81447435 0.456790 162 74 1666135102 0.048884 75 2001 BAL AL 67599540 0.388889 162 63 1960663313 0.034478 76 2002 BAL AL 60493487 0.413580 162 67 2024077522 0.029887 77 2003 BAL AL 73877500 0.435583 163 71 2128262128 0.034713 78 2004 BAL AL 51623333 0.481481 162 78 2070665943 0.024931 79 2005 BAL AL 73914333 0.456790 162 74 2188713398 0.033771 80 2006 BAL AL 72585582 0.432099 162 70 2321472617 0.031267 81 2007 BAL AL 93174808 0.425926 162 69 2476688987 0.037621 82 2008 BAL AL 67196246 0.422360 161 68 2684858670 0.025028 83 2009 BAL AL 67101666 0.395062 162 64 2664726994 0.025181 84 2010 BAL AL 81612500 0.407407 162 66 2721359865 0.029990 85 2011 BAL AL 85304038 0.425926 162 69 2784505291 0.030635 86 2012 BAL AL 77353999 0.574074 162 93 2932741192 0.026376 87 2013 BAL AL 84393333 0.524691 162 85 3034525648 0.027811 88 2014 BAL AL 103416000 0.592593 162 96 3192317623 0.032395 89 2015 BAL AL 115044833 0.500000 162 81 3514142569 0.032738 90 2016 BAL AL 161863456 0.549383 162 89 3750137392 0.043162 91 1985 BOS AL 10897560 0.496933 163 81 261964696 0.041599 92 1986 BOS AL 14402239 0.590062 161 95 307854518 0.046783 93 1987 BOS AL 10144167 0.481481 162 78 272575375 0.037216 94 1988 BOS AL 13896092 0.549383 162 89 300452424 0.046251 95 1989 BOS AL 17481748 0.512346 162 83 359995711 0.048561 96 1990 BOS AL 20558333 0.543210 162 88 443881193 0.046315 97 1991 BOS AL 35167500 0.518519 162 84 613048418 0.057365 98 1992 BOS AL 43610584 0.450617 162 73 805543323 0.054138 99 1993 BOS AL 37120583 0.493827 162 80 901740134 0.041165 100 1994 BOS AL 37859084 0.469565 115 54 927836287 0.040804 101 1995 BOS AL 32455518 0.597222 144 86 951469367 0.034111 102 1996 BOS AL 42393500 0.524691 162 85 956983550 0.044299 103 1997 BOS AL 43558750 0.481481 162 78 1127285885 0.038640 104 1998 BOS AL 56757000 0.567901 162 92 1278282871 0.044401 105 1999 BOS AL 63497500 0.580247 162 94 1494228750 0.042495 106 2000 BOS AL 77940333 0.524691 162 85 1666135102 0.046779 107 2001 BOS AL 110035833 0.509317 161 82 1960663313 0.056122 108 2002 BOS AL 108366060 0.574074 162 93 2024077522 0.053538 109 2003 BOS AL 99946500 0.586420 162 95 2128262128 0.046962 110 2004 BOS AL 127298500 0.604938 162 98 2070665943 0.061477 111 2005 BOS AL 123505125 0.586420 162 95 2188713398 0.056428 112 2006 BOS AL 120099824 0.530864 162 86 2321472617 0.051734 113 2007 BOS AL 143026214 0.592593 162 96 2476688987 0.057749 114 2008 BOS AL 133390035 0.586420 162 95 2684858670 0.049682 115 2009 BOS AL 121345999 0.586420 162 95 2664726994 0.045538 116 2010 BOS AL 162447333 0.549383 162 89 2721359865 0.059693 117 2011 BOS AL 161762475 0.555556 162 90 2784505291 0.058094 118 2012 BOS AL 173186617 0.425926 162 69 2932741192 0.059053 119 2013 BOS AL 151530000 0.598765 162 97 3034525648 0.049935 120 2014 BOS AL 139019929 0.438272 162 71 3192317623 0.043548 121 2015 BOS AL 181103400 0.481481 162 78 3514142569 0.051536 122 2016 BOS AL 188545761 0.574074 162 93 3750137392 0.050277 123 1985 CAL AL 14427894 0.555556 162 90 261964696 0.055076 124 1986 CAL AL 14427258 0.567901 162 92 307854518 0.046864 125 1987 CAL AL 12843499 0.462963 162 75 272575375 0.047119 126 1988 CAL AL 11947388 0.462963 162 75 300452424 0.039765 127 1989 CAL AL 15097833 0.561728 162 91 359995711 0.041939 128 1990 CAL AL 21720000 0.493827 162 80 443881193 0.048932 129 1991 CAL AL 33060001 0.500000 162 81 613048418 0.053927 130 1992 CAL AL 34749334 0.444444 162 72 805543323 0.043138 131 1993 CAL AL 28588334 0.438272 162 71 901740134 0.031704 132 1994 CAL AL 25156218 0.408696 115 47 927836287 0.027113 133 1995 CAL AL 31223171 0.537931 145 78 951469367 0.032816 134 1996 CAL AL 28738000 0.434783 161 70 956983550 0.030030 135 1985 CHA AL 9846178 0.521472 163 85 261964696 0.037586 136 1986 CHA AL 10418819 0.444444 162 72 307854518 0.033843 137 1987 CHA AL 10641843 0.475309 162 77 272575375 0.039042 138 1988 CHA AL 6390000 0.440994 161 71 300452424 0.021268 139 1989 CHA AL 7265410 0.428571 161 69 359995711 0.020182 140 1990 CHA AL 9491500 0.580247 162 94 443881193 0.021383 141 1991 CHA AL 16919667 0.537037 162 87 613048418 0.027599 142 1992 CHA AL 30160833 0.530864 162 86 805543323 0.037442 143 1993 CHA AL 39696166 0.580247 162 94 901740134 0.044022 144 1994 CHA AL 39183836 0.592920 113 67 927836287 0.042231 145 1995 CHA AL 46961282 0.468966 145 68 951469367 0.049357 146 1996 CHA AL 45139500 0.524691 162 85 956983550 0.047169 147 1997 CHA AL 57740000 0.496894 161 80 1127285885 0.051220 148 1998 CHA AL 38335000 0.490798 163 80 1278282871 0.029989 149 1999 CHA AL 25620000 0.462963 162 75 1494228750 0.017146 150 2000 CHA AL 31133500 0.586420 162 95 1666135102 0.018686 151 2001 CHA AL 65653667 0.512346 162 83 1960663313 0.033485 152 2002 CHA AL 57052833 0.500000 162 81 2024077522 0.028187 153 2003 CHA AL 51010000 0.530864 162 86 2128262128 0.023968 154 2004 CHA AL 65212500 0.512346 162 83 2070665943 0.031493 155 2005 CHA AL 75178000 0.611111 162 99 2188713398 0.034348 156 2006 CHA AL 102750667 0.555556 162 90 2321472617 0.044261 157 2007 CHA AL 108671833 0.444444 162 72 2476688987 0.043878 158 2008 CHA AL 121189332 0.546012 163 89 2684858670 0.045138 159 2009 CHA AL 96068500 0.487654 162 79 2664726994 0.036052 160 2010 CHA AL 105530000 0.543210 162 88 2721359865 0.038778 161 2011 CHA AL 127789000 0.487654 162 79 2784505291 0.045893 162 2012 CHA AL 96919500 0.524691 162 85 2932741192 0.033047 163 2013 CHA AL 120065277 0.388889 162 63 3034525648 0.039566 164 2014 CHA AL 81830500 0.450617 162 73 3192317623 0.025634 165 2015 CHA AL 112373700 0.469136 162 76 3514142569 0.031978 166 2016 CHA AL 112998667 0.481481 162 78 3750137392 0.030132 167 1985 CHN NL 12702917 0.475309 162 77 261964696 0.048491 168 1986 CHN NL 17208165 0.437500 160 70 307854518 0.055897 169 1987 CHN NL 14307999 0.472050 161 76 272575375 0.052492 170 1988 CHN NL 13119198 0.472393 163 77 300452424 0.043665 171 1989 CHN NL 10668000 0.574074 162 93 359995711 0.029634 172 1990 CHN NL 13624000 0.475309 162 77 443881193 0.030693 173 1991 CHN NL 23175667 0.481250 160 77 613048418 0.037804 174 1992 CHN NL 29829686 0.481481 162 78 805543323 0.037031 175 1993 CHN NL 39386666 0.515337 163 84 901740134 0.043679 176 1994 CHN NL 36287333 0.433628 113 49 927836287 0.039110 177 1995 CHN NL 29505834 0.506944 144 73 951469367 0.031011 178 1996 CHN NL 33081000 0.469136 162 76 956983550 0.034568 179 1997 CHN NL 42155333 0.419753 162 68 1127285885 0.037395 180 1998 CHN NL 50838000 0.552147 163 90 1278282871 0.039771 181 1999 CHN NL 62343000 0.413580 162 67 1494228750 0.041723 182 2000 CHN NL 60539333 0.401235 162 65 1666135102 0.036335 183 2001 CHN NL 64715833 0.543210 162 88 1960663313 0.033007 184 2002 CHN NL 75690833 0.413580 162 67 2024077522 0.037395 185 2003 CHN NL 79868333 0.543210 162 88 2128262128 0.037527 186 2004 CHN NL 90560000 0.549383 162 89 2070665943 0.043735 187 2005 CHN NL 87032933 0.487654 162 79 2188713398 0.039764 188 2006 CHN NL 94424499 0.407407 162 66 2321472617 0.040674 189 2007 CHN NL 99670332 0.524691 162 85 2476688987 0.040243 190 2008 CHN NL 118345833 0.602484 161 97 2684858670 0.044079 191 2009 CHN NL 134809000 0.515528 161 83 2664726994 0.050590 192 2010 CHN NL 146609000 0.462963 162 75 2721359865 0.053873 193 2011 CHN NL 125047329 0.438272 162 71 2784505291 0.044908 194 2012 CHN NL 88197033 0.376543 162 61 2932741192 0.030073 195 2013 CHN NL 100567726 0.407407 162 66 3034525648 0.033141 196 2014 CHN NL 65522500 0.450617 162 73 3192317623 0.020525 197 2015 CHN NL 115879310 0.598765 162 97 3514142569 0.032975 198 2016 CHN NL 154067668 0.635802 162 103 3750137392 0.041083 199 1985 CIN NL 8359917 0.549383 162 89 261964696 0.031912 200 1986 CIN NL 11906388 0.530864 162 86 307854518 0.038675 201 1987 CIN NL 9281500 0.518519 162 84 272575375 0.034051 202 1988 CIN NL 8888409 0.540373 161 87 300452424 0.029583 203 1989 CIN NL 11072000 0.462963 162 75 359995711 0.030756 204 1990 CIN NL 14370000 0.561728 162 91 443881193 0.032374 205 1991 CIN NL 26305333 0.456790 162 74 613048418 0.042909 206 1992 CIN NL 35931499 0.555556 162 90 805543323 0.044605 207 1993 CIN NL 44879666 0.450617 162 73 901740134 0.049770 208 1994 CIN NL 40961833 0.573913 115 66 927836287 0.044148 209 1995 CIN NL 43144670 0.590278 144 85 951469367 0.045345 210 1996 CIN NL 42526334 0.500000 162 81 956983550 0.044438 211 1997 CIN NL 49768000 0.469136 162 76 1127285885 0.044149 212 1998 CIN NL 23005000 0.475309 162 77 1278282871 0.017997 213 1999 CIN NL 33962761 0.588957 163 96 1494228750 0.022729 214 2000 CIN NL 46867200 0.521472 163 85 1666135102 0.028129 215 2001 CIN NL 48986000 0.407407 162 66 1960663313 0.024984 216 2002 CIN NL 45050390 0.481481 162 78 2024077522 0.022257 217 2003 CIN NL 59355667 0.425926 162 69 2128262128 0.027889 218 2004 CIN NL 46615250 0.469136 162 76 2070665943 0.022512 219 2005 CIN NL 61892583 0.447853 163 73 2188713398 0.028278 220 2006 CIN NL 60909519 0.493827 162 80 2321472617 0.026237 221 2007 CIN NL 68524980 0.444444 162 72 2476688987 0.027668 222 2008 CIN NL 74117695 0.456790 162 74 2684858670 0.027606 223 2009 CIN NL 73558500 0.481481 162 78 2664726994 0.027605 224 2010 CIN NL 71761542 0.561728 162 91 2721359865 0.026370 225 2011 CIN NL 75947134 0.487654 162 79 2784505291 0.027275 226 2012 CIN NL 82203616 0.598765 162 97 2932741192 0.028030 227 2013 CIN NL 106404462 0.555556 162 90 3034525648 0.035065 228 2014 CIN NL 108217500 0.469136 162 76 3192317623 0.033899 229 2015 CIN NL 113072286 0.395062 162 64 3514142569 0.032176 230 2016 CIN NL 88940059 0.419753 162 68 3750137392 0.023716 231 1985 CLE AL 6551666 0.370370 162 60 261964696 0.025010 232 1986 CLE AL 7809500 0.515337 163 84 307854518 0.025368 233 1987 CLE AL 8513750 0.376543 162 61 272575375 0.031234 234 1988 CLE AL 8936500 0.481481 162 78 300452424 0.029743 235 1989 CLE AL 9094500 0.450617 162 73 359995711 0.025263 236 1990 CLE AL 14487000 0.475309 162 77 443881193 0.032637 237 1991 CLE AL 17635000 0.351852 162 57 613048418 0.028766 238 1992 CLE AL 9373044 0.469136 162 76 805543323 0.011636 239 1993 CLE AL 18561000 0.469136 162 76 901740134 0.020584 240 1994 CLE AL 30490500 0.584071 113 66 927836287 0.032862 241 1995 CLE AL 37937835 0.694444 144 100 951469367 0.039873 242 1996 CLE AL 48107360 0.614907 161 99 956983550 0.050270 243 1997 CLE AL 56802460 0.534161 161 86 1127285885 0.050389 244 1998 CLE AL 60800166 0.549383 162 89 1278282871 0.047564 245 1999 CLE AL 72978462 0.598765 162 97 1494228750 0.048840 246 2000 CLE AL 75880771 0.555556 162 90 1666135102 0.045543 247 2001 CLE AL 93152001 0.561728 162 91 1960663313 0.047510 248 2002 CLE AL 78909449 0.456790 162 74 2024077522 0.038985 249 2003 CLE AL 48584834 0.419753 162 68 2128262128 0.022828 250 2004 CLE AL 34319300 0.493827 162 80 2070665943 0.016574 251 2005 CLE AL 41502500 0.574074 162 93 2188713398 0.018962 252 2006 CLE AL 56031500 0.481481 162 78 2321472617 0.024136 253 2007 CLE AL 61673267 0.592593 162 96 2476688987 0.024901 254 2008 CLE AL 78970066 0.500000 162 81 2684858670 0.029413 255 2009 CLE AL 81579166 0.401235 162 65 2664726994 0.030614 256 2010 CLE AL 61203966 0.425926 162 69 2721359865 0.022490 257 2011 CLE AL 48776566 0.493827 162 80 2784505291 0.017517 258 2012 CLE AL 78430300 0.419753 162 68 2932741192 0.026743 259 2013 CLE AL 75771800 0.567901 162 92 3034525648 0.024970 260 2014 CLE AL 82151899 0.524691 162 85 3192317623 0.025734 261 2015 CLE AL 87663766 0.503106 161 81 3514142569 0.024946 262 2016 CLE AL 74311900 0.583851 161 94 3750137392 0.019816 263 1993 COL NL 10353500 0.413580 162 67 901740134 0.011482 264 1994 COL NL 23887333 0.452991 117 53 927836287 0.025745 265 1995 COL NL 34154717 0.534722 144 77 951469367 0.035897 266 1996 COL NL 40179823 0.512346 162 83 956983550 0.041986 267 1997 COL NL 43559667 0.512346 162 83 1127285885 0.038641 268 1998 COL NL 50484648 0.475309 162 77 1278282871 0.039494 269 1999 COL NL 61935837 0.444444 162 72 1494228750 0.041450 270 2000 COL NL 61111190 0.506173 162 82 1666135102 0.036678 271 2001 COL NL 71541334 0.450617 162 73 1960663313 0.036488 272 2002 COL NL 56851043 0.450617 162 73 2024077522 0.028087 273 2003 COL NL 67179667 0.456790 162 74 2128262128 0.031566 274 2004 COL NL 65445167 0.419753 162 68 2070665943 0.031606 275 2005 COL NL 47839000 0.413580 162 67 2188713398 0.021857 276 2006 COL NL 41233000 0.469136 162 76 2321472617 0.017762 277 2007 COL NL 54041000 0.552147 163 90 2476688987 0.021820 278 2008 COL NL 68655500 0.456790 162 74 2684858670 0.025571 279 2009 COL NL 75201000 0.567901 162 92 2664726994 0.028221 280 2010 COL NL 84227000 0.512346 162 83 2721359865 0.030950 281 2011 COL NL 88148071 0.450617 162 73 2784505291 0.031657 282 2012 COL NL 78069571 0.395062 162 64 2932741192 0.026620 283 2013 COL NL 74409071 0.456790 162 74 3034525648 0.024521 284 2014 COL NL 95403500 0.407407 162 66 3192317623 0.029885 285 2015 COL NL 95688600 0.419753 162 68 3514142569 0.027230 286 2016 COL NL 112645071 0.462963 162 75 3750137392 0.030038 287 1985 DET AL 10348143 0.521739 161 84 261964696 0.039502 288 1986 DET AL 12335714 0.537037 162 87 307854518 0.040070 289 1987 DET AL 12122881 0.604938 162 98 272575375 0.044475 290 1988 DET AL 12869571 0.543210 162 88 300452424 0.042834 291 1989 DET AL 15146404 0.364198 162 59 359995711 0.042074 292 1990 DET AL 17593238 0.487654 162 79 443881193 0.039635 293 1991 DET AL 23838333 0.518519 162 84 613048418 0.038885 294 1992 DET AL 27322834 0.462963 162 75 805543323 0.033919 295 1993 DET AL 38150165 0.524691 162 85 901740134 0.042307 296 1994 DET AL 41446501 0.460870 115 53 927836287 0.044670 297 1995 DET AL 37044168 0.416667 144 60 951469367 0.038934 298 1996 DET AL 23438000 0.327160 162 53 956983550 0.024492 299 1997 DET AL 17272000 0.487654 162 79 1127285885 0.015322 300 1998 DET AL 24065000 0.401235 162 65 1278282871 0.018826 301 1999 DET AL 36489666 0.428571 161 69 1494228750 0.024420 302 2000 DET AL 58265167 0.487654 162 79 1666135102 0.034970 303 2001 DET AL 53416167 0.407407 162 66 1960663313 0.027244 304 2002 DET AL 55048000 0.341615 161 55 2024077522 0.027197 305 2003 DET AL 49168000 0.265432 162 43 2128262128 0.023102 306 2004 DET AL 46832000 0.444444 162 72 2070665943 0.022617 307 2005 DET AL 69092000 0.438272 162 71 2188713398 0.031567 308 2006 DET AL 82612866 0.586420 162 95 2321472617 0.035586 309 2007 DET AL 94800369 0.543210 162 88 2476688987 0.038277 310 2008 DET AL 137685196 0.456790 162 74 2684858670 0.051282 311 2009 DET AL 115085145 0.527607 163 86 2664726994 0.043188 312 2010 DET AL 122864928 0.500000 162 81 2721359865 0.045148 313 2011 DET AL 105700231 0.586420 162 95 2784505291 0.037960 314 2012 DET AL 132300000 0.543210 162 88 2932741192 0.045111 315 2013 DET AL 145989500 0.574074 162 93 3034525648 0.048109 316 2014 DET AL 152855500 0.555556 162 90 3192317623 0.047882 317 2015 DET AL 172284750 0.459627 161 74 3514142569 0.049026 318 2016 DET AL 194876481 0.534161 161 86 3750137392 0.051965 319 1993 FLO NL 19330545 0.395062 162 64 901740134 0.021437 320 1994 FLO NL 21633000 0.443478 115 51 927836287 0.023316 321 1995 FLO NL 24515781 0.468531 143 67 951469367 0.025766 322 1996 FLO NL 31022500 0.493827 162 80 956983550 0.032417 323 1997 FLO NL 48692500 0.567901 162 92 1127285885 0.043194 324 1998 FLO NL 41322667 0.333333 162 54 1278282871 0.032327 325 1999 FLO NL 21085000 0.395062 162 64 1494228750 0.014111 326 2000 FLO NL 19872000 0.490683 161 79 1666135102 0.011927 327 2001 FLO NL 35762500 0.469136 162 76 1960663313 0.018240 328 2002 FLO NL 41979917 0.487654 162 79 2024077522 0.020740 329 2003 FLO NL 49450000 0.561728 162 91 2128262128 0.023235 330 2004 FLO NL 42143042 0.512346 162 83 2070665943 0.020352 331 2005 FLO NL 60408834 0.512346 162 83 2188713398 0.027600 332 2006 FLO NL 14671500 0.481481 162 78 2321472617 0.006320 333 2007 FLO NL 30507000 0.438272 162 71 2476688987 0.012318 334 2008 FLO NL 21811500 0.521739 161 84 2684858670 0.008124 335 2009 FLO NL 36834000 0.537037 162 87 2664726994 0.013823 336 2010 FLO NL 57029719 0.493827 162 80 2721359865 0.020956 337 2011 FLO NL 56944000 0.444444 162 72 2784505291 0.020450 338 1985 HOU NL 9993051 0.512346 162 83 261964696 0.038147 339 1986 HOU NL 9873276 0.592593 162 96 307854518 0.032071 340 1987 HOU NL 12608371 0.469136 162 76 272575375 0.046256 341 1988 HOU NL 12286167 0.506173 162 82 300452424 0.040892 342 1989 HOU NL 15029500 0.530864 162 86 359995711 0.041749 343 1990 HOU NL 18330000 0.462963 162 75 443881193 0.041295 344 1991 HOU NL 12852500 0.401235 162 65 613048418 0.020965 345 1992 HOU NL 15407500 0.500000 162 81 805543323 0.019127 346 1993 HOU NL 30210500 0.524691 162 85 901740134 0.033502 347 1994 HOU NL 33126000 0.573913 115 66 927836287 0.035702 348 1995 HOU NL 34169834 0.527778 144 76 951469367 0.035913 349 1996 HOU NL 28487000 0.506173 162 82 956983550 0.029767 350 1997 HOU NL 34777500 0.518519 162 84 1127285885 0.030851 351 1998 HOU NL 42374000 0.629630 162 102 1278282871 0.033149 352 1999 HOU NL 54914000 0.598765 162 97 1494228750 0.036751 353 2000 HOU NL 51289111 0.444444 162 72 1666135102 0.030783 354 2001 HOU NL 60612667 0.574074 162 93 1960663313 0.030914 355 2002 HOU NL 63448417 0.518519 162 84 2024077522 0.031347 356 2003 HOU NL 71040000 0.537037 162 87 2128262128 0.033379 357 2004 HOU NL 75397000 0.567901 162 92 2070665943 0.036412 358 2005 HOU NL 76779000 0.546012 163 89 2188713398 0.035080 359 2006 HOU NL 88694435 0.506173 162 82 2321472617 0.038206 360 2007 HOU NL 87759000 0.450617 162 73 2476688987 0.035434 361 2008 HOU NL 88930414 0.534161 161 86 2684858670 0.033123 362 2009 HOU NL 102996414 0.456790 162 74 2664726994 0.038652 363 2010 HOU NL 92355500 0.469136 162 76 2721359865 0.033937 364 2011 HOU NL 70694000 0.345679 162 56 2784505291 0.025388 365 2012 HOU NL 60651000 0.339506 162 55 2932741192 0.020681 366 2013 HOU AL 17890700 0.314815 162 51 3034525648 0.005896 367 2014 HOU AL 35116300 0.432099 162 70 3192317623 0.011000 368 2015 HOU AL 72256200 0.530864 162 86 3514142569 0.020562 369 2016 HOU AL 94893700 0.518519 162 84 3750137392 0.025304 370 1985 KCA AL 9321179 0.561728 162 91 261964696 0.035582 371 1986 KCA AL 13043698 0.469136 162 76 307854518 0.042370 372 1987 KCA AL 11828056 0.512346 162 83 272575375 0.043394 373 1988 KCA AL 14556562 0.521739 161 84 300452424 0.048449 374 1989 KCA AL 18683568 0.567901 162 92 359995711 0.051899 375 1990 KCA AL 23361084 0.465839 161 75 443881193 0.052629 376 1991 KCA AL 26319834 0.506173 162 82 613048418 0.042933 377 1992 KCA AL 33893834 0.444444 162 72 805543323 0.042076 378 1993 KCA AL 41346167 0.518519 162 84 901740134 0.045852 379 1994 KCA AL 40541334 0.556522 115 64 927836287 0.043694 380 1995 KCA AL 29532834 0.486111 144 70 951469367 0.031039 381 1996 KCA AL 20281250 0.465839 161 75 956983550 0.021193 382 1997 KCA AL 34655000 0.416149 161 67 1127285885 0.030742 383 1998 KCA AL 36862500 0.447205 161 72 1278282871 0.028838 384 1999 KCA AL 26225000 0.397516 161 64 1494228750 0.017551 385 2000 KCA AL 23433000 0.475309 162 77 1666135102 0.014064 386 2001 KCA AL 35422500 0.401235 162 65 1960663313 0.018067 387 2002 KCA AL 47257000 0.382716 162 62 2024077522 0.023347 388 2003 KCA AL 40518000 0.512346 162 83 2128262128 0.019038 389 2004 KCA AL 47609000 0.358025 162 58 2070665943 0.022992 390 2005 KCA AL 36881000 0.345679 162 56 2188713398 0.016851 391 2006 KCA AL 47294000 0.382716 162 62 2321472617 0.020372 392 2007 KCA AL 67116500 0.425926 162 69 2476688987 0.027099 393 2008 KCA AL 58245500 0.462963 162 75 2684858670 0.021694 394 2009 KCA AL 70519333 0.401235 162 65 2664726994 0.026464 395 2010 KCA AL 71405210 0.413580 162 67 2721359865 0.026239 396 2011 KCA AL 35712000 0.438272 162 71 2784505291 0.012825 397 2012 KCA AL 60916225 0.444444 162 72 2932741192 0.020771 398 2013 KCA AL 80091725 0.530864 162 86 3034525648 0.026393 399 2014 KCA AL 74594075 0.549383 162 89 3192317623 0.023367 400 2015 KCA AL 112107025 0.586420 162 95 3514142569 0.031902 401 2016 KCA AL 131487125 0.500000 162 81 3750137392 0.035062 402 2005 LAA AL 94867822 0.586420 162 95 2188713398 0.043344 403 2006 LAA AL 103472000 0.549383 162 89 2321472617 0.044572 404 2007 LAA AL 109251333 0.580247 162 94 2476688987 0.044112 405 2008 LAA AL 119216333 0.617284 162 100 2684858670 0.044403 406 2009 LAA AL 113709000 0.598765 162 97 2664726994 0.042672 407 2010 LAA AL 104963866 0.493827 162 80 2721359865 0.038570 408 2011 LAA AL 138543166 0.530864 162 86 2784505291 0.049755 409 2012 LAA AL 154485166 0.549383 162 89 2932741192 0.052676 410 2013 LAA AL 124174750 0.481481 162 78 3034525648 0.040921 411 2014 LAA AL 121988250 0.604938 162 98 3192317623 0.038213 412 2015 LAA AL 120005415 0.524691 162 85 3514142569 0.034149 413 2016 LAA AL 137251333 0.456790 162 74 3750137392 0.036599 414 1985 LAN NL 10967917 0.586420 162 95 261964696 0.041868 415 1986 LAN NL 14913776 0.450617 162 73 307854518 0.048444 416 1987 LAN NL 13675403 0.450617 162 73 272575375 0.050171 417 1988 LAN NL 16850515 0.580247 162 94 300452424 0.056084 418 1989 LAN NL 21071562 0.481250 160 77 359995711 0.058533 419 1990 LAN NL 21318704 0.530864 162 86 443881193 0.048028 420 1991 LAN NL 32790664 0.574074 162 93 613048418 0.053488 421 1992 LAN NL 44788166 0.388889 162 63 805543323 0.055600 422 1993 LAN NL 39331999 0.500000 162 81 901740134 0.043618 423 1994 LAN NL 38000001 0.508772 114 58 927836287 0.040956 424 1995 LAN NL 39273201 0.541667 144 78 951469367 0.041276 425 1996 LAN NL 35355000 0.555556 162 90 956983550 0.036944 426 1997 LAN NL 45380304 0.543210 162 88 1127285885 0.040256 427 1998 LAN NL 48820000 0.512346 162 83 1278282871 0.038192 428 1999 LAN NL 80862453 0.475309 162 77 1494228750 0.054117 429 2000 LAN NL 87924286 0.530864 162 86 1666135102 0.052771 430 2001 LAN NL 109105953 0.530864 162 86 1960663313 0.055647 431 2002 LAN NL 94850953 0.567901 162 92 2024077522 0.046861 432 2003 LAN NL 105572620 0.524691 162 85 2128262128 0.049605 433 2004 LAN NL 92902001 0.574074 162 93 2070665943 0.044866 434 2005 LAN NL 83039000 0.438272 162 71 2188713398 0.037940 435 2006 LAN NL 98447187 0.543210 162 88 2321472617 0.042407 436 2007 LAN NL 108454524 0.506173 162 82 2476688987 0.043790 437 2008 LAN NL 118588536 0.518519 162 84 2684858670 0.044169 438 2009 LAN NL 100414592 0.586420 162 95 2664726994 0.037683 439 2010 LAN NL 95358016 0.493827 162 80 2721359865 0.035041 440 2011 LAN NL 104188999 0.509317 161 82 2784505291 0.037417 441 2012 LAN NL 95143575 0.530864 162 86 2932741192 0.032442 442 2013 LAN NL 223362196 0.567901 162 92 3034525648 0.073607 443 2014 LAN NL 217014600 0.580247 162 94 3192317623 0.067980 444 2015 LAN NL 215792000 0.567901 162 92 3514142569 0.061407 445 2016 LAN NL 221288380 0.561728 162 91 3750137392 0.059008 446 2012 MIA NL 118078000 0.425926 162 69 2932741192 0.040262 447 2013 MIA NL 33601900 0.382716 162 62 3034525648 0.011073 448 2014 MIA NL 41836900 0.475309 162 77 3192317623 0.013105 449 2015 MIA NL 68056500 0.438272 162 71 3514142569 0.019366 450 2016 MIA NL 77314202 0.490683 161 79 3750137392 0.020616 451 1998 MIL NL 33914904 0.456790 162 74 1278282871 0.026532 452 1999 MIL NL 43377395 0.459627 161 74 1494228750 0.029030 453 2000 MIL NL 36505333 0.447853 163 73 1666135102 0.021910 454 2001 MIL NL 43886833 0.419753 162 68 1960663313 0.022384 455 2002 MIL NL 50287833 0.345679 162 56 2024077522 0.024845 456 2003 MIL NL 40627000 0.419753 162 68 2128262128 0.019089 457 2004 MIL NL 27528500 0.416149 161 67 2070665943 0.013295 458 2005 MIL NL 39934833 0.500000 162 81 2188713398 0.018246 459 2006 MIL NL 57568333 0.462963 162 75 2321472617 0.024798 460 2007 MIL NL 70986500 0.512346 162 83 2476688987 0.028662 461 2008 MIL NL 80937499 0.555556 162 90 2684858670 0.030146 462 2009 MIL NL 80182502 0.493827 162 80 2664726994 0.030090 463 2010 MIL NL 81108278 0.475309 162 77 2721359865 0.029804 464 2011 MIL NL 85497333 0.592593 162 96 2784505291 0.030705 465 2012 MIL NL 97653944 0.512346 162 83 2932741192 0.033298 466 2013 MIL NL 76947033 0.456790 162 74 3034525648 0.025357 467 2014 MIL NL 101217000 0.506173 162 82 3192317623 0.031706 468 2015 MIL NL 100850000 0.419753 162 68 3514142569 0.028698 469 2016 MIL NL 68775237 0.450617 162 73 3750137392 0.018339 470 1985 MIN AL 5764821 0.475309 162 77 261964696 0.022006 471 1986 MIN AL 8748167 0.438272 162 71 307854518 0.028417 472 1987 MIN AL 6397500 0.524691 162 85 272575375 0.023471 473 1988 MIN AL 12462666 0.561728 162 91 300452424 0.041480 474 1989 MIN AL 15531666 0.493827 162 80 359995711 0.043144 475 1990 MIN AL 14602000 0.456790 162 74 443881193 0.032896 476 1991 MIN AL 23361833 0.586420 162 95 613048418 0.038108 477 1992 MIN AL 28027834 0.555556 162 90 805543323 0.034794 478 1993 MIN AL 28217933 0.438272 162 71 901740134 0.031293 479 1994 MIN AL 28438500 0.469027 113 53 927836287 0.030650 480 1995 MIN AL 25410500 0.388889 144 56 951469367 0.026707 481 1996 MIN AL 23117000 0.481481 162 78 956983550 0.024156 482 1997 MIN AL 34072500 0.419753 162 68 1127285885 0.030225 483 1998 MIN AL 27927500 0.432099 162 70 1278282871 0.021848 484 1999 MIN AL 21257500 0.391304 161 63 1494228750 0.014226 485 2000 MIN AL 16519500 0.425926 162 69 1666135102 0.009915 486 2001 MIN AL 24130000 0.524691 162 85 1960663313 0.012307 487 2002 MIN AL 40425000 0.583851 161 94 2024077522 0.019972 488 2003 MIN AL 55505000 0.555556 162 90 2128262128 0.026080 489 2004 MIN AL 53585000 0.567901 162 92 2070665943 0.025878 490 2005 MIN AL 56186000 0.512346 162 83 2188713398 0.025671 491 2006 MIN AL 63396006 0.592593 162 96 2321472617 0.027309 492 2007 MIN AL 71439500 0.487654 162 79 2476688987 0.028845 493 2008 MIN AL 56932766 0.539877 163 88 2684858670 0.021205 494 2009 MIN AL 65299266 0.533742 163 87 2664726994 0.024505 495 2010 MIN AL 97559166 0.580247 162 94 2721359865 0.035849 496 2011 MIN AL 112737000 0.388889 162 63 2784505291 0.040487 497 2012 MIN AL 94085000 0.407407 162 66 2932741192 0.032081 498 2013 MIN AL 75337500 0.407407 162 66 3034525648 0.024827 499 2014 MIN AL 83762500 0.432099 162 70 3192317623 0.026239 500 2015 MIN AL 107755000 0.512346 162 83 3514142569 0.030663 501 2016 MIN AL 102583200 0.364198 162 59 3750137392 0.027355 502 1985 ML4 AL 11284107 0.440994 161 71 261964696 0.043075 503 1986 ML4 AL 9943642 0.478261 161 77 307854518 0.032300 504 1987 ML4 AL 7293224 0.561728 162 91 272575375 0.026757 505 1988 ML4 AL 8402000 0.537037 162 87 300452424 0.027964 506 1989 ML4 AL 11533000 0.500000 162 81 359995711 0.032036 507 1990 ML4 AL 19719167 0.456790 162 74 443881193 0.044424 508 1991 ML4 AL 23115500 0.512346 162 83 613048418 0.037706 509 1992 ML4 AL 31013667 0.567901 162 92 805543323 0.038500 510 1993 ML4 AL 23806834 0.425926 162 69 901740134 0.026401 511 1994 ML4 AL 24350500 0.460870 115 53 927836287 0.026244 512 1995 ML4 AL 17798825 0.451389 144 65 951469367 0.018707 513 1996 ML4 AL 21730000 0.493827 162 80 956983550 0.022707 514 1997 ML4 AL 23655338 0.484472 161 78 1127285885 0.020984 515 1985 MON NL 9470166 0.521739 161 84 261964696 0.036151 516 1986 MON NL 11103600 0.484472 161 78 307854518 0.036068 517 1987 MON NL 6942052 0.561728 162 91 272575375 0.025468 518 1988 MON NL 9603333 0.496933 163 81 300452424 0.031963 519 1989 MON NL 13807389 0.500000 162 81 359995711 0.038354 520 1990 MON NL 16586388 0.524691 162 85 443881193 0.037367 521 1991 MON NL 10732333 0.440994 161 71 613048418 0.017507 522 1992 MON NL 15822334 0.537037 162 87 805543323 0.019642 523 1993 MON NL 18899333 0.576687 163 94 901740134 0.020959 524 1994 MON NL 19098000 0.649123 114 74 927836287 0.020583 525 1995 MON NL 12364000 0.458333 144 66 951469367 0.012995 526 1996 MON NL 16264500 0.543210 162 88 956983550 0.016996 527 1997 MON NL 19295500 0.481481 162 78 1127285885 0.017117 528 1998 MON NL 10641500 0.401235 162 65 1278282871 0.008325 529 1999 MON NL 17903000 0.419753 162 68 1494228750 0.011981 530 2000 MON NL 32994333 0.413580 162 67 1666135102 0.019803 531 2001 MON NL 35159500 0.419753 162 68 1960663313 0.017932 532 2002 MON NL 38670500 0.512346 162 83 2024077522 0.019105 533 2003 MON NL 51948500 0.512346 162 83 2128262128 0.024409 534 2004 MON NL 40897500 0.413580 162 67 2070665943 0.019751 535 1985 NYA AL 14238204 0.602484 161 97 261964696 0.054352 536 1986 NYA AL 18494253 0.555556 162 90 307854518 0.060075 537 1987 NYA AL 17099714 0.549383 162 89 272575375 0.062734 538 1988 NYA AL 19441152 0.527950 161 85 300452424 0.064706 539 1989 NYA AL 17114375 0.459627 161 74 359995711 0.047540 540 1990 NYA AL 20912318 0.413580 162 67 443881193 0.047112 541 1991 NYA AL 27344168 0.438272 162 71 613048418 0.044604 542 1992 NYA AL 37543334 0.469136 162 76 805543323 0.046606 543 1993 NYA AL 42624900 0.543210 162 88 901740134 0.047270 544 1994 NYA AL 45731334 0.619469 113 70 927836287 0.049288 545 1995 NYA AL 48874851 0.544828 145 79 951469367 0.051368 546 1996 NYA AL 54191792 0.567901 162 92 956983550 0.056628 547 1997 NYA AL 62241545 0.592593 162 96 1127285885 0.055214 548 1998 NYA AL 66806867 0.703704 162 114 1278282871 0.052263 549 1999 NYA AL 86734359 0.604938 162 98 1494228750 0.058046 550 2000 NYA AL 92338260 0.540373 161 87 1666135102 0.055421 551 2001 NYA AL 112287143 0.590062 161 95 1960663313 0.057270 552 2002 NYA AL 125928583 0.639752 161 103 2024077522 0.062215 553 2003 NYA AL 152749814 0.619632 163 101 2128262128 0.071772 554 2004 NYA AL 184193950 0.623457 162 101 2070665943 0.088954 555 2005 NYA AL 208306817 0.586420 162 95 2188713398 0.095173 556 2006 NYA AL 194663079 0.598765 162 97 2321472617 0.083853 557 2007 NYA AL 189259045 0.580247 162 94 2476688987 0.076416 558 2008 NYA AL 207896789 0.549383 162 89 2684858670 0.077433 559 2009 NYA AL 201449189 0.635802 162 103 2664726994 0.075598 560 2010 NYA AL 206333389 0.586420 162 95 2721359865 0.075820 561 2011 NYA AL 202275028 0.598765 162 97 2784505291 0.072643 562 2012 NYA AL 196522289 0.586420 162 95 2932741192 0.067010 563 2013 NYA AL 231978886 0.524691 162 85 3034525648 0.076447 564 2014 NYA AL 197543907 0.518519 162 84 3192317623 0.061881 565 2015 NYA AL 212751957 0.537037 162 87 3514142569 0.060542 566 2016 NYA AL 222997792 0.518519 162 84 3750137392 0.059464 567 1985 NYN NL 10834762 0.604938 162 98 261964696 0.041360 568 1986 NYN NL 15393714 0.666667 162 108 307854518 0.050003 569 1987 NYN NL 13846714 0.567901 162 92 272575375 0.050800 570 1988 NYN NL 15269314 0.625000 160 100 300452424 0.050821 571 1989 NYN NL 19885071 0.537037 162 87 359995711 0.055237 572 1990 NYN NL 21722834 0.561728 162 91 443881193 0.048938 573 1991 NYN NL 32590001 0.478261 161 77 613048418 0.053161 574 1992 NYN NL 44602002 0.444444 162 72 805543323 0.055369 575 1993 NYN NL 39043667 0.364198 162 59 901740134 0.043298 576 1994 NYN NL 30956583 0.486726 113 55 927836287 0.033364 577 1995 NYN NL 27674992 0.479167 144 69 951469367 0.029087 578 1996 NYN NL 24479500 0.438272 162 71 956983550 0.025580 579 1997 NYN NL 39800400 0.543210 162 88 1127285885 0.035306 580 1998 NYN NL 52077999 0.543210 162 88 1278282871 0.040741 581 1999 NYN NL 65092092 0.595092 163 97 1494228750 0.043562 582 2000 NYN NL 79509776 0.580247 162 94 1666135102 0.047721 583 2001 NYN NL 93174428 0.506173 162 82 1960663313 0.047522 584 2002 NYN NL 94633593 0.465839 161 75 2024077522 0.046754 585 2003 NYN NL 116876429 0.409938 161 66 2128262128 0.054916 586 2004 NYN NL 96660970 0.438272 162 71 2070665943 0.046681 587 2005 NYN NL 101305821 0.512346 162 83 2188713398 0.046286 588 2006 NYN NL 101084963 0.598765 162 97 2321472617 0.043543 589 2007 NYN NL 115231663 0.543210 162 88 2476688987 0.046526 590 2008 NYN NL 137793376 0.549383 162 89 2684858670 0.051322 591 2009 NYN NL 149373987 0.432099 162 70 2664726994 0.056056 592 2010 NYN NL 134422942 0.487654 162 79 2721359865 0.049396 593 2011 NYN NL 118847309 0.475309 162 77 2784505291 0.042682 594 2012 NYN NL 93353983 0.456790 162 74 2932741192 0.031832 595 2013 NYN NL 49448346 0.456790 162 74 3034525648 0.016295 596 2014 NYN NL 85556990 0.487654 162 79 3192317623 0.026801 597 2015 NYN NL 96766683 0.555556 162 90 3514142569 0.027536 598 2016 NYN NL 133889129 0.537037 162 87 3750137392 0.035702 599 1985 OAK AL 9058606 0.475309 162 77 261964696 0.034579 600 1986 OAK AL 9779421 0.469136 162 76 307854518 0.031766 601 1987 OAK AL 11680839 0.500000 162 81 272575375 0.042854 602 1988 OAK AL 9690000 0.641975 162 104 300452424 0.032251 603 1989 OAK AL 15613070 0.611111 162 99 359995711 0.043370 604 1990 OAK AL 19887501 0.635802 162 103 443881193 0.044804 605 1991 OAK AL 36999167 0.518519 162 84 613048418 0.060353 606 1992 OAK AL 41035000 0.592593 162 96 805543323 0.050941 607 1993 OAK AL 37812333 0.419753 162 68 901740134 0.041933 608 1994 OAK AL 34172500 0.447368 114 51 927836287 0.036830 609 1995 OAK AL 37739225 0.465278 144 67 951469367 0.039664 610 1996 OAK AL 21243000 0.481481 162 78 956983550 0.022198 611 1997 OAK AL 24018500 0.401235 162 65 1127285885 0.021306 612 1998 OAK AL 21303000 0.456790 162 74 1278282871 0.016665 613 1999 OAK AL 24431833 0.537037 162 87 1494228750 0.016351 614 2000 OAK AL 31971333 0.565217 161 91 1666135102 0.019189 615 2001 OAK AL 33810750 0.629630 162 102 1960663313 0.017245 616 2002 OAK AL 40004167 0.635802 162 103 2024077522 0.019764 617 2003 OAK AL 50260834 0.592593 162 96 2128262128 0.023616 618 2004 OAK AL 59425667 0.561728 162 91 2070665943 0.028699 619 2005 OAK AL 55425762 0.543210 162 88 2188713398 0.025323 620 2006 OAK AL 62243079 0.574074 162 93 2321472617 0.026812 621 2007 OAK AL 79366940 0.469136 162 76 2476688987 0.032046 622 2008 OAK AL 47967126 0.465839 161 75 2684858670 0.017866 623 2009 OAK AL 61910000 0.462963 162 75 2664726994 0.023233 624 2010 OAK AL 55254900 0.500000 162 81 2721359865 0.020304 625 2011 OAK AL 66536500 0.456790 162 74 2784505291 0.023895 626 2012 OAK AL 55372500 0.580247 162 94 2932741192 0.018881 627 2013 OAK AL 60132500 0.592593 162 96 3034525648 0.019816 628 2014 OAK AL 72408400 0.543210 162 88 3192317623 0.022682 629 2015 OAK AL 79053501 0.419753 162 68 3514142569 0.022496 630 2016 OAK AL 86806234 0.425926 162 69 3750137392 0.023147 631 1985 PHI NL 10124966 0.462963 162 75 261964696 0.038650 632 1986 PHI NL 11590166 0.534161 161 86 307854518 0.037648 633 1987 PHI NL 11514233 0.493827 162 80 272575375 0.042242 634 1988 PHI NL 13838000 0.401235 162 65 300452424 0.046057 635 1989 PHI NL 10604000 0.411043 163 67 359995711 0.029456 636 1990 PHI NL 13173667 0.475309 162 77 443881193 0.029678 637 1991 PHI NL 22487332 0.481481 162 78 613048418 0.036681 638 1992 PHI NL 24383834 0.432099 162 70 805543323 0.030270 639 1993 PHI NL 28538334 0.598765 162 97 901740134 0.031648 640 1994 PHI NL 31599000 0.469565 115 54 927836287 0.034057 641 1995 PHI NL 30555945 0.479167 144 69 951469367 0.032114 642 1996 PHI NL 34314500 0.413580 162 67 956983550 0.035857 643 1997 PHI NL 36656500 0.419753 162 68 1127285885 0.032517 644 1998 PHI NL 36297500 0.462963 162 75 1278282871 0.028396 645 1999 PHI NL 31692500 0.475309 162 77 1494228750 0.021210 646 2000 PHI NL 47308000 0.401235 162 65 1666135102 0.028394 647 2001 PHI NL 41663833 0.530864 162 86 1960663313 0.021250 648 2002 PHI NL 57954999 0.496894 161 80 2024077522 0.028633 649 2003 PHI NL 70780000 0.530864 162 86 2128262128 0.033257 650 2004 PHI NL 92919167 0.530864 162 86 2070665943 0.044874 651 2005 PHI NL 95522000 0.543210 162 88 2188713398 0.043643 652 2006 PHI NL 88273333 0.524691 162 85 2321472617 0.038025 653 2007 PHI NL 89428213 0.549383 162 89 2476688987 0.036108 654 2008 PHI NL 97879880 0.567901 162 92 2684858670 0.036456 655 2009 PHI NL 113004046 0.574074 162 93 2664726994 0.042407 656 2010 PHI NL 141928379 0.598765 162 97 2721359865 0.052153 657 2011 PHI NL 172976379 0.629630 162 102 2784505291 0.062121 658 2012 PHI NL 174538938 0.500000 162 81 2932741192 0.059514 659 2013 PHI NL 169863189 0.450617 162 73 3034525648 0.055977 660 2014 PHI NL 180944967 0.450617 162 73 3192317623 0.056681 661 2015 PHI NL 111693000 0.388889 162 63 3514142569 0.031784 662 2016 PHI NL 58980000 0.438272 162 71 3750137392 0.015727 663 1985 PIT NL 9227500 0.354037 161 57 261964696 0.035224 664 1986 PIT NL 10843500 0.395062 162 64 307854518 0.035223 665 1987 PIT NL 7652000 0.493827 162 80 272575375 0.028073 666 1988 PIT NL 5998500 0.531250 160 85 300452424 0.019965 667 1989 PIT NL 12737500 0.451220 164 74 359995711 0.035382 668 1990 PIT NL 15556000 0.586420 162 95 443881193 0.035045 669 1991 PIT NL 23634667 0.604938 162 98 613048418 0.038553 670 1992 PIT NL 33944167 0.592593 162 96 805543323 0.042138 671 1993 PIT NL 24822467 0.462963 162 75 901740134 0.027527 672 1994 PIT NL 24217250 0.464912 114 53 927836287 0.026101 673 1995 PIT NL 18355345 0.402778 144 58 951469367 0.019292 674 1996 PIT NL 23017500 0.450617 162 73 956983550 0.024052 675 1997 PIT NL 10771667 0.487654 162 79 1127285885 0.009555 676 1998 PIT NL 15065000 0.423313 163 69 1278282871 0.011785 677 1999 PIT NL 24697666 0.484472 161 78 1494228750 0.016529 678 2000 PIT NL 28928334 0.425926 162 69 1666135102 0.017363 679 2001 PIT NL 57760833 0.382716 162 62 1960663313 0.029460 680 2002 PIT NL 42323599 0.447205 161 72 2024077522 0.020910 681 2003 PIT NL 54812429 0.462963 162 75 2128262128 0.025755 682 2004 PIT NL 32227929 0.447205 161 72 2070665943 0.015564 683 2005 PIT NL 38133000 0.413580 162 67 2188713398 0.017423 684 2006 PIT NL 46717750 0.413580 162 67 2321472617 0.020124 685 2007 PIT NL 38537833 0.419753 162 68 2476688987 0.015560 686 2008 PIT NL 48689783 0.413580 162 67 2684858670 0.018135 687 2009 PIT NL 48693000 0.385093 161 62 2664726994 0.018273 688 2010 PIT NL 34943000 0.351852 162 57 2721359865 0.012840 689 2011 PIT NL 45047000 0.444444 162 72 2784505291 0.016178 690 2012 PIT NL 62951999 0.487654 162 79 2932741192 0.021465 691 2013 PIT NL 77062000 0.580247 162 94 3034525648 0.025395 692 2014 PIT NL 77178000 0.543210 162 88 3192317623 0.024176 693 2015 PIT NL 88892499 0.604938 162 98 3514142569 0.025296 694 2016 PIT NL 103778833 0.481481 162 78 3750137392 0.027673 695 1985 SDN NL 11036583 0.512346 162 83 261964696 0.042130 696 1986 SDN NL 11380693 0.456790 162 74 307854518 0.036968 697 1987 SDN NL 11065796 0.401235 162 65 272575375 0.040597 698 1988 SDN NL 9561002 0.515528 161 83 300452424 0.031822 699 1989 SDN NL 14195000 0.549383 162 89 359995711 0.039431 700 1990 SDN NL 17588334 0.462963 162 75 443881193 0.039624 701 1991 SDN NL 22150001 0.518519 162 84 613048418 0.036131 702 1992 SDN NL 26854167 0.506173 162 82 805543323 0.033337 703 1993 SDN NL 25511333 0.376543 162 61 901740134 0.028291 704 1994 SDN NL 14916333 0.401709 117 47 927836287 0.016076 705 1995 SDN NL 26382334 0.486111 144 70 951469367 0.027728 706 1996 SDN NL 28348172 0.561728 162 91 956983550 0.029622 707 1997 SDN NL 37363672 0.469136 162 76 1127285885 0.033145 708 1998 SDN NL 46861500 0.604938 162 98 1278282871 0.036660 709 1999 SDN NL 49768179 0.456790 162 74 1494228750 0.033307 710 2000 SDN NL 54821000 0.469136 162 76 1666135102 0.032903 711 2001 SDN NL 39182833 0.487654 162 79 1960663313 0.019984 712 2002 SDN NL 41425000 0.407407 162 66 2024077522 0.020466 713 2003 SDN NL 45210000 0.395062 162 64 2128262128 0.021243 714 2004 SDN NL 55384833 0.537037 162 87 2070665943 0.026747 715 2005 SDN NL 63290833 0.506173 162 82 2188713398 0.028917 716 2006 SDN NL 69896141 0.543210 162 88 2321472617 0.030109 717 2007 SDN NL 58110567 0.546012 163 89 2476688987 0.023463 718 2008 SDN NL 73677616 0.388889 162 63 2684858670 0.027442 719 2009 SDN NL 43333700 0.462963 162 75 2664726994 0.016262 720 2010 SDN NL 37799300 0.555556 162 90 2721359865 0.013890 721 2011 SDN NL 45869140 0.438272 162 71 2784505291 0.016473 722 2012 SDN NL 55244700 0.469136 162 76 2932741192 0.018837 723 2013 SDN NL 65585500 0.469136 162 76 3034525648 0.021613 724 2014 SDN NL 75685700 0.475309 162 77 3192317623 0.023709 725 2015 SDN NL 118441300 0.456790 162 74 3514142569 0.033704 726 2016 SDN NL 101424814 0.419753 162 68 3750137392 0.027046 727 1985 SEA AL 4613000 0.456790 162 74 261964696 0.017609 728 1986 SEA AL 5958309 0.413580 162 67 307854518 0.019354 729 1987 SEA AL 2263500 0.481481 162 78 272575375 0.008304 730 1988 SEA AL 7342450 0.422360 161 68 300452424 0.024438 731 1989 SEA AL 9779500 0.450617 162 73 359995711 0.027166 732 1990 SEA AL 12553667 0.475309 162 77 443881193 0.028282 733 1991 SEA AL 15691833 0.512346 162 83 613048418 0.025596 734 1992 SEA AL 23179833 0.395062 162 64 805543323 0.028775 735 1993 SEA AL 32696333 0.506173 162 82 901740134 0.036259 736 1994 SEA AL 29228500 0.437500 112 49 927836287 0.031502 737 1995 SEA AL 36481311 0.544828 145 79 951469367 0.038342 738 1996 SEA AL 41328501 0.527950 161 85 956983550 0.043186 739 1997 SEA AL 41540661 0.555556 162 90 1127285885 0.036850 740 1998 SEA AL 54087036 0.472050 161 76 1278282871 0.042312 741 1999 SEA AL 54125003 0.487654 162 79 1494228750 0.036223 742 2000 SEA AL 58915000 0.561728 162 91 1666135102 0.035360 743 2001 SEA AL 74720834 0.716049 162 116 1960663313 0.038110 744 2002 SEA AL 80282668 0.574074 162 93 2024077522 0.039664 745 2003 SEA AL 86959167 0.574074 162 93 2128262128 0.040859 746 2004 SEA AL 81515834 0.388889 162 63 2070665943 0.039367 747 2005 SEA AL 87754334 0.425926 162 69 2188713398 0.040094 748 2006 SEA AL 87959833 0.481481 162 78 2321472617 0.037890 749 2007 SEA AL 106460833 0.543210 162 88 2476688987 0.042985 750 2008 SEA AL 117666482 0.376543 162 61 2684858670 0.043826 751 2009 SEA AL 98904166 0.524691 162 85 2664726994 0.037116 752 2010 SEA AL 86510000 0.376543 162 61 2721359865 0.031789 753 2011 SEA AL 86110600 0.413580 162 67 2784505291 0.030925 754 2012 SEA AL 81978100 0.462963 162 75 2932741192 0.027953 755 2013 SEA AL 74005043 0.438272 162 71 3034525648 0.024388 756 2014 SEA AL 92531100 0.537037 162 87 3192317623 0.028986 757 2015 SEA AL 122208700 0.469136 162 76 3514142569 0.034776 758 2016 SEA AL 135683339 0.530864 162 86 3750137392 0.036181 759 1985 SFN NL 8221714 0.382716 162 62 261964696 0.031385 760 1986 SFN NL 8947000 0.512346 162 83 307854518 0.029062 761 1987 SFN NL 7290000 0.555556 162 90 272575375 0.026745 762 1988 SFN NL 12380000 0.512346 162 83 300452424 0.041205 763 1989 SFN NL 14962834 0.567901 162 92 359995711 0.041564 764 1990 SFN NL 19335333 0.524691 162 85 443881193 0.043560 765 1991 SFN NL 30967666 0.462963 162 75 613048418 0.050514 766 1992 SFN NL 33163168 0.444444 162 72 805543323 0.041169 767 1993 SFN NL 35050000 0.635802 162 103 901740134 0.038869 768 1994 SFN NL 42638666 0.478261 115 55 927836287 0.045955 769 1995 SFN NL 36462777 0.465278 144 67 951469367 0.038323 770 1996 SFN NL 37144725 0.419753 162 68 956983550 0.038814 771 1997 SFN NL 35592378 0.555556 162 90 1127285885 0.031574 772 1998 SFN NL 42565834 0.546012 163 89 1278282871 0.033299 773 1999 SFN NL 46595057 0.530864 162 86 1494228750 0.031183 774 2000 SFN NL 53737826 0.598765 162 97 1666135102 0.032253 775 2001 SFN NL 63280167 0.555556 162 90 1960663313 0.032275 776 2002 SFN NL 78299835 0.586420 162 95 2024077522 0.038684 777 2003 SFN NL 82852167 0.621118 161 100 2128262128 0.038929 778 2004 SFN NL 82019166 0.561728 162 91 2070665943 0.039610 779 2005 SFN NL 90199500 0.462963 162 75 2188713398 0.041211 780 2006 SFN NL 90056419 0.472050 161 76 2321472617 0.038793 781 2007 SFN NL 90219056 0.438272 162 71 2476688987 0.036427 782 2008 SFN NL 76594500 0.444444 162 72 2684858670 0.028528 783 2009 SFN NL 83026450 0.543210 162 88 2664726994 0.031158 784 2010 SFN NL 98641333 0.567901 162 92 2721359865 0.036247 785 2011 SFN NL 118198333 0.530864 162 86 2784505291 0.042449 786 2012 SFN NL 117620683 0.580247 162 94 2932741192 0.040106 787 2013 SFN NL 140180334 0.469136 162 76 3034525648 0.046195 788 2014 SFN NL 163510167 0.543210 162 88 3192317623 0.051220 789 2015 SFN NL 164701500 0.518519 162 84 3514142569 0.046868 790 2016 SFN NL 172253778 0.537037 162 87 3750137392 0.045933 791 1985 SLN NL 11817083 0.623457 162 101 261964696 0.045109 792 1986 SLN NL 9875010 0.490683 161 79 307854518 0.032077 793 1987 SLN NL 11758000 0.586420 162 95 272575375 0.043137 794 1988 SLN NL 12880000 0.469136 162 76 300452424 0.042869 795 1989 SLN NL 16078833 0.524390 164 86 359995711 0.044664 796 1990 SLN NL 20523334 0.432099 162 70 443881193 0.046236 797 1991 SLN NL 21860001 0.518519 162 84 613048418 0.035658 798 1992 SLN NL 27583836 0.512346 162 83 805543323 0.034243 799 1993 SLN NL 23367334 0.537037 162 87 901740134 0.025914 800 1994 SLN NL 29275601 0.460870 115 53 927836287 0.031553 801 1995 SLN NL 37101000 0.433566 143 62 951469367 0.038993 802 1996 SLN NL 40269667 0.543210 162 88 956983550 0.042080 803 1997 SLN NL 45456667 0.450617 162 73 1127285885 0.040324 804 1998 SLN NL 54672521 0.509202 163 83 1278282871 0.042770 805 1999 SLN NL 49778195 0.465839 161 75 1494228750 0.033314 806 2000 SLN NL 61453863 0.586420 162 95 1666135102 0.036884 807 2001 SLN NL 78538333 0.574074 162 93 1960663313 0.040057 808 2002 SLN NL 74660875 0.598765 162 97 2024077522 0.036886 809 2003 SLN NL 83786666 0.524691 162 85 2128262128 0.039369 810 2004 SLN NL 83228333 0.648148 162 105 2070665943 0.040194 811 2005 SLN NL 92106833 0.617284 162 100 2188713398 0.042083 812 2006 SLN NL 88891371 0.515528 161 83 2321472617 0.038291 813 2007 SLN NL 90286823 0.481481 162 78 2476688987 0.036455 814 2008 SLN NL 99624449 0.530864 162 86 2684858670 0.037106 815 2009 SLN NL 88528409 0.561728 162 91 2664726994 0.033222 816 2010 SLN NL 93540751 0.530864 162 86 2721359865 0.034373 817 2011 SLN NL 105433572 0.555556 162 90 2784505291 0.037864 818 2012 SLN NL 110300862 0.543210 162 88 2932741192 0.037610 819 2013 SLN NL 92260110 0.598765 162 97 3034525648 0.030403 820 2014 SLN NL 120693000 0.555556 162 90 3192317623 0.037807 821 2015 SLN NL 119241500 0.617284 162 100 3514142569 0.033932 822 2016 SLN NL 143053500 0.530864 162 86 3750137392 0.038146 823 1998 TBA AL 27280000 0.388889 162 63 1278282871 0.021341 824 1999 TBA AL 38870000 0.425926 162 69 1494228750 0.026013 825 2000 TBA AL 62765129 0.428571 161 69 1666135102 0.037671 826 2001 TBA AL 56980000 0.382716 162 62 1960663313 0.029062 827 2002 TBA AL 34380000 0.341615 161 55 2024077522 0.016986 828 2003 TBA AL 19630000 0.388889 162 63 2128262128 0.009223 829 2004 TBA AL 29556667 0.434783 161 70 2070665943 0.014274 830 2005 TBA AL 29679067 0.413580 162 67 2188713398 0.013560 831 2006 TBA AL 34917967 0.376543 162 61 2321472617 0.015041 832 2007 TBA AL 24123500 0.407407 162 66 2476688987 0.009740 833 2008 TBA AL 43820597 0.598765 162 97 2684858670 0.016321 834 2009 TBA AL 63313034 0.518519 162 84 2664726994 0.023760 835 2010 TBA AL 71923471 0.592593 162 96 2721359865 0.026429 836 2011 TBA AL 41053571 0.561728 162 91 2784505291 0.014744 837 2012 TBA AL 64173500 0.555556 162 90 2932741192 0.021882 838 2013 TBA AL 52955272 0.564417 163 92 3034525648 0.017451 839 2014 TBA AL 72689100 0.475309 162 77 3192317623 0.022770 840 2015 TBA AL 64521233 0.493827 162 80 3514142569 0.018360 841 2016 TBA AL 57097310 0.419753 162 68 3750137392 0.015225 842 1985 TEX AL 7676500 0.385093 161 62 261964696 0.029304 843 1986 TEX AL 6743119 0.537037 162 87 307854518 0.021904 844 1987 TEX AL 880000 0.462963 162 75 272575375 0.003228 845 1988 TEX AL 5342131 0.434783 161 70 300452424 0.017780 846 1989 TEX AL 11893781 0.512346 162 83 359995711 0.033039 847 1990 TEX AL 14874372 0.512346 162 83 443881193 0.033510 848 1991 TEX AL 18224500 0.524691 162 85 613048418 0.029728 849 1992 TEX AL 30128167 0.475309 162 77 805543323 0.037401 850 1993 TEX AL 36376959 0.530864 162 86 901740134 0.040341 851 1994 TEX AL 32973597 0.456140 114 52 927836287 0.035538 852 1995 TEX AL 34581451 0.513889 144 74 951469367 0.036345 853 1996 TEX AL 39041528 0.552147 163 90 956983550 0.040796 854 1997 TEX AL 53448838 0.475309 162 77 1127285885 0.047414 855 1998 TEX AL 56572095 0.543210 162 88 1278282871 0.044256 856 1999 TEX AL 76709931 0.586420 162 95 1494228750 0.051337 857 2000 TEX AL 70795921 0.438272 162 71 1666135102 0.042491 858 2001 TEX AL 88633500 0.450617 162 73 1960663313 0.045206 859 2002 TEX AL 105526122 0.444444 162 72 2024077522 0.052135 860 2003 TEX AL 103491667 0.438272 162 71 2128262128 0.048627 861 2004 TEX AL 55050417 0.549383 162 89 2070665943 0.026586 862 2005 TEX AL 55849000 0.487654 162 79 2188713398 0.025517 863 2006 TEX AL 68228662 0.493827 162 80 2321472617 0.029390 864 2007 TEX AL 68318675 0.462963 162 75 2476688987 0.027585 865 2008 TEX AL 67712326 0.487654 162 79 2684858670 0.025220 866 2009 TEX AL 68178798 0.537037 162 87 2664726994 0.025586 867 2010 TEX AL 55250544 0.555556 162 90 2721359865 0.020303 868 2011 TEX AL 92299264 0.592593 162 96 2784505291 0.033147 869 2012 TEX AL 120510974 0.574074 162 93 2932741192 0.041092 870 2013 TEX AL 112522600 0.558282 163 91 3034525648 0.037081 871 2014 TEX AL 112255059 0.413580 162 67 3192317623 0.035164 872 2015 TEX AL 143742789 0.543210 162 88 3514142569 0.040904 873 2016 TEX AL 176038723 0.586420 162 95 3750137392 0.046942 874 1985 TOR AL 8812550 0.614907 161 99 261964696 0.033640 875 1986 TOR AL 12611047 0.527607 163 86 307854518 0.040964 876 1987 TOR AL 10479501 0.592593 162 96 272575375 0.038446 877 1988 TOR AL 12241225 0.537037 162 87 300452424 0.040743 878 1989 TOR AL 16261666 0.549383 162 89 359995711 0.045172 879 1990 TOR AL 17756834 0.530864 162 86 443881193 0.040004 880 1991 TOR AL 19902417 0.561728 162 91 613048418 0.032465 881 1992 TOR AL 44788666 0.592593 162 96 805543323 0.055601 882 1993 TOR AL 47279166 0.586420 162 95 901740134 0.052431 883 1994 TOR AL 43433668 0.478261 115 55 927836287 0.046812 884 1995 TOR AL 50590000 0.388889 144 56 951469367 0.053170 885 1996 TOR AL 29555083 0.456790 162 74 956983550 0.030884 886 1997 TOR AL 47079833 0.469136 162 76 1127285885 0.041764 887 1998 TOR AL 51376000 0.539877 163 88 1278282871 0.040191 888 1999 TOR AL 45444333 0.518519 162 84 1494228750 0.030413 889 2000 TOR AL 44838332 0.512346 162 83 1666135102 0.026912 890 2001 TOR AL 76895999 0.493827 162 80 1960663313 0.039219 891 2002 TOR AL 76864333 0.481481 162 78 2024077522 0.037975 892 2003 TOR AL 51269000 0.530864 162 86 2128262128 0.024090 893 2004 TOR AL 50017000 0.416149 161 67 2070665943 0.024155 894 2005 TOR AL 45719500 0.493827 162 80 2188713398 0.020889 895 2006 TOR AL 71365000 0.537037 162 87 2321472617 0.030741 896 2007 TOR AL 81942800 0.512346 162 83 2476688987 0.033086 897 2008 TOR AL 97793900 0.530864 162 86 2684858670 0.036424 898 2009 TOR AL 80538300 0.462963 162 75 2664726994 0.030224 899 2010 TOR AL 62234000 0.524691 162 85 2721359865 0.022869 900 2011 TOR AL 62567800 0.500000 162 81 2784505291 0.022470 901 2012 TOR AL 75009200 0.450617 162 73 2932741192 0.025576 902 2013 TOR AL 126288100 0.456790 162 74 3034525648 0.041617 903 2014 TOR AL 109920100 0.512346 162 83 3192317623 0.034433 904 2015 TOR AL 112992400 0.574074 162 93 3514142569 0.032154 905 2016 TOR AL 138701700 0.549383 162 89 3750137392 0.036986 906 2005 WAS NL 48581500 0.500000 162 81 2188713398 0.022196 907 2006 WAS NL 63143000 0.438272 162 71 2321472617 0.027200 908 2007 WAS NL 36947500 0.450617 162 73 2476688987 0.014918 909 2008 WAS NL 54961000 0.366460 161 59 2684858670 0.020471 910 2009 WAS NL 59928000 0.364198 162 59 2664726994 0.022489 911 2010 WAS NL 61400000 0.425926 162 69 2721359865 0.022562 912 2011 WAS NL 63856928 0.496894 161 80 2784505291 0.022933 913 2012 WAS NL 80855143 0.604938 162 98 2932741192 0.027570 914 2013 WAS NL 113703270 0.530864 162 86 3034525648 0.037470 915 2014 WAS NL 131983680 0.592593 162 96 3192317623 0.041344 916 2015 WAS NL 155587472 0.512346 162 83 3514142569 0.044275 917 2016 WAS NL 141652646 0.586420 162 95 3750137392 0.037773 In\u00a0[14]: Copied! <pre># now we create the lagged dependend variable\n\nMLB['wpc_lag'] = MLB.groupby('Team')['wpc'].shift(1)\nMLB\n</pre> # now we create the lagged dependend variable  MLB['wpc_lag'] = MLB.groupby('Team')['wpc'].shift(1) MLB Out[14]: season Team lgID salaries wpc G W allsal relsal wpc_lag 0 1997 ANA AL 31135472 0.518519 162 84 1127285885 0.027620 NaN 1 1998 ANA AL 41281000 0.524691 162 85 1278282871 0.032294 0.518519 2 1999 ANA AL 55388166 0.432099 162 70 1494228750 0.037068 0.524691 3 2000 ANA AL 51464167 0.506173 162 82 1666135102 0.030888 0.432099 4 2001 ANA AL 47535167 0.462963 162 75 1960663313 0.024244 0.506173 5 2002 ANA AL 61721667 0.611111 162 99 2024077522 0.030494 0.462963 6 2003 ANA AL 79031667 0.475309 162 77 2128262128 0.037134 0.611111 7 2004 ANA AL 100534667 0.567901 162 92 2070665943 0.048552 0.475309 8 1998 ARI NL 32347000 0.401235 162 65 1278282871 0.025305 NaN 9 1999 ARI NL 68703999 0.617284 162 100 1494228750 0.045980 0.401235 10 2000 ARI NL 81027833 0.524691 162 85 1666135102 0.048632 0.617284 11 2001 ARI NL 85082999 0.567901 162 92 1960663313 0.043395 0.524691 12 2002 ARI NL 102819999 0.604938 162 98 2024077522 0.050798 0.567901 13 2003 ARI NL 80657000 0.518519 162 84 2128262128 0.037898 0.604938 14 2004 ARI NL 69780750 0.314815 162 51 2070665943 0.033700 0.518519 15 2005 ARI NL 62329166 0.475309 162 77 2188713398 0.028478 0.314815 16 2006 ARI NL 59684226 0.469136 162 76 2321472617 0.025710 0.475309 17 2007 ARI NL 52067546 0.555556 162 90 2476688987 0.021023 0.469136 18 2008 ARI NL 66202712 0.506173 162 82 2684858670 0.024658 0.555556 19 2009 ARI NL 73115666 0.432099 162 70 2664726994 0.027438 0.506173 20 2010 ARI NL 60718166 0.401235 162 65 2721359865 0.022312 0.432099 21 2011 ARI NL 53639833 0.580247 162 94 2784505291 0.019264 0.401235 22 2012 ARI NL 73804833 0.500000 162 81 2932741192 0.025166 0.580247 23 2013 ARI NL 90132000 0.500000 162 81 3034525648 0.029702 0.500000 24 2014 ARI NL 97861500 0.395062 162 64 3192317623 0.030655 0.500000 25 2015 ARI NL 61834000 0.487654 162 79 3514142569 0.017596 0.395062 26 2016 ARI NL 87439063 0.425926 162 69 3750137392 0.023316 0.487654 27 1985 ATL NL 14807000 0.407407 162 66 261964696 0.056523 NaN 28 1986 ATL NL 17102786 0.447205 161 72 307854518 0.055555 0.407407 29 1987 ATL NL 16544560 0.428571 161 69 272575375 0.060697 0.447205 30 1988 ATL NL 12728174 0.337500 160 54 300452424 0.042363 0.428571 31 1989 ATL NL 11112334 0.391304 161 63 359995711 0.030868 0.337500 32 1990 ATL NL 14555501 0.401235 162 65 443881193 0.032791 0.391304 33 1991 ATL NL 18403500 0.580247 162 94 613048418 0.030020 0.401235 34 1992 ATL NL 34625333 0.604938 162 98 805543323 0.042984 0.580247 35 1993 ATL NL 41641417 0.641975 162 104 901740134 0.046179 0.604938 36 1994 ATL NL 49383513 0.596491 114 68 927836287 0.053224 0.641975 37 1995 ATL NL 47235445 0.625000 144 90 951469367 0.049645 0.596491 38 1996 ATL NL 49698500 0.592593 162 96 956983550 0.051932 0.625000 39 1997 ATL NL 52278500 0.623457 162 101 1127285885 0.046376 0.592593 40 1998 ATL NL 61186000 0.654321 162 106 1278282871 0.047866 0.623457 41 1999 ATL NL 73140000 0.635802 162 103 1494228750 0.048948 0.654321 42 2000 ATL NL 84537836 0.586420 162 95 1666135102 0.050739 0.635802 43 2001 ATL NL 91936166 0.543210 162 88 1960663313 0.046890 0.586420 44 2002 ATL NL 92870367 0.627329 161 101 2024077522 0.045883 0.543210 45 2003 ATL NL 106243667 0.623457 162 101 2128262128 0.049920 0.627329 46 2004 ATL NL 90182500 0.592593 162 96 2070665943 0.043552 0.623457 47 2005 ATL NL 86457302 0.555556 162 90 2188713398 0.039501 0.592593 48 2006 ATL NL 90156876 0.487654 162 79 2321472617 0.038836 0.555556 49 2007 ATL NL 87290833 0.518519 162 84 2476688987 0.035245 0.487654 50 2008 ATL NL 102365683 0.444444 162 72 2684858670 0.038127 0.518519 51 2009 ATL NL 96726166 0.530864 162 86 2664726994 0.036299 0.444444 52 2010 ATL NL 84423666 0.561728 162 91 2721359865 0.031023 0.530864 53 2011 ATL NL 87002692 0.549383 162 89 2784505291 0.031245 0.561728 54 2012 ATL NL 82829942 0.580247 162 94 2932741192 0.028243 0.549383 55 2013 ATL NL 87871525 0.592593 162 96 3034525648 0.028957 0.580247 56 2014 ATL NL 97609000 0.487654 162 79 3192317623 0.030576 0.592593 57 2015 ATL NL 71781250 0.413580 162 67 3514142569 0.020426 0.487654 58 2016 ATL NL 68498291 0.422360 161 68 3750137392 0.018266 0.413580 59 1985 BAL AL 11560712 0.515528 161 83 261964696 0.044131 NaN 60 1986 BAL AL 13001258 0.450617 162 73 307854518 0.042232 0.515528 61 1987 BAL AL 13900273 0.413580 162 67 272575375 0.050996 0.450617 62 1988 BAL AL 13532075 0.335404 161 54 300452424 0.045039 0.413580 63 1989 BAL AL 8275167 0.537037 162 87 359995711 0.022987 0.335404 64 1990 BAL AL 9680084 0.472050 161 76 443881193 0.021808 0.537037 65 1991 BAL AL 17519000 0.413580 162 67 613048418 0.028577 0.472050 66 1992 BAL AL 23780667 0.549383 162 89 805543323 0.029521 0.413580 67 1993 BAL AL 29096500 0.524691 162 85 901740134 0.032267 0.549383 68 1994 BAL AL 38849769 0.562500 112 63 927836287 0.041871 0.524691 69 1995 BAL AL 43942521 0.493056 144 71 951469367 0.046184 0.562500 70 1996 BAL AL 54490315 0.539877 163 88 956983550 0.056940 0.493056 71 1997 BAL AL 58516400 0.604938 162 98 1127285885 0.051909 0.539877 72 1998 BAL AL 72355634 0.487654 162 79 1278282871 0.056604 0.604938 73 1999 BAL AL 80605863 0.481481 162 78 1494228750 0.053945 0.487654 74 2000 BAL AL 81447435 0.456790 162 74 1666135102 0.048884 0.481481 75 2001 BAL AL 67599540 0.388889 162 63 1960663313 0.034478 0.456790 76 2002 BAL AL 60493487 0.413580 162 67 2024077522 0.029887 0.388889 77 2003 BAL AL 73877500 0.435583 163 71 2128262128 0.034713 0.413580 78 2004 BAL AL 51623333 0.481481 162 78 2070665943 0.024931 0.435583 79 2005 BAL AL 73914333 0.456790 162 74 2188713398 0.033771 0.481481 80 2006 BAL AL 72585582 0.432099 162 70 2321472617 0.031267 0.456790 81 2007 BAL AL 93174808 0.425926 162 69 2476688987 0.037621 0.432099 82 2008 BAL AL 67196246 0.422360 161 68 2684858670 0.025028 0.425926 83 2009 BAL AL 67101666 0.395062 162 64 2664726994 0.025181 0.422360 84 2010 BAL AL 81612500 0.407407 162 66 2721359865 0.029990 0.395062 85 2011 BAL AL 85304038 0.425926 162 69 2784505291 0.030635 0.407407 86 2012 BAL AL 77353999 0.574074 162 93 2932741192 0.026376 0.425926 87 2013 BAL AL 84393333 0.524691 162 85 3034525648 0.027811 0.574074 88 2014 BAL AL 103416000 0.592593 162 96 3192317623 0.032395 0.524691 89 2015 BAL AL 115044833 0.500000 162 81 3514142569 0.032738 0.592593 90 2016 BAL AL 161863456 0.549383 162 89 3750137392 0.043162 0.500000 91 1985 BOS AL 10897560 0.496933 163 81 261964696 0.041599 NaN 92 1986 BOS AL 14402239 0.590062 161 95 307854518 0.046783 0.496933 93 1987 BOS AL 10144167 0.481481 162 78 272575375 0.037216 0.590062 94 1988 BOS AL 13896092 0.549383 162 89 300452424 0.046251 0.481481 95 1989 BOS AL 17481748 0.512346 162 83 359995711 0.048561 0.549383 96 1990 BOS AL 20558333 0.543210 162 88 443881193 0.046315 0.512346 97 1991 BOS AL 35167500 0.518519 162 84 613048418 0.057365 0.543210 98 1992 BOS AL 43610584 0.450617 162 73 805543323 0.054138 0.518519 99 1993 BOS AL 37120583 0.493827 162 80 901740134 0.041165 0.450617 100 1994 BOS AL 37859084 0.469565 115 54 927836287 0.040804 0.493827 101 1995 BOS AL 32455518 0.597222 144 86 951469367 0.034111 0.469565 102 1996 BOS AL 42393500 0.524691 162 85 956983550 0.044299 0.597222 103 1997 BOS AL 43558750 0.481481 162 78 1127285885 0.038640 0.524691 104 1998 BOS AL 56757000 0.567901 162 92 1278282871 0.044401 0.481481 105 1999 BOS AL 63497500 0.580247 162 94 1494228750 0.042495 0.567901 106 2000 BOS AL 77940333 0.524691 162 85 1666135102 0.046779 0.580247 107 2001 BOS AL 110035833 0.509317 161 82 1960663313 0.056122 0.524691 108 2002 BOS AL 108366060 0.574074 162 93 2024077522 0.053538 0.509317 109 2003 BOS AL 99946500 0.586420 162 95 2128262128 0.046962 0.574074 110 2004 BOS AL 127298500 0.604938 162 98 2070665943 0.061477 0.586420 111 2005 BOS AL 123505125 0.586420 162 95 2188713398 0.056428 0.604938 112 2006 BOS AL 120099824 0.530864 162 86 2321472617 0.051734 0.586420 113 2007 BOS AL 143026214 0.592593 162 96 2476688987 0.057749 0.530864 114 2008 BOS AL 133390035 0.586420 162 95 2684858670 0.049682 0.592593 115 2009 BOS AL 121345999 0.586420 162 95 2664726994 0.045538 0.586420 116 2010 BOS AL 162447333 0.549383 162 89 2721359865 0.059693 0.586420 117 2011 BOS AL 161762475 0.555556 162 90 2784505291 0.058094 0.549383 118 2012 BOS AL 173186617 0.425926 162 69 2932741192 0.059053 0.555556 119 2013 BOS AL 151530000 0.598765 162 97 3034525648 0.049935 0.425926 120 2014 BOS AL 139019929 0.438272 162 71 3192317623 0.043548 0.598765 121 2015 BOS AL 181103400 0.481481 162 78 3514142569 0.051536 0.438272 122 2016 BOS AL 188545761 0.574074 162 93 3750137392 0.050277 0.481481 123 1985 CAL AL 14427894 0.555556 162 90 261964696 0.055076 NaN 124 1986 CAL AL 14427258 0.567901 162 92 307854518 0.046864 0.555556 125 1987 CAL AL 12843499 0.462963 162 75 272575375 0.047119 0.567901 126 1988 CAL AL 11947388 0.462963 162 75 300452424 0.039765 0.462963 127 1989 CAL AL 15097833 0.561728 162 91 359995711 0.041939 0.462963 128 1990 CAL AL 21720000 0.493827 162 80 443881193 0.048932 0.561728 129 1991 CAL AL 33060001 0.500000 162 81 613048418 0.053927 0.493827 130 1992 CAL AL 34749334 0.444444 162 72 805543323 0.043138 0.500000 131 1993 CAL AL 28588334 0.438272 162 71 901740134 0.031704 0.444444 132 1994 CAL AL 25156218 0.408696 115 47 927836287 0.027113 0.438272 133 1995 CAL AL 31223171 0.537931 145 78 951469367 0.032816 0.408696 134 1996 CAL AL 28738000 0.434783 161 70 956983550 0.030030 0.537931 135 1985 CHA AL 9846178 0.521472 163 85 261964696 0.037586 NaN 136 1986 CHA AL 10418819 0.444444 162 72 307854518 0.033843 0.521472 137 1987 CHA AL 10641843 0.475309 162 77 272575375 0.039042 0.444444 138 1988 CHA AL 6390000 0.440994 161 71 300452424 0.021268 0.475309 139 1989 CHA AL 7265410 0.428571 161 69 359995711 0.020182 0.440994 140 1990 CHA AL 9491500 0.580247 162 94 443881193 0.021383 0.428571 141 1991 CHA AL 16919667 0.537037 162 87 613048418 0.027599 0.580247 142 1992 CHA AL 30160833 0.530864 162 86 805543323 0.037442 0.537037 143 1993 CHA AL 39696166 0.580247 162 94 901740134 0.044022 0.530864 144 1994 CHA AL 39183836 0.592920 113 67 927836287 0.042231 0.580247 145 1995 CHA AL 46961282 0.468966 145 68 951469367 0.049357 0.592920 146 1996 CHA AL 45139500 0.524691 162 85 956983550 0.047169 0.468966 147 1997 CHA AL 57740000 0.496894 161 80 1127285885 0.051220 0.524691 148 1998 CHA AL 38335000 0.490798 163 80 1278282871 0.029989 0.496894 149 1999 CHA AL 25620000 0.462963 162 75 1494228750 0.017146 0.490798 150 2000 CHA AL 31133500 0.586420 162 95 1666135102 0.018686 0.462963 151 2001 CHA AL 65653667 0.512346 162 83 1960663313 0.033485 0.586420 152 2002 CHA AL 57052833 0.500000 162 81 2024077522 0.028187 0.512346 153 2003 CHA AL 51010000 0.530864 162 86 2128262128 0.023968 0.500000 154 2004 CHA AL 65212500 0.512346 162 83 2070665943 0.031493 0.530864 155 2005 CHA AL 75178000 0.611111 162 99 2188713398 0.034348 0.512346 156 2006 CHA AL 102750667 0.555556 162 90 2321472617 0.044261 0.611111 157 2007 CHA AL 108671833 0.444444 162 72 2476688987 0.043878 0.555556 158 2008 CHA AL 121189332 0.546012 163 89 2684858670 0.045138 0.444444 159 2009 CHA AL 96068500 0.487654 162 79 2664726994 0.036052 0.546012 160 2010 CHA AL 105530000 0.543210 162 88 2721359865 0.038778 0.487654 161 2011 CHA AL 127789000 0.487654 162 79 2784505291 0.045893 0.543210 162 2012 CHA AL 96919500 0.524691 162 85 2932741192 0.033047 0.487654 163 2013 CHA AL 120065277 0.388889 162 63 3034525648 0.039566 0.524691 164 2014 CHA AL 81830500 0.450617 162 73 3192317623 0.025634 0.388889 165 2015 CHA AL 112373700 0.469136 162 76 3514142569 0.031978 0.450617 166 2016 CHA AL 112998667 0.481481 162 78 3750137392 0.030132 0.469136 167 1985 CHN NL 12702917 0.475309 162 77 261964696 0.048491 NaN 168 1986 CHN NL 17208165 0.437500 160 70 307854518 0.055897 0.475309 169 1987 CHN NL 14307999 0.472050 161 76 272575375 0.052492 0.437500 170 1988 CHN NL 13119198 0.472393 163 77 300452424 0.043665 0.472050 171 1989 CHN NL 10668000 0.574074 162 93 359995711 0.029634 0.472393 172 1990 CHN NL 13624000 0.475309 162 77 443881193 0.030693 0.574074 173 1991 CHN NL 23175667 0.481250 160 77 613048418 0.037804 0.475309 174 1992 CHN NL 29829686 0.481481 162 78 805543323 0.037031 0.481250 175 1993 CHN NL 39386666 0.515337 163 84 901740134 0.043679 0.481481 176 1994 CHN NL 36287333 0.433628 113 49 927836287 0.039110 0.515337 177 1995 CHN NL 29505834 0.506944 144 73 951469367 0.031011 0.433628 178 1996 CHN NL 33081000 0.469136 162 76 956983550 0.034568 0.506944 179 1997 CHN NL 42155333 0.419753 162 68 1127285885 0.037395 0.469136 180 1998 CHN NL 50838000 0.552147 163 90 1278282871 0.039771 0.419753 181 1999 CHN NL 62343000 0.413580 162 67 1494228750 0.041723 0.552147 182 2000 CHN NL 60539333 0.401235 162 65 1666135102 0.036335 0.413580 183 2001 CHN NL 64715833 0.543210 162 88 1960663313 0.033007 0.401235 184 2002 CHN NL 75690833 0.413580 162 67 2024077522 0.037395 0.543210 185 2003 CHN NL 79868333 0.543210 162 88 2128262128 0.037527 0.413580 186 2004 CHN NL 90560000 0.549383 162 89 2070665943 0.043735 0.543210 187 2005 CHN NL 87032933 0.487654 162 79 2188713398 0.039764 0.549383 188 2006 CHN NL 94424499 0.407407 162 66 2321472617 0.040674 0.487654 189 2007 CHN NL 99670332 0.524691 162 85 2476688987 0.040243 0.407407 190 2008 CHN NL 118345833 0.602484 161 97 2684858670 0.044079 0.524691 191 2009 CHN NL 134809000 0.515528 161 83 2664726994 0.050590 0.602484 192 2010 CHN NL 146609000 0.462963 162 75 2721359865 0.053873 0.515528 193 2011 CHN NL 125047329 0.438272 162 71 2784505291 0.044908 0.462963 194 2012 CHN NL 88197033 0.376543 162 61 2932741192 0.030073 0.438272 195 2013 CHN NL 100567726 0.407407 162 66 3034525648 0.033141 0.376543 196 2014 CHN NL 65522500 0.450617 162 73 3192317623 0.020525 0.407407 197 2015 CHN NL 115879310 0.598765 162 97 3514142569 0.032975 0.450617 198 2016 CHN NL 154067668 0.635802 162 103 3750137392 0.041083 0.598765 199 1985 CIN NL 8359917 0.549383 162 89 261964696 0.031912 NaN 200 1986 CIN NL 11906388 0.530864 162 86 307854518 0.038675 0.549383 201 1987 CIN NL 9281500 0.518519 162 84 272575375 0.034051 0.530864 202 1988 CIN NL 8888409 0.540373 161 87 300452424 0.029583 0.518519 203 1989 CIN NL 11072000 0.462963 162 75 359995711 0.030756 0.540373 204 1990 CIN NL 14370000 0.561728 162 91 443881193 0.032374 0.462963 205 1991 CIN NL 26305333 0.456790 162 74 613048418 0.042909 0.561728 206 1992 CIN NL 35931499 0.555556 162 90 805543323 0.044605 0.456790 207 1993 CIN NL 44879666 0.450617 162 73 901740134 0.049770 0.555556 208 1994 CIN NL 40961833 0.573913 115 66 927836287 0.044148 0.450617 209 1995 CIN NL 43144670 0.590278 144 85 951469367 0.045345 0.573913 210 1996 CIN NL 42526334 0.500000 162 81 956983550 0.044438 0.590278 211 1997 CIN NL 49768000 0.469136 162 76 1127285885 0.044149 0.500000 212 1998 CIN NL 23005000 0.475309 162 77 1278282871 0.017997 0.469136 213 1999 CIN NL 33962761 0.588957 163 96 1494228750 0.022729 0.475309 214 2000 CIN NL 46867200 0.521472 163 85 1666135102 0.028129 0.588957 215 2001 CIN NL 48986000 0.407407 162 66 1960663313 0.024984 0.521472 216 2002 CIN NL 45050390 0.481481 162 78 2024077522 0.022257 0.407407 217 2003 CIN NL 59355667 0.425926 162 69 2128262128 0.027889 0.481481 218 2004 CIN NL 46615250 0.469136 162 76 2070665943 0.022512 0.425926 219 2005 CIN NL 61892583 0.447853 163 73 2188713398 0.028278 0.469136 220 2006 CIN NL 60909519 0.493827 162 80 2321472617 0.026237 0.447853 221 2007 CIN NL 68524980 0.444444 162 72 2476688987 0.027668 0.493827 222 2008 CIN NL 74117695 0.456790 162 74 2684858670 0.027606 0.444444 223 2009 CIN NL 73558500 0.481481 162 78 2664726994 0.027605 0.456790 224 2010 CIN NL 71761542 0.561728 162 91 2721359865 0.026370 0.481481 225 2011 CIN NL 75947134 0.487654 162 79 2784505291 0.027275 0.561728 226 2012 CIN NL 82203616 0.598765 162 97 2932741192 0.028030 0.487654 227 2013 CIN NL 106404462 0.555556 162 90 3034525648 0.035065 0.598765 228 2014 CIN NL 108217500 0.469136 162 76 3192317623 0.033899 0.555556 229 2015 CIN NL 113072286 0.395062 162 64 3514142569 0.032176 0.469136 230 2016 CIN NL 88940059 0.419753 162 68 3750137392 0.023716 0.395062 231 1985 CLE AL 6551666 0.370370 162 60 261964696 0.025010 NaN 232 1986 CLE AL 7809500 0.515337 163 84 307854518 0.025368 0.370370 233 1987 CLE AL 8513750 0.376543 162 61 272575375 0.031234 0.515337 234 1988 CLE AL 8936500 0.481481 162 78 300452424 0.029743 0.376543 235 1989 CLE AL 9094500 0.450617 162 73 359995711 0.025263 0.481481 236 1990 CLE AL 14487000 0.475309 162 77 443881193 0.032637 0.450617 237 1991 CLE AL 17635000 0.351852 162 57 613048418 0.028766 0.475309 238 1992 CLE AL 9373044 0.469136 162 76 805543323 0.011636 0.351852 239 1993 CLE AL 18561000 0.469136 162 76 901740134 0.020584 0.469136 240 1994 CLE AL 30490500 0.584071 113 66 927836287 0.032862 0.469136 241 1995 CLE AL 37937835 0.694444 144 100 951469367 0.039873 0.584071 242 1996 CLE AL 48107360 0.614907 161 99 956983550 0.050270 0.694444 243 1997 CLE AL 56802460 0.534161 161 86 1127285885 0.050389 0.614907 244 1998 CLE AL 60800166 0.549383 162 89 1278282871 0.047564 0.534161 245 1999 CLE AL 72978462 0.598765 162 97 1494228750 0.048840 0.549383 246 2000 CLE AL 75880771 0.555556 162 90 1666135102 0.045543 0.598765 247 2001 CLE AL 93152001 0.561728 162 91 1960663313 0.047510 0.555556 248 2002 CLE AL 78909449 0.456790 162 74 2024077522 0.038985 0.561728 249 2003 CLE AL 48584834 0.419753 162 68 2128262128 0.022828 0.456790 250 2004 CLE AL 34319300 0.493827 162 80 2070665943 0.016574 0.419753 251 2005 CLE AL 41502500 0.574074 162 93 2188713398 0.018962 0.493827 252 2006 CLE AL 56031500 0.481481 162 78 2321472617 0.024136 0.574074 253 2007 CLE AL 61673267 0.592593 162 96 2476688987 0.024901 0.481481 254 2008 CLE AL 78970066 0.500000 162 81 2684858670 0.029413 0.592593 255 2009 CLE AL 81579166 0.401235 162 65 2664726994 0.030614 0.500000 256 2010 CLE AL 61203966 0.425926 162 69 2721359865 0.022490 0.401235 257 2011 CLE AL 48776566 0.493827 162 80 2784505291 0.017517 0.425926 258 2012 CLE AL 78430300 0.419753 162 68 2932741192 0.026743 0.493827 259 2013 CLE AL 75771800 0.567901 162 92 3034525648 0.024970 0.419753 260 2014 CLE AL 82151899 0.524691 162 85 3192317623 0.025734 0.567901 261 2015 CLE AL 87663766 0.503106 161 81 3514142569 0.024946 0.524691 262 2016 CLE AL 74311900 0.583851 161 94 3750137392 0.019816 0.503106 263 1993 COL NL 10353500 0.413580 162 67 901740134 0.011482 NaN 264 1994 COL NL 23887333 0.452991 117 53 927836287 0.025745 0.413580 265 1995 COL NL 34154717 0.534722 144 77 951469367 0.035897 0.452991 266 1996 COL NL 40179823 0.512346 162 83 956983550 0.041986 0.534722 267 1997 COL NL 43559667 0.512346 162 83 1127285885 0.038641 0.512346 268 1998 COL NL 50484648 0.475309 162 77 1278282871 0.039494 0.512346 269 1999 COL NL 61935837 0.444444 162 72 1494228750 0.041450 0.475309 270 2000 COL NL 61111190 0.506173 162 82 1666135102 0.036678 0.444444 271 2001 COL NL 71541334 0.450617 162 73 1960663313 0.036488 0.506173 272 2002 COL NL 56851043 0.450617 162 73 2024077522 0.028087 0.450617 273 2003 COL NL 67179667 0.456790 162 74 2128262128 0.031566 0.450617 274 2004 COL NL 65445167 0.419753 162 68 2070665943 0.031606 0.456790 275 2005 COL NL 47839000 0.413580 162 67 2188713398 0.021857 0.419753 276 2006 COL NL 41233000 0.469136 162 76 2321472617 0.017762 0.413580 277 2007 COL NL 54041000 0.552147 163 90 2476688987 0.021820 0.469136 278 2008 COL NL 68655500 0.456790 162 74 2684858670 0.025571 0.552147 279 2009 COL NL 75201000 0.567901 162 92 2664726994 0.028221 0.456790 280 2010 COL NL 84227000 0.512346 162 83 2721359865 0.030950 0.567901 281 2011 COL NL 88148071 0.450617 162 73 2784505291 0.031657 0.512346 282 2012 COL NL 78069571 0.395062 162 64 2932741192 0.026620 0.450617 283 2013 COL NL 74409071 0.456790 162 74 3034525648 0.024521 0.395062 284 2014 COL NL 95403500 0.407407 162 66 3192317623 0.029885 0.456790 285 2015 COL NL 95688600 0.419753 162 68 3514142569 0.027230 0.407407 286 2016 COL NL 112645071 0.462963 162 75 3750137392 0.030038 0.419753 287 1985 DET AL 10348143 0.521739 161 84 261964696 0.039502 NaN 288 1986 DET AL 12335714 0.537037 162 87 307854518 0.040070 0.521739 289 1987 DET AL 12122881 0.604938 162 98 272575375 0.044475 0.537037 290 1988 DET AL 12869571 0.543210 162 88 300452424 0.042834 0.604938 291 1989 DET AL 15146404 0.364198 162 59 359995711 0.042074 0.543210 292 1990 DET AL 17593238 0.487654 162 79 443881193 0.039635 0.364198 293 1991 DET AL 23838333 0.518519 162 84 613048418 0.038885 0.487654 294 1992 DET AL 27322834 0.462963 162 75 805543323 0.033919 0.518519 295 1993 DET AL 38150165 0.524691 162 85 901740134 0.042307 0.462963 296 1994 DET AL 41446501 0.460870 115 53 927836287 0.044670 0.524691 297 1995 DET AL 37044168 0.416667 144 60 951469367 0.038934 0.460870 298 1996 DET AL 23438000 0.327160 162 53 956983550 0.024492 0.416667 299 1997 DET AL 17272000 0.487654 162 79 1127285885 0.015322 0.327160 300 1998 DET AL 24065000 0.401235 162 65 1278282871 0.018826 0.487654 301 1999 DET AL 36489666 0.428571 161 69 1494228750 0.024420 0.401235 302 2000 DET AL 58265167 0.487654 162 79 1666135102 0.034970 0.428571 303 2001 DET AL 53416167 0.407407 162 66 1960663313 0.027244 0.487654 304 2002 DET AL 55048000 0.341615 161 55 2024077522 0.027197 0.407407 305 2003 DET AL 49168000 0.265432 162 43 2128262128 0.023102 0.341615 306 2004 DET AL 46832000 0.444444 162 72 2070665943 0.022617 0.265432 307 2005 DET AL 69092000 0.438272 162 71 2188713398 0.031567 0.444444 308 2006 DET AL 82612866 0.586420 162 95 2321472617 0.035586 0.438272 309 2007 DET AL 94800369 0.543210 162 88 2476688987 0.038277 0.586420 310 2008 DET AL 137685196 0.456790 162 74 2684858670 0.051282 0.543210 311 2009 DET AL 115085145 0.527607 163 86 2664726994 0.043188 0.456790 312 2010 DET AL 122864928 0.500000 162 81 2721359865 0.045148 0.527607 313 2011 DET AL 105700231 0.586420 162 95 2784505291 0.037960 0.500000 314 2012 DET AL 132300000 0.543210 162 88 2932741192 0.045111 0.586420 315 2013 DET AL 145989500 0.574074 162 93 3034525648 0.048109 0.543210 316 2014 DET AL 152855500 0.555556 162 90 3192317623 0.047882 0.574074 317 2015 DET AL 172284750 0.459627 161 74 3514142569 0.049026 0.555556 318 2016 DET AL 194876481 0.534161 161 86 3750137392 0.051965 0.459627 319 1993 FLO NL 19330545 0.395062 162 64 901740134 0.021437 NaN 320 1994 FLO NL 21633000 0.443478 115 51 927836287 0.023316 0.395062 321 1995 FLO NL 24515781 0.468531 143 67 951469367 0.025766 0.443478 322 1996 FLO NL 31022500 0.493827 162 80 956983550 0.032417 0.468531 323 1997 FLO NL 48692500 0.567901 162 92 1127285885 0.043194 0.493827 324 1998 FLO NL 41322667 0.333333 162 54 1278282871 0.032327 0.567901 325 1999 FLO NL 21085000 0.395062 162 64 1494228750 0.014111 0.333333 326 2000 FLO NL 19872000 0.490683 161 79 1666135102 0.011927 0.395062 327 2001 FLO NL 35762500 0.469136 162 76 1960663313 0.018240 0.490683 328 2002 FLO NL 41979917 0.487654 162 79 2024077522 0.020740 0.469136 329 2003 FLO NL 49450000 0.561728 162 91 2128262128 0.023235 0.487654 330 2004 FLO NL 42143042 0.512346 162 83 2070665943 0.020352 0.561728 331 2005 FLO NL 60408834 0.512346 162 83 2188713398 0.027600 0.512346 332 2006 FLO NL 14671500 0.481481 162 78 2321472617 0.006320 0.512346 333 2007 FLO NL 30507000 0.438272 162 71 2476688987 0.012318 0.481481 334 2008 FLO NL 21811500 0.521739 161 84 2684858670 0.008124 0.438272 335 2009 FLO NL 36834000 0.537037 162 87 2664726994 0.013823 0.521739 336 2010 FLO NL 57029719 0.493827 162 80 2721359865 0.020956 0.537037 337 2011 FLO NL 56944000 0.444444 162 72 2784505291 0.020450 0.493827 338 1985 HOU NL 9993051 0.512346 162 83 261964696 0.038147 NaN 339 1986 HOU NL 9873276 0.592593 162 96 307854518 0.032071 0.512346 340 1987 HOU NL 12608371 0.469136 162 76 272575375 0.046256 0.592593 341 1988 HOU NL 12286167 0.506173 162 82 300452424 0.040892 0.469136 342 1989 HOU NL 15029500 0.530864 162 86 359995711 0.041749 0.506173 343 1990 HOU NL 18330000 0.462963 162 75 443881193 0.041295 0.530864 344 1991 HOU NL 12852500 0.401235 162 65 613048418 0.020965 0.462963 345 1992 HOU NL 15407500 0.500000 162 81 805543323 0.019127 0.401235 346 1993 HOU NL 30210500 0.524691 162 85 901740134 0.033502 0.500000 347 1994 HOU NL 33126000 0.573913 115 66 927836287 0.035702 0.524691 348 1995 HOU NL 34169834 0.527778 144 76 951469367 0.035913 0.573913 349 1996 HOU NL 28487000 0.506173 162 82 956983550 0.029767 0.527778 350 1997 HOU NL 34777500 0.518519 162 84 1127285885 0.030851 0.506173 351 1998 HOU NL 42374000 0.629630 162 102 1278282871 0.033149 0.518519 352 1999 HOU NL 54914000 0.598765 162 97 1494228750 0.036751 0.629630 353 2000 HOU NL 51289111 0.444444 162 72 1666135102 0.030783 0.598765 354 2001 HOU NL 60612667 0.574074 162 93 1960663313 0.030914 0.444444 355 2002 HOU NL 63448417 0.518519 162 84 2024077522 0.031347 0.574074 356 2003 HOU NL 71040000 0.537037 162 87 2128262128 0.033379 0.518519 357 2004 HOU NL 75397000 0.567901 162 92 2070665943 0.036412 0.537037 358 2005 HOU NL 76779000 0.546012 163 89 2188713398 0.035080 0.567901 359 2006 HOU NL 88694435 0.506173 162 82 2321472617 0.038206 0.546012 360 2007 HOU NL 87759000 0.450617 162 73 2476688987 0.035434 0.506173 361 2008 HOU NL 88930414 0.534161 161 86 2684858670 0.033123 0.450617 362 2009 HOU NL 102996414 0.456790 162 74 2664726994 0.038652 0.534161 363 2010 HOU NL 92355500 0.469136 162 76 2721359865 0.033937 0.456790 364 2011 HOU NL 70694000 0.345679 162 56 2784505291 0.025388 0.469136 365 2012 HOU NL 60651000 0.339506 162 55 2932741192 0.020681 0.345679 366 2013 HOU AL 17890700 0.314815 162 51 3034525648 0.005896 0.339506 367 2014 HOU AL 35116300 0.432099 162 70 3192317623 0.011000 0.314815 368 2015 HOU AL 72256200 0.530864 162 86 3514142569 0.020562 0.432099 369 2016 HOU AL 94893700 0.518519 162 84 3750137392 0.025304 0.530864 370 1985 KCA AL 9321179 0.561728 162 91 261964696 0.035582 NaN 371 1986 KCA AL 13043698 0.469136 162 76 307854518 0.042370 0.561728 372 1987 KCA AL 11828056 0.512346 162 83 272575375 0.043394 0.469136 373 1988 KCA AL 14556562 0.521739 161 84 300452424 0.048449 0.512346 374 1989 KCA AL 18683568 0.567901 162 92 359995711 0.051899 0.521739 375 1990 KCA AL 23361084 0.465839 161 75 443881193 0.052629 0.567901 376 1991 KCA AL 26319834 0.506173 162 82 613048418 0.042933 0.465839 377 1992 KCA AL 33893834 0.444444 162 72 805543323 0.042076 0.506173 378 1993 KCA AL 41346167 0.518519 162 84 901740134 0.045852 0.444444 379 1994 KCA AL 40541334 0.556522 115 64 927836287 0.043694 0.518519 380 1995 KCA AL 29532834 0.486111 144 70 951469367 0.031039 0.556522 381 1996 KCA AL 20281250 0.465839 161 75 956983550 0.021193 0.486111 382 1997 KCA AL 34655000 0.416149 161 67 1127285885 0.030742 0.465839 383 1998 KCA AL 36862500 0.447205 161 72 1278282871 0.028838 0.416149 384 1999 KCA AL 26225000 0.397516 161 64 1494228750 0.017551 0.447205 385 2000 KCA AL 23433000 0.475309 162 77 1666135102 0.014064 0.397516 386 2001 KCA AL 35422500 0.401235 162 65 1960663313 0.018067 0.475309 387 2002 KCA AL 47257000 0.382716 162 62 2024077522 0.023347 0.401235 388 2003 KCA AL 40518000 0.512346 162 83 2128262128 0.019038 0.382716 389 2004 KCA AL 47609000 0.358025 162 58 2070665943 0.022992 0.512346 390 2005 KCA AL 36881000 0.345679 162 56 2188713398 0.016851 0.358025 391 2006 KCA AL 47294000 0.382716 162 62 2321472617 0.020372 0.345679 392 2007 KCA AL 67116500 0.425926 162 69 2476688987 0.027099 0.382716 393 2008 KCA AL 58245500 0.462963 162 75 2684858670 0.021694 0.425926 394 2009 KCA AL 70519333 0.401235 162 65 2664726994 0.026464 0.462963 395 2010 KCA AL 71405210 0.413580 162 67 2721359865 0.026239 0.401235 396 2011 KCA AL 35712000 0.438272 162 71 2784505291 0.012825 0.413580 397 2012 KCA AL 60916225 0.444444 162 72 2932741192 0.020771 0.438272 398 2013 KCA AL 80091725 0.530864 162 86 3034525648 0.026393 0.444444 399 2014 KCA AL 74594075 0.549383 162 89 3192317623 0.023367 0.530864 400 2015 KCA AL 112107025 0.586420 162 95 3514142569 0.031902 0.549383 401 2016 KCA AL 131487125 0.500000 162 81 3750137392 0.035062 0.586420 402 2005 LAA AL 94867822 0.586420 162 95 2188713398 0.043344 NaN 403 2006 LAA AL 103472000 0.549383 162 89 2321472617 0.044572 0.586420 404 2007 LAA AL 109251333 0.580247 162 94 2476688987 0.044112 0.549383 405 2008 LAA AL 119216333 0.617284 162 100 2684858670 0.044403 0.580247 406 2009 LAA AL 113709000 0.598765 162 97 2664726994 0.042672 0.617284 407 2010 LAA AL 104963866 0.493827 162 80 2721359865 0.038570 0.598765 408 2011 LAA AL 138543166 0.530864 162 86 2784505291 0.049755 0.493827 409 2012 LAA AL 154485166 0.549383 162 89 2932741192 0.052676 0.530864 410 2013 LAA AL 124174750 0.481481 162 78 3034525648 0.040921 0.549383 411 2014 LAA AL 121988250 0.604938 162 98 3192317623 0.038213 0.481481 412 2015 LAA AL 120005415 0.524691 162 85 3514142569 0.034149 0.604938 413 2016 LAA AL 137251333 0.456790 162 74 3750137392 0.036599 0.524691 414 1985 LAN NL 10967917 0.586420 162 95 261964696 0.041868 NaN 415 1986 LAN NL 14913776 0.450617 162 73 307854518 0.048444 0.586420 416 1987 LAN NL 13675403 0.450617 162 73 272575375 0.050171 0.450617 417 1988 LAN NL 16850515 0.580247 162 94 300452424 0.056084 0.450617 418 1989 LAN NL 21071562 0.481250 160 77 359995711 0.058533 0.580247 419 1990 LAN NL 21318704 0.530864 162 86 443881193 0.048028 0.481250 420 1991 LAN NL 32790664 0.574074 162 93 613048418 0.053488 0.530864 421 1992 LAN NL 44788166 0.388889 162 63 805543323 0.055600 0.574074 422 1993 LAN NL 39331999 0.500000 162 81 901740134 0.043618 0.388889 423 1994 LAN NL 38000001 0.508772 114 58 927836287 0.040956 0.500000 424 1995 LAN NL 39273201 0.541667 144 78 951469367 0.041276 0.508772 425 1996 LAN NL 35355000 0.555556 162 90 956983550 0.036944 0.541667 426 1997 LAN NL 45380304 0.543210 162 88 1127285885 0.040256 0.555556 427 1998 LAN NL 48820000 0.512346 162 83 1278282871 0.038192 0.543210 428 1999 LAN NL 80862453 0.475309 162 77 1494228750 0.054117 0.512346 429 2000 LAN NL 87924286 0.530864 162 86 1666135102 0.052771 0.475309 430 2001 LAN NL 109105953 0.530864 162 86 1960663313 0.055647 0.530864 431 2002 LAN NL 94850953 0.567901 162 92 2024077522 0.046861 0.530864 432 2003 LAN NL 105572620 0.524691 162 85 2128262128 0.049605 0.567901 433 2004 LAN NL 92902001 0.574074 162 93 2070665943 0.044866 0.524691 434 2005 LAN NL 83039000 0.438272 162 71 2188713398 0.037940 0.574074 435 2006 LAN NL 98447187 0.543210 162 88 2321472617 0.042407 0.438272 436 2007 LAN NL 108454524 0.506173 162 82 2476688987 0.043790 0.543210 437 2008 LAN NL 118588536 0.518519 162 84 2684858670 0.044169 0.506173 438 2009 LAN NL 100414592 0.586420 162 95 2664726994 0.037683 0.518519 439 2010 LAN NL 95358016 0.493827 162 80 2721359865 0.035041 0.586420 440 2011 LAN NL 104188999 0.509317 161 82 2784505291 0.037417 0.493827 441 2012 LAN NL 95143575 0.530864 162 86 2932741192 0.032442 0.509317 442 2013 LAN NL 223362196 0.567901 162 92 3034525648 0.073607 0.530864 443 2014 LAN NL 217014600 0.580247 162 94 3192317623 0.067980 0.567901 444 2015 LAN NL 215792000 0.567901 162 92 3514142569 0.061407 0.580247 445 2016 LAN NL 221288380 0.561728 162 91 3750137392 0.059008 0.567901 446 2012 MIA NL 118078000 0.425926 162 69 2932741192 0.040262 NaN 447 2013 MIA NL 33601900 0.382716 162 62 3034525648 0.011073 0.425926 448 2014 MIA NL 41836900 0.475309 162 77 3192317623 0.013105 0.382716 449 2015 MIA NL 68056500 0.438272 162 71 3514142569 0.019366 0.475309 450 2016 MIA NL 77314202 0.490683 161 79 3750137392 0.020616 0.438272 451 1998 MIL NL 33914904 0.456790 162 74 1278282871 0.026532 NaN 452 1999 MIL NL 43377395 0.459627 161 74 1494228750 0.029030 0.456790 453 2000 MIL NL 36505333 0.447853 163 73 1666135102 0.021910 0.459627 454 2001 MIL NL 43886833 0.419753 162 68 1960663313 0.022384 0.447853 455 2002 MIL NL 50287833 0.345679 162 56 2024077522 0.024845 0.419753 456 2003 MIL NL 40627000 0.419753 162 68 2128262128 0.019089 0.345679 457 2004 MIL NL 27528500 0.416149 161 67 2070665943 0.013295 0.419753 458 2005 MIL NL 39934833 0.500000 162 81 2188713398 0.018246 0.416149 459 2006 MIL NL 57568333 0.462963 162 75 2321472617 0.024798 0.500000 460 2007 MIL NL 70986500 0.512346 162 83 2476688987 0.028662 0.462963 461 2008 MIL NL 80937499 0.555556 162 90 2684858670 0.030146 0.512346 462 2009 MIL NL 80182502 0.493827 162 80 2664726994 0.030090 0.555556 463 2010 MIL NL 81108278 0.475309 162 77 2721359865 0.029804 0.493827 464 2011 MIL NL 85497333 0.592593 162 96 2784505291 0.030705 0.475309 465 2012 MIL NL 97653944 0.512346 162 83 2932741192 0.033298 0.592593 466 2013 MIL NL 76947033 0.456790 162 74 3034525648 0.025357 0.512346 467 2014 MIL NL 101217000 0.506173 162 82 3192317623 0.031706 0.456790 468 2015 MIL NL 100850000 0.419753 162 68 3514142569 0.028698 0.506173 469 2016 MIL NL 68775237 0.450617 162 73 3750137392 0.018339 0.419753 470 1985 MIN AL 5764821 0.475309 162 77 261964696 0.022006 NaN 471 1986 MIN AL 8748167 0.438272 162 71 307854518 0.028417 0.475309 472 1987 MIN AL 6397500 0.524691 162 85 272575375 0.023471 0.438272 473 1988 MIN AL 12462666 0.561728 162 91 300452424 0.041480 0.524691 474 1989 MIN AL 15531666 0.493827 162 80 359995711 0.043144 0.561728 475 1990 MIN AL 14602000 0.456790 162 74 443881193 0.032896 0.493827 476 1991 MIN AL 23361833 0.586420 162 95 613048418 0.038108 0.456790 477 1992 MIN AL 28027834 0.555556 162 90 805543323 0.034794 0.586420 478 1993 MIN AL 28217933 0.438272 162 71 901740134 0.031293 0.555556 479 1994 MIN AL 28438500 0.469027 113 53 927836287 0.030650 0.438272 480 1995 MIN AL 25410500 0.388889 144 56 951469367 0.026707 0.469027 481 1996 MIN AL 23117000 0.481481 162 78 956983550 0.024156 0.388889 482 1997 MIN AL 34072500 0.419753 162 68 1127285885 0.030225 0.481481 483 1998 MIN AL 27927500 0.432099 162 70 1278282871 0.021848 0.419753 484 1999 MIN AL 21257500 0.391304 161 63 1494228750 0.014226 0.432099 485 2000 MIN AL 16519500 0.425926 162 69 1666135102 0.009915 0.391304 486 2001 MIN AL 24130000 0.524691 162 85 1960663313 0.012307 0.425926 487 2002 MIN AL 40425000 0.583851 161 94 2024077522 0.019972 0.524691 488 2003 MIN AL 55505000 0.555556 162 90 2128262128 0.026080 0.583851 489 2004 MIN AL 53585000 0.567901 162 92 2070665943 0.025878 0.555556 490 2005 MIN AL 56186000 0.512346 162 83 2188713398 0.025671 0.567901 491 2006 MIN AL 63396006 0.592593 162 96 2321472617 0.027309 0.512346 492 2007 MIN AL 71439500 0.487654 162 79 2476688987 0.028845 0.592593 493 2008 MIN AL 56932766 0.539877 163 88 2684858670 0.021205 0.487654 494 2009 MIN AL 65299266 0.533742 163 87 2664726994 0.024505 0.539877 495 2010 MIN AL 97559166 0.580247 162 94 2721359865 0.035849 0.533742 496 2011 MIN AL 112737000 0.388889 162 63 2784505291 0.040487 0.580247 497 2012 MIN AL 94085000 0.407407 162 66 2932741192 0.032081 0.388889 498 2013 MIN AL 75337500 0.407407 162 66 3034525648 0.024827 0.407407 499 2014 MIN AL 83762500 0.432099 162 70 3192317623 0.026239 0.407407 500 2015 MIN AL 107755000 0.512346 162 83 3514142569 0.030663 0.432099 501 2016 MIN AL 102583200 0.364198 162 59 3750137392 0.027355 0.512346 502 1985 ML4 AL 11284107 0.440994 161 71 261964696 0.043075 NaN 503 1986 ML4 AL 9943642 0.478261 161 77 307854518 0.032300 0.440994 504 1987 ML4 AL 7293224 0.561728 162 91 272575375 0.026757 0.478261 505 1988 ML4 AL 8402000 0.537037 162 87 300452424 0.027964 0.561728 506 1989 ML4 AL 11533000 0.500000 162 81 359995711 0.032036 0.537037 507 1990 ML4 AL 19719167 0.456790 162 74 443881193 0.044424 0.500000 508 1991 ML4 AL 23115500 0.512346 162 83 613048418 0.037706 0.456790 509 1992 ML4 AL 31013667 0.567901 162 92 805543323 0.038500 0.512346 510 1993 ML4 AL 23806834 0.425926 162 69 901740134 0.026401 0.567901 511 1994 ML4 AL 24350500 0.460870 115 53 927836287 0.026244 0.425926 512 1995 ML4 AL 17798825 0.451389 144 65 951469367 0.018707 0.460870 513 1996 ML4 AL 21730000 0.493827 162 80 956983550 0.022707 0.451389 514 1997 ML4 AL 23655338 0.484472 161 78 1127285885 0.020984 0.493827 515 1985 MON NL 9470166 0.521739 161 84 261964696 0.036151 NaN 516 1986 MON NL 11103600 0.484472 161 78 307854518 0.036068 0.521739 517 1987 MON NL 6942052 0.561728 162 91 272575375 0.025468 0.484472 518 1988 MON NL 9603333 0.496933 163 81 300452424 0.031963 0.561728 519 1989 MON NL 13807389 0.500000 162 81 359995711 0.038354 0.496933 520 1990 MON NL 16586388 0.524691 162 85 443881193 0.037367 0.500000 521 1991 MON NL 10732333 0.440994 161 71 613048418 0.017507 0.524691 522 1992 MON NL 15822334 0.537037 162 87 805543323 0.019642 0.440994 523 1993 MON NL 18899333 0.576687 163 94 901740134 0.020959 0.537037 524 1994 MON NL 19098000 0.649123 114 74 927836287 0.020583 0.576687 525 1995 MON NL 12364000 0.458333 144 66 951469367 0.012995 0.649123 526 1996 MON NL 16264500 0.543210 162 88 956983550 0.016996 0.458333 527 1997 MON NL 19295500 0.481481 162 78 1127285885 0.017117 0.543210 528 1998 MON NL 10641500 0.401235 162 65 1278282871 0.008325 0.481481 529 1999 MON NL 17903000 0.419753 162 68 1494228750 0.011981 0.401235 530 2000 MON NL 32994333 0.413580 162 67 1666135102 0.019803 0.419753 531 2001 MON NL 35159500 0.419753 162 68 1960663313 0.017932 0.413580 532 2002 MON NL 38670500 0.512346 162 83 2024077522 0.019105 0.419753 533 2003 MON NL 51948500 0.512346 162 83 2128262128 0.024409 0.512346 534 2004 MON NL 40897500 0.413580 162 67 2070665943 0.019751 0.512346 535 1985 NYA AL 14238204 0.602484 161 97 261964696 0.054352 NaN 536 1986 NYA AL 18494253 0.555556 162 90 307854518 0.060075 0.602484 537 1987 NYA AL 17099714 0.549383 162 89 272575375 0.062734 0.555556 538 1988 NYA AL 19441152 0.527950 161 85 300452424 0.064706 0.549383 539 1989 NYA AL 17114375 0.459627 161 74 359995711 0.047540 0.527950 540 1990 NYA AL 20912318 0.413580 162 67 443881193 0.047112 0.459627 541 1991 NYA AL 27344168 0.438272 162 71 613048418 0.044604 0.413580 542 1992 NYA AL 37543334 0.469136 162 76 805543323 0.046606 0.438272 543 1993 NYA AL 42624900 0.543210 162 88 901740134 0.047270 0.469136 544 1994 NYA AL 45731334 0.619469 113 70 927836287 0.049288 0.543210 545 1995 NYA AL 48874851 0.544828 145 79 951469367 0.051368 0.619469 546 1996 NYA AL 54191792 0.567901 162 92 956983550 0.056628 0.544828 547 1997 NYA AL 62241545 0.592593 162 96 1127285885 0.055214 0.567901 548 1998 NYA AL 66806867 0.703704 162 114 1278282871 0.052263 0.592593 549 1999 NYA AL 86734359 0.604938 162 98 1494228750 0.058046 0.703704 550 2000 NYA AL 92338260 0.540373 161 87 1666135102 0.055421 0.604938 551 2001 NYA AL 112287143 0.590062 161 95 1960663313 0.057270 0.540373 552 2002 NYA AL 125928583 0.639752 161 103 2024077522 0.062215 0.590062 553 2003 NYA AL 152749814 0.619632 163 101 2128262128 0.071772 0.639752 554 2004 NYA AL 184193950 0.623457 162 101 2070665943 0.088954 0.619632 555 2005 NYA AL 208306817 0.586420 162 95 2188713398 0.095173 0.623457 556 2006 NYA AL 194663079 0.598765 162 97 2321472617 0.083853 0.586420 557 2007 NYA AL 189259045 0.580247 162 94 2476688987 0.076416 0.598765 558 2008 NYA AL 207896789 0.549383 162 89 2684858670 0.077433 0.580247 559 2009 NYA AL 201449189 0.635802 162 103 2664726994 0.075598 0.549383 560 2010 NYA AL 206333389 0.586420 162 95 2721359865 0.075820 0.635802 561 2011 NYA AL 202275028 0.598765 162 97 2784505291 0.072643 0.586420 562 2012 NYA AL 196522289 0.586420 162 95 2932741192 0.067010 0.598765 563 2013 NYA AL 231978886 0.524691 162 85 3034525648 0.076447 0.586420 564 2014 NYA AL 197543907 0.518519 162 84 3192317623 0.061881 0.524691 565 2015 NYA AL 212751957 0.537037 162 87 3514142569 0.060542 0.518519 566 2016 NYA AL 222997792 0.518519 162 84 3750137392 0.059464 0.537037 567 1985 NYN NL 10834762 0.604938 162 98 261964696 0.041360 NaN 568 1986 NYN NL 15393714 0.666667 162 108 307854518 0.050003 0.604938 569 1987 NYN NL 13846714 0.567901 162 92 272575375 0.050800 0.666667 570 1988 NYN NL 15269314 0.625000 160 100 300452424 0.050821 0.567901 571 1989 NYN NL 19885071 0.537037 162 87 359995711 0.055237 0.625000 572 1990 NYN NL 21722834 0.561728 162 91 443881193 0.048938 0.537037 573 1991 NYN NL 32590001 0.478261 161 77 613048418 0.053161 0.561728 574 1992 NYN NL 44602002 0.444444 162 72 805543323 0.055369 0.478261 575 1993 NYN NL 39043667 0.364198 162 59 901740134 0.043298 0.444444 576 1994 NYN NL 30956583 0.486726 113 55 927836287 0.033364 0.364198 577 1995 NYN NL 27674992 0.479167 144 69 951469367 0.029087 0.486726 578 1996 NYN NL 24479500 0.438272 162 71 956983550 0.025580 0.479167 579 1997 NYN NL 39800400 0.543210 162 88 1127285885 0.035306 0.438272 580 1998 NYN NL 52077999 0.543210 162 88 1278282871 0.040741 0.543210 581 1999 NYN NL 65092092 0.595092 163 97 1494228750 0.043562 0.543210 582 2000 NYN NL 79509776 0.580247 162 94 1666135102 0.047721 0.595092 583 2001 NYN NL 93174428 0.506173 162 82 1960663313 0.047522 0.580247 584 2002 NYN NL 94633593 0.465839 161 75 2024077522 0.046754 0.506173 585 2003 NYN NL 116876429 0.409938 161 66 2128262128 0.054916 0.465839 586 2004 NYN NL 96660970 0.438272 162 71 2070665943 0.046681 0.409938 587 2005 NYN NL 101305821 0.512346 162 83 2188713398 0.046286 0.438272 588 2006 NYN NL 101084963 0.598765 162 97 2321472617 0.043543 0.512346 589 2007 NYN NL 115231663 0.543210 162 88 2476688987 0.046526 0.598765 590 2008 NYN NL 137793376 0.549383 162 89 2684858670 0.051322 0.543210 591 2009 NYN NL 149373987 0.432099 162 70 2664726994 0.056056 0.549383 592 2010 NYN NL 134422942 0.487654 162 79 2721359865 0.049396 0.432099 593 2011 NYN NL 118847309 0.475309 162 77 2784505291 0.042682 0.487654 594 2012 NYN NL 93353983 0.456790 162 74 2932741192 0.031832 0.475309 595 2013 NYN NL 49448346 0.456790 162 74 3034525648 0.016295 0.456790 596 2014 NYN NL 85556990 0.487654 162 79 3192317623 0.026801 0.456790 597 2015 NYN NL 96766683 0.555556 162 90 3514142569 0.027536 0.487654 598 2016 NYN NL 133889129 0.537037 162 87 3750137392 0.035702 0.555556 599 1985 OAK AL 9058606 0.475309 162 77 261964696 0.034579 NaN 600 1986 OAK AL 9779421 0.469136 162 76 307854518 0.031766 0.475309 601 1987 OAK AL 11680839 0.500000 162 81 272575375 0.042854 0.469136 602 1988 OAK AL 9690000 0.641975 162 104 300452424 0.032251 0.500000 603 1989 OAK AL 15613070 0.611111 162 99 359995711 0.043370 0.641975 604 1990 OAK AL 19887501 0.635802 162 103 443881193 0.044804 0.611111 605 1991 OAK AL 36999167 0.518519 162 84 613048418 0.060353 0.635802 606 1992 OAK AL 41035000 0.592593 162 96 805543323 0.050941 0.518519 607 1993 OAK AL 37812333 0.419753 162 68 901740134 0.041933 0.592593 608 1994 OAK AL 34172500 0.447368 114 51 927836287 0.036830 0.419753 609 1995 OAK AL 37739225 0.465278 144 67 951469367 0.039664 0.447368 610 1996 OAK AL 21243000 0.481481 162 78 956983550 0.022198 0.465278 611 1997 OAK AL 24018500 0.401235 162 65 1127285885 0.021306 0.481481 612 1998 OAK AL 21303000 0.456790 162 74 1278282871 0.016665 0.401235 613 1999 OAK AL 24431833 0.537037 162 87 1494228750 0.016351 0.456790 614 2000 OAK AL 31971333 0.565217 161 91 1666135102 0.019189 0.537037 615 2001 OAK AL 33810750 0.629630 162 102 1960663313 0.017245 0.565217 616 2002 OAK AL 40004167 0.635802 162 103 2024077522 0.019764 0.629630 617 2003 OAK AL 50260834 0.592593 162 96 2128262128 0.023616 0.635802 618 2004 OAK AL 59425667 0.561728 162 91 2070665943 0.028699 0.592593 619 2005 OAK AL 55425762 0.543210 162 88 2188713398 0.025323 0.561728 620 2006 OAK AL 62243079 0.574074 162 93 2321472617 0.026812 0.543210 621 2007 OAK AL 79366940 0.469136 162 76 2476688987 0.032046 0.574074 622 2008 OAK AL 47967126 0.465839 161 75 2684858670 0.017866 0.469136 623 2009 OAK AL 61910000 0.462963 162 75 2664726994 0.023233 0.465839 624 2010 OAK AL 55254900 0.500000 162 81 2721359865 0.020304 0.462963 625 2011 OAK AL 66536500 0.456790 162 74 2784505291 0.023895 0.500000 626 2012 OAK AL 55372500 0.580247 162 94 2932741192 0.018881 0.456790 627 2013 OAK AL 60132500 0.592593 162 96 3034525648 0.019816 0.580247 628 2014 OAK AL 72408400 0.543210 162 88 3192317623 0.022682 0.592593 629 2015 OAK AL 79053501 0.419753 162 68 3514142569 0.022496 0.543210 630 2016 OAK AL 86806234 0.425926 162 69 3750137392 0.023147 0.419753 631 1985 PHI NL 10124966 0.462963 162 75 261964696 0.038650 NaN 632 1986 PHI NL 11590166 0.534161 161 86 307854518 0.037648 0.462963 633 1987 PHI NL 11514233 0.493827 162 80 272575375 0.042242 0.534161 634 1988 PHI NL 13838000 0.401235 162 65 300452424 0.046057 0.493827 635 1989 PHI NL 10604000 0.411043 163 67 359995711 0.029456 0.401235 636 1990 PHI NL 13173667 0.475309 162 77 443881193 0.029678 0.411043 637 1991 PHI NL 22487332 0.481481 162 78 613048418 0.036681 0.475309 638 1992 PHI NL 24383834 0.432099 162 70 805543323 0.030270 0.481481 639 1993 PHI NL 28538334 0.598765 162 97 901740134 0.031648 0.432099 640 1994 PHI NL 31599000 0.469565 115 54 927836287 0.034057 0.598765 641 1995 PHI NL 30555945 0.479167 144 69 951469367 0.032114 0.469565 642 1996 PHI NL 34314500 0.413580 162 67 956983550 0.035857 0.479167 643 1997 PHI NL 36656500 0.419753 162 68 1127285885 0.032517 0.413580 644 1998 PHI NL 36297500 0.462963 162 75 1278282871 0.028396 0.419753 645 1999 PHI NL 31692500 0.475309 162 77 1494228750 0.021210 0.462963 646 2000 PHI NL 47308000 0.401235 162 65 1666135102 0.028394 0.475309 647 2001 PHI NL 41663833 0.530864 162 86 1960663313 0.021250 0.401235 648 2002 PHI NL 57954999 0.496894 161 80 2024077522 0.028633 0.530864 649 2003 PHI NL 70780000 0.530864 162 86 2128262128 0.033257 0.496894 650 2004 PHI NL 92919167 0.530864 162 86 2070665943 0.044874 0.530864 651 2005 PHI NL 95522000 0.543210 162 88 2188713398 0.043643 0.530864 652 2006 PHI NL 88273333 0.524691 162 85 2321472617 0.038025 0.543210 653 2007 PHI NL 89428213 0.549383 162 89 2476688987 0.036108 0.524691 654 2008 PHI NL 97879880 0.567901 162 92 2684858670 0.036456 0.549383 655 2009 PHI NL 113004046 0.574074 162 93 2664726994 0.042407 0.567901 656 2010 PHI NL 141928379 0.598765 162 97 2721359865 0.052153 0.574074 657 2011 PHI NL 172976379 0.629630 162 102 2784505291 0.062121 0.598765 658 2012 PHI NL 174538938 0.500000 162 81 2932741192 0.059514 0.629630 659 2013 PHI NL 169863189 0.450617 162 73 3034525648 0.055977 0.500000 660 2014 PHI NL 180944967 0.450617 162 73 3192317623 0.056681 0.450617 661 2015 PHI NL 111693000 0.388889 162 63 3514142569 0.031784 0.450617 662 2016 PHI NL 58980000 0.438272 162 71 3750137392 0.015727 0.388889 663 1985 PIT NL 9227500 0.354037 161 57 261964696 0.035224 NaN 664 1986 PIT NL 10843500 0.395062 162 64 307854518 0.035223 0.354037 665 1987 PIT NL 7652000 0.493827 162 80 272575375 0.028073 0.395062 666 1988 PIT NL 5998500 0.531250 160 85 300452424 0.019965 0.493827 667 1989 PIT NL 12737500 0.451220 164 74 359995711 0.035382 0.531250 668 1990 PIT NL 15556000 0.586420 162 95 443881193 0.035045 0.451220 669 1991 PIT NL 23634667 0.604938 162 98 613048418 0.038553 0.586420 670 1992 PIT NL 33944167 0.592593 162 96 805543323 0.042138 0.604938 671 1993 PIT NL 24822467 0.462963 162 75 901740134 0.027527 0.592593 672 1994 PIT NL 24217250 0.464912 114 53 927836287 0.026101 0.462963 673 1995 PIT NL 18355345 0.402778 144 58 951469367 0.019292 0.464912 674 1996 PIT NL 23017500 0.450617 162 73 956983550 0.024052 0.402778 675 1997 PIT NL 10771667 0.487654 162 79 1127285885 0.009555 0.450617 676 1998 PIT NL 15065000 0.423313 163 69 1278282871 0.011785 0.487654 677 1999 PIT NL 24697666 0.484472 161 78 1494228750 0.016529 0.423313 678 2000 PIT NL 28928334 0.425926 162 69 1666135102 0.017363 0.484472 679 2001 PIT NL 57760833 0.382716 162 62 1960663313 0.029460 0.425926 680 2002 PIT NL 42323599 0.447205 161 72 2024077522 0.020910 0.382716 681 2003 PIT NL 54812429 0.462963 162 75 2128262128 0.025755 0.447205 682 2004 PIT NL 32227929 0.447205 161 72 2070665943 0.015564 0.462963 683 2005 PIT NL 38133000 0.413580 162 67 2188713398 0.017423 0.447205 684 2006 PIT NL 46717750 0.413580 162 67 2321472617 0.020124 0.413580 685 2007 PIT NL 38537833 0.419753 162 68 2476688987 0.015560 0.413580 686 2008 PIT NL 48689783 0.413580 162 67 2684858670 0.018135 0.419753 687 2009 PIT NL 48693000 0.385093 161 62 2664726994 0.018273 0.413580 688 2010 PIT NL 34943000 0.351852 162 57 2721359865 0.012840 0.385093 689 2011 PIT NL 45047000 0.444444 162 72 2784505291 0.016178 0.351852 690 2012 PIT NL 62951999 0.487654 162 79 2932741192 0.021465 0.444444 691 2013 PIT NL 77062000 0.580247 162 94 3034525648 0.025395 0.487654 692 2014 PIT NL 77178000 0.543210 162 88 3192317623 0.024176 0.580247 693 2015 PIT NL 88892499 0.604938 162 98 3514142569 0.025296 0.543210 694 2016 PIT NL 103778833 0.481481 162 78 3750137392 0.027673 0.604938 695 1985 SDN NL 11036583 0.512346 162 83 261964696 0.042130 NaN 696 1986 SDN NL 11380693 0.456790 162 74 307854518 0.036968 0.512346 697 1987 SDN NL 11065796 0.401235 162 65 272575375 0.040597 0.456790 698 1988 SDN NL 9561002 0.515528 161 83 300452424 0.031822 0.401235 699 1989 SDN NL 14195000 0.549383 162 89 359995711 0.039431 0.515528 700 1990 SDN NL 17588334 0.462963 162 75 443881193 0.039624 0.549383 701 1991 SDN NL 22150001 0.518519 162 84 613048418 0.036131 0.462963 702 1992 SDN NL 26854167 0.506173 162 82 805543323 0.033337 0.518519 703 1993 SDN NL 25511333 0.376543 162 61 901740134 0.028291 0.506173 704 1994 SDN NL 14916333 0.401709 117 47 927836287 0.016076 0.376543 705 1995 SDN NL 26382334 0.486111 144 70 951469367 0.027728 0.401709 706 1996 SDN NL 28348172 0.561728 162 91 956983550 0.029622 0.486111 707 1997 SDN NL 37363672 0.469136 162 76 1127285885 0.033145 0.561728 708 1998 SDN NL 46861500 0.604938 162 98 1278282871 0.036660 0.469136 709 1999 SDN NL 49768179 0.456790 162 74 1494228750 0.033307 0.604938 710 2000 SDN NL 54821000 0.469136 162 76 1666135102 0.032903 0.456790 711 2001 SDN NL 39182833 0.487654 162 79 1960663313 0.019984 0.469136 712 2002 SDN NL 41425000 0.407407 162 66 2024077522 0.020466 0.487654 713 2003 SDN NL 45210000 0.395062 162 64 2128262128 0.021243 0.407407 714 2004 SDN NL 55384833 0.537037 162 87 2070665943 0.026747 0.395062 715 2005 SDN NL 63290833 0.506173 162 82 2188713398 0.028917 0.537037 716 2006 SDN NL 69896141 0.543210 162 88 2321472617 0.030109 0.506173 717 2007 SDN NL 58110567 0.546012 163 89 2476688987 0.023463 0.543210 718 2008 SDN NL 73677616 0.388889 162 63 2684858670 0.027442 0.546012 719 2009 SDN NL 43333700 0.462963 162 75 2664726994 0.016262 0.388889 720 2010 SDN NL 37799300 0.555556 162 90 2721359865 0.013890 0.462963 721 2011 SDN NL 45869140 0.438272 162 71 2784505291 0.016473 0.555556 722 2012 SDN NL 55244700 0.469136 162 76 2932741192 0.018837 0.438272 723 2013 SDN NL 65585500 0.469136 162 76 3034525648 0.021613 0.469136 724 2014 SDN NL 75685700 0.475309 162 77 3192317623 0.023709 0.469136 725 2015 SDN NL 118441300 0.456790 162 74 3514142569 0.033704 0.475309 726 2016 SDN NL 101424814 0.419753 162 68 3750137392 0.027046 0.456790 727 1985 SEA AL 4613000 0.456790 162 74 261964696 0.017609 NaN 728 1986 SEA AL 5958309 0.413580 162 67 307854518 0.019354 0.456790 729 1987 SEA AL 2263500 0.481481 162 78 272575375 0.008304 0.413580 730 1988 SEA AL 7342450 0.422360 161 68 300452424 0.024438 0.481481 731 1989 SEA AL 9779500 0.450617 162 73 359995711 0.027166 0.422360 732 1990 SEA AL 12553667 0.475309 162 77 443881193 0.028282 0.450617 733 1991 SEA AL 15691833 0.512346 162 83 613048418 0.025596 0.475309 734 1992 SEA AL 23179833 0.395062 162 64 805543323 0.028775 0.512346 735 1993 SEA AL 32696333 0.506173 162 82 901740134 0.036259 0.395062 736 1994 SEA AL 29228500 0.437500 112 49 927836287 0.031502 0.506173 737 1995 SEA AL 36481311 0.544828 145 79 951469367 0.038342 0.437500 738 1996 SEA AL 41328501 0.527950 161 85 956983550 0.043186 0.544828 739 1997 SEA AL 41540661 0.555556 162 90 1127285885 0.036850 0.527950 740 1998 SEA AL 54087036 0.472050 161 76 1278282871 0.042312 0.555556 741 1999 SEA AL 54125003 0.487654 162 79 1494228750 0.036223 0.472050 742 2000 SEA AL 58915000 0.561728 162 91 1666135102 0.035360 0.487654 743 2001 SEA AL 74720834 0.716049 162 116 1960663313 0.038110 0.561728 744 2002 SEA AL 80282668 0.574074 162 93 2024077522 0.039664 0.716049 745 2003 SEA AL 86959167 0.574074 162 93 2128262128 0.040859 0.574074 746 2004 SEA AL 81515834 0.388889 162 63 2070665943 0.039367 0.574074 747 2005 SEA AL 87754334 0.425926 162 69 2188713398 0.040094 0.388889 748 2006 SEA AL 87959833 0.481481 162 78 2321472617 0.037890 0.425926 749 2007 SEA AL 106460833 0.543210 162 88 2476688987 0.042985 0.481481 750 2008 SEA AL 117666482 0.376543 162 61 2684858670 0.043826 0.543210 751 2009 SEA AL 98904166 0.524691 162 85 2664726994 0.037116 0.376543 752 2010 SEA AL 86510000 0.376543 162 61 2721359865 0.031789 0.524691 753 2011 SEA AL 86110600 0.413580 162 67 2784505291 0.030925 0.376543 754 2012 SEA AL 81978100 0.462963 162 75 2932741192 0.027953 0.413580 755 2013 SEA AL 74005043 0.438272 162 71 3034525648 0.024388 0.462963 756 2014 SEA AL 92531100 0.537037 162 87 3192317623 0.028986 0.438272 757 2015 SEA AL 122208700 0.469136 162 76 3514142569 0.034776 0.537037 758 2016 SEA AL 135683339 0.530864 162 86 3750137392 0.036181 0.469136 759 1985 SFN NL 8221714 0.382716 162 62 261964696 0.031385 NaN 760 1986 SFN NL 8947000 0.512346 162 83 307854518 0.029062 0.382716 761 1987 SFN NL 7290000 0.555556 162 90 272575375 0.026745 0.512346 762 1988 SFN NL 12380000 0.512346 162 83 300452424 0.041205 0.555556 763 1989 SFN NL 14962834 0.567901 162 92 359995711 0.041564 0.512346 764 1990 SFN NL 19335333 0.524691 162 85 443881193 0.043560 0.567901 765 1991 SFN NL 30967666 0.462963 162 75 613048418 0.050514 0.524691 766 1992 SFN NL 33163168 0.444444 162 72 805543323 0.041169 0.462963 767 1993 SFN NL 35050000 0.635802 162 103 901740134 0.038869 0.444444 768 1994 SFN NL 42638666 0.478261 115 55 927836287 0.045955 0.635802 769 1995 SFN NL 36462777 0.465278 144 67 951469367 0.038323 0.478261 770 1996 SFN NL 37144725 0.419753 162 68 956983550 0.038814 0.465278 771 1997 SFN NL 35592378 0.555556 162 90 1127285885 0.031574 0.419753 772 1998 SFN NL 42565834 0.546012 163 89 1278282871 0.033299 0.555556 773 1999 SFN NL 46595057 0.530864 162 86 1494228750 0.031183 0.546012 774 2000 SFN NL 53737826 0.598765 162 97 1666135102 0.032253 0.530864 775 2001 SFN NL 63280167 0.555556 162 90 1960663313 0.032275 0.598765 776 2002 SFN NL 78299835 0.586420 162 95 2024077522 0.038684 0.555556 777 2003 SFN NL 82852167 0.621118 161 100 2128262128 0.038929 0.586420 778 2004 SFN NL 82019166 0.561728 162 91 2070665943 0.039610 0.621118 779 2005 SFN NL 90199500 0.462963 162 75 2188713398 0.041211 0.561728 780 2006 SFN NL 90056419 0.472050 161 76 2321472617 0.038793 0.462963 781 2007 SFN NL 90219056 0.438272 162 71 2476688987 0.036427 0.472050 782 2008 SFN NL 76594500 0.444444 162 72 2684858670 0.028528 0.438272 783 2009 SFN NL 83026450 0.543210 162 88 2664726994 0.031158 0.444444 784 2010 SFN NL 98641333 0.567901 162 92 2721359865 0.036247 0.543210 785 2011 SFN NL 118198333 0.530864 162 86 2784505291 0.042449 0.567901 786 2012 SFN NL 117620683 0.580247 162 94 2932741192 0.040106 0.530864 787 2013 SFN NL 140180334 0.469136 162 76 3034525648 0.046195 0.580247 788 2014 SFN NL 163510167 0.543210 162 88 3192317623 0.051220 0.469136 789 2015 SFN NL 164701500 0.518519 162 84 3514142569 0.046868 0.543210 790 2016 SFN NL 172253778 0.537037 162 87 3750137392 0.045933 0.518519 791 1985 SLN NL 11817083 0.623457 162 101 261964696 0.045109 NaN 792 1986 SLN NL 9875010 0.490683 161 79 307854518 0.032077 0.623457 793 1987 SLN NL 11758000 0.586420 162 95 272575375 0.043137 0.490683 794 1988 SLN NL 12880000 0.469136 162 76 300452424 0.042869 0.586420 795 1989 SLN NL 16078833 0.524390 164 86 359995711 0.044664 0.469136 796 1990 SLN NL 20523334 0.432099 162 70 443881193 0.046236 0.524390 797 1991 SLN NL 21860001 0.518519 162 84 613048418 0.035658 0.432099 798 1992 SLN NL 27583836 0.512346 162 83 805543323 0.034243 0.518519 799 1993 SLN NL 23367334 0.537037 162 87 901740134 0.025914 0.512346 800 1994 SLN NL 29275601 0.460870 115 53 927836287 0.031553 0.537037 801 1995 SLN NL 37101000 0.433566 143 62 951469367 0.038993 0.460870 802 1996 SLN NL 40269667 0.543210 162 88 956983550 0.042080 0.433566 803 1997 SLN NL 45456667 0.450617 162 73 1127285885 0.040324 0.543210 804 1998 SLN NL 54672521 0.509202 163 83 1278282871 0.042770 0.450617 805 1999 SLN NL 49778195 0.465839 161 75 1494228750 0.033314 0.509202 806 2000 SLN NL 61453863 0.586420 162 95 1666135102 0.036884 0.465839 807 2001 SLN NL 78538333 0.574074 162 93 1960663313 0.040057 0.586420 808 2002 SLN NL 74660875 0.598765 162 97 2024077522 0.036886 0.574074 809 2003 SLN NL 83786666 0.524691 162 85 2128262128 0.039369 0.598765 810 2004 SLN NL 83228333 0.648148 162 105 2070665943 0.040194 0.524691 811 2005 SLN NL 92106833 0.617284 162 100 2188713398 0.042083 0.648148 812 2006 SLN NL 88891371 0.515528 161 83 2321472617 0.038291 0.617284 813 2007 SLN NL 90286823 0.481481 162 78 2476688987 0.036455 0.515528 814 2008 SLN NL 99624449 0.530864 162 86 2684858670 0.037106 0.481481 815 2009 SLN NL 88528409 0.561728 162 91 2664726994 0.033222 0.530864 816 2010 SLN NL 93540751 0.530864 162 86 2721359865 0.034373 0.561728 817 2011 SLN NL 105433572 0.555556 162 90 2784505291 0.037864 0.530864 818 2012 SLN NL 110300862 0.543210 162 88 2932741192 0.037610 0.555556 819 2013 SLN NL 92260110 0.598765 162 97 3034525648 0.030403 0.543210 820 2014 SLN NL 120693000 0.555556 162 90 3192317623 0.037807 0.598765 821 2015 SLN NL 119241500 0.617284 162 100 3514142569 0.033932 0.555556 822 2016 SLN NL 143053500 0.530864 162 86 3750137392 0.038146 0.617284 823 1998 TBA AL 27280000 0.388889 162 63 1278282871 0.021341 NaN 824 1999 TBA AL 38870000 0.425926 162 69 1494228750 0.026013 0.388889 825 2000 TBA AL 62765129 0.428571 161 69 1666135102 0.037671 0.425926 826 2001 TBA AL 56980000 0.382716 162 62 1960663313 0.029062 0.428571 827 2002 TBA AL 34380000 0.341615 161 55 2024077522 0.016986 0.382716 828 2003 TBA AL 19630000 0.388889 162 63 2128262128 0.009223 0.341615 829 2004 TBA AL 29556667 0.434783 161 70 2070665943 0.014274 0.388889 830 2005 TBA AL 29679067 0.413580 162 67 2188713398 0.013560 0.434783 831 2006 TBA AL 34917967 0.376543 162 61 2321472617 0.015041 0.413580 832 2007 TBA AL 24123500 0.407407 162 66 2476688987 0.009740 0.376543 833 2008 TBA AL 43820597 0.598765 162 97 2684858670 0.016321 0.407407 834 2009 TBA AL 63313034 0.518519 162 84 2664726994 0.023760 0.598765 835 2010 TBA AL 71923471 0.592593 162 96 2721359865 0.026429 0.518519 836 2011 TBA AL 41053571 0.561728 162 91 2784505291 0.014744 0.592593 837 2012 TBA AL 64173500 0.555556 162 90 2932741192 0.021882 0.561728 838 2013 TBA AL 52955272 0.564417 163 92 3034525648 0.017451 0.555556 839 2014 TBA AL 72689100 0.475309 162 77 3192317623 0.022770 0.564417 840 2015 TBA AL 64521233 0.493827 162 80 3514142569 0.018360 0.475309 841 2016 TBA AL 57097310 0.419753 162 68 3750137392 0.015225 0.493827 842 1985 TEX AL 7676500 0.385093 161 62 261964696 0.029304 NaN 843 1986 TEX AL 6743119 0.537037 162 87 307854518 0.021904 0.385093 844 1987 TEX AL 880000 0.462963 162 75 272575375 0.003228 0.537037 845 1988 TEX AL 5342131 0.434783 161 70 300452424 0.017780 0.462963 846 1989 TEX AL 11893781 0.512346 162 83 359995711 0.033039 0.434783 847 1990 TEX AL 14874372 0.512346 162 83 443881193 0.033510 0.512346 848 1991 TEX AL 18224500 0.524691 162 85 613048418 0.029728 0.512346 849 1992 TEX AL 30128167 0.475309 162 77 805543323 0.037401 0.524691 850 1993 TEX AL 36376959 0.530864 162 86 901740134 0.040341 0.475309 851 1994 TEX AL 32973597 0.456140 114 52 927836287 0.035538 0.530864 852 1995 TEX AL 34581451 0.513889 144 74 951469367 0.036345 0.456140 853 1996 TEX AL 39041528 0.552147 163 90 956983550 0.040796 0.513889 854 1997 TEX AL 53448838 0.475309 162 77 1127285885 0.047414 0.552147 855 1998 TEX AL 56572095 0.543210 162 88 1278282871 0.044256 0.475309 856 1999 TEX AL 76709931 0.586420 162 95 1494228750 0.051337 0.543210 857 2000 TEX AL 70795921 0.438272 162 71 1666135102 0.042491 0.586420 858 2001 TEX AL 88633500 0.450617 162 73 1960663313 0.045206 0.438272 859 2002 TEX AL 105526122 0.444444 162 72 2024077522 0.052135 0.450617 860 2003 TEX AL 103491667 0.438272 162 71 2128262128 0.048627 0.444444 861 2004 TEX AL 55050417 0.549383 162 89 2070665943 0.026586 0.438272 862 2005 TEX AL 55849000 0.487654 162 79 2188713398 0.025517 0.549383 863 2006 TEX AL 68228662 0.493827 162 80 2321472617 0.029390 0.487654 864 2007 TEX AL 68318675 0.462963 162 75 2476688987 0.027585 0.493827 865 2008 TEX AL 67712326 0.487654 162 79 2684858670 0.025220 0.462963 866 2009 TEX AL 68178798 0.537037 162 87 2664726994 0.025586 0.487654 867 2010 TEX AL 55250544 0.555556 162 90 2721359865 0.020303 0.537037 868 2011 TEX AL 92299264 0.592593 162 96 2784505291 0.033147 0.555556 869 2012 TEX AL 120510974 0.574074 162 93 2932741192 0.041092 0.592593 870 2013 TEX AL 112522600 0.558282 163 91 3034525648 0.037081 0.574074 871 2014 TEX AL 112255059 0.413580 162 67 3192317623 0.035164 0.558282 872 2015 TEX AL 143742789 0.543210 162 88 3514142569 0.040904 0.413580 873 2016 TEX AL 176038723 0.586420 162 95 3750137392 0.046942 0.543210 874 1985 TOR AL 8812550 0.614907 161 99 261964696 0.033640 NaN 875 1986 TOR AL 12611047 0.527607 163 86 307854518 0.040964 0.614907 876 1987 TOR AL 10479501 0.592593 162 96 272575375 0.038446 0.527607 877 1988 TOR AL 12241225 0.537037 162 87 300452424 0.040743 0.592593 878 1989 TOR AL 16261666 0.549383 162 89 359995711 0.045172 0.537037 879 1990 TOR AL 17756834 0.530864 162 86 443881193 0.040004 0.549383 880 1991 TOR AL 19902417 0.561728 162 91 613048418 0.032465 0.530864 881 1992 TOR AL 44788666 0.592593 162 96 805543323 0.055601 0.561728 882 1993 TOR AL 47279166 0.586420 162 95 901740134 0.052431 0.592593 883 1994 TOR AL 43433668 0.478261 115 55 927836287 0.046812 0.586420 884 1995 TOR AL 50590000 0.388889 144 56 951469367 0.053170 0.478261 885 1996 TOR AL 29555083 0.456790 162 74 956983550 0.030884 0.388889 886 1997 TOR AL 47079833 0.469136 162 76 1127285885 0.041764 0.456790 887 1998 TOR AL 51376000 0.539877 163 88 1278282871 0.040191 0.469136 888 1999 TOR AL 45444333 0.518519 162 84 1494228750 0.030413 0.539877 889 2000 TOR AL 44838332 0.512346 162 83 1666135102 0.026912 0.518519 890 2001 TOR AL 76895999 0.493827 162 80 1960663313 0.039219 0.512346 891 2002 TOR AL 76864333 0.481481 162 78 2024077522 0.037975 0.493827 892 2003 TOR AL 51269000 0.530864 162 86 2128262128 0.024090 0.481481 893 2004 TOR AL 50017000 0.416149 161 67 2070665943 0.024155 0.530864 894 2005 TOR AL 45719500 0.493827 162 80 2188713398 0.020889 0.416149 895 2006 TOR AL 71365000 0.537037 162 87 2321472617 0.030741 0.493827 896 2007 TOR AL 81942800 0.512346 162 83 2476688987 0.033086 0.537037 897 2008 TOR AL 97793900 0.530864 162 86 2684858670 0.036424 0.512346 898 2009 TOR AL 80538300 0.462963 162 75 2664726994 0.030224 0.530864 899 2010 TOR AL 62234000 0.524691 162 85 2721359865 0.022869 0.462963 900 2011 TOR AL 62567800 0.500000 162 81 2784505291 0.022470 0.524691 901 2012 TOR AL 75009200 0.450617 162 73 2932741192 0.025576 0.500000 902 2013 TOR AL 126288100 0.456790 162 74 3034525648 0.041617 0.450617 903 2014 TOR AL 109920100 0.512346 162 83 3192317623 0.034433 0.456790 904 2015 TOR AL 112992400 0.574074 162 93 3514142569 0.032154 0.512346 905 2016 TOR AL 138701700 0.549383 162 89 3750137392 0.036986 0.574074 906 2005 WAS NL 48581500 0.500000 162 81 2188713398 0.022196 NaN 907 2006 WAS NL 63143000 0.438272 162 71 2321472617 0.027200 0.500000 908 2007 WAS NL 36947500 0.450617 162 73 2476688987 0.014918 0.438272 909 2008 WAS NL 54961000 0.366460 161 59 2684858670 0.020471 0.450617 910 2009 WAS NL 59928000 0.364198 162 59 2664726994 0.022489 0.366460 911 2010 WAS NL 61400000 0.425926 162 69 2721359865 0.022562 0.364198 912 2011 WAS NL 63856928 0.496894 161 80 2784505291 0.022933 0.425926 913 2012 WAS NL 80855143 0.604938 162 98 2932741192 0.027570 0.496894 914 2013 WAS NL 113703270 0.530864 162 86 3034525648 0.037470 0.604938 915 2014 WAS NL 131983680 0.592593 162 96 3192317623 0.041344 0.530864 916 2015 WAS NL 155587472 0.512346 162 83 3514142569 0.044275 0.592593 917 2016 WAS NL 141652646 0.586420 162 95 3750137392 0.037773 0.512346 <p>We now run our regression again, but adding wpc_lag into the regression equation:</p> In\u00a0[15]: Copied! <pre>wpcsal2_lm = smf.ols(formula = 'wpc ~wpc_lag + relsal', data=MLB).fit()\nprint(wpcsal2_lm.summary())\n</pre> wpcsal2_lm = smf.ols(formula = 'wpc ~wpc_lag + relsal', data=MLB).fit() print(wpcsal2_lm.summary()) <pre>                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    wpc   R-squared:                       0.234\nModel:                            OLS   Adj. R-squared:                  0.233\nMethod:                 Least Squares   F-statistic:                     134.6\nDate:                Fri, 11 Jun 2021   Prob (F-statistic):           9.68e-52\nTime:                        16:58:38   Log-Likelihood:                 1235.0\nNo. Observations:                 883   AIC:                            -2464.\nDf Residuals:                     880   BIC:                            -2450.\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.2839      0.015     19.093      0.000       0.255       0.313\nwpc_lag        0.3614      0.033     10.840      0.000       0.296       0.427\nrelsal         1.0259      0.182      5.641      0.000       0.669       1.383\n==============================================================================\nOmnibus:                        1.749   Durbin-Watson:                   2.023\nProb(Omnibus):                  0.417   Jarque-Bera (JB):                1.704\nSkew:                          -0.048   Prob(JB):                        0.426\nKurtosis:                       2.807   Cond. No.                         101.\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n</pre> <p>The lagged dependent variable here is much smaller than it was in the case of the NBA (0.6), which implies that last year's performance matters much less in determining this year's performance. There could be several reasons for this, e,g, greater player turnover in MLB, or a lower probability that player's from last year will be repeated in the current year.</p> <p>As was the case with the NBA, the addition of the lagged dependent variable has reduced the size of the coefficient for relsal, halving it, but still this is not as dramatic as the reduction in the NBA case, where the variable also became statistically insignificant, which is not the case here. The R-squared has not risen as much either.</p> <p>Overall, however, we can conclude that adding the lagged dependent variable has reduced the possibility of omitted variable bias.</p> <p>Now we add the fixed effects to the regression:</p> In\u00a0[16]: Copied! <pre>wpcsal3_lm = smf.ols(formula = 'wpc ~wpc_lag + relsal +C(Team)', data=MLB).fit()\nprint(wpcsal3_lm.summary())\n</pre> wpcsal3_lm = smf.ols(formula = 'wpc ~wpc_lag + relsal +C(Team)', data=MLB).fit() print(wpcsal3_lm.summary()) <pre>                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    wpc   R-squared:                       0.263\nModel:                            OLS   Adj. R-squared:                  0.232\nMethod:                 Least Squares   F-statistic:                     8.401\nDate:                Fri, 11 Jun 2021   Prob (F-statistic):           2.02e-36\nTime:                        16:58:38   Log-Likelihood:                 1252.0\nNo. Observations:                 883   AIC:                            -2430.\nDf Residuals:                     846   BIC:                            -2253.\nDf Model:                          36                                         \nCovariance Type:            nonrobust                                         \n==================================================================================\n                     coef    std err          t      P&gt;|t|      [0.025      0.975]\n----------------------------------------------------------------------------------\nIntercept          0.3224      0.028     11.637      0.000       0.268       0.377\nC(Team)[T.ARI]    -0.0112      0.027     -0.421      0.674      -0.064       0.041\nC(Team)[T.ATL]     0.0108      0.025      0.429      0.668      -0.039       0.060\nC(Team)[T.BAL]    -0.0282      0.025     -1.124      0.261      -0.077       0.021\nC(Team)[T.BOS]     0.0034      0.025      0.136      0.892      -0.046       0.053\nC(Team)[T.CAL]    -0.0304      0.029     -1.049      0.294      -0.087       0.026\nC(Team)[T.CHA]    -0.0064      0.025     -0.254      0.800      -0.056       0.043\nC(Team)[T.CHN]    -0.0224      0.025     -0.891      0.373      -0.072       0.027\nC(Team)[T.CIN]    -0.0116      0.025     -0.464      0.643      -0.061       0.038\nC(Team)[T.CLE]     0.0007      0.025      0.028      0.978      -0.049       0.050\nC(Team)[T.COL]    -0.0275      0.026     -1.062      0.288      -0.078       0.023\nC(Team)[T.DET]    -0.0275      0.025     -1.096      0.273      -0.077       0.022\nC(Team)[T.FLO]    -0.0104      0.027     -0.387      0.699      -0.063       0.042\nC(Team)[T.HOU]    -0.0087      0.025     -0.346      0.730      -0.058       0.041\nC(Team)[T.KCA]    -0.0314      0.025     -1.252      0.211      -0.081       0.018\nC(Team)[T.LAA]     0.0095      0.029      0.326      0.744      -0.047       0.066\nC(Team)[T.LAN]    -0.0064      0.025     -0.254      0.800      -0.056       0.043\nC(Team)[T.MIA]    -0.0252      0.038     -0.667      0.505      -0.099       0.049\nC(Team)[T.MIL]    -0.0234      0.027     -0.875      0.382      -0.076       0.029\nC(Team)[T.MIN]    -0.0152      0.025     -0.604      0.546      -0.064       0.034\nC(Team)[T.ML4]    -0.0086      0.028     -0.302      0.762      -0.065       0.047\nC(Team)[T.MON]    -0.0063      0.027     -0.235      0.814      -0.059       0.046\nC(Team)[T.NYA]     0.0059      0.026      0.229      0.819      -0.045       0.057\nC(Team)[T.NYN]    -0.0113      0.025     -0.448      0.654      -0.061       0.038\nC(Team)[T.OAK]     0.0100      0.025      0.398      0.691      -0.039       0.059\nC(Team)[T.PHI]    -0.0183      0.025     -0.730      0.466      -0.068       0.031\nC(Team)[T.PIT]    -0.0202      0.025     -0.801      0.423      -0.070       0.029\nC(Team)[T.SDN]    -0.0209      0.025     -0.831      0.406      -0.070       0.028\nC(Team)[T.SEA]    -0.0178      0.025     -0.712      0.477      -0.067       0.031\nC(Team)[T.SFN]     0.0041      0.025      0.164      0.870      -0.045       0.053\nC(Team)[T.SLN]     0.0084      0.025      0.334      0.738      -0.041       0.058\nC(Team)[T.TBA]    -0.0198      0.027     -0.735      0.462      -0.073       0.033\nC(Team)[T.TEX]    -0.0032      0.025     -0.128      0.898      -0.052       0.046\nC(Team)[T.TOR]    -0.0038      0.025     -0.152      0.879      -0.053       0.045\nC(Team)[T.WAS]    -0.0110      0.029     -0.378      0.706      -0.068       0.046\nwpc_lag            0.3141      0.035      8.980      0.000       0.245       0.383\nrelsal             0.8908      0.247      3.609      0.000       0.406       1.375\n==============================================================================\nOmnibus:                        0.852   Durbin-Watson:                   1.992\nProb(Omnibus):                  0.653   Jarque-Bera (JB):                0.936\nSkew:                          -0.048   Prob(JB):                        0.626\nKurtosis:                       2.873   Cond. No.                         139.\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n</pre> <p>The result here is a very sharp contrast to the NBA model, where a number of the fixed effects were statistically significant; for MLB, none of them are.</p> <p>When you add variables that are not statistically significant, it is logical that the R-squared will not go up very much, since you are not explaining very much. That is the case here, where the R-squared increases to only 0.26.</p> <p>You may have noticed that under the R-squared is \"Adj. R-squared\" - where \"adj.\" is short for \"adjusted\". This is useful to consider in this case. A simple fact about regression is that when you add variables, no matter if they are irrelevant, then you will increase the unadjusted R-squared. This is a consequence of the underlying algebra. We are trying to reproduce the relationship between a set of points, using a linear model, which is just an equation that produces another set of points. The closer the two sets of points, the better the model. But in the end, we could reproduce the original set of points by copying them - and in the algebra of regression this would mean providing a separate variable for each point. For example, in this regression we have 883 observations - and so if we had 883 variables in our regression we would fit the data exactly and the R-squared would be 1.0! Note that this would be true even if the variables had no logical connection with our data. The upshot of this is that adding variables increases R-squared, regardless of whether the variables really explain the data any better. Adjusted R-squared is an attempt to compensate for this effect, by reducing the value of R-squared as the number of variables in the regression increases. If the variables are statistically significant, then adjusted R-squared can still increase, but in this case we can see that with the addition of the fixed effects, adjusted R-squared has in fact fallen from 0.233 to 0.232. This is a strong suggestion that we should ignore the fixed effects.</p> <p>The conclusion of this is that our second model, with just relsal and the lagged dependent variable, was our best model.</p> <p>What is the impact of spending and performance in this model?</p> <p>Our preferred regression model is wpc(t) = 0.284 + 0.361 x wpc(t-1) + 1.026 x relsal (t), where t refers to the season.</p> <p>To work out the impact of relsal we need to eliminate the the lagged dependent variable from the equation, which we do by assuming a \"steady state\"- where wpc(t) = wpc(t-1). If this were the case then we would have</p> <p>wpc = 1/(1-0.361) x (0.284 + 1.026 x relsal)</p> <p>We can then work out these values of win percentage for very low relsal (0.01), average relsal (0.035) and very high relsal (0.06):</p> In\u00a0[17]: Copied! <pre>print(1/(1-0.361)*(0.284 + 1.026*.01))\nprint(1/(1-0.361)*(0.284 + 1.026*.035))\nprint(1/(1-0.361)*(0.284 + 1.026*.06))\n</pre> print(1/(1-0.361)*(0.284 + 1.026*.01)) print(1/(1-0.361)*(0.284 + 1.026*.035)) print(1/(1-0.361)*(0.284 + 1.026*.06))  <pre>0.4605007824726134\n0.500641627543036\n0.5407824726134585\n</pre> <p>The results suggest that while it is possible to buy success in MLB by increasing spending relative to your competitors, it is not that easy to do so. Even the very highest spending does not deliver a dominant performance. This might be a disappointment for those who think markets ought to work perfectly, but on the other hand, we would suggest, this is good news for baseball fans.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/05.03/#salaries-and-performance-in-major-league-baseball","title":"Salaries and Performance in Major League Baseball\u00b6","text":"<p>We might expect that the salary performance relationship in baseball will be more like the NBA than the EPL, given that the organizational structure has many similarities with the NBA.</p> <p>We follow the same steps as we did for both those leagues.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/05.03/#self-test","title":"Self test\u00b6","text":"<p>Based on this model, what would be the win percentage of a team for whom the value of relsal was 4%?</p> <p>Recall that we asked the same question when looking at the NBA. Compare you two answers. What do you think explains the difference?</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/05.03/#self-test","title":"Self test\u00b6","text":"<p>The model implies that win percentage of a team in year t, wpc(t) = 0.2839 +0.3614 x wpc_lag + 1.0259 x relsal</p> <p>Suppose relsal is 4% (0.04), calculate the value of wpc(t) if wpc(t-1) equals (a) 0.6 and (b) 0.4. How do you account for your answer?</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/05.03/#self-test","title":"Self test\u00b6","text":"<p>Suppose, as for the NBA, the value of the lagged dependent variable was 0.6. Use that value instead of 0.361 in the above equations. What difference does it make? Can you explain why?</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/05.03/#conclusion","title":"Conclusion\u00b6","text":"<p>The case of MLB has much more in common with the NBA than the EPL because of similarities of the league systems. We ran essentially the same models as we did for the NBA, but we also identified a number of differences. Comparing with the NBA, we found that the lagged dependent variable was less important and all of the fixed effects were insignficant. Given our main focus was on relsal, we found that in MLB win percentage was notably less sensitive the relative wage spending than the NBA.</p> <p>We conclude this week by looking at one more league that operates under the North American model, the National Hockey League (NHL).</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/05.04/","title":"Explaining Relationships Using Regression Analysis","text":"In\u00a0[\u00a0]: Copied! <pre># As usual, we begin by loading the packages we will need\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.formula.api as smf\n</pre> # As usual, we begin by loading the packages we will need  import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import statsmodels.formula.api as smf In\u00a0[\u00a0]: Copied! <pre># Now we load the data\n\nNBA=pd.read_excel(\"../../Data/Week 5/NBA pay and performance.xlsx\")\n</pre> # Now we load the data  NBA=pd.read_excel(\"../../Data/Week 5/NBA pay and performance.xlsx\") <p>We use .describe() to look at the summary statistics for the data. From this we can see that we have 210 observations, for teams running from 2012 to 2018 (7 seasons). Our two variables of interest are win percentage and team salaries. We can also use .info() to summarize the dataframe.</p> In\u00a0[\u00a0]: Copied! <pre>NBA.describe()\n</pre> NBA.describe() In\u00a0[\u00a0]: Copied! <pre>NBA.info()\n</pre> NBA.info() <p>If salaries reflect player ability, then the the success of a team should depend on how much more or less it pays than its rivals. However, if we look at salaries paid out in different seasons, there is clearly inflation in player salaries. We can see this if we use the .groupby() command to look at total salaries over the seven seasons:</p> In\u00a0[\u00a0]: Copied! <pre>Sumsal = NBA.groupby(['season'])['salaries'].sum().reset_index().rename(columns={'salaries':'allsal'})\nSumsal\n</pre> Sumsal = NBA.groupby(['season'])['salaries'].sum().reset_index().rename(columns={'salaries':'allsal'}) Sumsal <p>We can see that salaries in 2018 were more than double the level in 2012, and increased consistently from year to year. This does not imply that the players were gettign better from season to season. Rather, this is a reflection of the growing revenues of the NBA, and the ability of players to bargain for a more or less constant share of this growing revenue.</p> <p>So, if we now want to account for team performance in terms of salaries, we need to make sure we compare like with like. What $1 million would buy in 2012 was not the same as what it would buy in 2018. It's easy to adjust for this. We simply divide the salary of each team in each season by the total spending of all teams in that season, so that we have a measure of salary spending relative to the competition.</p> <p>To do this we first use pd.merge() to add the aggregate salaries for each season to our original dataframe:</p> In\u00a0[\u00a0]: Copied! <pre>NBA = pd.merge(NBA, Sumsal, on=['season'], how='left')\ndisplay(NBA)\n</pre> NBA = pd.merge(NBA, Sumsal, on=['season'], how='left') display(NBA) <p>We can now create a variable which we call 'relsal', which measures the share of team's salary spend in the total spending of all teams in that season:</p> In\u00a0[\u00a0]: Copied! <pre>NBA['relsal']= NBA['salaries']/NBA['allsal']\n</pre> NBA['relsal']= NBA['salaries']/NBA['allsal'] <p>Before running a regression, it makes sense to look at the relationship between salaries and win percentage on a chart. To do this we use sns.reglot(). Since our argument is that higher relative salaries mean better players which in turns leads to more wins, we put relsal on the x axis and wpc on the y axis.</p> In\u00a0[\u00a0]: Copied! <pre>sns.regplot(x=\"relsal\", y=\"wpc\", data = NBA, ci=False)\n</pre> sns.regplot(x=\"relsal\", y=\"wpc\", data = NBA, ci=False) <p>It's clear from the data that there is a positive correlation between relsal and wpc, as shown by the regression line which regplot adds to the scatter diagram. We now run a regression using smf.ols() in order to derive the coefficients of the regression and other diagnostic statistics.</p> In\u00a0[\u00a0]: Copied! <pre>wpcsal1_lm = smf.ols(formula = 'wpc ~ relsal', data=NBA).fit()\nprint(wpcsal1_lm.summary())\n</pre> wpcsal1_lm = smf.ols(formula = 'wpc ~ relsal', data=NBA).fit() print(wpcsal1_lm.summary()) <p>The first things to look at in any regression are the coefficients of the explanatory variables and their statistical significance. We can see here that the coefficient on relsal is 11.3009. This means that every one percentage (0.01) increase in the share of the team in total salaries leads to and 11.3 x .01 increase in win percentage - that is roughly .11 , or eleven percentage points. That is a very large increase, but not that the share of salaries, as can be seen on the x axis of the chart above, ranges from around .02 (2%) to .05 (5%). Thus going from the lowest salary to the highest share (from 2% to 5%) will produce a 3 x .11 = .33 increase in win percentage - from around roughly 33% to 66%.</p> <p>This estimate is statistically significant. The coefficient estimate is more than six times larger than its standard error (this ratio is called the t- statistic (6.577). The p-value (P&gt;|t|) tells us the probability of observing such estimate if the true value were actually zero. The p-value here is shown as \"0.000\" - however, it can never be exactly zero. It is just that, in this case, it is so small that is does not register up to three decmial places. The usual standard for statistical significance is a p-value below 0.05. Clearly, in this case the estimate clearly beats that standard.</p> <p>How much of the variation in team performance is captured by this relsal? We can see this from the R-squared coefficient which is 0.172, or 17.2%. Clearly, there is much else to team performance than salaries alone.</p> <p>Is the coefficient estimate plausible? Our regression estimate is the best estimate we have of the effect, but only under the assumption that our regression includes all of the relevant variables. If there are other variables which influence performance other than salary share, then our regression estimate will be biased, either upward or downward. This is the problem known as \"omitted variable bias\" (OVB). There is no way to be certain that OVB is not a problem, it requires good judgment and careful thought to decide on whether there are other variables that should be included.</p> <p>The fact that the R-squared value was only 0.172 might give one to think that there are other factors to include, but it is always possible that the remainder is just random - the effect of luck, which no doubt plays a role in every game.</p> <p>But in this case, it is possible to think of other factors that might be relevant and therefore should be included. One such is \"lagged dependent variable\", which here means the value of win percentage in the previous season. While the salary level should capture many aspects of team quality, salaries are not renegotiated every year, and many aspects of team quality would have been in pace in the previous season. So we can add this lagged dependent variable to our regression, and then see if this changes our estimate of the impact of salaries.</p> <p>We create this variable in two stages. First, we sort the data by team, and then by season, using .sort_values()</p> In\u00a0[\u00a0]: Copied! <pre>NBA.sort_values(by=['Team','season'], ascending=True)\n</pre> NBA.sort_values(by=['Team','season'], ascending=True) <p>Second, we create the lagged value of wpc. This done by using .groupby() together with .shift(1). The value 1 in .shift() signifies the value is for the previous season that is being added to each row. If we used .shift(2) it would add the value from 2 seasons before, and so on.</p> <p>Note that because we define the lagged value to apply to each team, there is a missing value (NaN) for each team in 2012, which is the first year in our data.</p> In\u00a0[\u00a0]: Copied! <pre>NBA['wpc_lag'] = NBA.groupby('Team')['wpc'].shift(1)\nNBA\n</pre> NBA['wpc_lag'] = NBA.groupby('Team')['wpc'].shift(1) NBA In\u00a0[\u00a0]: Copied! <pre># this command allows us to see all rows in the window\n\npd.set_option('display.max_rows', 250)\nNBA\n</pre> # this command allows us to see all rows in the window  pd.set_option('display.max_rows', 250) NBA <p>We now run our regression again, but adding wpc_lag into the regression equation:</p> In\u00a0[\u00a0]: Copied! <pre>wpcsal2_lm = smf.ols(formula = 'wpc ~wpc_lag + relsal', data=NBA).fit()\nprint(wpcsal2_lm.summary())\n</pre> wpcsal2_lm = smf.ols(formula = 'wpc ~wpc_lag + relsal', data=NBA).fit() print(wpcsal2_lm.summary()) <p>The result of this change is quite dramatic. Not only has the coefficient on relsal fallen to only 2.0165, but it is not statistically insignificant (p_value is 0.28, greater than the critical value of .05). Last year's win percentage is highly significant and the R-squared of the regression has risen to 0.416.</p> <p>It is quite usual for the lagged dependent variable to be significant in situations such as these- history matters. But should we now abandon our theory that wages influence performance? One should also be cautious about a theory that relies only on the lagged dependent variable - while history matters, one should also expect there to be specific, quantifiable factors that drive history.</p> <p>While the omission of the lagged dependent variable in the first regression suggests that there may have been OVB that biased the estimate upwards, there is also the possibility that OVB can bias the estimate downwards.</p> <p>Clearly, not all teams are identical, while our regression estimates thus far treat each team as if it were, in the sense that spending would affect the performance of each team in the same way, and that last year's win percentage would imact this year's in the same way. In our regression specification we want to find balance between treating each team as if it were identical, and treating each team as if it were completely unique. The truth is likely to be that there are common factors affecting all teams, but that there are also idiosyncrasies. This is often described as heterogeneity.</p> <p>One way we can introduce heterogeneity is through fixed effects. Fixed effects are dummy variables. For each team there is a fixed effect, equal to one if the row relates to the team in question, and zero otherwise. Each team can have its own fixed effects. Estimation of fixed effects allows us to identify differences between the teams that are independent of the impact of salaries or of the lagged dependent variable.</p> <p>Adding fixed effects is very easy in Python. The variable 'Team' identifies the team names, and if we add \"C(Team)\" to the regression formula Python will estimate a fixed effect for each team.</p> <p>We now run the regression with the lagged dependent variable and fixed effects, to see what impact this has on the estimate of the salary coefficient.</p> In\u00a0[\u00a0]: Copied! <pre>wpcsal3_lm = smf.ols(formula = 'wpc ~ wpc_lag + relsal +C(Team)', data=NBA).fit()\nprint(wpcsal3_lm.summary())\n</pre> wpcsal3_lm = smf.ols(formula = 'wpc ~ wpc_lag + relsal +C(Team)', data=NBA).fit() print(wpcsal3_lm.summary()) <p>There are 30 fixed effects listed in the output, one for each team. A positive fixed effect means that in some way the team was able to perform above average, and a negative fixed effect implies below average performance. However, most of the fixed effects are not statistically significant. The significant fixed effects are: The Brooklyn Nets (negative), the Golden State Warriors (positive), The LA Lakers (negative), Orlando Magic (negative), San Antonio Spurs (positive).</p> <p>Looking at our main variable of interest, relsal, it is clear that this variable is once again statistically significant with the addition of the fixed effects. Thus, we might conclude that the absence of the fixed effects biased the coefficient estimate downwards. The coefficient is statistically significant at the 5% level, and the value of 4.9388 implies that increasing salary share in the NBA by 1 percentage point (e.g. from 2% to 3%) will lead to an increase in win percentage of almost 5%. This is smaller than our original estimate, but also because of the presence of the lagged dependent variable, spending which increases win percentage this season will also have an effect, albeit a smaller one, on the following season. Indeed, an increase in spending today will create a ripple effect which will be discernible in performance for a number of years into the future.</p> <p>Also note that the size of the lagged dependent variable is smaller once we add the fixed effects. Finally, with this third specification the R-squared of the regression has now risen to 0.585 (close to 60%), which accounts for significant fraction of the overall variation.</p> <p>We should never expect to explain 100% of the variation of outcomes in sport - if we could do that then each game would be perfectly predictable - and then what would be the point of watching?</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/05.04/#explaining-relationships-using-regression-analysis","title":"Explaining Relationships Using Regression Analysis\u00b6","text":"<p>A regression model can be used for two different purposes. It can be used to explain how one variable (y) depends on another variable (x). Indeed, here we are often trying to identify causal inferences, to explain how a system works. It can also be used to make forecasts about future outcomes.</p> <p>During this week we are going to focus on the first purpose, thinking about potential causal links in the performance of professional sports teams. We are going to focus on the performance of teams across an entire season, in terms of either win percentage or league position.</p> <p>The main input to any team is, of course, the players themselves. Teams compete to hire the best players and player agents seek to find the best financial deal for their clients. It is reasonable to expect, therefore, that team expenditure on player salaries should be an important factor in determining team performance. To be clear, the logic of this is NOT that paying higher salaries will make players perform better. At the top level of professional sports, all players are highly motivated, and salary probably does not play a significant motivational role. Rather, competition for players means that salaries are likely to reflect relative abilities. Better players command higher salaries, and as a result the aggregate pay of players on the team is likely to be a good predictor of team performance.</p> <p>There are a number of sources for player salary data. In the North American major leagues, salary negotiations are framed by collective bargaining agreements with player unions, which often publish individual player salary data. In European soccer leagues, aggregate salary data is to be found in audited financial statements of professional clubs, which are often available to the public (notably in England). Cricket players in the Indian Premier League have their salaries determined in a public auction.</p> <p>This week we are going to examine the wage-performance relationship in four different leagues - the NBA, the English Premier League, Major League Baseball and the National Hockey League. While our focus is on the role of salaries, we will also consider other factors that might be relevant, which will help us to think about some of the issues that arise in regression analysis.</p> <p>We start with the NBA.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/05.04/#self-test","title":"Self test\u00b6","text":"<p>Based on this model, what would be the win percentage of a team for whom the value of relsal was 4%?</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/05.04/#self-test","title":"Self test\u00b6","text":"<p>Based on this model, what would be the win percentage of a team that (a) had 0.5 win percentage last season and (b) had a value of relsal equal to 3%?</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/05.04/#self-test","title":"Self test\u00b6","text":"<p>Run the regression of win percentage on relsal with fixed effects but without the lagged depedent variable. Compare your output results. Compare this to the previous three regressions. Which do you think is the best representation of the data. Why?</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/05.04/#conclusion","title":"Conclusion\u00b6","text":"<p>In this notebook we have explored the possibility of using regression analysis to explore the validity of a causal explanation of team success. That causal explanation was itself not derived from the data, but based on a theory that player quality will be reflected in salaries and therefore salaries will predict team success.</p> <p>You might be wondering about why this works at all with the NBA, since the league operates a salary cap system which is intended to equalize resources among the teams. If each team spent the same amount of money on players, our theory predicts that each team can expect to win 50% of its games, and team performances will vary randomly around this mean. However, the NBA cap is a \"soft cap\", meaning that there are many exemptions, so teams spend varying amounts in reality. Some leagues, such as the NFL, operate a hard cap, which strictly prohibit spending above the cap. The NFL also has a salary floor, which prevents teams from spending a lot less than average. When looking at NFL data, therefore, it is much harder to identifty the effect of wage spending on performance.</p> <p>We next turn to look at the salary performance relationship in the English Premier League.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.01/","title":"06.01","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport numpy as np\n\nShotlog=pd.read_csv(\"../../Data/Week 6/Shotlog_16_17.csv\")\nShotlog.head()\n</pre> import pandas as pd import numpy as np  Shotlog=pd.read_csv(\"../../Data/Week 6/Shotlog_16_17.csv\") Shotlog.head() In\u00a0[\u00a0]: Copied! <pre>Shotlog.shape\n</pre> Shotlog.shape In\u00a0[\u00a0]: Copied! <pre>Shotlog.info()\n</pre> Shotlog.info() In\u00a0[\u00a0]: Copied! <pre>Shotlog['current_shot_hit'] = np.where(Shotlog['current_shot_outcome']==\"SCORED\", 1, 0)\nShotlog.head()\n</pre> Shotlog['current_shot_hit'] = np.where(Shotlog['current_shot_outcome']==\"SCORED\", 1, 0) Shotlog.head() <ul> <li>Make sure the variable \"date\" is stored as a date type variable.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>import datetime as dt\nShotlog['date']=pd.to_datetime(Shotlog['date'])\n</pre> import datetime as dt Shotlog['date']=pd.to_datetime(Shotlog['date']) <ul> <li>Convert the variable \"time\" to be datetime type variable</li> </ul> <ol> <li>We will first add the hour (00) to the time variable since the time variable will be stored in the format 'HH:MM:SS';</li> <li>We will use \"to_timedelta\" to work with variable with only time information.</li> </ol> In\u00a0[\u00a0]: Copied! <pre>Shotlog['time'] = pd.to_timedelta('00:'+ Shotlog['time'])\nShotlog['time'].describe()\n</pre> Shotlog['time'] = pd.to_timedelta('00:'+ Shotlog['time']) Shotlog['time'].describe() <ul> <li>Create lagged variable to indicate the result of the previous shot by the same player in the same game.</li> </ul> <ol> <li>We will first sort the shot outcome by the quarter and time in the game;</li> <li>We will group the data by player and game (date) and use the \"shift\" command to create a lag variable.</li> </ol> In\u00a0[\u00a0]: Copied! <pre>Shotlog['lag_shot_hit']=Shotlog.sort_values(by=['quarter','time'], ascending=[True, True]).groupby(['shoot_player','date'])['current_shot_hit'].shift(1)\nShotlog.head()\n</pre> Shotlog['lag_shot_hit']=Shotlog.sort_values(by=['quarter','time'], ascending=[True, True]).groupby(['shoot_player','date'])['current_shot_hit'].shift(1) Shotlog.head() In\u00a0[\u00a0]: Copied! <pre>Shotlog.sort_values(by=['shoot_player', 'date', 'quarter', 'time'], ascending=[True, True, True, True])\n</pre> Shotlog.sort_values(by=['shoot_player', 'date', 'quarter', 'time'], ascending=[True, True, True, True]) <p>Notice that for the first shots of the game by the given players, the lagged outcome variable will have missing value.</p> In\u00a0[\u00a0]: Copied! <pre>Player_Stats=Shotlog.groupby(['shoot_player'])['current_shot_hit'].mean()\nPlayer_Stats=Player_Stats.reset_index()\nPlayer_Stats.head()\n</pre> Player_Stats=Shotlog.groupby(['shoot_player'])['current_shot_hit'].mean() Player_Stats=Player_Stats.reset_index() Player_Stats.head() <ul> <li>Let's rename the \"current_shot_hit\" variable in the newly created date frame as \"average_hit\".</li> </ul> In\u00a0[\u00a0]: Copied! <pre>Player_Stats.rename(columns={'current_shot_hit':'average_hit'}, inplace=True)\n</pre> Player_Stats.rename(columns={'current_shot_hit':'average_hit'}, inplace=True) In\u00a0[\u00a0]: Copied! <pre>Shotlog=pd.merge(Shotlog, Player_Stats, on=['shoot_player'])\nShotlog.head()\n</pre> Shotlog=pd.merge(Shotlog, Player_Stats, on=['shoot_player']) Shotlog.head() <ul> <li>Create a variable to indicate the total number of shots recorded in the dataset for each player.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>Player_Shots=Shotlog.groupby(['shoot_player']).size().reset_index(name='shot_count')\n</pre> Player_Shots=Shotlog.groupby(['shoot_player']).size().reset_index(name='shot_count') In\u00a0[\u00a0]: Copied! <pre>Player_Shots.sort_values(by=['shot_count'], ascending=[False]).head()\n</pre> Player_Shots.sort_values(by=['shot_count'], ascending=[False]).head() <p>We should also note that players have different number of shots in each individual game. We will need to treat the data differently for a player who had only two shots in a game compared to those who had attempted 30 in a game.</p> <ul> <li>Create a variable to indicate the number of shots in each game for by each player.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>Player_Game=Shotlog.groupby(['shoot_player','date']).size().reset_index(name='shot_per_game')\nPlayer_Game.head()\n</pre> Player_Game=Shotlog.groupby(['shoot_player','date']).size().reset_index(name='shot_per_game') Player_Game.head() In\u00a0[\u00a0]: Copied! <pre>Shotlog=pd.merge(Shotlog, Player_Shots, on=['shoot_player'])\nShotlog=pd.merge(Shotlog, Player_Game, on=['shoot_player','date'])\ndisplay(Shotlog)\n</pre> Shotlog=pd.merge(Shotlog, Player_Shots, on=['shoot_player']) Shotlog=pd.merge(Shotlog, Player_Game, on=['shoot_player','date']) display(Shotlog) In\u00a0[\u00a0]: Copied! <pre>Shotlog.sort_values(by=['shoot_player', 'date', 'quarter', 'time'], ascending=[True, True, True, True])\n</pre> Shotlog.sort_values(by=['shoot_player', 'date', 'quarter', 'time'], ascending=[True, True, True, True]) In\u00a0[\u00a0]: Copied! <pre>Shotlog['points'] = Shotlog['points'].astype(object)\nShotlog['quarter'] = Shotlog['quarter'].astype(object)\n</pre> Shotlog['points'] = Shotlog['points'].astype(object) Shotlog['quarter'] = Shotlog['quarter'].astype(object) In\u00a0[\u00a0]: Copied! <pre>Shotlog=Shotlog[pd.notnull(Shotlog[\"lag_shot_hit\"])]\n</pre> Shotlog=Shotlog[pd.notnull(Shotlog[\"lag_shot_hit\"])] In\u00a0[\u00a0]: Copied! <pre>Shotlog.shape\n</pre> Shotlog.shape In\u00a0[\u00a0]: Copied! <pre>Shotlog.to_csv(\"../../Data/Week 6/Shotlog1.csv\", index=False)\nPlayer_Stats.to_csv(\"../../Data/Week 6/Player_Stats1.csv\", index=False)\nPlayer_Shots.to_csv(\"../../Data/Week 6/Player_Shots1.csv\", index=False)\nPlayer_Game.to_csv(\"../../Data/Week 6/Player_Game1.csv\", index=False)\n</pre> Shotlog.to_csv(\"../../Data/Week 6/Shotlog1.csv\", index=False) Player_Stats.to_csv(\"../../Data/Week 6/Player_Stats1.csv\", index=False) Player_Shots.to_csv(\"../../Data/Week 6/Player_Shots1.csv\", index=False) Player_Game.to_csv(\"../../Data/Week 6/Player_Game1.csv\", index=False) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.01/#we-will-use-the-2016-2017-basketball-shot-log-data-to-demonstrate-how-to-test-the-hot-hand","title":"We will use the 2016-2017 basketball shot log data to demonstrate how to test the hot hand.\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.01/#import-useful-libraries-and-the-shot-log-data","title":"Import useful libraries and the shot log data\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.01/#please-note-that-the-3-lecture-notebooks-for-this-week-must-be-run-in-order-as-the-following-notebooks-rely-on-the-output-of-the-previous","title":"Please note that the 3 lecture notebooks for this week must be run in order, as the following notebooks rely on the output of the previous\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.01/#data-preparation","title":"Data Preparation\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.01/#missing-value","title":"Missing Value\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.01/#lets-create-some-useful-variables","title":"Let\u2019s create some useful variables.\u00b6","text":"<ul> <li>Create dummy variables to indicate hit or miss of current shot and previous shot.</li> </ul>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.01/#we-can-sort-the-shot-log-data-by-player-gamedate-quarter-and-time-of-the-shot","title":"We can sort the shot log data by player, game(date),  quarter, and time of the shot.\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.01/#lets-create-a-dataframe-for-average-success-rate-of-players-over-the-season","title":"Let's create a dataframe for average success rate of players over the season.\u00b6","text":"<p>Since the \"current_shot_hit\" variable is a dummy variable (=1 if hit, =0 if miss), the average of this variable would indicate the success rate of the player over the season.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.01/#we-will-use-the-player-statistics-to-analyze-the-hot-hand-so-we-will-merge-average-player-statistics-dataframe-back-to-the-shot-log-dataframe","title":"We will use the player statistics to analyze the hot hand. So we will merge average player statistics dataframe back to the shot log dataframe.\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.01/#we-will-merge-the-shot-count-data-frames-back-to-the-shot-log-dataframe","title":"We will merge the shot count data frames back to the shot log dataframe.\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.01/#we-will-sort-the-data-again-after-merging","title":"We will sort the data again after merging.\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.01/#we-will-treat-the-points-and-quarter-variables-as-objects","title":"We will treat the \"points\" and \"quarter\" variables as objects.\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.01/#missing-values","title":"Missing values\u00b6","text":"<ul> <li>Drop observations with missing value in lagged variable.</li> </ul>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.01/#lets-take-a-quick-look-at-the-number-of-variables-and-the-number-of-observations-in-our-clean-dataframe","title":"Let's take a quick look at the number of variables and the number of observations in our clean dataframe.\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.01/#save-our-updated-data","title":"Save our updated data\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.02/","title":"Using Summary Statistics to Examine the \"Hot Hand\"","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport datetime as dt\n\nShotlog=pd.read_csv(\"../../Data/Week 6/Shotlog1.csv\")\nPlayer_Stats=pd.read_csv(\"../../Data/Week 6/Player_Stats1.csv\")\nPlayer_Shots=pd.read_csv(\"../../Data/Week 6/Player_Shots1.csv\")\nPlayer_Game=pd.read_csv(\"../../Data/Week 6/Player_Game1.csv\")\nShotlog.head()\n</pre> import pandas as pd import numpy as np import datetime as dt  Shotlog=pd.read_csv(\"../../Data/Week 6/Shotlog1.csv\") Player_Stats=pd.read_csv(\"../../Data/Week 6/Player_Stats1.csv\") Player_Shots=pd.read_csv(\"../../Data/Week 6/Player_Shots1.csv\") Player_Game=pd.read_csv(\"../../Data/Week 6/Player_Game1.csv\") Shotlog.head() In\u00a0[\u00a0]: Copied! <pre>Shotlog['conse_shot_hit'] = np.where((Shotlog['current_shot_hit']==1)&amp;(Shotlog['lag_shot_hit']==1), 1, 0) \nShotlog.head()\n</pre> Shotlog['conse_shot_hit'] = np.where((Shotlog['current_shot_hit']==1)&amp;(Shotlog['lag_shot_hit']==1), 1, 0)  Shotlog.head() <p>We can create a player level dataframe. The average of the variable \"conse_shot_hit\" would be the joint probability of making current and previous shots. We will also calculate the average of \"lag_shot_hit\" to indicate the probability of making the previous shot.</p> In\u00a0[\u00a0]: Copied! <pre>Player_Prob=Shotlog.groupby(['shoot_player'])['conse_shot_hit','lag_shot_hit'].mean()\nPlayer_Prob=Player_Prob.reset_index()\nPlayer_Prob.rename(columns={'lag_shot_hit':'average_lag_hit'}, inplace=True)\nPlayer_Prob.head()\n</pre> Player_Prob=Shotlog.groupby(['shoot_player'])['conse_shot_hit','lag_shot_hit'].mean() Player_Prob=Player_Prob.reset_index() Player_Prob.rename(columns={'lag_shot_hit':'average_lag_hit'}, inplace=True) Player_Prob.head() In\u00a0[\u00a0]: Copied! <pre>Player_Prob['conditional_prob']=Player_Prob['conse_shot_hit']/Player_Prob['average_lag_hit']\nPlayer_Prob.head()\n</pre> Player_Prob['conditional_prob']=Player_Prob['conse_shot_hit']/Player_Prob['average_lag_hit'] Player_Prob.head() <p>We can merge the \"Player_Prob\" data frame with the \"Player_Stats\" data frame we created earlier to compare the conditional probability and the unconditional probability. If the two probabilities are the same, or almost the same, then we fail to find evidence that the making the current shot depends on making the previous shot.</p> In\u00a0[\u00a0]: Copied! <pre>Player_Stats=pd.merge(Player_Prob, Player_Stats, on=['shoot_player'])\nPlayer_Stats.head(10)\n</pre> Player_Stats=pd.merge(Player_Prob, Player_Stats, on=['shoot_player']) Player_Stats.head(10) <p>Let's first take a quick look at our \"Player_Stats\" data frame.</p> In\u00a0[\u00a0]: Copied! <pre>Player_Stats.info()\n</pre> Player_Stats.info() <p>Note that when we created the \"conditional_prob\" variable, some observations may have missing value since the \"average_lag_shot\" variable may contain zero value. We will delete these observations with missing values in conditional probability.</p> In\u00a0[\u00a0]: Copied! <pre>Player_Stats=Player_Stats[pd.notnull(Player_Stats[\"conditional_prob\"])]\n</pre> Player_Stats=Player_Stats[pd.notnull(Player_Stats[\"conditional_prob\"])] <p>We can first check which players have the highest conditional probability, i.e., more likely to have hot hand.</p> <p>Let's sort the data by conditional probability.</p> In\u00a0[\u00a0]: Copied! <pre>Player_Stats.sort_values(by=['conditional_prob'], ascending=[False]).head(10)\n</pre> Player_Stats.sort_values(by=['conditional_prob'], ascending=[False]).head(10) <p>Comparing the \"conditional_prob\" variable and the \"average_hit\" variable, some players have a slightly higher conditional probability but some also have a lower conditional probability.</p> <p>We can sort the data by the value of difference between conditional and unconditional probabilities.</p> In\u00a0[\u00a0]: Copied! <pre>Player_Stats['diff_prob']=Player_Stats['conditional_prob']-Player_Stats['average_hit']\nPlayer_Stats=pd.merge(Player_Stats, Player_Shots, on=['shoot_player'])\nPlayer_Stats.sort_values(by=['diff_prob'], ascending=[False]).head(10)\n</pre> Player_Stats['diff_prob']=Player_Stats['conditional_prob']-Player_Stats['average_hit'] Player_Stats=pd.merge(Player_Stats, Player_Shots, on=['shoot_player']) Player_Stats.sort_values(by=['diff_prob'], ascending=[False]).head(10) <p>Comparing the \"conditional_prob\" variable and the \"average_hit\" variable, some players have a slightly higher conditional probability but some also have a lower conditional probability. We can sort the data by the value of the difference between conditional and unconditional probabilities. We can see that Lamar Patterson has the highest difference between the two probabilities, at 30%. But we could also see that the sample size for Patterson is pretty small. For Joe Young and Damjan Rudez, we have about 80 observations and the difference in the probabilities is about 20%.</p> In\u00a0[\u00a0]: Copied! <pre>import scipy.stats as sp\n</pre> import scipy.stats as sp In\u00a0[\u00a0]: Copied! <pre>sp.stats.ttest_ind(Player_Stats['conditional_prob'], Player_Stats['average_hit'])\n</pre> sp.stats.ttest_ind(Player_Stats['conditional_prob'], Player_Stats['average_hit']) <p>The first number is the t-statistics and the second number is the p-value.</p> <p>Note that the p-value for the t test is about 0.10, which is higher than the conventional significance level 0.05. Thus the conditional probability is not statistically significantly different than the average success rate. In other words, in the analysis of conditional probability, we fail to find evidence to support the \"hot hand\".</p> In\u00a0[\u00a0]: Copied! <pre>Shotlog['current_shot_hit'].corr(Shotlog['lag_shot_hit'])\n</pre> Shotlog['current_shot_hit'].corr(Shotlog['lag_shot_hit']) <p>As we can see, the autocorrelation coefficient is negative and the magnitude is very small and close to zero.</p> <p>Since some players may have \u201chot hand\u201d, and hence strong correlation between outcomes of adjacent shots, while some may not.  We can also calculate autocorrelation coefficient for each player.</p> In\u00a0[\u00a0]: Copied! <pre>Shotlog.groupby('shoot_player')[['current_shot_hit','lag_shot_hit']].corr().head(10)\n</pre> Shotlog.groupby('shoot_player')[['current_shot_hit','lag_shot_hit']].corr().head(10) In\u00a0[\u00a0]: Copied! <pre>Autocorr_Hit=Shotlog.groupby('shoot_player')[['current_shot_hit','lag_shot_hit']].corr().unstack()\nAutocorr_Hit.head()\n</pre> Autocorr_Hit=Shotlog.groupby('shoot_player')[['current_shot_hit','lag_shot_hit']].corr().unstack() Autocorr_Hit.head() <p>Note that now each row represents a single player. But we still have duplicate information in the columns.</p> In\u00a0[\u00a0]: Copied! <pre>Autocorr_Hit=Shotlog.groupby('shoot_player')[['current_shot_hit','lag_shot_hit']].corr().unstack().iloc[:,1].reset_index()\nAutocorr_Hit.head()\n</pre> Autocorr_Hit=Shotlog.groupby('shoot_player')[['current_shot_hit','lag_shot_hit']].corr().unstack().iloc[:,1].reset_index() Autocorr_Hit.head() <p>Notice that we still have two levels of variable names.</p> In\u00a0[\u00a0]: Copied! <pre>Autocorr_Hit.columns=Autocorr_Hit.columns.get_level_values(0)\nAutocorr_Hit.head()\n</pre> Autocorr_Hit.columns=Autocorr_Hit.columns.get_level_values(0) Autocorr_Hit.head() <p>Let's rename the variable capturing autocorrelation coefficient.</p> In\u00a0[\u00a0]: Copied! <pre>Autocorr_Hit.rename(columns={'current_shot_hit':'autocorr'}, inplace=True)\nAutocorr_Hit.head()\n</pre> Autocorr_Hit.rename(columns={'current_shot_hit':'autocorr'}, inplace=True) Autocorr_Hit.head() In\u00a0[\u00a0]: Copied! <pre>Player_Game_Shot=Player_Game.groupby([\"shoot_player\"])['shot_per_game'].mean().reset_index(name='avg_shot_game')\nPlayer_Game_Shot.head()\n</pre> Player_Game_Shot=Player_Game.groupby([\"shoot_player\"])['shot_per_game'].mean().reset_index(name='avg_shot_game') Player_Game_Shot.head() In\u00a0[\u00a0]: Copied! <pre>Autocorr_Hit=pd.merge(Autocorr_Hit, Player_Game_Shot, on=['shoot_player'])\nAutocorr_Hit.sort_values(by=['autocorr'], ascending=[False]).head(10)\n</pre> Autocorr_Hit=pd.merge(Autocorr_Hit, Player_Game_Shot, on=['shoot_player']) Autocorr_Hit.sort_values(by=['autocorr'], ascending=[False]).head(10) <p>We will merge the Player_Game_Shot dataframe to the Player_Shots dataframe since both dataframes are measured in player level and both contain information on the number of shots.</p> In\u00a0[\u00a0]: Copied! <pre>Player_Shots=pd.merge(Player_Shots, Player_Game_Shot, on=['shoot_player'])\nPlayer_Shots.head()\n</pre> Player_Shots=pd.merge(Player_Shots, Player_Game_Shot, on=['shoot_player']) Player_Shots.head() In\u00a0[\u00a0]: Copied! <pre>Shotlog.to_csv(\"../../Data/Week 6/Shotlog2.csv\", index=False)\nPlayer_Stats.to_csv(\"../../Data/Week 6/Player_Stats2.csv\", index=False)\nPlayer_Shots.to_csv(\"../../Data/Week 6/Player_Shots2.csv\", index=False)\n</pre> Shotlog.to_csv(\"../../Data/Week 6/Shotlog2.csv\", index=False) Player_Stats.to_csv(\"../../Data/Week 6/Player_Stats2.csv\", index=False) Player_Shots.to_csv(\"../../Data/Week 6/Player_Shots2.csv\", index=False) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.02/#using-summary-statistics-to-examine-the-hot-hand","title":"Using Summary Statistics to Examine the \"Hot Hand\"\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.02/#import-useful-libraries-and-the-updated-shot-log-data","title":"Import useful libraries and the updated shot log data\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.02/#conditional-probability","title":"Conditional Probability\u00b6","text":"<p>We can first calculate the conditional probability of making a shot in the current period conditional on making the previous shot. $$Conditional \\ Probability=\\frac{Probability \\ of \\ Making \\ Consecutive \\ Shots}{Probability \\ of \\ Making \\ Previous \\ Shot}$$</p> <p>We will need to create a variable that indicates a player made consecutive shots.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.02/#calculate-conditional-probability-for-each-player","title":"Calculate conditional probability for each player\u00b6","text":"<p>We can calculate the conditional probability by dividing the joint probability by the probability of making the previous shot.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.02/#t-test-for-statistical-significance-on-the-difference","title":"T-test for statistical significance on the difference\u00b6","text":"<p>More rigorously, we can use a t-test to test if the players\u2019 probability of hitting the goal is statistically significantly different than their conditional probability.</p> <p>We need to choose a significance level before we perform the test. If the test produces a p-value less than the chosen significance level, then we say that there is a statistically significant difference between the two probabilities; otherwise, we fail to find evidence to support that the two probabilities are statistically significantly different from each other.</p> <p>The most commonly used significance level is 0.05.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.02/#to-perform-a-t-test-we-need-to-import-a-new-library-scipystats","title":"To perform a t-test, we need to import a new library, \"scipy.stats.\"\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.02/#we-can-use-the-ttest_ind-function-to-calculate-the-test-statistics","title":"We can use the ttest_ind() function to calculate the test statistics.\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.02/#autocorrelation-coefficient","title":"Autocorrelation Coefficient\u00b6","text":"<p>We can calculate the autocorrelation coefficient by calculating the correlation coefficient between the \u201ccurrent_shot_hit\u201d variable and the \u201clag_shot_hit\u201d variable.</p> <p>Note: in python, you could use \u201cautocorr(lag=1)\u201d to calculate first order autocorrelation coefficient. This command is not very useful in our case since we want to look at the autocorrelation coefficient within each game. Using the built-in autocorrelation coefficient function in python, we will be treating the last shot from the previous game and the first shot of the subsequent game as a pair.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.02/#we-may-not-want-to-print-out-a-2-by-2-matrix-for-every-player-we-can-use-the-unstack-command-to-reshape-the-data","title":"We may not want to print out a 2 by 2 matrix for every player. We can use the \"unstack()\" command to reshape the data.\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.02/#we-can-use-the-iloc-command-to-select-the-columns-that-we-need","title":"We can use the \".iloc\" command to select the columns that we need.\u00b6","text":"<ul> <li>In the iloc[,] command, we first specify the rows we want to select, then the columns, i.e., [rows, columns]</li> <li>We want to select all rows, so we will have iloc[:,]</li> <li>We only want to select the second column, which is indexed 1 (first column would be indexed 0, etc.)</li> <li>So we will use the command iloc[:,1]</li> </ul> <p>Lastly, we will also reset the index so that the player names would become a variable.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.02/#we-can-use-the-get_level_values-command-to-reset-the-variable-name-to-the-first-level-index-0","title":"We can use the \"get_level_values\" command to reset the variable name to the first level (index 0).\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.02/#how-informative-the-autocorrelation-coefficient-also-depends-on-the-number-of-shots-per-game-for-each-player-lets-add-the-number-of-shots-and-the-number-of-shots-per-game-to-the-autocorrelation-matrix-and-sort-the-data-by-the-size-of-autocorrelation-coefficient","title":"How informative the autocorrelation coefficient also depends on the number of shots per game for each player. Let's add the number of shots and the number of shots per game to the autocorrelation matrix  and sort the data by the size of autocorrelation coefficient.\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.02/#save-updated-data","title":"Save updated data\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.03/","title":"06.03","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport datetime as dt\nimport statsmodels.formula.api as sm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nShotlog=pd.read_csv(\"../../Data/Week 6/Shotlog2.csv\")\nPlayer_Stats=pd.read_csv(\"../../Data/Week 6/Player_Stats2.csv\")\nPlayer_Shots=pd.read_csv(\"../../Data/Week 6/Player_Shots2.csv\")\nShotlog.head()\n</pre> import pandas as pd import numpy as np import datetime as dt import statsmodels.formula.api as sm import matplotlib.pyplot as plt import seaborn as sns  Shotlog=pd.read_csv(\"../../Data/Week 6/Shotlog2.csv\") Player_Stats=pd.read_csv(\"../../Data/Week 6/Player_Stats2.csv\") Player_Shots=pd.read_csv(\"../../Data/Week 6/Player_Shots2.csv\") Shotlog.head() In\u00a0[\u00a0]: Copied! <pre>Shotlog['error']=Shotlog['current_shot_hit']-Shotlog['average_hit']\nShotlog['lagerror']=Shotlog['lag_shot_hit']-Shotlog['average_hit']\n</pre> Shotlog['error']=Shotlog['current_shot_hit']-Shotlog['average_hit'] Shotlog['lagerror']=Shotlog['lag_shot_hit']-Shotlog['average_hit'] <p>We can graph the outcome of the shots to see if there is any pattern over time in the variable.</p> <p>We will look at LeBron James' performance during the regular season as an example.</p> In\u00a0[\u00a0]: Copied! <pre>Shotlog['time'] = pd.to_timedelta(Shotlog['time'])\nShotlog['time'].describe()\n</pre> Shotlog['time'] = pd.to_timedelta(Shotlog['time']) Shotlog['time'].describe() <p>We will first graph the outcome of LeBron James' shots in a single game on April 9th, 2017.</p> <p>(To make this graph, we use a small trick. Instead of asking Python to produce a scatter plot with the \"plot.scatter\" command, we ask Python to graph a line plot, but specify the width of the line to be 0. So essentially we produce a scatter plot. The reason we do it this way is because in Python, scatter plot requires the x axis to be numeric. It does not allow scatter plot where the x axis is a date or time variable.)</p> In\u00a0[\u00a0]: Copied! <pre>Shotlog[(Shotlog.shoot_player == 'LeBron James')&amp;(Shotlog.date=='2017-04-09')].plot(x='time', y='current_shot_hit', marker='o', linewidth=0)\n</pre> Shotlog[(Shotlog.shoot_player == 'LeBron James')&amp;(Shotlog.date=='2017-04-09')].plot(x='time', y='current_shot_hit', marker='o', linewidth=0) <p>Let's create a graph of the outcomes of individual shots for Lebron James throughout the regular season. We will create a subgraph for each game he played.</p> <p>We will first subset a dataset that includes only LeBron James' data.</p> In\u00a0[\u00a0]: Copied! <pre>LeBron_James=Shotlog[(Shotlog.shoot_player == 'LeBron James')]\nLeBron_James.head()\n</pre> LeBron_James=Shotlog[(Shotlog.shoot_player == 'LeBron James')] LeBron_James.head() <p>Now we can graph prediction error for LeBron James for all the games separately in the season.</p> In\u00a0[\u00a0]: Copied! <pre>g = sns.FacetGrid(LeBron_James, col=\"date\", col_wrap=4)\ng = g.map(plt.plot, \"time\", \"current_shot_hit\", marker='o', linewidth=0)\ng.set_axis_labels(\"Game\", \"Shots\");\n</pre> g = sns.FacetGrid(LeBron_James, col=\"date\", col_wrap=4) g = g.map(plt.plot, \"time\", \"current_shot_hit\", marker='o', linewidth=0) g.set_axis_labels(\"Game\", \"Shots\"); <p>We will do a similar exercise for the statistics of Cheick Diallo.</p> In\u00a0[\u00a0]: Copied! <pre>Cheick_Diallo=Shotlog[(Shotlog.shoot_player == 'Cheick Diallo')]\ng = sns.FacetGrid(Cheick_Diallo, col=\"date\", col_wrap=4)\ng = g.map(plt.plot, \"time\", \"current_shot_hit\", marker='o', linewidth=0)\n</pre> Cheick_Diallo=Shotlog[(Shotlog.shoot_player == 'Cheick Diallo')] g = sns.FacetGrid(Cheick_Diallo, col=\"date\", col_wrap=4) g = g.map(plt.plot, \"time\", \"current_shot_hit\", marker='o', linewidth=0) In\u00a0[\u00a0]: Copied! <pre>#Your Code Here\n</pre> #Your Code Here In\u00a0[\u00a0]: Copied! <pre>reg1 = sm.ols(formula = 'error ~ lagerror', data= Shotlog).fit()\nprint(reg1.summary())\n</pre> reg1 = sm.ols(formula = 'error ~ lagerror', data= Shotlog).fit() print(reg1.summary()) <p>The estimated coefficient of the lagged error is statistically significant. However, the R-Squared for this regression is also zero. This means that our specified linear model is not a good fit for our data at all!</p> <p>There are a lot of factors that may influence the success of shot, for example, the player\u2019s own skill as a shooter, the type of the shot, the atmosphere of the stadium (whether it is home or away game), and whether it is at the beginning or towards the end of the game. Let\u2019s add these control variables in our regression.</p> In\u00a0[\u00a0]: Copied! <pre>reg2 = sm.ols(formula = 'error ~ lagerror+player_position+home_game+opponent_previous_shot+C(points)+time_from_last_shot+C(quarter)', data= Shotlog).fit()\nprint(reg2.summary())\n</pre> reg2 = sm.ols(formula = 'error ~ lagerror+player_position+home_game+opponent_previous_shot+C(points)+time_from_last_shot+C(quarter)', data= Shotlog).fit() print(reg2.summary()) <p>We can see that the R-squared is now increased to 0.015 which is still very small. The estimate on lagerror is statistically significant, but the magnitude of the estimate is -0.0136 which is still very small. And it is negative, meaning that the success of the previous shot would hurt the chance of the subsequent shot. This is contrary to what the hot hand predicts.</p> In\u00a0[\u00a0]: Copied! <pre>reg3 = sm.wls(formula = 'error ~ lagerror+player_position+home_game+opponent_previous_shot+points+time_from_last_shot+quarter',  weights=1/Shotlog['shot_per_game'] , data= Shotlog).fit()\nprint(reg3.summary())\n</pre> reg3 = sm.wls(formula = 'error ~ lagerror+player_position+home_game+opponent_previous_shot+points+time_from_last_shot+quarter',  weights=1/Shotlog['shot_per_game'] , data= Shotlog).fit() print(reg3.summary()) <p>From our summary statistics, some players exhibit a stream of the success while some don\u2019t. In our previous regressions, we are grouping all the players together. Let\u2019s see if we can find any effect if we look at individual players.</p> In\u00a0[\u00a0]: Copied! <pre>reg_LeBron = sm.ols(formula = 'error ~ lagerror+home_game+opponent_previous_shot+C(points)+time_from_last_shot+C(quarter)', data= LeBron_James).fit()\nprint(reg_LeBron.summary())\n</pre> reg_LeBron = sm.ols(formula = 'error ~ lagerror+home_game+opponent_previous_shot+C(points)+time_from_last_shot+C(quarter)', data= LeBron_James).fit() print(reg_LeBron.summary()) <p>Similarly, we can run a weighted least squares estimation on LeBron James\u2019 prediction error, weighted by the number of shot he made in each game.</p> In\u00a0[\u00a0]: Copied! <pre>reg_LeBron_wls = sm.wls(formula = 'error ~ lagerror+home_game+opponent_previous_shot+points+time_from_last_shot+quarter',  weights=1/LeBron_James['shot_per_game'] , data= LeBron_James).fit()\nprint(reg_LeBron_wls.summary())\n</pre> reg_LeBron_wls = sm.wls(formula = 'error ~ lagerror+home_game+opponent_previous_shot+points+time_from_last_shot+quarter',  weights=1/LeBron_James['shot_per_game'] , data= LeBron_James).fit() print(reg_LeBron_wls.summary()) <p>We can also take a look back at LeBron James\u2019 autocorrelation coefficient.</p> In\u00a0[\u00a0]: Copied! <pre>Shotlog[(Shotlog.shoot_player == 'LeBron James')][['current_shot_hit','lag_shot_hit']].corr()\n</pre> Shotlog[(Shotlog.shoot_player == 'LeBron James')][['current_shot_hit','lag_shot_hit']].corr() <p>The autocorrelation coefficient between the outcomes of the current shot and the previous shot for LeBron James is very small.</p> <p>We can do a similar exercise for James Jones. We will start with an ordinary least square regression.</p> In\u00a0[\u00a0]: Copied! <pre>reg_Jones = sm.ols(formula = 'error ~ lagerror+home_game+opponent_previous_shot+C(points)+time_from_last_shot+C(quarter)', data= James_Jones).fit()\nprint(reg_Jones.summary())\n</pre> reg_Jones = sm.ols(formula = 'error ~ lagerror+home_game+opponent_previous_shot+C(points)+time_from_last_shot+C(quarter)', data= James_Jones).fit() print(reg_Jones.summary()) <p>We will also run a weighted least squares estimation on Jones' statistics. Weight=1/shot_per_game.</p> In\u00a0[\u00a0]: Copied! <pre>reg_Jones_wls = sm.wls(formula = 'error ~ lagerror+home_game+opponent_previous_shot+points+time_from_last_shot+quarter',  weights=1/James_Jones['shot_per_game'] , data= James_Jones).fit()\nprint(reg_Jones_wls.summary())\n</pre> reg_Jones_wls = sm.wls(formula = 'error ~ lagerror+home_game+opponent_previous_shot+points+time_from_last_shot+quarter',  weights=1/James_Jones['shot_per_game'] , data= James_Jones).fit() print(reg_Jones_wls.summary()) In\u00a0[\u00a0]: Copied! <pre>#Your Code Here\n</pre> #Your Code Here In\u00a0[\u00a0]: Copied! <pre>#Your Code Here\n</pre> #Your Code Here In\u00a0[\u00a0]: Copied! <pre>def reg_player(player):\n    Shotlog_player=Shotlog[Shotlog.shoot_player==player]\n    reg_player=sm.ols(formula = 'error ~ lagerror+home_game+opponent_previous_shot+points+time_from_last_shot+quarter', data= Shotlog_player).fit()\n    print(reg_player.summary())\n    return;\n</pre> def reg_player(player):     Shotlog_player=Shotlog[Shotlog.shoot_player==player]     reg_player=sm.ols(formula = 'error ~ lagerror+home_game+opponent_previous_shot+points+time_from_last_shot+quarter', data= Shotlog_player).fit()     print(reg_player.summary())     return;  <p>We can then use this function for individual player, for example, Russell Westbrook.</p> In\u00a0[\u00a0]: Copied! <pre>reg_player('Russell Westbrook')\n</pre> reg_player('Russell Westbrook') <ul> <li>Define a function to run weighted least square regression by player.</li> </ul> In\u00a0[\u00a0]: Copied! <pre>def reg_wls_player(player):\n    Shotlog_player=Shotlog[Shotlog.shoot_player==player]\n    reg_wls_player=sm.wls(formula = 'error ~ lagerror+home_game+opponent_previous_shot+points+time_from_last_shot+quarter',weights=1/Shotlog_player['shot_per_game'] , data= Shotlog_player).fit()\n    print(reg_wls_player.summary())\n    return;\n</pre> def reg_wls_player(player):     Shotlog_player=Shotlog[Shotlog.shoot_player==player]     reg_wls_player=sm.wls(formula = 'error ~ lagerror+home_game+opponent_previous_shot+points+time_from_last_shot+quarter',weights=1/Shotlog_player['shot_per_game'] , data= Shotlog_player).fit()     print(reg_wls_player.summary())     return;  <p>Let's use this function to run a weighted least squares estimation for Russell Westbrook.</p> In\u00a0[\u00a0]: Copied! <pre>reg_wls_player('Russell Westbrook')\n</pre> reg_wls_player('Russell Westbrook') In\u00a0[\u00a0]: Copied! <pre>player_list = np.array(Shotlog['shoot_player'])\nplayer_list = np.unique(player_list)\n</pre> player_list = np.array(Shotlog['shoot_player']) player_list = np.unique(player_list) In\u00a0[\u00a0]: Copied! <pre>player_list[0]\n</pre> player_list[0] <ul> <li>Run weighted least squares regression for each player by specifying \"shoot_play==player_list[index]\"</li> </ul> In\u00a0[\u00a0]: Copied! <pre>Shotlog_player=Shotlog[Shotlog.shoot_player==player_list[0]]\nreg_player=sm.wls(formula = 'error ~ lagerror+home_game+opponent_previous_shot+points+time_from_last_shot+quarter', weights=1/Shotlog_player['shot_per_game'], data= Shotlog_player).fit()\nprint(reg_player.summary())\n</pre> Shotlog_player=Shotlog[Shotlog.shoot_player==player_list[0]] reg_player=sm.wls(formula = 'error ~ lagerror+home_game+opponent_previous_shot+points+time_from_last_shot+quarter', weights=1/Shotlog_player['shot_per_game'], data= Shotlog_player).fit() print(reg_player.summary()) <ul> <li>Extract the estimated coefficients, along with the p-value and t-statistics of the estimates and store them in a dataframe</li> </ul> In\u00a0[\u00a0]: Copied! <pre>RegParams = pd.DataFrame(reg_player.params).reset_index()\nRegTvals = pd.DataFrame(reg_player.tvalues).reset_index()\nRegPvals = pd.DataFrame(reg_player.pvalues).reset_index()\n\nRegOutput = pd.merge(RegParams, RegTvals, on=['index'])\nRegOutput = pd.merge(RegOutput, RegPvals, on=['index'])\nRegOutput\n</pre> RegParams = pd.DataFrame(reg_player.params).reset_index() RegTvals = pd.DataFrame(reg_player.tvalues).reset_index() RegPvals = pd.DataFrame(reg_player.pvalues).reset_index()  RegOutput = pd.merge(RegParams, RegTvals, on=['index']) RegOutput = pd.merge(RegOutput, RegPvals, on=['index']) RegOutput <ul> <li>Write a loop to extract regression outputs for each player</li> </ul> In\u00a0[\u00a0]: Copied! <pre>i = 0 \nPlayer_Results = {}\nwhile i &lt;= len(player_list) - 1:\n    Shotlog_player=Shotlog[Shotlog.shoot_player==player_list[i]]\n    reg_player=sm.wls(formula = 'error ~ lagerror+home_game+opponent_previous_shot+points+time_from_last_shot+quarter', weights=1/Shotlog_player['shot_per_game'], data= Shotlog_player).fit()\n    RegParams = pd.DataFrame(reg_player.params).reset_index()\n    RegTvals = pd.DataFrame(reg_player.tvalues).reset_index()\n    RegPvals = pd.DataFrame(reg_player.pvalues).reset_index()\n\n    RegOutput = pd.merge(RegParams, RegTvals, on=['index'])\n    RegOutput = pd.merge(RegOutput, RegPvals, on=['index'])\n    RegOutput\n    \n    LagErr = RegOutput[RegOutput['index'] == 'lagerror']\n    LagErr = LagErr.drop(columns=['index'])\n    LagErr = LagErr.rename(columns={\"0_x\":\"Coef\", \"0_y\":\"T_Statistics\", 0:\"P_Value\"})\n    LagErr['shoot_player'] = player_list[i]\n    Headers = ['shoot_player', 'Coef', 'T_Statistics', 'P_Value']\n    Player_Results[i] = LagErr[Headers]\n    i = i+1\n</pre> i = 0  Player_Results = {} while i &lt;= len(player_list) - 1:     Shotlog_player=Shotlog[Shotlog.shoot_player==player_list[i]]     reg_player=sm.wls(formula = 'error ~ lagerror+home_game+opponent_previous_shot+points+time_from_last_shot+quarter', weights=1/Shotlog_player['shot_per_game'], data= Shotlog_player).fit()     RegParams = pd.DataFrame(reg_player.params).reset_index()     RegTvals = pd.DataFrame(reg_player.tvalues).reset_index()     RegPvals = pd.DataFrame(reg_player.pvalues).reset_index()      RegOutput = pd.merge(RegParams, RegTvals, on=['index'])     RegOutput = pd.merge(RegOutput, RegPvals, on=['index'])     RegOutput          LagErr = RegOutput[RegOutput['index'] == 'lagerror']     LagErr = LagErr.drop(columns=['index'])     LagErr = LagErr.rename(columns={\"0_x\":\"Coef\", \"0_y\":\"T_Statistics\", 0:\"P_Value\"})     LagErr['shoot_player'] = player_list[i]     Headers = ['shoot_player', 'Coef', 'T_Statistics', 'P_Value']     Player_Results[i] = LagErr[Headers]     i = i+1 <ul> <li>Write another loop to build a dataframe to store the regression output for all the players</li> </ul> In\u00a0[\u00a0]: Copied! <pre>RegPlayer = Player_Results[0]\nj = 1\nwhile j &lt;= len(player_list) - 1:\n    RegPlayer = RegPlayer.append(Player_Results[j])\n    j = j+1\nRegPlayer = RegPlayer.reset_index()\nRegPlayer = RegPlayer.drop(columns=['index'])\nRegPlayer\n</pre> RegPlayer = Player_Results[0] j = 1 while j &lt;= len(player_list) - 1:     RegPlayer = RegPlayer.append(Player_Results[j])     j = j+1 RegPlayer = RegPlayer.reset_index() RegPlayer = RegPlayer.drop(columns=['index']) RegPlayer <ul> <li>Merge the total number of shots captured in \"Player_Shots\" to the regression result dataframe. This total number of shots represents the sample size of each regression</li> </ul> In\u00a0[\u00a0]: Copied! <pre>RegPlayer=pd.merge(RegPlayer, Player_Shots, on=['shoot_player'])\nRegPlayer.head()\n</pre> RegPlayer=pd.merge(RegPlayer, Player_Shots, on=['shoot_player']) RegPlayer.head() <ul> <li>Display players with statistically significant estimates on the lagged error variable</li> </ul> In\u00a0[\u00a0]: Copied! <pre>display(RegPlayer.loc[RegPlayer['P_Value']&lt;=0.05])\n</pre> display(RegPlayer.loc[RegPlayer['P_Value']&lt;=0.05]) <p>There are a total of 38 players with statistically significant estimates on the lagged error variable, that is, the success of their previous shots impact the success rate of their current shot. Interestingly, more than half of these estimates are negative, which means that a success in the previous shot actually hurts the chance of scoring in the current shot. This is the opposite of a \"hot hand.\"</p> <p>Overall from our regression analyses, 8 players, Boris Diaw, Brandon Rush, Frank Kaminsky, Joe Young, Jose Calderon, Kyle Wiltjer, Omri Casspi, Robert Covington, and Tony Parker have positive and statistically significant estimate on the lagged error variable. Thus, these players may have \"hot hand.\" Note that the estimate for Kyle Wiltjer is 1 and there are only a total of 14 observations for him. We need to interpret his result with caution.</p> In\u00a0[\u00a0]: Copied! <pre>#Save updated data to csv file\nShotlog.to_csv(\"../../Data/Week 6/Shotlog3.csv\")\nPlayer_Stats.to_csv(\"../../Data/Week 6/Player_Stats3.csv\", index=False)\nPlayer_Shots.to_csv(\"../../Data/Week 6/Player_Shots3.csv\", index=False)\n</pre> #Save updated data to csv file Shotlog.to_csv(\"../../Data/Week 6/Shotlog3.csv\") Player_Stats.to_csv(\"../../Data/Week 6/Player_Stats3.csv\", index=False) Player_Shots.to_csv(\"../../Data/Week 6/Player_Shots3.csv\", index=False) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.03/#using-regression-analysis-to-test-the-hot-hand","title":"Using Regression Analysis to Test the \"Hot Hand\"\u00b6","text":"<p>In this section, we will use regression analysis to test for the \"hot hand.\"</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.03/#import-useful-libraries-and-the-shot-log-data","title":"Import useful libraries and the shot log data\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.03/#prediction-error","title":"Prediction Error\u00b6","text":"<p>Let's create a variable that equals to the difference between the outcome of the shot and the average success rate. Since we typically use the average success rate to predict the outcome of the shot, this difference will capture the prediction error.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.03/#self-test-1","title":"Self Test - 1\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.03/#graph-the-prediction-error-for-james-jones","title":"Graph the prediction error for James Jones\u00b6","text":"<ul> <li>Separate the shots by game</li> <li>Interpret your result</li> </ul>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.03/#regression-analysis-on-prediction-error","title":"Regression analysis on prediction error\u00b6","text":"<p>We will first run a simple regression of the prediction error of current period on the prediction error of previous period.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.03/#weighted-least-squares-regression","title":"Weighted least squares regression\u00b6","text":"<p>As we have seen, some players had a lot of shot per game while some just had a few. Different players may have different variations in their success rate in the shots. We can run a weighted least squared regression to address this problem.</p> <p>Weighted least squares estimation weights the observations proportional to the reciprocal of the error variance of the observation. Thus weighted least squares can overcome the issue of non-constant variance.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.03/#we-can-use-the-smwls-command-to-run-the-weighted-least-square-regression-weighted-by-the-number-of-shot-per-game-weight1shot_per_game","title":"We can use the \u201csm.wls\u201d command to run the weighted least square regression weighted by the number of shot per game (weight=1/shot_per_game).\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.03/#regression-analysis-on-individual-players","title":"Regression analysis on individual players\u00b6","text":"<p>Run a regression of current error on lagged error for LeBron James.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.03/#self-test-2","title":"Self Test - 2\u00b6","text":"<p>Use regression analysis to test \"hot hand\" for Cheick Diallo</p> <ol> <li>Run an ordinary least square regression of current error on lagged error for Cheick Diallo.</li> <li>Run a weighted least sqaure regression of current error on lagged error for Cheick Diallo, weight=1/shot_per_game.</li> <li>Interpret your regression results.</li> </ol>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.03/#more-generally-we-can-define-functions-to-run-regressions-for-each-individual-player","title":"More generally, we can define functions to run regressions for each individual player.\u00b6","text":"<ul> <li>Define a function to run ordinary least square regression by player.</li> </ul>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.03/#we-can-extract-estimated-coefficient-on-the-lagged-error-for-each-player","title":"We can extract estimated coefficient on the lagged error for each player.\u00b6","text":"<ul> <li>Create a list of unique player names</li> </ul>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.04/","title":"06.04","text":"In\u00a0[\u00a0]: Copied! <pre>James_Jones=Shotlog[(Shotlog.shoot_player == 'James Jones')]\ng = sns.FacetGrid(James_Jones, col=\"date\", col_wrap=4)\ng = g.map(plt.plot, \"time\", \"current_shot_hit\", marker='o', linewidth=0)\n</pre> James_Jones=Shotlog[(Shotlog.shoot_player == 'James Jones')] g = sns.FacetGrid(James_Jones, col=\"date\", col_wrap=4) g = g.map(plt.plot, \"time\", \"current_shot_hit\", marker='o', linewidth=0) In\u00a0[\u00a0]: Copied! <pre>reg_Diallo = sm.ols(formula = 'error ~ lagerror+home_game+opponent_previous_shot+points+time_from_last_shot+quarter', data= Cheick_Diallo).fit()\nprint(reg_Diallo.summary())\n</pre> reg_Diallo = sm.ols(formula = 'error ~ lagerror+home_game+opponent_previous_shot+points+time_from_last_shot+quarter', data= Cheick_Diallo).fit() print(reg_Diallo.summary()) In\u00a0[\u00a0]: Copied! <pre>reg_Diallo_wls = sm.wls(formula = 'error ~ lagerror+home_game+opponent_previous_shot+points+time_from_last_shot+quarter',  weights=1/Cheick_Diallo['shot_per_game'] , data=Cheick_Diallo).fit()\nprint(reg_Diallo_wls.summary())\n</pre> reg_Diallo_wls = sm.wls(formula = 'error ~ lagerror+home_game+opponent_previous_shot+points+time_from_last_shot+quarter',  weights=1/Cheick_Diallo['shot_per_game'] , data=Cheick_Diallo).fit() print(reg_Diallo_wls.summary())"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.04/#note-on-self-tests-for-week-6","title":"Note on Self-Tests for Week 6\u00b6","text":"<p>Due to the amount of code used in this week's lecture notebooks, only the solutions to the self-tests are included here rather than all code required to run the self-tests. Attempting to run this code will fail, it is intended for reference only.</p>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.04/#self-test-1-solution","title":"Self Test - 1 Solution\u00b6","text":""},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.04/#graph-the-prediction-error-for-james-jones","title":"Graph the prediction error for James Jones\u00b6","text":"<ul> <li>Separate the shots by game</li> <li>Interpret your result</li> </ul>"},{"location":"CS_Electives/Sports%20Analytics/01%20Foundations/nb/06.04/#self-test-2-solution","title":"Self Test - 2 Solution\u00b6","text":"<p>Use regression analysis to test \"hot hand\" for Cheick Diallo</p> <ol> <li>Run an ordinary least square regression of current error on lagged error for Cheick Diallo.</li> <li>Run a weighted least sqaure regression of current error on lagged error for Cheick Diallo, weight=1/shot_per_game.</li> <li>Interpret your regression results.</li> </ol>"},{"location":"CS_Electives/Tiny_ML/","title":"TinyML","text":"<p>Rather than adding more compute power, focus on improving compute efficiency</p> <p>Will mainly focus on the following applications: Speech, Computer Vision, NLP</p>"},{"location":"CS_Electives/Tiny_ML/#topics","title":"Topics","text":"<ul> <li>Hardware</li> <li>Architecture &amp; Dataflow</li> <li>Metrics and Analysis</li> <li>Efficiency</li> <li>Micro-architecture/Circuits</li> <li>Model Optimization</li> <li>Quantization</li> <li>Pruning</li> <li>Knowledge distillation</li> <li>AutoML</li> <li>Software: Optimize DNN operations through software compilation/kernel implementations</li> <li>Domain-specific compilers; eg: TVM</li> <li>Kernel implementations</li> <li>Mapping onto hardware</li> <li>Systems</li> <li>Pre/Post Processing</li> <li>Distributed training</li> <li>Federated learning</li> <li>Environmental issues</li> </ul>"},{"location":"CS_Electives/Tiny_ML/#pre-requisites","title":"Pre-Requisites","text":"<ul> <li>Computer archictecture</li> <li>Machine Learning</li> <li>Python programming</li> <li>PyTorch Basics</li> </ul>"},{"location":"CS_Electives/Tiny_ML/#reading","title":"Reading","text":"Textbook Efficient processing of deep neural networks Introduction A New Golden Age for Computer Architecture- PDF- HTML DNN Computations TB Chap 1, 2What\u2019s the backward-forward FLOP ratio for Neural Networks?Optimizing RNNs in CuDNN 5What are keys, queries, and values in attention mechanisms?Attention is all you need Hardware Book: Chapter 3In-datacenter Performance Analysis of a Tensor Processing UnitOptional: Computer Architecture: A Quantitative Approach. Ch 7Book: Chapter 5Think Fast: a TSP for Accelerating Deep Learning WorkloadsFYI: OCP Microscaling (MX) Format SpecificationBook: Chapter 8Serving DNNs in Real-time with Project BrainwaveTen Lessons from 3 Generations Shaped Google TPUv4iOptional: EIE: Efficient Inference Engine on Compressed DNNOptional: Survey on sparse hardware acceleration Microarchitecture Deep learning with INT8 on Xilinx devicesOn-Chip Memory Design for Low-Power CNN AcceleratorsOptional: Making Floating Point Math Highly Efficient for AI HardwareOptional: Book: Chapter 10 Quantization Book: Chapter 7Quantization and Training of NNs for Efficient INT-only InferenceTraining DNNs with 8-bit Floating-Point Numbers Pruning Book: Chapter 8Learning Both Weights and Connections for Efficient NNsThe Lottery Ticket Hypothesis TinyML TinyML Progress, Challenges and Roadmap Knowledge Distillation Distilling the Knowledge in a Neural NetworkKnowledge Distillation: A Survey Neural Architecture Search Book: Chapter 9Neural Architecture Search with Reinforcement LearningBRP-NAS: Prediction-based NAS using GCNsAutoML Codesign of a CNN and its Hardware Accelerator Kernel Computation Book: Chapter 4Fast algorithms for CNNsEnd-to-end ASR Model Compression using Reinforcement LearningOptional: TNet Mapping Book: Chapter 6Optimizing RNNs on GPUsDLA: Compiler and FPGA Overlay for DNN Inference Acceleration TVM TVM: An Automated Optimizing Compiler for Deep Learning Pre-/Postprocessing AI Tax: The Hidden Cost of AI Data-Center ApplicationsRethinking Data Storage and Preprocessing in DatacentersFaster Neural Networks Straight from JPEG Distributed Training Horovod: Fast and Easy Distributed Deep Learning in TensorflowLarge Scale Distributed Deep Learning Federated Learning Google AI Blog Post on FLCommunication Efficient Learning of DNNs from Decentralized DataTowards Federated Learning at Scale: System Design Ethical/Environmental Issues Chasing Carbon: The Elusive Environmental Footprint of ComputingOn the Dangers of Stochastic Parrots: Can Language Models be Too BigThe Carbon Footprint of ML Training will Plateau, then Shrink"},{"location":"CS_Electives/Tiny_ML/#references","title":"References","text":"<ul> <li> Machine Learning Hardware and Systems (Cornell Tech, Spring 2022)</li> <li> Videos</li> <li> Material</li> <li> TinyML and Efficient Deep Learning Computing | EfficientML.ai - MIT HAN Lab</li> <li> Tiny Machine Learning | UPenn</li> <li> AutoDL | Applied Deep Learning</li> </ul>"},{"location":"CS_Electives/Tiny_ML/00_Introduction/","title":"Introduction","text":"<p>Hardware and systems are essential for the progress of deep learning.</p>"},{"location":"CS_Electives/Tiny_ML/00_Introduction/#importance-of-optimization","title":"Importance of Optimization","text":""},{"location":"CS_Electives/Tiny_ML/00_Introduction/#hardware","title":"Hardware","text":"<p>No more \u201cfree lunch\u201d from material science improvements</p> Comment Moore\u2019s law Slowing downIn 1970-2010, we were able to put more transistors on a chip and get exponentially more performance; but now this is ending Dennard scaling essential stopped <p>Costly for companies to use cloud-based systems; would prefer edge-computing to reduce their energy consumption</p> <p>Can\u2019t rely on material technology for performance: After a point in shrinking size of transistors to fit more on a single chip, side-effects (such as electrons shoot in unwanted directions) cause higher power usage. Hence, domain-specific H/W architectures (GPUs, TPUs) are important</p>"},{"location":"CS_Electives/Tiny_ML/00_Introduction/#model","title":"Model","text":"<p>DNN Compression reduces the FLOPS, Model size</p>"},{"location":"CS_Electives/Tiny_ML/00_Introduction/#software","title":"Software","text":"<p>Domain-specific compilation</p>"},{"location":"CS_Electives/Tiny_ML/00_Introduction/#systems","title":"Systems","text":""},{"location":"CS_Electives/Tiny_ML/01_DNN_Computations/","title":"DNN Computations","text":""},{"location":"CS_Electives/Tiny_ML/01_DNN_Computations/#computational-view","title":"Computational View","text":"Aspect Questions Memory Parameters SizeDoes it fit on-chipHow long does it take to load from off-chipCan I overlap loading with computationIs there re-use of loaded parameters Activation SizeDoes it fit on-chipHow long does it need to be on-chip Compute MACs(Multiply-ACcumulates) What is the available parallelism in each layer?Does it fit the HW?Can I overlap compute with memory access Data dependencies Which (parts of) tensors need to be ready before starting to process a layer <p>Operation in an epoch</p> Bottleneck operation Compute utilization ratio Memory utilization ratio Forward pass Multiplication Backward pass Multiplication 2x of forward pass 1x of forward pass Gradient update Subtraction"},{"location":"CS_Electives/Tiny_ML/01_DNN_Computations/#io-bound-vs-cpu-bound","title":"I/O-Bound vs CPU-Bound","text":"<p>Memory-Bound vs Compute-Bound</p> <ul> <li>Does it take more time to do computation or fetch data from memory</li> <li>Depends on factors</li> <li>Memory bandwidth</li> <li>Compute speed/parallelism</li> <li>Size of operands</li> <li>Size of (intermediate) result</li> <li>On-Chip buffering resources</li> </ul> Memory-Bound Compute-Bound Parameters to fetch Many Few Stalls other Compute Memory ImplicationEven if you have the best __, it won\u2019t make a difference GPU RAM, hard drive Not concerning \u274c \u2705 IDK Unnecessarily over-optimized memory controller"},{"location":"CS_Electives/Tiny_ML/01_DNN_Computations/#double-buffering","title":"Double-Buffering","text":"<p>DNN \u201cfit\u201d on a processor: DNN\u2019s parameters and activations fit on the processor\u2019s external memory</p> <p>Why do GPUs have smaller external memories, ie why is VRAM smaller than RAM? Because VRAM is much faster, and hence more expensive</p>"},{"location":"CS_Electives/Tiny_ML/01_DNN_Computations/#model-checkpointing","title":"Model Checkpointing","text":"<ul> <li>General: Store all activations from forward pass, to use during backward pass</li> <li>Checkpointing: skips some of those activations and recalculate on the fly during backward pass</li> </ul> <p>Implication: less memory usage, but more computation</p>"},{"location":"CS_Electives/Tiny_ML/01_DNN_Computations/#common-dnn-layers","title":"Common DNN Layers","text":"Type of bound Compute complexity(Operations) Memory complexity(No of parameters) Comment Convolution Compute \\(k \\times 2 \\times rs \\times w h \\times c\\) \\(k \\times rs \\times c\\) Depth-wise convolution Compute \\(k \\times 2 \\times rs \\times w h\\) \\(k \\times rs\\) Linear/Fully-Connected Memory \\(k\\) Batched Linear Equal \\(k\\) Pooling Equal \\(O(1)\\) \\(O(1)\\) Can reuse hardware for convolutions with max/avg filter Normalization Equal \\(O(1)\\) \\(O(1)\\) Batch-norm becomes a simple scale+shift operation during inference Activation Functions Equal \\(O(1)\\) \\(O(1)\\) AF that cannot be compute in-place would need gradients to be computed before &amp; after the AFSome AF have parameters Convolution \\(w\\) Input width \\(h\\) Input height \\(c\\) No of input channels \\(s\\) Filter width \\(r\\) Filter height \\(k\\) No of filters in convolutionNo of weight tensors"},{"location":"CS_Electives/Tiny_ML/02_Efficiency/","title":"Efficiency","text":"<p>All elements on a single layer of a network are parallelizable</p>"},{"location":"CS_Electives/Tiny_ML/02_Efficiency/#cpu-chip-area","title":"CPU Chip Area","text":""},{"location":"CS_Electives/Tiny_ML/02_Efficiency/#hardware-types","title":"Hardware Types","text":"Purpose General CPU (Central Processing Unit) Low LatencyControl Flow GPU (Graphics Processing Unit) High throughputData flow TPU (Tensor Processing Unit)NPU (Neural Processing Unit) Specialized FPGA (Field Programmable Gate Assembly) Re-Programmable Logic ASIC (Application Specific Integrated Circuit) Fixed logic"},{"location":"CS_Electives/Tiny_ML/02_Efficiency/#performance-metrics","title":"Performance Metrics","text":"Metric Common Units Affected by Hardware Affected by DNN Compute FLOPs/s **F**loating-point **op**erations per **s**econd \u2705 \u274c OPs/s Non floating-point **op**erations per **s**econd \u2705 \u274c MACs/s Multipy-Accumulate Ops/s Half FLOPs/s \u2705 \u2705 Latency No of sec per operation s \u2705 \u2705 Throughput No of operations per second Ops/s \u2705 \u2705 Memory Capacity GB \u274c \u274c Bandwidth GB/s \u274c \u274c Workload Operational intensity Op/B \u274c \u2705 HW Utilization \u2705 \u2705"},{"location":"CS_Electives/Tiny_ML/02_Efficiency/#ops","title":"OPs","text":"\\[ \\begin{aligned} &amp;\\text{OPs} \\\\ &amp;= \\text{Ops/sec} \\\\ &amp;= \\underbrace{ \\dfrac{1}{\\text{Cycles/Op}} \\times \\text{Cycles/sec} }_\\text{for single PE} \\times \\text{No of PEs} \\end{aligned} \\] <p>PE = Processing Element</p>"},{"location":"CS_Electives/Tiny_ML/02_Efficiency/#roofline-plot","title":"Roofline Plot","text":"<p>Characterize performance of given hardware device across different workloads, to help identify if a workload is memory-bound or compute-bound</p> <p></p> Speed up Technique Memory-bound Algorithmic improvement(reduce precision) Faster memory chip Compute-Bound Faster PE(Overclocking)"},{"location":"CS_Electives/Tiny_ML/02_Efficiency/#operational-intensity","title":"Operational Intensity","text":"\\[ \\begin{aligned} \\text{Operational Intensity} &amp;= \\dfrac{\\text{No of Ops}}{\\text{Mem Footprint}} \\\\ \\text{No of Ops} &amp;= \\text{Multiplications} + \\text{Additions} \\\\ \\text{Mem Footprint} &amp;= \\text{Size of parameters} + \\text{Size of activations} \\end{aligned} \\] <p>Quantifies the ratio of computations to memory footprint of a DNN</p> <p>The same DNN can have different operational intensity on different hardware, if each device supports different numerical precision (Size of data affects operational intensity)</p> <p></p>"},{"location":"CS_Electives/Tiny_ML/02_Efficiency/#idk","title":"IDK","text":""},{"location":"CS_Electives/Tiny_ML/02_Efficiency/#performance-bottlenecks","title":"Performance Bottlenecks","text":"<ul> <li>Memory access efficiency</li> <li>Uncoalesced reads</li> <li>Compute utilization</li> <li>Overhead of control logic</li> <li>Complex DNN topologies</li> <li>Control flow and data hazards may stall execution even if hardware is available</li> </ul>"},{"location":"CS_Electives/Tiny_ML/02_Efficiency/#hardware-efficiency","title":"Hardware Efficiency","text":""},{"location":"CS_Electives/Tiny_ML/02_Efficiency/#energy-breakdown","title":"Energy breakdown","text":""},{"location":"CS_Electives/Tiny_ML/02_Efficiency/#hardware-efficiency-approaches","title":"Hardware Efficiency Approaches","text":"Approach Technique Arithmetic Specialized instructions Amortize overheadReduce overhead fractionPerform complex/fused operations with the same data fetchSIMDMatrix Multiple UnitHFMAHDP4AHMMA Quantization Lower numerical precision Memory Locality Move data to inexpensive on-chip memory Re-use Avoid expensive memory fetchesTemporal: Read once, use same data multiple times by same PESIMD, SIMTSpatial: Read once, use data multiple times by multiple PEsDataflow processingWeights stationary (CNNs)Input stationary (Fully-Connected Layers)Output stationary Operations Sparsity Skip ineffectual operationsActivation Sparsity (Sparse Activation Functions: ReLU)Weight Sparsity (Regularization/Pruning)Block SparsityCoarse-grainedFine-grained - Overhead Interleaving Model storage CSC Representation(Compressed Sparse Column) Model Optim:Change DNN arch (and hence workload) to better fit HW Compression Distillation AutoML <p>Floating-point <code>add</code> is more expensive relative to <code>integer</code>, compared to multiplication , due to shifting operations</p>"},{"location":"CS_Electives/Tiny_ML/02_Efficiency/#guidelines-for-dsas","title":"Guidelines for DSAs","text":"<p>Domain-Specific Architectures</p> <ul> <li>Dedicated memory to minimize distance of data transfer</li> <li>Invest resources saved from dropping advanced micro-architectural optimizations into more arithmetic units/larger memories</li> <li>Use easiest form of parallelism that matches the domain</li> <li>Reduce data size and type to simplest needed for the domain</li> <li>Use domain-specific programming language to port code to DSA</li> </ul>"},{"location":"CS_Electives/Tiny_ML/03_Microarchitecture/","title":"Microarchitecture","text":"<ul> <li>Arithmetic unit design</li> <li>Memory organization</li> </ul>"},{"location":"CS_Electives/Tiny_ML/03_Microarchitecture/#processing-element","title":"Processing Element","text":"<p>Should support dot product</p> <ul> <li>Multiplier with 2 elements</li> <li>Accumulator with 2 elements</li> </ul> <p>Accumulator: Adder that keeps result in storage</p> <p>Inference in INT8 precision =&gt; Multipliers are INT8, because adders and accumulators need wide range to perform accurate accumulation of many numbers</p> <p></p>"},{"location":"CS_Electives/Tiny_ML/03_Microarchitecture/#sequential","title":"Sequential","text":"Step 1 2"},{"location":"CS_Electives/Tiny_ML/03_Microarchitecture/#paralllelvectorized","title":"Paralllel/Vectorized","text":"Step 1 2"},{"location":"CS_Electives/Tiny_ML/03_Microarchitecture/#pipelined","title":"Pipelined","text":"<p>Initiation interval: How often we can start computation of a new element in a loop</p> <p>Break down computation into multiple steps with intermediate registers</p>"},{"location":"CS_Electives/Tiny_ML/03_Microarchitecture/#interleaved","title":"Interleaved","text":""},{"location":"CS_Electives/Tiny_ML/03_Microarchitecture/#precision","title":"Precision","text":"<p>Block Floating Point</p> <ul> <li>One exponent for each exponent</li> </ul> <p></p>"},{"location":"CS_Electives/Tiny_ML/03_Microarchitecture/#on-chip-memory","title":"On-Chip Memory","text":"<p>Bit-width of address = no of data entries</p> <p>Connecting RAM to MAC</p> Simple Use separate memories for 2 operands Increase no of read ports Problems with adding many read ports to SRAM1. Large size2. Inc power consumption3. Slow4. In FPGA, you need to duplicate your memorie Banking Use multiple small memories"},{"location":"CS_Electives/Tiny_ML/03_Microarchitecture/#computing-paradigms","title":"Computing Paradigms","text":"Processing Why? In-Sensor Data movement from sensor to processor is costlyFor eg, if you only need class label as output, why unnecessarily transfer 8MP image to processor Near-Memory In-Memory(Analog Processing) - Weights stored as charges- Activations delivered as analog voltages- By activating pre-charge circuity on the word &amp; bit lines, we can perform multiplication between input activation voltage &amp; stored weights"},{"location":"CS_Electives/Tiny_ML/04_Tiny_ML/","title":"TinyML","text":"<p>ML on embedded systems</p> <p>Often overlooked</p>"},{"location":"CS_Electives/Tiny_ML/04_Tiny_ML/#embedded-system","title":"Embedded System","text":"<p>Sensor, Processor, Output all in one small device</p>"},{"location":"CS_Electives/Tiny_ML/04_Tiny_ML/#sensors","title":"Sensors","text":"<p>IMU: Inertial Measurement Unit</p> <ul> <li>Accelerometer</li> <li>Gyroscope</li> <li>Magnetometer</li> </ul> <p>Challenges with IMU sensors</p> <ul> <li>Interpretability</li> <li>Sensor drift: Sensors need to recalibrated regularly</li> </ul>"},{"location":"CS_Electives/Tiny_ML/04_Tiny_ML/#processor","title":"Processor","text":"<p>MCU: MicroController Unit</p> <p>Advantages</p> <ul> <li>Small size</li> <li>Low power</li> <li>Low cost</li> </ul>"},{"location":"CS_Electives/Tiny_ML/04_Tiny_ML/#existing-systems","title":"Existing Systems","text":"<p>Nano 33 BLE Sense: AI-enabled ARM-based developmental microcontroller board</p> <p></p> <p>OV 7675 Camera module</p> <p>TinyML Shield: Alternative to Breadboard</p> <p></p>"},{"location":"CS_Electives/Tiny_ML/04_Tiny_ML/#arm-cortex-processor-profiles","title":"ARM Cortex Processor Profiles","text":"<ul> <li>ARM designs the processor core &amp; ISA, but they don\u2019t fabricate the chips</li> <li>The company (Qualcomm, Apple) bundles it with other design for system-on-chip</li> <li>The company (Google, Samsung, etc) places order to fabrication company (TSMC)</li> </ul>"},{"location":"CS_Electives/Tiny_ML/04_Tiny_ML/#cortex-m-isa","title":"Cortex-M ISA","text":""},{"location":"CS_Electives/Tiny_ML/04_Tiny_ML/#embedded-systems-os","title":"Embedded Systems OS","text":"<ul> <li>RTOS</li> <li>Arm MBED OS</li> </ul>"},{"location":"CS_Electives/Tiny_ML/04_Tiny_ML/#idk","title":"IDK","text":""},{"location":"CS_Electives/Tiny_ML/04_Tiny_ML/#memory-usage","title":"Memory Usage","text":"<ul> <li>Need to be resource-aware</li> <li>Less compute</li> <li>Less memory</li> <li>Use quantization</li> </ul>"},{"location":"CS_Electives/Tiny_ML/04_Tiny_ML/#tfl-micro","title":"TFL Micro","text":"<p>Tensorflow Lite Micro</p> <p>Built to fit ML on embedded systems</p> <ul> <li>Very small binary footprint</li> <li>No dynamic memory allocation</li> <li>No dependencies on complex parts of the standard C/C++ libraries</li> <li>No operating system dependencies, can run on bare metal</li> <li>Designed to be portable across a wide variety of systems</li> </ul> <p></p>"},{"location":"CS_Electives/Tiny_ML/04_Tiny_ML/#g_model","title":"<code>g_model</code>","text":"<ul> <li>Array of bytes</li> <li>Acts as equivalent of a file on disk</li> <li>Holds all info about</li> <li>model</li> <li>operators</li> <li>connections</li> <li>trained weights</li> </ul>"},{"location":"CS_Electives/Tiny_ML/04_Tiny_ML/#conversion-using-tfl-micro","title":"Conversion using TFL Micro","text":"<ul> <li>ONIX</li> <li>Does not matter if you use PyTorch/Tensorflow</li> </ul>"},{"location":"CS_Electives/Tiny_ML/04_Tiny_ML/#hardware-software-constraints","title":"Hardware &amp; Software Constraints","text":"<ul> <li>OS support</li> <li>Compute</li> <li>Memory</li> </ul> <ul> <li>Long-Running</li> <li>Products are expected to run for months/years which pose challenges for memory allocation</li> <li>Need to guarantee that memory allocation will not end up fragmented<ul> <li>Contiguous memory cannot be allocated even if there is enough memory overall</li> </ul> </li> </ul>"},{"location":"CS_Electives/Tiny_ML/04_Tiny_ML/#how-tfl-micro-solves-these-challenges","title":"How TFL micro solves these challenges","text":"<ul> <li>Ask developers to supply a contiguous array of memory ton interpreter</li> <li>The framework avoids any other memory allocations</li> <li>Framework guarantees that it won\u2019t allocate from this \u201carena\u201d after initialization, so long-running applications won\u2019t fail due to to fragmentation</li> <li>Ensures</li> <li>clear budget for the memory used by ML</li> <li>framework has no dependency on OS facilities needed by <code>malloc</code> or <code>new</code></li> </ul> <p>Size of tensor arena</p> <ul> <li>Operator variables</li> <li>Interpreter state</li> <li>Operator I/O</li> </ul> <p>Finding ideal size of arena</p> <ul> <li>Trial and error</li> <li>Create as large an arena as possible</li> <li>Use <code>arena_used_bytes()</code> to find actual size used</li> <li>Resize arena to this length and rebuild</li> <li>Best to do this for every deployment platform, since different op implementations may need varying scratch buffer sizes</li> </ul> <p>Ops specification</p> <p></p>"},{"location":"CS_Electives/Tiny_ML/04_Tiny_ML/#workflow","title":"Workflow","text":""},{"location":"CS_Electives/Tiny_ML/04_Tiny_ML/#example","title":"Example","text":""},{"location":"CS_Electives/Tiny_ML/05_Quantization/","title":"Quantization","text":"<p>Mapping input values from a large set (often continuous) to output values in a (countable) smaller set (often discrete)</p>"},{"location":"CS_Electives/Tiny_ML/05_Quantization/#advantages","title":"Advantages","text":"<ol> <li>Lower memory usage</li> <li>Lower power consumption</li> <li>Lowe latency</li> <li>Smaller chip area</li> </ol>"},{"location":"CS_Electives/Tiny_ML/05_Quantization/#idk","title":"IDk","text":"<ul> <li> <p>Weights/Activations</p> </li> <li> <p>Linear/Non-linear</p> </li> <li> <p>Symmetric/assymetric</p> </li> <li> <p>Quantization-aware training</p> </li> <li> <p>Training/inference</p> </li> <li> <p>Error</p> </li> </ul> <p>Tradeoff between 2 types</p> <ul> <li>Clipping<ul> <li>To reduce, you need to inc the range: dec \\(r_\\min\\) and inc \\(r_\\max\\)</li> </ul> </li> <li> <p>Rounding</p> <ul> <li>To reduce, you need to reduce the range: inc \\(r_\\min\\)\u00a0and dec \\(r_\\max\\)</li> </ul> </li> <li> <p>Per-tensor, per-channel</p> </li> </ul>"},{"location":"CS_Electives/Tiny_ML/05_Quantization/#rounding","title":"Rounding","text":"Speed Accuracy Nearest Good Ceil Poor Floor Fastest Poor Stochastic(Randomly up/down) Good(but large standard deviation) Logarithmic Adaptive rounding"},{"location":"CS_Electives/Tiny_ML/05_Quantization/#linear-quantization","title":"Linear Quantization","text":"\\[ \\begin{aligned} q &amp;= \\text{clip} \\{ \\ \\text{round}(r/s) + z, 0, 2^b - 1 \\ \\} \\\\ s &amp;= \\dfrac{r_\\max - r_\\min}{2^b - 1} \\end{aligned} \\] <p>where</p> <ul> <li>\\(S =\\) scale factor</li> <li>\\(z =\\) zero point</li> <li>If \\(z=0:\\) symmetric quantization</li> <li>If \\(z \\ne 0:\\) asymmetric quantization</li> <li>\\(r_\\min\\) and \\(r_\\max\\) is usually 10<sup>th</sup> and 90<sup>th</sup> percentile</li> <li>Clip function works on any values outside the range</li> </ul> <p>Dequantization $$ r = S(q - Z) $$</p> \\[ \\begin{aligned} \\hat y &amp;= W X \\\\ &amp; \\approx S_W (W_q - Z_w) S_X (X_q -Z_x) \\\\ &amp; \\approx S_W W_q S_X Z_x &amp; (\\text{Symmetric}) \\\\ &amp; \\approx S_W W_q S_X (W_q X_q) \\\\ &amp; \\quad + \\underbrace{S_w Z_w S_x Z_x + S_x Z_x S_w W_q}_{\\text{precomputed offline } \\&amp; \\\\ \\text{ folded into bias addition}} \\\\ &amp; \\qquad  + \\underbrace{S_w Z_w S_x X_q}_\\text{Online overhead} &amp; \\text{(Assymmetric)} \\end{aligned} \\] <ul> <li>\\(W_q =\\) constant during inference</li> <li>\\(X_q =\\) new data</li> </ul> <p>Online overhead is roughly equivalent to cost of adding 1 channel</p> <ul> <li>for multiple channels, it is insignificant overhead</li> <li>for single channels, it is significant overhead</li> </ul> <p>Solution</p> <ul> <li>Use symmetric quantization for weights</li> <li>Use asymmetric quantization for activations</li> </ul>"},{"location":"CS_Electives/Tiny_ML/05_Quantization/#scale-value","title":"Scale Value","text":"Per-Tensor Per-Channel Meaning Single scale/zero for entire tensor Single scale/zero for each channel in tensor Accuracy Poor(as channels can have different ranges) Good Support in hardware Good Decent?"},{"location":"CS_Electives/Tiny_ML/05_Quantization/#types","title":"Types","text":""},{"location":"CS_Electives/Tiny_ML/05_Quantization/#post-training","title":"Post Training","text":"<ul> <li>Training \u2013&gt; Quantization</li> <li>Weights</li> <li>Frozen &amp; do not change</li> <li>Can precompute scale &amp; zero point</li> <li>Activations</li> <li>scale &amp; zero-points can be calibrated using a mini batch of data</li> </ul> <p>How to find scale and zero-point</p> <ul> <li>Run \u2018representative\u2019 mini-batches of data through DNN &amp; capture \\(r_\\max\\) and \\(r_\\min\\)</li> </ul>"},{"location":"CS_Electives/Tiny_ML/05_Quantization/#quantization-aware-training","title":"Quantization-Aware Training","text":"<p>Simulate quantization during training to improve model robustness, and then use Post Training Quantization</p> <p></p> <p>\u201cFake quantization\u201d nodes</p> <ul> <li>Input: float</li> <li>Output: float</li> <li>Operation:</li> <li>Quantize</li> <li>Dequantize</li> <li>Result</li> <li>Floats that are clip and rounded like ints</li> </ul> <p></p>"},{"location":"CS_Electives/Tiny_ML/05_Quantization/#non-linear-quantization","title":"Non-Linear Quantization","text":"<p>K-means clustering</p> <ul> <li>Represent each group of numbers by a single \u201ccentroid\u201d</li> <li>Choose how many clusters you want</li> <li>For \\(k\\)\u00a0Clusters, you need \\(\\log_2 k\\)\u00a0bits to represent centroids</li> </ul> <p></p> <p>De-quantization function: Lookup table</p> Cluster Index Centroid Value 0 0.153 1 0.223 \u2026 \u2026 <p></p> <p>Advantages</p> <ul> <li>No effect on operation itself - still full precision</li> <li>Storage footprint reduced drastically</li> <li>Accuracy [typically] well preserved</li> </ul> <p>Disadvantages</p> <ul> <li>Only applicable to weights</li> </ul> <p>Outliers in LLM quantization</p> <p></p>"},{"location":"CS_Electives/Tiny_ML/05_Quantization/#product-quantization","title":"Product Quantization","text":"<p>Quantize group of numbers into a \u2018prototype\u2019</p> <p>Steps</p> <ol> <li>Prototype learning</li> <li>In an initial, offline training phase, cluster the rows of \\(A\\) (or training set \\(\\tilde A\\)) using \\(k\\) means to create prototypes</li> <li>A separate \\(k\\) means is run in each of the \\(C\\) disjoint subspaces to produce \\(C\\) sets of \\(k\\)\u00a0prototypes</li> <li>Encoding function</li> <li>Determine the most similar prototype to \\(a\\) in each subspace</li> <li>Store these assignments as integer indices using \\(C \\log_2 (k)\\) bits</li> <li>Table construction</li> <li>Precompute the dot products between \\(b\\) and each prototype in each subspace</li> <li>Store these partial dot products in \\(C\\)\u00a0lookup tables of size \\(k\\)</li> <li>Aggregation</li> <li>Use the indices and tables to lookup the estimated partial \\(a^T b\\) in each subspace</li> <li>Sum the results across all \\(C\\)\u00a0subspaces</li> </ol> <p></p>"},{"location":"CS_Electives/Tiny_ML/06_Pruning/","title":"Pruning","text":"<ol> <li>Train ANN</li> <li>Remove \\(p \\%\\) of parameters with smallest magnitude</li> </ol> <p>Terms</p> Pruning Rate \\(p\\) Sparsity Rate \\(1-p\\) Compression \\(1/s\\)"},{"location":"CS_Electives/Tiny_ML/06_Pruning/#pruning-mask","title":"Pruning Mask","text":"<p>The filter that causes the pruning $$ Y = X \\cdot (W \\odot M) $$</p>"},{"location":"CS_Electives/Tiny_ML/06_Pruning/#weight-distribution","title":"Weight Distribution","text":""},{"location":"CS_Electives/Tiny_ML/06_Pruning/#when","title":"When","text":"<ul> <li>Before training: at initiationization</li> <li>During training</li> <li>After training</li> </ul> <p>Pruning at initialization</p> <p></p>"},{"location":"CS_Electives/Tiny_ML/06_Pruning/#what","title":"What","text":"Local Global Remove least important \\(p \\%\\) of weights in each layer Remove least important \\(p \\%\\) of weights in entire DNN Better Disadvantage May cause layer collapse: Removal of all parameters of certain layer <p>Solution for Layer collapse</p> <ul> <li>Hard constraints: if lyre collapse then stop pruning</li> <li>Iterative pruning: fine-tuning helps to equalize the weight magnitudes</li> </ul> Unstructured Structured Remove blocks of size of dot product in hardware Remove Individual weights ChannelsFilters (N:M) Advantage Can be accelerated on any hardware Higher rates Compression Better Limitation May cause shape mismatch: no of channels not equal for inputs into future layer, thereby not allowing it <p></p> <p></p>"},{"location":"CS_Electives/Tiny_ML/06_Pruning/#encourage-sparsity","title":"Encourage Sparsity","text":"<p>Add regularization penalty to weights</p> <ul> <li>L1-norm</li> <li>L2-norm</li> <li>L\\(\\infty\\)-norm</li> </ul> <p></p>"},{"location":"CS_Electives/Tiny_ML/06_Pruning/#pruning-criteria","title":"Pruning Criteria","text":"<p>How to prune</p> <p>Remove least important</p> <ul> <li>Magnitude</li> <li>Gradient</li> <li>Learned</li> <li>Information</li> <li>Salience: The change in the loss function with and without a parameter</li> <li>Gradient-based</li> <li>Information-based</li> </ul>"},{"location":"CS_Electives/Tiny_ML/06_Pruning/#salience","title":"Salience","text":"\\[ \\Delta L_j (w; D) = L( 1 \\cdot w; D) - L \\Big( (1-e_j) \\cdot w; D \\Big) \\] <p>where</p> <ul> <li>\\(\\Delta L_j =\\) change in loss function</li> <li>\\(w=\\) weights</li> <li>\\(D =\\) dataset</li> <li>\\(1 =\\) unit vector</li> <li>\\((1-e_j) =\\)\u00a0pruning mask</li> </ul> <p>However, this is intractable</p>"},{"location":"CS_Electives/Tiny_ML/06_Pruning/#optimal-brain-damage","title":"Optimal brain damage","text":"<p>The hessian of a neural network is intractable to compute</p> <p>Salience $$ s_j = \\begin{cases} \\vert w_j \\vert &amp; \\text{magnitude-based} \\ \\dfrac{w^2_j}{2 \\times H^{-1}_{jj}} &amp; \\text{gradient-based}  \\end{cases} $$ Delete the lowest saliency parameters</p>"},{"location":"CS_Electives/Tiny_ML/06_Pruning/#mutual-information","title":"Mutual Information","text":"<p>Measure of how information is present in one var about another var $$ \\begin{aligned} \\vert \\Delta L(h_i) \\vert &amp;= \\vert L(D, h_i=0) - L(D, h_i=1) \\vert &amp; \\text{(1)} \\ &amp; = \\left \\vert \\dfrac{\\partial L}{\\partial h_i} h_i \\right \\vert &amp; \\text{(2)} \\end{aligned} $$</p> <ol> <li>Impact on loss function \\(L\\) when activation channel \\(h_i\\) is set to 0 and when it is not</li> <li>Gradient of loss function wrt activation map -&gt; intuitively prunes channels that don\u2019t impact loss function</li> </ol>"},{"location":"CS_Electives/Tiny_ML/06_Pruning/#fisher-information","title":"Fisher information","text":""},{"location":"CS_Electives/Tiny_ML/06_Pruning/#how-often","title":"How often","text":"<ul> <li>One-shot</li> <li>Iterative</li> </ul>"},{"location":"CS_Electives/Tiny_ML/06_Pruning/#iterative-magnitude-pruning-imp","title":"Iterative Magnitude  Pruning (IMP)","text":"<ol> <li>Train ANN</li> <li>Remove \\(p \\%\\) of params with smallest magnitude</li> <li>Retrain ANN: \u2018Fine-tune\u2019</li> <li>Repeat steps 2-3</li> </ol>"},{"location":"CS_Electives/Tiny_ML/06_Pruning/#weight-distributions","title":"Weight distributions","text":""},{"location":"CS_Electives/Tiny_ML/06_Pruning/#idk","title":"IDK","text":"<ul> <li>Dynamic iterative pruning: Allow parameters to \u2018come back to life\u2019: Correction of a pruning decision</li> <li>Runtime pruning: Learn which parameters/channels to drop at runtime based on input data</li> <li>Learnt pruning mask: Use back propagation to learn aspects of pruning algo during training</li> <li>What to prune</li> <li>Pruning Threshold per layer</li> </ul>"},{"location":"CS_Electives/Tiny_ML/07_Knowledge_Distillation/","title":"Knowledge Distillation","text":"<p>Distill \u201cknowledge\u201d from large ANN to small ANN</p> <ul> <li>Larger DNNs are easier to train</li> <li>Smaller DNNs are easier to deploy</li> </ul> <p>Targets</p> <ul> <li>Hard targets: No info about wrong classes</li> <li>Soft targets: Have info about wrong classes</li> <li>Get using expert annotation</li> <li>From a trained NN</li> </ul> <p></p> <ul> <li>Training</li> <li>Use softmax with temperature, usually \\(T=5\\) </li> <li>Loss function: Distillation loss + Student loss</li> <li>Inference</li> <li>T=1</li> </ul> <p>Teacher target can be from an ensemble of</p> <ul> <li>multiple initializations</li> <li>multiple teacher architectures</li> <li>Specialists &amp; generalists</li> </ul>"},{"location":"CS_Electives/Tiny_ML/07_Knowledge_Distillation/#distillation-types","title":"Distillation Types","text":"Type Offline Pre-trained teacher network Collaborative/mutual learning Teacher &amp; student trained simultaneously Self-distillation Eg: Progressive hierarchical inference"},{"location":"CS_Electives/Tiny_ML/07_Knowledge_Distillation/#distillation-algorithms","title":"Distillation Algorithms","text":"Adversarial Teacher also acts as discriminator in GAN to supplement training data to \u201cteach\u201d true data distribution Multi-Teacher Cross-Modal Teacher trained on RGB distills info to student learning on heat maps.Unlabeled image pairs needed Graph-Based Attention-Based Data-Free Quantized Use full-precision network to transfer knowledge to quantized network Lifelong NAS-Based"},{"location":"CS_Electives/Tiny_ML/07_Knowledge_Distillation/#knowledge-types","title":"Knowledge Types","text":"Response-based Output probs as soft targets Most common Feature-based - Output/weights of 1 or more \u201chint layers\u201d and minimize MSE lossor- Minimize difference in attention maps between student &amp; teacher Relation-based Correlations between feature maps; eg: Gramian"},{"location":"CS_Electives/Tiny_ML/08_NAS/","title":"Neural Architecture Search","text":"<ul> <li>Number of layers</li> <li>Operation at each layer</li> <li>Hyper-parameters of each operation</li> <li>Topology &amp; connectivity</li> </ul>"},{"location":"CS_Electives/Tiny_ML/08_NAS/#objective","title":"Objective","text":"<ul> <li>Minimize: Loss/Accuracy</li> <li>Minimize: Time taken to train</li> <li>Minimize: Latency</li> </ul>"},{"location":"CS_Electives/Tiny_ML/08_NAS/#search-space","title":"Search Space","text":"<ul> <li>Operation types \\(o\\)</li> <li>Number of layers \\(n\\)</li> </ul> <p>Total number of possible DNNs \\(= o^d\\)</p>"},{"location":"CS_Electives/Tiny_ML/08_NAS/#training-techniques","title":"Training Techniques","text":"<ul> <li>Full training</li> <li>Proxy instead of training</li> <li>Parameter sharing</li> </ul>"},{"location":"CS_Electives/Tiny_ML/08_NAS/#algorithms","title":"Algorithms","text":"<ul> <li>Random search</li> <li>Reinforcement learning</li> <li>Genetic algorithms</li> <li>Prediction-based search</li> <li>Differential architecture search</li> </ul>"},{"location":"CS_Electives/Tiny_ML/08_NAS/#reinforcement-learning","title":"Reinforcement Learning","text":"<pre><code>flowchart LR\nc[\"Controller&lt;br&gt;DNN\"] ---&gt;\n|Sample arhicterue A&lt;br&gt;with prob p| t[\"Train child network&lt;br&gt;with architecture A&lt;br&gt;to obtain accuracy R\"] ---&gt;\n|Compute gradient of p&lt;br&gt;and scale it by R&lt;br&gt;to update controller| c</code></pre> <p>Policy gradient algorithm</p> <ul> <li>Function parameterized with parameters \\(\\theta\\): DNN</li> <li>Action: DNN architecture parameters</li> <li>State space: All set of possible DNNs in search space</li> <li>Reward to update \\(\\theta\\): Objective (accuracy)</li> </ul>"},{"location":"CS_Electives/Tiny_ML/08_NAS/#predictor-based-search","title":"Predictor-based search","text":"<p>We don\u2019t need exact accuracy of architecture; just need the relative rankings of different architectures</p> <ol> <li>Train predictor on \\(T\\) DNNs</li> <li>Use predictor to sort \\(N\\) models, where \\(N &gt;&gt; T\\)</li> </ol> <p></p>"},{"location":"CS_Electives/Tiny_ML/08_NAS/#differentiable-architecture-search","title":"Differentiable Architecture Search","text":"<p>Optimize architecture parameters \\(\\alpha\\) during training, then keep operations with largest \\(\\alpha\\)</p> <ul> <li>Bi-level optimization</li> <li>DARTS</li> </ul> <p></p>"},{"location":"CS_Electives/Tiny_ML/08_NAS/#nas-efficiency","title":"NAS Efficiency","text":"<ul> <li>More efficient search algorithm</li> <li>Smaller search space</li> <li>Inexpensive accuracy proxy rather than training</li> </ul>"},{"location":"CS_Electives/Tiny_ML/08_NAS/#cell-based-nas","title":"Cell-based NAS","text":"<p>\\(c\\) cells, \\(o\\) ops per cell, each op has \\(k\\) choices</p> Search Space Micro Search for a \u201ccell\u201dPlace the same cell everywhere \\(k^o\\) Macro Search for each cell independently \\(k^{o^c}\\) <p></p>"},{"location":"CS_Electives/Tiny_ML/08_NAS/#parameter-sharing","title":"Parameter Sharing","text":"<p>Use trained parameters from previous sample</p> <p>1000x faster than conventional NAS</p> <p>Inspired by Transfer Learning</p>"},{"location":"CS_Electives/Tiny_ML/08_NAS/#training-proxies","title":"Training Proxies","text":"<p>Instead of training to convergence to evaluate each model, estimate accuracy by reducing</p> <ul> <li>Number of epochs</li> <li>Subsample training set</li> <li>Lower data resolution</li> <li>Downscale model being evaluated</li> </ul>"},{"location":"CS_Electives/Tiny_ML/08_NAS/#hardware-aware-nas","title":"Hardware-aware NAS","text":"<p>Customize DNN architecture for specific device</p> <ul> <li>Hard constraint: Reject models slower than target</li> <li>Soft constraint: Weighted sum of accuracy &amp; latency</li> </ul> <p>Latency evaluation</p> <ul> <li>Run inference on target device</li> <li>Estimation</li> <li>Graph neural networks</li> <li>No of parameters</li> <li>Use FLOPs</li> </ul>"},{"location":"CS_Electives/Tiny_ML/08_NAS/#codesign-nas","title":"Codesign NAS","text":"<p>Add hardware search space</p>"},{"location":"CS_Electives/Tiny_ML/09_Kernel_Computation/","title":"Kernel Computation","text":""},{"location":"CS_Electives/Tiny_ML/09_Kernel_Computation/#machine-learning-software-stack","title":"Machine Learning Software Stack","text":"<p>PyTorch is just a wrapper for writing CuDNN/MKL-DNN code</p> <p></p>"},{"location":"CS_Electives/Tiny_ML/09_Kernel_Computation/#kernel-implementations","title":"Kernel Implementations","text":"Limitation Im2col Convert image windows to columns of matrixorReplicate weights instead and flatten imageUse to implement convolution as matrix multiplication Data replication at algorithmic level may increase demand for external memory bandwidth Strassen\u2019s MM transform Reduce no of multiplications in MM through reorganizing operations and offline computation Transform limitation Winograd Conversion transform Reduce no of multiplications in Conv through reorganizing operations and offline computationSpecific to- supported filter size- tile size of input Transform limitation Alpha Tensor FFT-Transform Conv becomes multiplicationFilter needs to zero-pad to ensure same size as outputOnly useful for filter size &gt;= log of output size for effectiveness, else IFFT overhead exceeds the gain Transform limitationIFFT is costly overhead Log-domain multiplication \\(ab = 2^x 2^y = 2^{x+y} = 2^z\\)Only convert magnitude of numbersCompute sign using small circuit\\(s_c = s_a \\oplus s_b\\) Finding log &amp; exponents at high precision is expensiveNo straightforward add operation in log domain <p>Transform limitation: Requires transform to be performed at high precision to avoid accuracy detoriation</p>"},{"location":"CS_Electives/Tiny_ML/09_Kernel_Computation/#low-rank-approximation","title":"Low-Rank Approximation","text":"SVD: Singular Value Decomposition \\(M = U \\Sigma V\\)Speedup = \\(\\dfrac{mn}{k (m+n)}\\) Tensor decomposition Tucker DecompositionCanonical Polyadic"},{"location":"CS_Electives/Tiny_ML/10_Mapping/","title":"Mapping &amp; Compilation","text":""},{"location":"CS_Electives/Tiny_ML/10_Mapping/#parts-of-compiler","title":"Parts of Compiler","text":"<ul> <li>Frontend: Prog lang to IR (Intermediate Representation)</li> <li>Middle-end (Optimizer)</li> <li>Backend: IR to Assembly/Machine code (Code generator)</li> </ul>"},{"location":"CS_Electives/Tiny_ML/10_Mapping/#ml-compilation-system","title":"ML Compilation System","text":""},{"location":"CS_Electives/Tiny_ML/10_Mapping/#mapping-onto-hardware","title":"Mapping onto Hardware","text":"<ul> <li>Dataflow choice</li> <li>Tiling</li> <li>Vectorization</li> <li>Bind ops to PEs</li> <li>On-chip memory management</li> </ul>"},{"location":"CS_Electives/Tiny_ML/10_Mapping/#dataflow-selection","title":"Dataflow selection","text":"<p>Compiler can re-order loops without changing functionality</p> <p>Compiler heuristics can model approximate effect on runtime and memory access</p> <p>Eg: 1D Convolution</p> Weight stationary Output stationary Input stationary"},{"location":"CS_Electives/Tiny_ML/10_Mapping/#tiling","title":"Tiling","text":"<p>Choose tile size on which to operate, to fit data in various parts of memory system</p> <p>Break a loop into nested loops, each of which can be mapped hierarchically onto memory system (DRAM, SRAM, Registers)</p> <p></p> <p>Other names</p> <ul> <li>CUDA: Thread Block</li> <li>OpenCL: Work Group</li> </ul> <p></p>"},{"location":"CS_Electives/Tiny_ML/10_Mapping/#vectorization","title":"Vectorization","text":"<p>Parallelize operations within smallest tile, to leverage hardware parallelism</p> <p></p> <p></p>"},{"location":"CS_Electives/Tiny_ML/10_Mapping/#binding","title":"Binding","text":"<p>Specify PE index \\(i\\) that will execute loop iteration \\(j\\)</p> <p>Applicable when no of PEs \\(\\ne\\) no of loop iterations</p>"},{"location":"CS_Electives/Tiny_ML/10_Mapping/#on-chip-memory-management","title":"On-chip memory management","text":""},{"location":"CS_Electives/Tiny_ML/10_Mapping/#graph-compiler","title":"Graph Compiler","text":""},{"location":"CS_Electives/Tiny_ML/10_Mapping/#on-chip-buffer","title":"On-Chip buffer","text":"<p>\u201cSpatio-temporal tetris\u201d</p> <p>Mem management passes:</p> <ul> <li>Scheduling: order of subgraph execution</li> <li>Allocation: where to put data in buffer</li> <li>Slicing/fusing: how to break/merge operations</li> </ul> <p></p> <p></p>"},{"location":"CS_Electives/Tiny_ML/10_Mapping/#mapping-space","title":"Mapping Space","text":"<p>Usually very large</p> <ul> <li>Many mappings are functionally-identical; eg: binding operations differently</li> <li>Many mappings are invalid; eg: tile size doesn\u2019t fit in memory</li> </ul> <p>Navigate space using heuristics</p> <p>Informed by device performance/energy models</p>"},{"location":"CS_Electives/Tiny_ML/10_Mapping/#dla-isa","title":"DLA ISA","text":"<ul> <li>Domain-specific, simple ISAs</li> <li>VLIW: Very Long Instruction Word is common</li> </ul> <p>As time progresses, for DLAs, compiler need not worry about loop nests/mapping data flows, as this is all handled in hardware</p> <p>Operation fusion: Coarse-grained optimizations</p> <p></p>"},{"location":"CS_Electives/Tiny_ML/11_Processing/","title":"Pre/Post Processing","text":"<p>Amdahl\u2019s Law</p>"},{"location":"CS_Electives/Tiny_ML/11_Processing/#layers-of-overhead-for-dnn","title":"Layers of Overhead for DNN","text":"<p>At the application level, \u201coverheads\u201d can take more time than the DNN itself</p> <p></p> <p>Example: Face Recognition</p> <p></p> <p></p> <p></p>"},{"location":"CS_Electives/Tiny_ML/11_Processing/#host-accelerator","title":"Host + Accelerator","text":"<ul> <li> <p>Model</p> </li> <li> <p>Sits in CPU main memory</p> </li> <li> <p>Transferred over PCIe to GPU mem</p> </li> <li> <p>Input data</p> </li> <li> <p>Arrives over ethernet to CPU</p> </li> <li>Transferred over PCIe to CPU</li> <li>Inference/Training</li> <li>Result send back to CPU over PCIe</li> </ul> <p>Latency impacted by data transfers</p>"},{"location":"CS_Electives/Tiny_ML/11_Processing/#solution-1-multiple-gpu-cores","title":"Solution 1: Multiple GPU Cores","text":""},{"location":"CS_Electives/Tiny_ML/11_Processing/#solution-2-multiple-cpu-cores","title":"Solution 2: Multiple CPU Cores","text":""},{"location":"CS_Electives/Tiny_ML/11_Processing/#solution-3-dedicated-gpu-gpu-connections","title":"Solution 3: Dedicated GPU-GPU connections","text":"<p>NVLink</p> <p></p>"},{"location":"CS_Electives/Tiny_ML/11_Processing/#dedicated-fpga-for-packet-processing","title":"Dedicated FPGA for Packet Processing","text":""},{"location":"CS_Electives/Tiny_ML/11_Processing/#algorithm-codesign-opportunities","title":"Algorithm Codesign Opportunities","text":""},{"location":"CS_Electives/Tiny_ML/11_Processing/#on-device-deployment","title":"On-Device Deployment","text":"<p>CPU, GPU, NPU share the same memory on the SOC (System On Chip)</p>"},{"location":"CS_Electives/Tiny_ML/11_Processing/#mobile-cloud-inference","title":"Mobile-Cloud Inference","text":""},{"location":"CS_Electives/Tiny_ML/12_Decentralized_Learning/","title":"Decentralized Learning","text":""},{"location":"CS_Electives/Tiny_ML/12_Decentralized_Learning/#types","title":"Types","text":"Distributed Offloading Federated Collaborative Learning Send data to central server for trainingDevice only used as sensor - Data never stored in data center- Encrypt data and only decrypt after averaging 1000 updates Each device maintains functional model Model Location Servers Servers DeviceAggregated on Cloud Device Data Location DeviceServers DeviceServers Device Device Design goals Speed PrivacyOnline learningSecurityScale Device Types Same Different Device Compute Power High Low Training Complex Simple Run training when phone chargingTransmit updates when WiFi available Training examples Next-word prediction Number of devices 10-1k 100k+ Network speed Fast Slow Network reliability Reliable Intermittent Data Distribution IID Non-IID(Each device has own data distribution)Not representative of training data Applications - Privacy  - Personal data from devices  - Health data from hospitals- Continuous data  - Smart home/city  - Autonomous vehicles Advantages - Save device battery- No need to support on-device training- Better accuracy? - Most secure: Data never aggregated to a central server that could be compromised- Most scalable: No central server with bandwidth limitations Limitations - poor privacy- worse scalability Not fully private: You can recover data from model parameters/gradient updatesConsumes higher total energy Challenges Poor network All challenges of FL Example Google Photos"},{"location":"CS_Electives/Tiny_ML/12_Decentralized_Learning/#terms","title":"Terms","text":"Straggler Device that doesn\u2019t return data on time Data Imbalance One devices has 10k samples, while 10k devices have 1 sample each"},{"location":"CS_Electives/Tiny_ML/12_Decentralized_Learning/#terms_1","title":"Terms","text":""},{"location":"CS_Electives/Tiny_ML/12_Decentralized_Learning/#compression","title":"Compression","text":"<ul> <li>Gradient</li> <li>Data</li> </ul>"},{"location":"CS_Electives/Tiny_ML/12_Decentralized_Learning/#quantization","title":"Quantization","text":"<p>Quantization to gradients before transmission</p> <p>Communication cost drops linearly with bit width</p>"},{"location":"CS_Electives/Tiny_ML/12_Decentralized_Learning/#pruning","title":"Pruning","text":"<p>Prune gradients based magnitude and compress zeroes</p>"},{"location":"CS_Electives/Tiny_ML/12_Decentralized_Learning/#distributed-training","title":"Distributed Training","text":"<ul> <li>Model Parallelism: Fully-Connected layers</li> <li>Data Parallelism: Convolutional layers</li> </ul>"},{"location":"CS_Electives/Tiny_ML/12_Decentralized_Learning/#single-gpu-system","title":"Single GPU-system","text":""},{"location":"CS_Electives/Tiny_ML/12_Decentralized_Learning/#model-parallelism","title":"Model Parallelism","text":"<p>All workers train on same batch</p> <p>Workers communicate as frequently as network allows</p> <p></p> <p>Necessary for models that do not fit on a single GPU</p> <p>No method to hide synchronization latency</p> <ul> <li>Have to wait for data to be sent from upstream model split</li> <li>Need to think about how pipelining would work for model-parallel training</li> </ul> <p>Types</p> <ul> <li>Inter-layer</li> <li>Intra-layer</li> </ul> <p>Limitations</p> <ul> <li>Overhead due to</li> <li>moving data from one GPU to another via CPU</li> <li>Synchronization</li> <li>Pipelining not easy</li> </ul>"},{"location":"CS_Electives/Tiny_ML/12_Decentralized_Learning/#data-parallelism","title":"Data Parallelism","text":"<p>Each worker trains the same convolutional layers on a different data batch</p> <p>Workers communicate as frequently as network allows</p> Communication Overhead Advantage Limitation Single-GPU Multiple GPU Average gradients across minibatch on all GPUsOver PCIe, ethernet, NVLink depending on system \\(kn(n-1)\\) High communication overhead Parameter Server Parallel Parameter Sharing \\(k\\) per worker\\(kn/s\\) for server Ring Allreduce Each GPU has different chunks of the mini-batch \\(2k\\dfrac{n-1}{n}\\) ScalableCommunication cost independent of \\(n\\) <p>where - \\(n=\\) no of client GPUs - \\(k =\\) no of gradients - \\(s=\\) no of server GPUs</p>"},{"location":"CS_Electives/Tiny_ML/12_Decentralized_Learning/#ring-allreduce","title":"Ring-Allreduce","text":""},{"location":"CS_Electives/Tiny_ML/12_Decentralized_Learning/#step-1-reduce-scatter","title":"Step 1: Reduce-Scatter","text":""},{"location":"CS_Electives/Tiny_ML/12_Decentralized_Learning/#step-2-allgather","title":"Step 2: Allgather","text":""},{"location":"CS_Electives/Tiny_ML/12_Decentralized_Learning/#weight-updates-types","title":"Weight Updates Types","text":"Synchronous Asynchronous Working - Before forward pass, fetch latest parameters from server- Compute loss on each GPU using these latest parmeters- Gradients sent back to server to update model Speed per epoch Slow Fast Training convergence Fast Slow Accuracy Better Worse"},{"location":"CS_Electives/Tiny_ML/12_Decentralized_Learning/#pipeline-parallelism","title":"Pipeline Parallelism","text":""},{"location":"CS_Electives/Tiny_ML/12_Decentralized_Learning/#federated-learning","title":"Federated Learning","text":"<p>\u201cFederated\u201d: Distributed but \u201creport to\u201d one central entity</p> <p>Conventional learning</p> <ul> <li>Data collection</li> <li>Data Labeling (if supervised)</li> <li>Data cleaning</li> <li>Model training</li> </ul> <p>But new data is generated very frequently</p>"},{"location":"CS_Electives/Tiny_ML/12_Decentralized_Learning/#steps","title":"Steps","text":"<ol> <li>Download model from cloud to devices</li> <li>Personalization: Each device trains model on its own local data</li> <li>Devices send their model updates back to server</li> <li>Update global model</li> <li>Repeat steps 1-4</li> </ol> <p>Each iteration of this loop is called \u201cround\u201d of learning</p>"},{"location":"CS_Electives/Tiny_ML/12_Decentralized_Learning/#algorithms","title":"Algorithms","text":"Handling Stragglers Handling Data Imbalance FedAvg The more data points a device has, the higher weight of device in updating global model Drop Poor FedProx Use partial results Discourage large weight updates through regularization\\(\\lambda {\\vert \\vert w' - w \\vert \\vert}^2\\)\\(w=\\) Weight of single device q-fed-avg Discourage large weight updates for any single device per-per-avg"},{"location":"CS_Electives/Tiny_ML/12_Decentralized_Learning/#data-labelling","title":"Data Labelling","text":"<p>How to get labels</p> <ul> <li>Sometimes explicit labeling not required: Next-work prediction</li> <li>Need to incentivize users to label own data: Google Photos</li> <li>Use data for unsupervised learning</li> </ul>"},{"location":"CS_Electives/Tiny_ML/12_Decentralized_Learning/#types_1","title":"Types","text":"Horizontal Vertical Transfer"},{"location":"Finance_Electives/Accounts/","title":"Fundamentals of Finance &amp; Accounts","text":"<p>This course serves as an introduction to the reporting system utilized by businesses to communicate financial information to external users, along with foundational concepts in financial markets and management.</p> <p>In the first part of the course, the focus is on understanding financial reports, which are the final products of the accounting system, and what insights they provide about a business enterprise.</p> <p>The second part emphasizes financial markets, covering topics such as market reforms, primary and secondary markets, sources of investment information, portfolio selection, and preliminary concepts of financial management.</p>"},{"location":"Finance_Electives/Accounts/#accounting-spreadsheets","title":"Accounting Spreadsheets","text":"<p>Available in different formats</p> <ul> <li>HTML</li> <li>Google Sheets</li> <li>Excel</li> </ul>"},{"location":"Finance_Electives/Accounts/#financial-statements-dashboard","title":"Financial Statements Dashboard","text":"<ul> <li>Dashboard</li> <li>GitHub Repository</li> </ul>"},{"location":"Finance_Electives/Accounts/01_Introduction/","title":"01 Introduction","text":""},{"location":"Finance_Electives/Accounts/01_Introduction/#accounts-vs-finance","title":"Accounts vs Finance","text":"Accounting Finance Past-Looking Future-Looking Aspect Reporting Planning Easy to automate \u2705 \u274c"},{"location":"Finance_Electives/Accounts/01_Introduction/#accounting","title":"Accounting","text":"<ul> <li>Measure performance of organization</li> <li>Provides useful info for decision-making</li> </ul>"},{"location":"Finance_Electives/Accounts/01_Introduction/#public-offerings","title":"Public Offerings","text":"<p>When a corporation wants to list itself in the stock market, there are offerings (for valuation of my company)</p> Meaning IPO Initial Public Offer FPO Follow-Up Public Offer"},{"location":"Finance_Electives/Accounts/01_Introduction/#earnings-management","title":"Earnings Management","text":"<p>Situation when management tries to increase their earnings, by intentionally focusing on increasing stock valuation, and nothing else.</p>"},{"location":"Finance_Electives/Accounts/01_Introduction/#brances-of-accounting","title":"Brances of Accounting","text":"Branch Alias Name Purpose External Financial Accounting Official records Internal Management Accounting(Revenue Optimization, Internal Audit) Decision process"},{"location":"Finance_Electives/Accounts/01_Introduction/#financial-accounting","title":"Financial Accounting","text":"<p>Systematic procedure of recording, classifying, summarizing, analyzing, and reporting a business\u2019 monetary transactions</p>"},{"location":"Finance_Electives/Accounts/01_Introduction/#advantages","title":"Advantages","text":"<p>The systematic record helps</p> <ul> <li>Protect property</li> <li>Communicate results to interested parties</li> <li>Comply with legalities</li> </ul>"},{"location":"Finance_Electives/Accounts/01_Introduction/#disadvantages","title":"Disadvantages","text":"<ul> <li> <p>Records only monetary transactions</p> </li> <li> <p>Inflation not considered</p> </li> <li> <p>Historical in nature</p> </li> <li> <p>Costly</p> </li> <li> <p>Window-dressing</p> </li> </ul> <p>Fund manager sells stocks with large losses and purchases high-flying stocks near the end of the quarter or year</p>"},{"location":"Finance_Electives/Accounts/01_Introduction/#interested-parties","title":"Interested Parties","text":"Party Why are they interested in accounts Investors If they will get return on their investment Creditors If they will get loan back Workers If a company closes, you will not get salary Customers If a company closes, you will not get maintanance/replacement Government Checking for Fraud"},{"location":"Finance_Electives/Accounts/01_Introduction/#financial-management","title":"Financial Management","text":"<p>Decisions on</p> <ul> <li>Investment</li> <li>Financing</li> <li>Dividend</li> </ul>"},{"location":"Finance_Electives/Accounts/01_Introduction/#goal-of-corporation","title":"Goal of Corporation","text":"<p>Maximizes current wealth, ie value of company; not maximize profit</p>"},{"location":"Finance_Electives/Accounts/01_Introduction/#expenditure-vs-expenses","title":"Expenditure vs Expenses","text":"Expenditure Expenses Meaning Buying assets for long-run gain Payment for something you have already consumed Example InvestmentBuying laptop Giving SalaryDepreciation of assetsBuying chocolates"},{"location":"Finance_Electives/Accounts/01_Introduction/#asset","title":"Asset","text":"<p>Anything that provides you right to use/gain renenue generated by that item</p> <p>eg: Prepaid Telephone Service, Accrued income (deferred income; revenue that's been earned, but has yet to be received)</p> Type Duration Nature changes Example Current Period of 12months Short \u2705 Inventory Non-current/Fixed Long \u274c Land, Buildings Fictitious deferred revenue expenditures with no resale value Set up costsCompany LogoGoodwill"},{"location":"Finance_Electives/Accounts/01_Introduction/#types-of-assets","title":"Types of Assets","text":"Real Asset Financial Asset/Securities Something that can generate revenue Contracts that provide a right to gain income from real assetsOne party\u2019s financial assets are another party\u2019s liability Can be tangible/intangible \u2705 \u2705 Liquidity Low High Examples Tangible (Machinery, property)Intangible (Technical know-ho) SharesBondsThese can be tangible (hard-copy)/intangible Purpose Functioning of businessRepresents society\u2019s wealth Separation of ownership &amp; managementEfficient allocation of capitalEfficient allocation of risk <p>Securities are actually financial assets with collateral, but they are used interchangeably</p>"},{"location":"Finance_Electives/Accounts/01_Introduction/#financial-assets","title":"Financial Assets","text":"Asset Income Ownership/Equity Claims(Equity Shares/Common Stocks) Variable Creditor Claims(Bonds) Fixed Hybrid Claims(Preferential Stocks) Hybrid Derivatives(Claims on other financial assets) Synthetic Indirect Financial Assets(Mutual funds, Hedge funds) Synthetic"},{"location":"Finance_Electives/Accounts/01_Introduction/#bonds","title":"Bonds","text":"<p>Interest rate of bond depends on</p> <ul> <li>Credit rating of corporation</li> <li>Duration</li> <li>Short-term bond has low interest rate</li> <li>Long-term bond has high interest rate</li> </ul>"},{"location":"Finance_Electives/Accounts/01_Introduction/#derivatives","title":"Derivatives","text":"<p>Synthetically-created instrument that obtains value from underlying set of assets</p> <ul> <li>Spot price</li> <li>Expiry</li> <li>Exercise price</li> </ul>"},{"location":"Finance_Electives/Accounts/01_Introduction/#types","title":"Types","text":"Type Forward Future contract Call Option Right to buy an asset Put Option Right to sell an asset Swap- CDS (Credit Default Swap)"},{"location":"Finance_Electives/Accounts/01_Introduction/#indirect-investing","title":"Indirect Investing","text":"<ul> <li>Hedge funds invest at a higher risk-return compared to Mutual funds</li> </ul>"},{"location":"Finance_Electives/Accounts/01_Introduction/#liability","title":"Liability","text":"<p>Anything that has requirement for you to pay for it</p> <p>eg: Outstanding expenses, Unaccrued income (receiving advance payment, bank overdrafting)</p> Type Example Current Short Term(Period of 12months) Rent Non-Current Long Term Loans Contingent Conditional Guarantee/Warranty"},{"location":"Finance_Electives/Accounts/01_Introduction/#keywords","title":"Keywords","text":"Underwriters Assurance that they will buy in case the value of a commodity reduceseg: Insurance Standards The standards we follow is INDAS (INDian Accounting Standards) Market Cap Valuation Free-Floating Ratio Ratio of stocks publicly available for purchase Liquidity Ratio of assets in cash BPL Below poverty line White Goods Household/domestic goods, such as fridge, dryers, etc"},{"location":"Finance_Electives/Accounts/01_Introduction/#documents","title":"Documents","text":"Document Meaning Time Shows Journal Chronological record of transactions continuous Debit, Credit Ledger Classification of transactions into different accounts continuous Debit, Credit Trial Balance list of balances of the ledger accounts at a point of time Debit, Credit Balance Sheet Shows the assets and liabilities at particular instant Assets, Liabilities Profit-Loss/Income Statement Shows the revenue and expenses relevant to the core operations of company during period of time Assets, Liabilities Cashflow Statement Show the source and uses of cash during period of time"},{"location":"Finance_Electives/Accounts/02_Principles_of_Accounting/","title":"02 Principles of Accounting","text":""},{"location":"Finance_Electives/Accounts/02_Principles_of_Accounting/#accounting-concepts","title":"Accounting Concepts","text":"Money Measurement All transactions are recorded in terms of currency (monetary value)Only money transactions are recorded Business Entity A corporation is considered as a separate legal entity from its owners Dual Aspect Assets = Liabilities + Capital(Assets + Expenses = Liabilities + Equity + Revenue) Going Concern Assumption that business will continue to trade for the foreseable futureAllows cost of purchases to be distributed across time Historical Cost Assets are quantified by their cost &amp; not their market value Accounting Period Matching Cost When calculating net income, only relevant revenue and relevant expenses for the specific accounting period should be used Revenue Realisation Sale should be considered as complete only after complete exchange of ownership between both parties, for the goods/services"},{"location":"Finance_Electives/Accounts/02_Principles_of_Accounting/#accounting-conventions","title":"Accounting Conventions","text":"Full Disclosure All information should be informed to users of statements Materiality Only those information should be stated that impacts the decision of the users of statement and unnecessary information should not be disclosed Conservatism Incorporate anticipated losses, but not anticipated profits Consistency Transactions/events are recorded in the same way, in every accounting period"},{"location":"Finance_Electives/Accounts/02_Principles_of_Accounting/#derivation-of-accounting-equation","title":"Derivation of Accounting Equation","text":"\\[ \\begin{aligned} \\text{Assets} &amp;= \\underbrace{\\text{Liabilities + Capital}}_\\text{Total Liabilities} \\\\ \\text{Capital} &amp;= \\text{Equity+Net Margin} \\\\ \\text{Net Margin} &amp;= \\text{Revenue-Expenses} \\\\ \\implies \\text{Assets + Expenses} &amp;= \\text{Liabilities + Equity + Revenue} \\end{aligned} \\] <ul> <li>Capital: Soft Contract</li> <li>Liabilities: Hard Contract</li> </ul>"},{"location":"Finance_Electives/Accounts/02_Principles_of_Accounting/#something","title":"Something","text":"<ul> <li>Debit: Indebted</li> <li> <p>Creditor: Gets credit for the transaction</p> </li> <li> <p>Capital is when owners invest funds</p> </li> <li>Drawings is when owners withdraw funds</li> </ul>"},{"location":"Finance_Electives/Accounts/03_Process_of_Accounting/","title":"03 Process of Accounting","text":""},{"location":"Finance_Electives/Accounts/03_Process_of_Accounting/#types-of-accounts","title":"Types of Accounts","text":"Account Debit Credit Personal Receiver Giver Real What comes in What goes out Nominal Expenses &amp; Losses Income &amp; Gains"},{"location":"Finance_Electives/Accounts/03_Process_of_Accounting/#accounting-equation","title":"Accounting Equation","text":"Assets + Expenses Liabilities + Equity + Revenue Increases Dr. Cr. Decreases Cr. Dr."},{"location":"Finance_Electives/Accounts/03_Process_of_Accounting/#accounting-cycle","title":"Accounting Cycle","text":"<pre><code>flowchart LR\nsd[Source&lt;br/&gt;Documents] --&gt;\n|Journalize| Journal --&gt;\n|Posting| Ledger --&gt;\ntb[Trial&lt;br/&gt;Balance]--&gt;\nfs\n\nsubgraph fs[Financial&lt;br/&gt;Statements]\n    direction LR\n    bs[Balance&lt;br/&gt;Sheet]\n    pl[Profit-Loss/&lt;br/&gt;Income]\n    cf[Cashflow]\nend</code></pre>"},{"location":"Finance_Electives/Accounts/03_Process_of_Accounting/#journal","title":"Journal","text":"<p>Record of transactions, regardless of income, expenses, etc</p> Date Particulars Debit(Dhs) Credit(Dhs) 2022-01-01 Cash A/C Dr. 20,000 To Capital A/C 20,000 (Being commencement of business) <p>To means: Debitor(Dr.) is indebted to Creditor(Cr.)</p>"},{"location":"Finance_Electives/Accounts/03_Process_of_Accounting/#purpose","title":"Purpose","text":"<ul> <li>provides permanent record</li> <li>provides information of debit and credit in an entry and an explanation</li> <li>reduces the possibility of error as both aspects of a business transaction are written side by side</li> </ul>"},{"location":"Finance_Electives/Accounts/03_Process_of_Accounting/#compound-journal-entries","title":"Compound Journal Entries","text":"Date Particulars Debit (Dhs) Credit(Dhs) 2022-01-01 Cash A/C Dr. 20,000 To Electricity Company 10,000 To Water Company 10,000 (Expenditure on Utilities) <p>Transactions involving Discount A/C are always compound journal entries</p>"},{"location":"Finance_Electives/Accounts/03_Process_of_Accounting/#ledger","title":"Ledger","text":"<p>Summary statement of all the transactions relating to a person, asset, expense or income which have taken place during a given period of time and shows their net effect</p> <ul> <li>Debit side = Receipts side</li> <li>Credit side = Payment side</li> </ul> <p>It is in a \u2018T\u2019 form</p> <p></p> Dr &lt; Account Name &gt; Cr. Date Particulars Amount Date Particulars Amount 2022-01-01 To Credit A/C 2022-01-01 By Debit A/C Total \\(d\\) Total \\(c\\)"},{"location":"Finance_Electives/Accounts/03_Process_of_Accounting/#overage-balance","title":"Overage Balance","text":"<p>Overage = surplus of debit/credit</p> Symbol Meaning C/D Carried Down B/D Brought Down Case Dr Cr \\(d&gt;c\\) To Balance B/D By Balance C/D \\(d&lt;c\\) To Balance C/D By Balance B/D"},{"location":"Finance_Electives/Accounts/03_Process_of_Accounting/#trial-balance","title":"Trial Balance","text":"<p>List of overages of the ledger accounts at a particular point of time</p> <p>The selected side (debit/credit) is the one having the amount brought down to next period.</p> <p>If everything is right, the trial balance should result in</p> \\[ \\sum \\text{Debit Overages} = \\sum \\text{Credit Overages} \\] S. No Name of A/C Debit (Dhs) Credit (Dhs) \\(a\\) \\(b\\) Total \\(k\\) \\(k\\)"},{"location":"Finance_Electives/Accounts/04_Financial_Statements/","title":"04 Financial Statements","text":"<p>In our course, we will work on the simpler \u2018horizontal\u2019 version of each financial statement.</p>"},{"location":"Finance_Electives/Accounts/04_Financial_Statements/#balance-sheet","title":"Balance Sheet","text":"Liabilities Amount Asset Amount"},{"location":"Finance_Electives/Accounts/04_Financial_Statements/#types","title":"Types","text":"Contains Standalone Company Consolidated Company,"},{"location":"Finance_Electives/Accounts/04_Financial_Statements/#cashflow-statement","title":"Cashflow Statement","text":"Particular Inflow Outflow Opening Balance $120,000 Operating Activities Sales $400,000 Supplies $9,000 Rent $60,000 Salaries $330,000 Investing Activities Additional Equipment $55,000 Financing Activities 0 Total Inflow $400,000 Total Outflow $454,000 Net Cashflow -$54,000 Closing Balance $66,000"},{"location":"Finance_Electives/Accounts/04_Financial_Statements/#profitloss-statement","title":"Profit/Loss Statement","text":"<p>Also called as comprehensive Income Statement</p> Amount Sales Cost of Goods Sold Gross Margin XXX - Operational Costs (Office &amp; Selling Expenses) Operating Profits XXX + Non-Operating Income + Non-Operating Losses EBIT (Earnings before Interests and Tax) XXX - Interest - Other financing costs PBT (Profit before Tax) XXX Provision for Tax PAT (Profit after Tax)/Net Margin XXX <p>You only include the expenses used for the revenue generation.</p>"},{"location":"Finance_Electives/Accounts/04_Financial_Statements/#change-in-equity-statement","title":"Change in Equity Statement","text":""},{"location":"Finance_Electives/Accounts/04_Financial_Statements/#notes","title":"Notes","text":"<p>Explanation notes of system you followed to obtain the above statements</p>"},{"location":"Finance_Electives/Accounts/05_Fixed_Assets_%26_Depreciation/","title":"05 Fixed Assets & Depreciation","text":""},{"location":"Finance_Electives/Accounts/05_Fixed_Assets_%26_Depreciation/#fixed-assets","title":"Fixed Assets","text":"<p>Also called as non-current assets</p> <p>Long-term assets that can be</p> <ul> <li>Tangible (Land, Buildings)</li> <li>Intangible (Goodwill, Patent, Knowhow)</li> </ul>"},{"location":"Finance_Electives/Accounts/05_Fixed_Assets_%26_Depreciation/#importance-of-fixed-assets-identification","title":"Importance of fixed assets identification","text":"Financial Statement Understand Why important? Profit Loss Statement Expensed Less profit this yearLess Depreciation in Coming yearsLess assets in Balance Sheet Balance Sheet Capitalized More profit this yearMore depreciation in coming yearsMore assets in Balance Sheet"},{"location":"Finance_Electives/Accounts/05_Fixed_Assets_%26_Depreciation/#cost-of-fixed-asset","title":"Cost of Fixed Asset","text":"<p>All expenses necessary to make asset ready for intented use</p> <ul> <li>Purchase price [Net ie after making adjustment for taxes(excluded), rebates, etc]</li> <li>Cost of site preparation</li> <li>Delivery and handling cost</li> <li>Installation cost</li> <li>Training/Professional fee</li> <li>Cost of trial run</li> <li>Maintanance</li> </ul>"},{"location":"Finance_Electives/Accounts/05_Fixed_Assets_%26_Depreciation/#useful-life-of-an-asset","title":"Useful life of an asset","text":"\\[ \\text{Useful Life} \\le \\text{Physical Life} \\] <p>Estimated based on - Expected physical wear and tear - Obsolescence (not used anmyroe - Legal/limits on use assets</p> <p>Expressed in terms of time period/production units (hrs, km)</p>"},{"location":"Finance_Electives/Accounts/05_Fixed_Assets_%26_Depreciation/#depreciation","title":"Depreciation","text":"<p>Means of cost allocation</p> <p>Depreciation \\(\\ne\\) Valuation</p>"},{"location":"Finance_Electives/Accounts/05_Fixed_Assets_%26_Depreciation/#methods-of-depreciation","title":"Methods of Depreciation","text":"Straight-line Declining balance/Written-Down value DepreciationAmount \\(D\\) \\(\\frac{\\text{Original Asset Cost} - \\text{Estimated Scrap Value}}{\\text{Estimated Life of Asset}}\\) Value at time \\(t\\) \\(V_0 - tD\\) \\(V_0 (1-r)^t\\)where \\(r =\\) depreciation rate Conclusion Acceptable Better Salvage Amount 0 \\(\\ge 0\\)"},{"location":"Finance_Electives/Accounts/05_Fixed_Assets_%26_Depreciation/#accounting-treatment","title":"Accounting Treatment","text":"Date Particulars Amount 1. Entry for purchase of asset Asset A/C Dr. To Bank A/C Cr. 2. Entries for providing depreciation at the end of each year Depreciation A/C Dr. To Asset A/C Cr. 3. Entry for amount realized on sale of asset Bank A/C Dr. To Asset A/C Cr."},{"location":"Finance_Electives/Accounts/05_Fixed_Assets_%26_Depreciation/#reasoning","title":"Reasoning","text":"<p>Refer to Accounting Equation</p> <ol> <li>Asset value inc; Bank (Asset) dec</li> <li>Depreciation (Expense) inc; Asset value dec</li> <li>Bank (Asset) inc; Asset value dec</li> </ol>"},{"location":"Finance_Electives/Accounts/05_Fixed_Assets_%26_Depreciation/#types-of-asset-values","title":"Types of Asset Values","text":"Value Meaning Salvage Estimated asset value at the end of its useful life Scrap Actual resale value at the end of its useful life Book Estimate asset value at any point in time <p>At the end of useful life</p> \\[ \\begin{aligned} \\text{Book Value} &amp;= \\text{Scrap Value} \\\\ \\text{Net margin for asset resale} &amp;= \\text{Scrap value} - \\text{Salvage value} \\end{aligned} \\]"},{"location":"Finance_Electives/Accounts/05_Fixed_Assets_%26_Depreciation/#law-of-marginal-utility","title":"Law of Marginal Utility","text":"\\[ \\text{Returns from consumption} \\propto \\frac{1}{\\text{Consumption Amount}} \\]"},{"location":"Finance_Electives/Accounts/05_Fixed_Assets_%26_Depreciation/#sinking-fund","title":"Sinking Fund","text":"<p>\u201cA sinking fund is an account a corporation uses to set aside money earmarked to pay off the debt from a bond or other debt issue\u201d</p> <ul> <li>Long-term provision</li> <li>Short-term provision</li> </ul>"},{"location":"Finance_Electives/Accounts/06_Cost_of_Sales_%26_Inventories/","title":"06 Cost of Sales & Inventories","text":""},{"location":"Finance_Electives/Accounts/06_Cost_of_Sales_%26_Inventories/#inventory","title":"Inventory","text":"<p>Assets that are either</p> <ul> <li>products for sale</li> <li>supplies</li> </ul>"},{"location":"Finance_Electives/Accounts/06_Cost_of_Sales_%26_Inventories/#supplies","title":"Supplies","text":"<p>Tangible items that will be consumed in the course of normal operations, such as lubricants, repair parts</p>"},{"location":"Finance_Electives/Accounts/06_Cost_of_Sales_%26_Inventories/#types-of-companies","title":"Types of Companies","text":"Type Manufacturing Merchandising Service Meaning Converts raw materials into finished goods Sells goods in the same form as acquired Provides intangible services Example Trading businesses, Grocery stores Hotels, beauty parlors, plumbers, professional service firms (accountingfirms, legal firms) Inventory MaterialsWork-in-progressFinished goods Materials Inventory costs Acquisition costs(includes cost of goods sold)"},{"location":"Finance_Electives/Accounts/06_Cost_of_Sales_%26_Inventories/#inventory-costs","title":"Inventory Costs","text":"<p>Includes</p> <ul> <li>Cost of purchase</li> <li>Net of trade discount</li> <li>Includes duties and taxes, freight inward</li> <li>Cost of conversion</li> <li>Direct Labour</li> <li>Factory Overheads (Rent, Insurance, Electricity)</li> <li>Transport costs</li> <li>Set-up costs</li> <li>Other normal losses</li> </ul> <p>Does not include</p> <ul> <li>Abnormal losses</li> <li>Interest cost</li> <li>Selling and distribution overheads</li> </ul>"},{"location":"Finance_Electives/Accounts/06_Cost_of_Sales_%26_Inventories/#some-special-costs","title":"Some Special Costs","text":"Cost Meaning Example Intangible Inventory CostsJobs in-progress/unbilled costs Costs incurred for client but not yet billed Shortage Costs Costs incurred by an organization when it has no inventory in stock. Loss of business from customers who go elsewhere to make purchasesLoss of the margin on sales that were not completedOvernight shipping costs to acquire goods that are not in stock"},{"location":"Finance_Electives/Accounts/06_Cost_of_Sales_%26_Inventories/#calculation-of-cost-of-goods-sold","title":"Calculation of Cost of Goods Sold","text":"A. Cost of Goods Consumed Opening stock of raw material \\(a\\) + Purchase (including freights etc.) \\(b\\) - Closing Stock of raw material \\(c\\) \\(c\\) B. Cost of Goods Produced/Production: Opening Work-in-progress (WIP) \\(d\\) + Cost of raw material consumed \\(c\\) + Conversion Cost \\(e\\) + Factory Overheads \\(f\\) - Closing WIP \\(g\\) \\(g\\) C. Cost of Goods Sold Opening Stock of Finished Goods \\(h\\) + Cost of Goods Produced \\(g\\) - Closing Stock of Finished Goods \\(i\\) \\(i\\)"},{"location":"Finance_Electives/Accounts/06_Cost_of_Sales_%26_Inventories/#inventory-cost-methods","title":"Inventory Cost Methods","text":"<p>Valuation of unsold inventory stock when preparing financial statements</p> Meaning Permittedin India? Formula FIFO(First In, First Out) Items bought first sold first \u2705 LIFO(Last In, First Out) Items bought last sold first \u274c WAC(Weighted Average Cost) \u2705 \\(\\dfrac{\\sum \\text{Count}_i \\times \\text{Rate}_i}{\\text{Total Count}}\\) <p></p>"},{"location":"Finance_Electives/Accounts/07_Analysis_of_Financial_Statements/","title":"07 Analysis of Financial Statements","text":""},{"location":"Finance_Electives/Accounts/07_Analysis_of_Financial_Statements/#types-of-analysis","title":"Types of Analysis","text":"Analysis Understand Values Formula Absolute Actual Values Horizontal/Dynamic Trend over the years All values compared to a base year \\(\\frac{V}{V_\\text{base}} \\times 100\\%\\) Vertical/Static Particular period All values compared to the total assets in the same period \\(\\frac{V}{\\text{Total Assets}}\\times 100\\%\\) (Balance Sheet)\\(\\frac{V}{\\text{Net Cashflow}}\\times 100\\%\\)(Cash Flow)\\(\\frac{V}{\\text{Total Revenue}}\\times 100\\%\\)(Income Statement) Ratio Take ratio to create a combination of values and infer something <p>All types of analysis is important, as they each may give different inferences.</p>"},{"location":"Finance_Electives/Accounts/07_Analysis_of_Financial_Statements/#balance-sheet","title":"Balance Sheet","text":"Variable Meaning SHAREHOLDER'S FUNDS Equity Share Capital Ownership stake(increases with Stock split) Total Share Capital Reserves and Surplus Retained earnings Total Reserves and Surplus Total Shareholders Funds Equity Share Application Money Hybrid/Debt/Other Securities NON-CURRENT LIABILITIES Pending after a year Long Term Borrowings Loans/Bonds Deferred Tax Liabilities [Net] Postponed taxes Other Long Term Liabilities Mortgages Long Term Provisions Conditional Guarantees/WarrantiesEmployment benefits, pensions Total Non-Current Liabilities CURRENT LIABILITIES Pending within a year Short Term Borrowings Trade Payables What you owe suppliersOther parties will only tolerate if you are dominant in the market Other Current Liabilities Current Portion of Long-Term Debt Short Term Provisions Provisions for immediate conditional compensations such as legal issues, etc Total Current Liabilities Total Capital And Liabilities ASSETS NON-CURRENT ASSETS Tangible Assets Physical Intangible Assets Non-Physical Capital Work-In-Progress Products in progress Intangible Assets Under Development R&amp;D Fixed Assets Non-Current Investments Long Term Loans And Advances Giving loans to subsidiaries, as the parent company get cheaper loansTax evasion Other Non-Current Assets Machinery, Equipment Total Non-Current Assets CURRENT ASSETS Current Investments Money market (loans between 7 &amp; 365 days) Inventories Trade Receivables Cash And Cash Equivalents Anything that can used as payment, such as Cash/Cheques/Credit slips Short Term Loans And Advances Other Current Assets Total Current Assets Total Assets <p>Trenches is when you securitize a loan that you lent, and then trade it to the public. 2008 financial crisis</p> <p>Sinking fund is the repayment of bond</p> <p>Generally, steel plants </p>"},{"location":"Finance_Electives/Accounts/07_Analysis_of_Financial_Statements/#income-statement","title":"Income Statement","text":"Meaning INCOME Revenue From Operations [Gross] Less: Excise/Sevice Tax/Other Levies Revenue From Operations [Net] Other Operating Revenues Total Operating Revenues Other Income Total Revenue EXPENSES Cost Of Materials Consumed Purchase of Stock-In Trade Purchase of ready-made input for production Changes In Inventories Of FG,WIP And Stock-In Trade Employee Benefit Expenses Finance Costs Depreciation And Amortisation Expenses Other Expenses Less: Amounts Transfer To Capital Accounts Total Expenses Profit/Loss Before Exceptional, ExtraOrdinary Items And Tax Exceptional Items Profit/Loss Before Tax Tax Expenses-Continued Operations Current Tax Deferred Tax Total Tax Expenses Profit/Loss After Tax And Before ExtraOrdinary Items Profit/Loss From Continuing Operations Profit/Loss For The Period OTHER ADDITIONAL INFORMATION EARNINGS PER SHARE Basic EPS <p>If total shareholders funds increases over time, the company is very good, regardless if equity share capital dec.</p>"},{"location":"Finance_Electives/Accounts/07_Analysis_of_Financial_Statements/#cashflow-statement","title":"Cashflow Statement","text":"NET PROFIT/LOSS BEFORE EXTRAORDINARY ITEMS AND TAX Net CashFlow From Operating Activities Net Cash Used In Investing Activities Net Cash Used From Financing Activities Foreign Exchange Gains / Losses Adjustments On Amalgamation Merger Demerger Others NET INC/DEC IN CASH AND CASH EQUIVALENTS Cash And Cash Equivalents Begin of Year Cash And Cash Equivalents End Of Year"},{"location":"Finance_Electives/Accounts/07_Analysis_of_Financial_Statements/#ratio-analysis","title":"Ratio Analysis","text":"Aspect Ratio Recommended Liquidity Current Ratio 2:1 Quick Ratio/Acid Test Cash Ratio Profitability Gross Profit Ratio Operating Profit Ratio Net Profit Ratio Solvency Ratio Debt-Equity Ratio Interest Coverage Ratio Turnover Ratio Fixed Asset Turnover Ratio Inventory Turnover Ratio Receivable Turnover Ratio Earning Ratio Profit-Earning Ratio Earning Per Share"},{"location":"Finance_Electives/Accounts/07_Analysis_of_Financial_Statements/#misc","title":"Misc","text":""},{"location":"Finance_Electives/Accounts/07_Analysis_of_Financial_Statements/#types-of-investments","title":"Types of Investments","text":"Relationship Amount Subsidiary &gt; 50% Associate 20% &lt; A &lt; 50% Minority Interest &lt; 20%"},{"location":"Finance_Electives/Accounts/07_Analysis_of_Financial_Statements/#shares","title":"Shares","text":"<p>\u2018Splitting\u2019 shares = fragmentation</p> <p>Base capital</p> Authorized capital Paid-up capital Equity shares <p>Reserve and Surplus involves</p> <ul> <li>Retained earnings</li> <li>Overage from premium surplus</li> </ul> Face Value Book/Intrinsic Value Market Value <p>Dividend of 200% means 200% of face value</p>"},{"location":"Finance_Electives/Accounts/07_Analysis_of_Financial_Statements/#stock-splits","title":"Stock Splits","text":"<p>Causes market noise</p>"},{"location":"Finance_Electives/Accounts/07_Analysis_of_Financial_Statements/#market-noise","title":"Market Noise","text":"<p>Unnecessary oscillation</p> <p>Herding behavior</p>"},{"location":"Finance_Electives/Accounts/07_Analysis_of_Financial_Statements/#re-investment-risk","title":"Re-Investment Risk","text":""},{"location":"Finance_Electives/Accounts/07_Analysis_of_Financial_Statements/#solvency","title":"Solvency","text":"<p>Ability of a company to cover its long-term financial obligations</p>"},{"location":"Finance_Electives/Accounts/07_Analysis_of_Financial_Statements/#insolvency","title":"Insolvency","text":""},{"location":"Finance_Electives/Accounts/07_Analysis_of_Financial_Statements/#financial-distress","title":"Financial Distress","text":"<p>When Current assets &lt; Hard Contracts</p>"},{"location":"Finance_Electives/Accounts/07_Analysis_of_Financial_Statements/#types-of-lease","title":"Types of Lease","text":"<p>An operating lease is a contract that permits the use of an asset without transferring the ownership rights of said asset. A finance lease is a contract that permits the use of an asset and transfers ownership after the lease period is complete, and the lessor meets all other contract obligations.</p>"},{"location":"Finance_Electives/Accounts/07_Analysis_of_Financial_Statements/#ratio-analysis_1","title":"Ratio Analysis","text":"Represents NI Net Income TE Total Equity IDK \\(\\tau\\) Effective Tax Rate Current Tax/PBT Working Capital Current Assets - Current Liabilities Cash Expenses per Day Total Expenses/365 Overall Performance Ratio ROA/ROTA (Before Tax) EBIT/TA ROA/ROTA (After Tax) EBIT*(1-\\(\\tau\\))/TA ROE NI/Equity ROCE (Before Tax) EBIT/TA-CL ROCE (After Tax) EBIT(1-t)/TA-CL Profit Margin Ratios EBITDA Margin EBITDA/Sales EBIT Margin/OPM EBIT/Sales EBT Margin EBT/Sales Net Profit Margin NI/Sales Asset Turnover Ratio Sales/Total Assets Two Factor Dupont Analysis ROA EBIT margin * ATR Three Factor DuPont NPM NI/Sales ATR Sales/Total Assets Total Leverage Total Assets/Equity ROE NPM * ATR * TL Five Factor Dupont Tax Factor NI/EBT Interest Factor EBT/EBIT EBIT Margin EBIT/Sales ATR Sales/TA TL TA/Equity ROE TF * IF * EM * ATR * TL Turnover or Efficiency Ratios Non-Current Asset Turnover Ratio Sales/NCA PPE Utilisation Ratio/ Capital Intensity Ratio Sales/PPE Current Asset Turnover Ratio Sales/CA Equity Turnover Ratio Sales/Equity Working Capital Ratios Working Capital Turnover Ratio Sales/Working Capital Inventory Turnover Ratio (ITR) Sales/Inventory Days Inventory 365/ITR Debtors Turnover Ratio (DTR) Sales/Accounts Receivable Days receivable or Average Collection Period 365/DTR Days Cash Cash/ Cash Expenses per Day Creditor Turnover Ratio (CTR) Material Consumed/Accounts Payable Days Creditors/Average Payment Period 365/CTR Cash Conversion Cycle (Days) Days inventory + Days debtors + Days Cash - Days Payable Insolvency Ratio Debt All interest bearing liabilites are debt Debt/Equity Ratio Total Debt/ Equity Debt Ratio /Debt Capitalisation Ratio Debt/ (Debt + Equity) Equity Ratio/ Equity Capitalisation Ratio Equity/(Debt + Equity) Interest Coverage Ratio EBIT/ Interest Total Debt Service Ratio EBIT/ (Interest + Debt) Test of Dividend Policy Dividend Per Share Dividend Declared/ No. of share outstanding Earning Per Share NI/ No of share outstanding Dividend Yeild Ratio Dividend/ Current Market Price Dividend Payout Ratio (D/P Ratio) DPS/EPS = Dividend Declared/NI Retension Ratio 1 - D/P Ratio Liquidity Ratios Current Ratio Current Assets/ Current Liabilities Quick Ratio/ Acid Test Ratio (CA-Inventory)/ CL Valuation Ratios Book Value per Share Total Equity/ No. of Share Outstanding Market Value Per Share (on balance sheet date) market value of share Earning Per share NI/ No of share outstanding Price Earning Ratio (P/E) MPS/EPS Price to Book Value Ratio (P/B) MPS/BVPS"},{"location":"Finance_Electives/Accounts/07_Analysis_of_Financial_Statements/#margin-vs-markup","title":"Margin vs Markup","text":""},{"location":"Finance_Electives/Accounts/07_Analysis_of_Financial_Statements/#return-vs-yield","title":"Return vs Yield","text":"Return Yield Calculated relative to Initial invested capital Current Market Price"},{"location":"Finance_Electives/Accounts/Side_Quests/Current_Maturity/","title":"Current Maturity","text":""},{"location":"Finance_Electives/Accounts/Side_Quests/Current_Maturity/#2-extra-credit-assignment","title":"\\(2 \\%\\) Extra-Credit Assignment","text":"<p>Will long-term borrowings will be reclassified into short-term borrowings?</p> <p>By: Ahmed Thahir, 2020A7PS0198U</p>"},{"location":"Finance_Electives/Accounts/Side_Quests/Current_Maturity/#summary","title":"Summary","text":"<p>No, not exactly. However, long-term borrowings will be reclassified into current liabilities (not exactly short-term borrowings) upon a certain date, using the concepts of</p> <ul> <li>Current Maturity of Long-Term Debt</li> <li>Current Portion of Long-Term Debt</li> </ul>"},{"location":"Finance_Electives/Accounts/Side_Quests/Current_Maturity/#explanation","title":"Explanation","text":"<p>The current maturity of a company\u2019s long-term debt refers to the stage when a long-term debt (or a portion of it) is due within the next 12 months.</p> <p>Current Portion of Long-Term Debt (CPLTD) refers to the portion of liabilities that have gone throught current maturity of long-term debt. It can be less than or equal to the Long-term Debt (LTD).</p> \\[ 0 \\le \\text{CPLTD} \\le \\text{LTD} \\] <p>This means that portion of long-term debt that is to be paid within a year is reclassified from a non-current liability to a current liability.</p>"},{"location":"Finance_Electives/Accounts/Side_Quests/Current_Maturity/#why","title":"Why?","text":"<p>This is to accurately analyze the liquidity of a company, since CPLTD must be paid within the coming year; liquidity is a company's ability to convert assets to cash or acquire cash\u2014through a loan or money in the bank\u2014to pay its liabilities/obligations within the current year.</p> \\[ \\begin{aligned} \\text{Current Ratio} &amp;= \\frac{\\text{Current Assets}}{\\text{Current Liabilities}} \\\\ &amp;= \\frac{\\text{Current Assets}}{\\text{Short-Term Borrowings + \\textcolor{hotpink}{CPLTD}}} \\\\ \\text{Quick Ratio} &amp;= \\frac{\\text{Current Assets - Inventories}}{\\text{Current Liabilities}} \\\\ &amp;= \\frac{\\text{Current Assets - Inventories}}{\\text{Short-Term Borrowings + \\textcolor{hotpink}{CPLTD}}} \\\\ \\text{Cash Ratio} &amp;= \\frac{\\text{Cash \\&amp; Cash Equivalent + Short-Term Investments}}{\\text{Current Liabilities}} \\\\ &amp;= \\frac{\\text{Cash \\&amp; Cash Equivalent + Short-Term Investments}}{\\text{Short-Term Borrowings + \\textcolor{hotpink}{CPLTD}}} \\end{aligned} \\] <p>As you can see in all forms of liquidity analysis, we need to include CPLTD to ensure an accurate analysis.</p>"},{"location":"Finance_Electives/Accounts/Side_Quests/Current_Maturity/#references","title":"References","text":"<p>[1] \u201cCurrent Maturity Definition,\u201d Investopedia. https://www.investopedia.com/terms/c/currentmaturity.asp (accessed Apr. 30, 2023).</p> <p>[2] \u201cWhat Is the Current Portion of Long-Term Debt (CPLTD)?,\u201d Investopedia. https://www.investopedia.com/terms/c/currentportionlongtermdebt.asp (accessed Apr. 30, 2023).</p>"},{"location":"Finance_Electives/Accounts/Side_Quests/Insolvency/","title":"Insolvency","text":"<p>Insolvency and banruptcy 2016</p>"},{"location":"Finance_Electives/Behavioral_Economics/","title":"Behavioral Economics","text":""},{"location":"Finance_Electives/Behavioral_Economics/#overview","title":"Overview","text":"<ol> <li>Introduction</li> <li>Preferences</li> <li>Time preferences</li> <li>Self-control</li> <li>Risk preferences</li> <li>Reference-dependent preferences</li> <li>Social preferences</li> <li>Unit</li> <li>Emotions, projection &amp; attribution bias</li> <li>Limited attention</li> <li>Beliefs &amp; learning</li> <li> <p>Mental accounting</p> </li> <li> <p>Unit</p> </li> <li>Malleability &amp; inaccessibility of preferences</li> <li>Happiness</li> <li>Mental health</li> <li> <p>Gender &amp; racial discrimination</p> </li> <li> <p>Unit</p> </li> <li>Frames, defaults, nudges</li> <li>Policy &amp; paternalism</li> <li>Poverty through lens of psychology</li> </ol>"},{"location":"Finance_Electives/Behavioral_Economics/#references","title":"References","text":"<ul> <li> MIT 14.13 Psychology and Economics</li> <li> Neuroeconomics | Saul Leung UvA</li> </ul>"},{"location":"Finance_Electives/Behavioral_Economics/#next-video","title":"Next video","text":"<p>https://www.youtube.com/watch?v=pwFsPEPPUGU&amp;list=PLUl4u3cNGP63Z979ri_UXXk_1zrvrF77Q&amp;index=7</p>"},{"location":"Finance_Electives/Behavioral_Economics/01_Introduction/","title":"Introduction","text":"<p>Behavioral economics discards the Assumptions of Classical Economics, and analyzes psychological factors.</p>"},{"location":"Finance_Electives/Behavioral_Economics/01_Introduction/#how-does-economics-view-human-behavior","title":"How does economics view human behavior?","text":"<p>Humans have goal-driven individual behavior with constrained optimization</p> <ol> <li>Utility function: What makes people happy</li> <li>Instantaneous</li> <li>Time, Risk, Social Preferences</li> <li>Beliefs: What do people believe about environment</li> <li>Physical environment</li> <li>Others\u2019 behavior</li> <li>Use of information to update beliefs</li> <li>Choice/Decision-making: How do people use the above to make decisions</li> <li>Some influences on behavior are not about utility/beliefs</li> <li>Frames, Defaults, Nudges; heuristics</li> </ol>"},{"location":"Finance_Electives/Behavioral_Economics/01_Introduction/#social-preferences","title":"Social Preferences","text":"<p>Classical economics assumes that humans are selfish</p> <p>Behavioral economics</p> <p>People care about others, but not pure altruism (behavior that benefits another at one\u2019s own expense)</p> <ol> <li>Helping others actually makes you happier</li> <li>To show-off to others</li> <li>Reciprocity</li> <li>Inequality aversion</li> </ol>"},{"location":"Finance_Electives/Behavioral_Economics/01_Introduction/#game-to-measure-peoples-preferences","title":"Game to measure people\u2019s preferences","text":"<p>Dictator\u2019s game</p> <p>Imagine you have been given <code>$10</code> to split between yourself and another, randomly-chosen person. You can keep any part of the <code>$10</code>, and give the rest to the other person.</p> <p>2 ways</p> <ol> <li>Recipient informed about the circumstances of the decision. Self-image</li> <li>Recipient might never notice (money is wired anonymously). Pure altruism </li> </ol>"},{"location":"Finance_Electives/Behavioral_Economics/01_Introduction/#anchoring","title":"Anchoring","text":"<p>People think in relative terms, not absolute terms of money</p>"},{"location":"Finance_Electives/Behavioral_Economics/02_Time_Preferences/","title":"Time Preferences","text":"<p>Most non-trivial economic choices involve </p> <ol> <li>Determine tradeoffs between costs and benefits that occur across time points</li> <li>Determine values (utility) of costs and benefits, by weighing costs &amp; benefits against each other</li> </ol>"},{"location":"Finance_Electives/Behavioral_Economics/02_Time_Preferences/#present-bias","title":"Present Bias","text":"<p>Humans have a tendency to put more weight into the present rather than the future when making decision</p>"},{"location":"Finance_Electives/Behavioral_Economics/02_Time_Preferences/#indifference-curves","title":"Indifference Curves","text":"<p>At any point on the curve, the combination of the two will leave the consumer equally well off or equally satisfied\u2014hence indifferent.</p> Shows various combinations of __ that consumers can choose Goods-Indifference Curve two commodities at the same time point Fisher\u2019s Time-Indifference Curve same commodity at different time points"},{"location":"Finance_Electives/Behavioral_Economics/02_Time_Preferences/#utility-function","title":"Utility Function","text":"\\[ \\begin{aligned} \\max U_t &amp;= \\sum_{t = 0}^\\infty D(t) \\cdot u_{t_0 + t} \\\\ u_t &amp;= u(c_t, l_t, \\dots) \\end{aligned} \\] <p>where</p> \\(u_t\\) Instantaneous utility Captures how person feels at a specific momentFunction of consumption, leisure \\(U_t\\) Discounted utility Captures total utility obtained until a specific moment \\(D(t)\\) Discount Function Specifies weights on utility derived in \\(t\\) time periodsMeasures how utility in alter periods is discounted relative to earlier periodsReplaces complex psychology of how people think about futureUsually \\(\\in (0, 1]\\) \\(\\rho(t)\\) Discount Rate Rate of decline in the discount functionSpecifies rate at which value of \\(u\\) declines with delay \\(\\dfrac{-D'(t)}{D(t)}\\)"},{"location":"Finance_Electives/Behavioral_Economics/02_Time_Preferences/#standard-utility-models","title":"Standard Utility Models","text":"Samuelson\u2019s Exponential Quasi-Hyperbolic Non-graphical model of Fisher\u2019s Time-Indifference CurveDeveloped as a simple approximation as a first start, not meant to be accurate \\(D(t)\\) \\(\\delta^t\\)\\(\\delta \\in [0, 1]\\) \\(\\beta \\delta^t\\)\\(\\delta \\to 1; \\delta \\approx 0.\\bar{9}\\)\\(\\beta \\in [0, 1]\\) \\(\\rho(t)\\) \\(- \\log \\vert \\delta \\vert \\approx 1-\\delta\\) Advantages Not affected by awareness issue - Separate short &amp; long-run discounting- Great patience for tradeoffs in the future than for tradeoff in present- Deals with preference reversals Limitation Constant discount rate1. Short vs Long-Run impatience2. Preference reversals3. Commitment devices Affected by awareness issue"},{"location":"Finance_Electives/Behavioral_Economics/02_Time_Preferences/#estimating-delta","title":"Estimating \\(\\delta\\)","text":"<p>Ask the person the following question</p> <p>What \\(X\\) makes you indifferent between receiving <code>$15</code> today and <code>$X</code> at various time point \\(t\\)</p> <ul> <li>\\(t=1\\) day</li> <li>\\(t=1\\) month</li> <li>\\(t=1\\) year</li> </ul> <p>Assumes that utility is linear in money, ie marginal utility is constant, ie \\(u(X) = X\\)</p>"},{"location":"Finance_Electives/Behavioral_Economics/02_Time_Preferences/#idk","title":"IDK","text":""},{"location":"Finance_Electives/Behavioral_Economics/02_Time_Preferences/#short-vs-long-run-impatience","title":"Short vs Long-Run Impatience","text":"<p>People tend to be more patient in the long-run than in the short-run</p> <p>Eg: Credit card loans</p>"},{"location":"Finance_Electives/Behavioral_Economics/02_Time_Preferences/#preference-reversals","title":"Preference Reversals","text":"<p>In reality, dynamic consistency is not followed. People don\u2019t always follow through with their plans.</p> <p>Hence</p> <ul> <li>When thinking ahead to the future, we want to be patient</li> <li>When the time actually comes, we are impatient</li> <li>People are over-confident about their self-control</li> </ul> <p>Eg: Dieting, Gym membership</p>"},{"location":"Finance_Electives/Behavioral_Economics/02_Time_Preferences/#dynamictime-consistency","title":"Dynamic/Time Consistency","text":"<ul> <li>The action a person thinks they should take in the future always coincides with the action that they actually prefer to take once the time comes</li> <li>A person\u2019s preferences at different points in time are consistent with each other; there are no \u201cintra-personal conflicts\u201d </li> </ul>"},{"location":"Finance_Electives/Behavioral_Economics/02_Time_Preferences/#commitment-devices","title":"Commitment Devices","text":"<p>Arrangement taken upon by agent to restrict their future choice set, by making certain choices more costly</p> <p>Demand for commitment requests (at least partial) sophistication</p> <p>When you know that your future preferences will be different from present preferences, you may engage in commitment devices to penalize (and hence eliminate) few options from the future. We disapprove of the tendency for instant gratification beforehand, but struggle to actually follow through</p>"},{"location":"Finance_Electives/Behavioral_Economics/02_Time_Preferences/#requirements-for-effectiveness-of-commitment-device","title":"Requirements for effectiveness of commitment device","text":"<ul> <li>Person needs to have a self-control problem: \\(\\beta &lt; 1\\)</li> <li>Person needs to at least partly be sophisticated: \\(\\hat \\beta &lt; 1\\)</li> <li>Commitment devices needs to be effective</li> <li>Person needs to believe that the commitment device is effective</li> </ul>"},{"location":"Finance_Electives/Behavioral_Economics/02_Time_Preferences/#things-that-worsen-effectiveness-of-commitment-devices","title":"Things that worsen effectiveness of commitment devices","text":"<ul> <li>Time-inconsistent preferences: Each time period\u2019s self restrict set of choices for their future selves, and hence there may be difference in assessment of best action at each time period</li> <li>Substitution: Substitution across temptation goods worsens effectiveness of commitment devices. Avoiding one temptation good may lead to increases consumption of another</li> <li>Naivete: Person underestimates their present bias, and might</li> <li>Naive: Not demand a helpful commitment device</li> <li>Partial naive: Demand a unhelpful commitment device</li> </ul>"},{"location":"Finance_Electives/Behavioral_Economics/02_Time_Preferences/#goods-types","title":"Goods Types","text":"Leisure Investment Example Eating Candy Going to GymFinishing assignmentsQuitting bad habitsFinding a job Costs Delayed Immediate Rewards Immediate Delayed ResultConsumption relative to long-run Over-consumption Under-consumption"},{"location":"Finance_Electives/Behavioral_Economics/02_Time_Preferences/#behavior-types","title":"Behavior Types","text":"PerfectExponential Discounter Na\u00efvet\u00e9 Partial Na\u00efvet\u00e9 Sophistication \\(\\beta\\) \\(1\\) \\(&lt; 1\\) \\(&lt;1\\) \\(&lt; 1\\) \\(\\hat \\beta\\)What you think \\(\\beta\\) is in the future \\(\\beta\\) \\(1\\) \\(&gt; \\beta\\)Measures belief about future \\(\\beta\\) \\(\\beta\\) Optimism Perfect Over-optimistic(Assumes future self will through on optimal plan) Underestimate degree of future present bias Pessimistic Person self-aware of preference reversal N/A(No preference reversal) \u274c \u2705 \u2705 Overcommitment/Self-control problem \u274c \u274c \u2705 \u274c Set deadlines optimally \u2705 \u274c(No perceived need to choose deadlines) \u26a0\ufe0f(Tries, but fails) \u2705 Deadlines help Deadline not required \u2705 \u26a0\ufe0f \u2705 Take advantage of commitment devices Not required \u274c \u26a0\ufe0f(Tries, but fails) \u2705 No surprises of future present bias No present bias \u274c \u274c \u2705 Overcomes short-run impatience No impatience \u274c \u274c \u2705 Utility Evaluation Forward1. Start at beginning2. Solve for optimal plan, assuming future self follows plan3. Person takes first step in that plan4. Go to next period5. Go to step 2 Backward &amp; Forward1. Start at the end2. Solve for what the person thinks they will do (using \\(\\hat \\beta\\)) (this is like solving for sophisticated person with \\(\\hat \\beta = \\beta\\))3. Work your way to first period using backward induction until period 2 (using \\(\\hat \\beta\\))4. Solve for optimal action in period 1 (using \\(\\beta\\) and already derived prediction on future behavior)5. Move to next period6. Go to step 2 Backward1. Start at end2. Solve for optimal action3. Go back to previous period4. Solve for optimal action, considering what happens in next period5. Go to step 3 Investment Goods: Behavior No procrastination Na\u00efve Procrastination Sophisticated Procrastination Investment Goods: Welfare Cost 0 Large Low Leisure Goods: Behavior No precrastination Na\u00efve precrastination Sophisticated precrastination(self-aware about impatience, and hence consumes earlier) Leisure Goods: Welfare Cost 0 Low Large(does not wait until max enjoyment)"},{"location":"Finance_Electives/Behavioral_Economics/02_Time_Preferences/#utility-evaluation","title":"Utility Evaluation","text":""},{"location":"Finance_Electives/Behavioral_Economics/02_Time_Preferences/#example","title":"Example","text":"<p>Consider the following table showing the utilities associated with watching a movie</p> <p></p> Na\u00efvet\u00e9 Sophistication Utility Evaluation t=0: Plans to go at t=3, so doesn\u2019t got=1: Plans to go at t=3, so doesn\u2019t got=2: Goes t=2: goes if she hasn\u2019tt=1: realizes she won\u2019t wait until t=3, she goest=0: realizes she won\u2019t wait until t=2 or 3, she goes Conclusion Goes at t=2, even though she planned to go at t=3 Just goes at t=0"},{"location":"Finance_Electives/Behavioral_Economics/02_Time_Preferences/#indicators-of-behavior-types","title":"Indicators of Behavior Types","text":"<ul> <li>Naivete: Person mis-predicting future behavior</li> <li>Sophistication: Person\u2019s use of commitment devices</li> </ul>"},{"location":"Finance_Electives/Behavioral_Economics/02_Time_Preferences/#uncertainties-about-future","title":"Uncertainties about Future","text":"<ul> <li>Present bias</li> <li>Planning Fallacy</li> </ul>"},{"location":"Finance_Electives/Behavioral_Economics/02_Time_Preferences/#planning-fallacy","title":"Planning Fallacy","text":"<p>Under-estimation of effort costs of tasks, leading to people always under-performing compared to original plan, even if they are aware of this.</p>"},{"location":"Finance_Electives/Behavioral_Economics/02_Time_Preferences/#case-studies","title":"Case Studies","text":""},{"location":"Finance_Electives/Behavioral_Economics/02_Time_Preferences/#assignment-deadlines","title":"Assignment Deadlines","text":"<p>Order of effectiveness</p> <ol> <li>Imposed deadlines</li> <li>Self-imposed assignments</li> <li>no deadline for assignments</li> </ol>"},{"location":"Finance_Electives/Behavioral_Economics/02_Time_Preferences/#gym-membership-purchase","title":"Gym Membership Purchase","text":""},{"location":"Finance_Electives/Behavioral_Economics/02_Time_Preferences/#work","title":"Work","text":"<p>Dominated contract increases production</p>"},{"location":"Finance_Electives/Behavioral_Economics/02_Time_Preferences/#credit-card-companies","title":"Credit Card Companies","text":"<p>Consider 3 types of loans with the following interest rates for different duration of loans</p> Deal &lt; 6M &gt;= 6M Standard 10% 20% Teaser Offer 5% 20% Post-Teaser Offer 10% 15% <p>Why would more people choose the teaser offer?</p> <ul> <li>Naive borrows believe they will repay loan quickly</li> <li>They borrow more than expected</li> <li>Sophisticated borrowers don\u2019t want to use their cards in the future, so choose high future interest rate to restrain future borrowing</li> <li>Expensive commitment device</li> <li>Substitution to other credit cards would undo this strategy</li> </ul> <p>How do credit card companies use this info</p> <ul> <li>Identify who are naive and who are sophisticated</li> <li>Credit card companies want to exploit people until the point that they won\u2019t default</li> <li>Naive households are more likely to be offered hidden-fee structures</li> <li>Low introductory/teaser rates</li> <li>Photos, colors, fine print</li> <li>After introductory period, these cards feature higher interest rates, late fees and over-limit fees</li> </ul>"},{"location":"Finance_Electives/Behavioral_Economics/02_Time_Preferences/#gym-membership-sale","title":"Gym Membership Sale","text":"<p>Similar to above, the gyms want to entice customers with cheap short-term fees but want to retain customers in the long-run as well</p>"},{"location":"Finance_Electives/Behavioral_Economics/02_Time_Preferences/#commitment-savings","title":"Commitment Savings","text":"Treatment Commitment offer:Restrict access to deposits SEED Encourage to save \u2705 Marketing Encourage to save \u274c Control None None <ul> <li>Offering commitment savings significant increased savings</li> <li>People still default on commitment contract</li> </ul>"},{"location":"Finance_Electives/Behavioral_Economics/02_Time_Preferences/#alcohol-consumption","title":"Alcohol consumption","text":"<p>Low-income workers tend to drink a lot of alcohol</p> <p>Many say that</p> <ul> <li>They would like to reduce drinking</li> <li>They would happier if liquor stores closed</li> </ul> <p>Physical pain from work appears to contribute to self-control problems</p> <ul> <li>Alcohol is powerful anesthetic</li> <li>Pain increases short-run benefits of drinking while leaving long-run costs unaffected</li> </ul> <p>They can\u2019t just \u2018stop drinking\u2019 as they will face severe withdrawal syndromes</p> <ul> <li>Sobriety incentives reduce day drinking</li> <li>Individuals mostly substitute to night drinking</li> <li>No impact of incentives on labor-market outcomes</li> <li>Increased savings</li> </ul>"},{"location":"Finance_Electives/Behavioral_Economics/02_Time_Preferences/#smoking","title":"Smoking","text":""},{"location":"Finance_Electives/Behavioral_Economics/02_Time_Preferences/#farmers-fertilizers","title":"Farmers Fertilizers","text":"<p>Give option to Pre-buy fertilizer at time of harvest, much before time of sowing</p> <p>Time of sowing = when fertilizer needed</p> <p>Compared to discount at time of sowing</p> <ul> <li>More effective at getting farmers to purchase</li> <li>Expensive to give subsidies</li> <li>Subsidies will get wasted on everyone</li> <li>Subsidies may incentivize some to over-use fertilizers</li> </ul>"},{"location":"Finance_Electives/Behavioral_Economics/03_Risk_Preferences/","title":"Risk Preferences","text":""},{"location":"Finance_Electives/Business_Analytics/","title":"Business Analytics","text":""},{"location":"Finance_Electives/Business_Analytics/#references","title":"References","text":"<ul> <li> Market Research | IIT Madras</li> <li> Business Analytics | IIT Madras</li> <li> Business Data Management | IIT Madras</li> <li> Customer Lifetime Value | Edward Malthouse</li> </ul>"},{"location":"Finance_Electives/Corporate_Finance/","title":"Financial Management","text":"<p>This course covers concepts relevant to managing finances of a corporation.</p> <p>Taught by Dr. Faisal Nazir Zargar</p>"},{"location":"Finance_Electives/Corporate_Finance/01_Introduction/","title":"01 Introduction","text":""},{"location":"Finance_Electives/Corporate_Finance/01_Introduction/#finance","title":"Finance","text":"<p>Finance is the field that deals with value; finance is not about money</p> <p>Financial system helps transfer money from savers to consumers.</p> <p>Finance is a child of Economics.</p> <p>Financial Management is a study of a corporation\u2019s finances. Hence, it is also called as Corporate Finance, Business Finance.</p> <p>2 variables involed are</p> <ul> <li>Time</li> <li>Risk</li> </ul>"},{"location":"Finance_Electives/Corporate_Finance/01_Introduction/#goal","title":"Goal","text":"<p>Maximize value of company</p> <p>This is measure by the price of the stock</p>"},{"location":"Finance_Electives/Corporate_Finance/01_Introduction/#financial-decisions-in-order-of-importance","title":"Financial Decisions (in order of importance)","text":"<ul> <li>Capital budgeting decisions (most important)</li> <li>Long term investments</li> <li>Purchase of real assets</li> <li>Captial structure decisions</li> <li>Financing decisiosn</li> <li>Sale of financial assets</li> <li>Working capital decisions</li> <li>Liquidity vs Profitability</li> <li>Dividend decisions (Residual of budgeting and structure - not very important)</li> <li>Pay/Retain</li> </ul>"},{"location":"Finance_Electives/Corporate_Finance/01_Introduction/#risk-vs-uncertainity","title":"Risk vs Uncertainity","text":"<p>For risk, we have known probabilities. For eg, we can estimate inflation, etc</p> <p>Uncertainity means we have no information.</p> <p>Usually,</p> \\[ \\text{Trade Return} \\propto \\text{Risk} \\]"},{"location":"Finance_Electives/Corporate_Finance/01_Introduction/#securities","title":"Securities","text":"<p>is a tradeable asset that gives you claim over something.</p> Risk-Free(Future cash flows guaranteed) Tradeable Loan \u2705 \u274c Bond \u2705 \u2705 Share \u274c \u2705"},{"location":"Finance_Electives/Corporate_Finance/01_Introduction/#ownership-forms","title":"Ownership Forms","text":"Form Sole Proprietorship Partnership Company/Corporation/Limited Liability Company No of owners 1 \\(\\ge 2\\) \\(n\\)(includes shareholders) Ease of starting Easy Intermediate Difficult Regulations Low Low High Keeping profits All Part Partial Ease of Valuation Difficult Difficult Easy Life length life of owner life of owners Unlimited Equity capital Limited to owner\u2019s personal wealth Limited to owners\u2019 personal wealth Liability Unlimited Unlimited Limited to investment of investor Ease for transfer of ownership Difficult Difficult Easy Ownership separate from Management \u274c \u274c \u2705 Agency Problems \u274c \u274c \u2705 Taxation Single* Single* Double <p>*Taxed only once as personal income (corporate tax is higher than personal)</p>"},{"location":"Finance_Electives/Corporate_Finance/01_Introduction/#agency-problems","title":"Agency Problems","text":"<p>Goals of managers different from owners</p> <p>The manager will be more short-term oriented, in order to boost their portfolio, so that they can move on to a better company</p>"},{"location":"Finance_Electives/Corporate_Finance/02_Corporation/","title":"02 Corporation","text":""},{"location":"Finance_Electives/Corporate_Finance/02_Corporation/#foundation","title":"Foundation","text":"<p>Founding starts by filing a charter with the state it wishes to incorporate</p>"},{"location":"Finance_Electives/Corporate_Finance/02_Corporation/#ownership","title":"Ownership","text":"<p>There is no limit to number of shareholders, and thus amounts of funds is theortically </p> <p>Before they list themselves, </p> <ol> <li>Value the company</li> <li>They make \\(\\le 49 \\%\\) available, to avoid a \u2018hostile takeover\u2019</li> </ol> <p>Owner of stock can be called as:</p> <ul> <li>Shareholder</li> <li>Stockholder</li> </ul> <p>Owners can expect (not promised - not legal obligation)</p> <ul> <li> <p>capital gain</p> </li> <li> <p>dividend payments</p> </li> </ul> \\[ \\begin{aligned} \\text{Earnings per share} &amp;= \\frac{\\text{net profit}}{\\text{no of shares}} \\\\ \\text{Dividend payout rate} &amp;= \\text{} \\end{aligned} \\] <p>The board decides [in consultation with CEO] whether to give out dividends or to re-invest</p>"},{"location":"Finance_Electives/Corporate_Finance/02_Corporation/#funds","title":"Funds","text":"\\[ \\text{Assets} = \\text{Liability} + \\text{Equity} \\] <p>The capital structure of a corporation</p> <ul> <li>Debt &amp; Bonds</li> <li>Equity = sum of all ownership value</li> </ul> <p>This is one of the things naive investors are not aware. Dividend should not be a factor when evaluating a stock. Dividends actually make the value of your stock lower, as the equity has reduced. (Similar to Law of Conservation of Energy). Distribution of profits should not occur during times of expansion. A company giving dividends is a sign that it has ran out of expansion.</p> <p>Double Taxation</p> <ul> <li>Dividend tax</li> <li>Capital gains tax</li> </ul> <p>Refer Clientele Effect</p>"},{"location":"Finance_Electives/Corporate_Finance/02_Corporation/#claim-on-assets","title":"Claim on Assets","text":"<ol> <li>Debt (Interest)</li> <li>Preference Shares (Preference Dividends)</li> <li>Owners/Common Shares (Dividends)</li> </ol>"},{"location":"Finance_Electives/Corporate_Finance/02_Corporation/#debt-bonds","title":"Debt &amp; Bonds","text":"<p>Lenders cannot be owners of the corporation. Payment of interest is a legal obligation.</p> <p>If company fails to pay interest, they have to file for bankruptcy. A legal authority comes in, and sells the company\u2019s assets to pay off the debt/bonds</p>"},{"location":"Finance_Electives/Corporate_Finance/02_Corporation/#company-summary","title":"Company Summary","text":"Category Sign Sales Revenue + Direct Costs - Indirect Costs - EBIT (Earnings Before InTerest) Interest - Taxes - Net Margin \u00b1 <p>EBITDA (I was too shy to ask sir)</p>"},{"location":"Finance_Electives/Corporate_Finance/02_Corporation/#clientele-effect","title":"Clientele Effect","text":""},{"location":"Finance_Electives/Corporate_Finance/03_Time_Value_of_Money/","title":"Time Value of Money","text":"<p>You should never compare money across different time instants. We can only compare at the same instant.</p> When we take cashflow ___ in time Name forward compounding backward discounting <p>Return for every investment is a compensation</p> <ul> <li>Time</li> <li>Inflation</li> <li>Risk</li> </ul> <p>In a finance interview, if you\u2019re not sure of the answer, just say it\u2019s compounding \ud83d\ude2d\ud83d\ude02</p>"},{"location":"Finance_Electives/Corporate_Finance/03_Time_Value_of_Money/#keywords","title":"Keywords","text":"Denotion Expressed as Value of something at Present Value PV Currency \\(t = 0\\) (not even \\(t \\approx 0\\)) Future Value FV Currency \\(t &gt; 0\\) Interest RateDiscount RateCompound RateOpportunity cost of capitalRequired return \\(r\\) % Exchange rate between present &amp; future value Number of Periods \\(n\\) or \\(t\\) Timeline Graphical reprsesentation of the timing of cash flows"},{"location":"Finance_Electives/Corporate_Finance/03_Time_Value_of_Money/#singular-cashflow-formula","title":"Singular Cashflow Formula","text":"\\[ \\begin{aligned} \\text{FV} &amp;= \\text{PV} \\times \\underbrace{(1+r)^t}_{\\text{Compound Factor}} \\\\ \\implies \\text{PV} &amp;= \\text{FV} \\times \\underbrace{\\frac{1}{(1+r)^t}}_\\text{Discount Factor} \\end{aligned} \\]"},{"location":"Finance_Electives/Corporate_Finance/03_Time_Value_of_Money/#multiple-cashflows","title":"Multiple Cashflows","text":"\\[ \\begin{aligned} \\text{FV} &amp;= \\sum_{t=0} c_t (1+r)^t \\\\ \\text{PV} &amp;= \\sum_{t=0} \\frac{c_t}{(1+r)^t} \\end{aligned} \\] <p>where \\(c_t\\) can be</p> Flow Type Inflow \\(c_t&gt;0\\) Outflow \\(c_t&lt;0\\)"},{"location":"Finance_Electives/Corporate_Finance/03_Time_Value_of_Money/#types-of-interest","title":"Types of Interest","text":"<p>If \\(P =\\) original principal amount</p> Type FV Simple \\(P \\times (1+r) \\times t\\) Compound(Default) \\(P \\times (1+r)^t\\)"},{"location":"Finance_Electives/Corporate_Finance/03_Time_Value_of_Money/#types-of-cashflows","title":"Types of Cashflows","text":"<p>Infinite series of cashflows which has</p> <p>eg: Preference share in a corporation</p> Perpetuity Annuity Finiteness Infinite Finite Term Forever Fixed Cashflow \u2705 \u2705 Occurs every time period \u2705 \u2705 Present Value \\(\\frac{c}{r}\\) \\(\\frac{c}{r} \\left[ 1-\\frac{1}{(1+r)^t} \\right]\\) Future Value N/A \\(\\frac{c}{r} \\left[ (1+r)^t - 1 \\right]\\)"},{"location":"Finance_Electives/Corporate_Finance/03_Time_Value_of_Money/#conceptual-understanding-of-long-term-loan","title":"Conceptual understanding of long-term loan","text":"<p>Every [equal] installment is actually a combination of</p> <ul> <li>interest payment</li> <li>principal repayment</li> </ul> <p>As time goes on, your installment will be constituting: less of interest repayment &amp; more of principal repayment</p>"},{"location":"Finance_Electives/Corporate_Finance/03_Time_Value_of_Money/#i-missed-a-few-classes","title":"I missed a few classes","text":""},{"location":"Finance_Electives/Corporate_Finance/03_Time_Value_of_Money/#interest-rates","title":"Interest Rates","text":""},{"location":"Finance_Electives/Corporate_Finance/03_Time_Value_of_Money/#apr","title":"APR","text":"<p>Annual Percentage Rate</p>"},{"location":"Finance_Electives/Corporate_Finance/03_Time_Value_of_Money/#ear","title":"EAR","text":"<p>Effective Annual Rate</p> <p>The actual interest rate you are paying</p> \\[ \\text{EAR } = \\left( 1 + \\frac{\\text{APR}}{m} \\right)^m - 1 \\] <p>where \\(m =\\) interest compounding frequency</p> <p>This is the value of \\(r\\) we use when calculating present/future value</p>"},{"location":"Finance_Electives/Corporate_Finance/03_Time_Value_of_Money/#compounding-frequency","title":"Compounding Frequency","text":"\\(m\\) Annual 1 Semi-Annual 2 Quarterly 4 Monthly 12 Daily 365 Hourly 365 * 24 Minutely 365 * 24 * 60 Second 365 * 24 * 60 * 60 <p>As we go from annual compounding towards more frequent compounding frequency, we are moving from discrete compounding to continuous compounding</p>"},{"location":"Finance_Electives/Corporate_Finance/03_Time_Value_of_Money/#compounding-cycle","title":"Compounding Cycle","text":"<p>Frequency of compounding</p> <p>Let \\(m\\) be the compounding cycle, ie number of compounding per year $$ \\begin{aligned} \\text{FV} &amp;= \\text{PV} \\cdot (1+r)^t \\ &amp;= \\text{PV} \\left(1 + \\dfrac{r}{m} \\right)^{mt} &amp; \\text{(Discrete)} \\ &amp;= \\text{PV} \\times e^{rt} &amp; \\text{(Continuous)} \\end{aligned} $$</p>"},{"location":"Finance_Electives/Corporate_Finance/03_Time_Value_of_Money/#idk","title":"IDK","text":"<p>If you are in the middle of time period, and certain cashflows have already been taken, $$ \\text{PV}' = \\text{PV} \\times \\dfrac{1}{(1+r)^{t_a/t_b}} $$</p> <ul> <li>\\(t_a=\\) Time elapsed in current time period</li> <li>\\(t_b=\\) Time Period</li> </ul>"},{"location":"Finance_Electives/Corporate_Finance/04_Capital_Budgeting_Decisions/","title":"04 Capital Budgeting Decisions","text":"<p>Corporations face multiple decisions, but have to pick wisely due to limited capital.</p>"},{"location":"Finance_Electives/Corporate_Finance/04_Capital_Budgeting_Decisions/#capital-budgeting","title":"Capital Budgeting","text":"<p>Process of evaluating firm\u2019s long-term investment opportunities</p> <p>Large investments usually consist of smaller investment decisions.</p>"},{"location":"Finance_Electives/Corporate_Finance/04_Capital_Budgeting_Decisions/#framework","title":"Framework","text":"<ol> <li>Generation of investment idea</li> <li>Estimation of cash flows</li> <li>Select the appropriate opportunity cost of capital</li> <li>Selection of ideas based on acceptance criteria</li> <li>Re-evaluation</li> </ol>"},{"location":"Finance_Electives/Corporate_Finance/04_Capital_Budgeting_Decisions/#types-of-investments","title":"Types of Investments","text":"<ul> <li>Revenue-enhancement</li> <li>Cost-reduction</li> <li>Mandatory [government] investments to meet regulations</li> </ul>"},{"location":"Finance_Electives/Corporate_Finance/04_Capital_Budgeting_Decisions/#net-present-value-primary","title":"Net Present Value (Primary)","text":"<p>It is in currency</p> <p>One of </p> \\[ \\text{NPV} = \\text{PV(Inflows)} - \\text{PV(Outflows)} \\] NPV Meaning Decision \\(&gt;0\\) Actual returns &gt; Minimum required return Accept \\(&lt;0\\) Actual returns &lt; Minimum required return Reject \\(0\\) Actual returns = Minimum required return Doesn\u2019t matter"},{"location":"Finance_Electives/Corporate_Finance/04_Capital_Budgeting_Decisions/#irr-primary","title":"IRR (Primary)","text":"<p>Internal Rate of Return</p> \\[ \\text{IRR} = \\text{Rate @ which NPV is 0} \\] <p>Actual return of your project</p> <p>We only know cashflows; no interest rates</p> <p>Calculating</p> <ol> <li>Derive an equation in terms of </li> </ol> \\[ \\text{NPV} = 0 \\\\ \\implies \\sum \\text{Discounted Cashflows} = 0 \\\\ \\] <ol> <li>Solve for \\(r\\)</li> </ol>"},{"location":"Finance_Electives/Corporate_Finance/04_Capital_Budgeting_Decisions/#profitability-index-secondary","title":"Profitability Index (Secondary)","text":"\\[ \\text{PI} = \\frac{ \\text{PV(Inflows)} }{ \\text{PV(Outflows)} } \\] <p>For every 1 unit of investment</p> \\[ \\begin{aligned} &amp;\\text{Additional value generated after taking minimum returns} \\\\ &amp;= (\\text{PI} - 1) \\times \\text{Original Investment} \\end{aligned} \\] NPV Meaning Decision \\(&gt;1\\) Actual returns &gt; Minimum required return Accept \\(&lt;1\\) Actual returns &lt; Minimum required return Reject \\(1\\) Actual returns = Minimum required return Doesn\u2019t matter"},{"location":"Finance_Electives/Corporate_Finance/04_Capital_Budgeting_Decisions/#payback-period-secondary","title":"Payback Period (Secondary)","text":"<ul> <li>Simplest explanation</li> <li>If you have low DPP, that means the investement is less risky</li> </ul>"},{"location":"Finance_Electives/Corporate_Finance/04_Capital_Budgeting_Decisions/#discounted-payback-period-secondary","title":"Discounted Payback Period (Secondary)","text":"\\[ \\text{DPP} = \\]"},{"location":"Finance_Electives/Corporate_Finance/04_Capital_Budgeting_Decisions/#disadvantages","title":"Disadvantages","text":"<ul> <li>Subjective payback period</li> <li>Only focusing on short-term gains</li> </ul>"},{"location":"Finance_Electives/Corporate_Finance/04_Capital_Budgeting_Decisions/#required-rate-of-return","title":"Required Rate of Return","text":"\\[ \\text{RRR} = R_f + \\beta \\cdot \\text{RP} \\] <p>where</p> <ul> <li>\\(R_f=\\) Risk Free Return</li> <li>\\(\\text{RP} =\\)\u00a0Risk Premium</li> </ul>"},{"location":"Finance_Electives/Corporate_Finance/05_Cost_of_Capital/","title":"Cost of Capital","text":"<p>Depends primarily on the use of funds, not the source, because every investment has a different risk associated with it.</p> <p>Debt is almost always the cheapest source of capital, but has some trouble associated with it. This will be covered in a future topic.</p> <p>It is always calculated as WACC (Weighted average cost of capital)</p> <p>Lower the WACC the better</p> Alias Name Perspective of Required return Investor Appropriate discount rate Firm Compound rate Calculations Opportunity cost of capital idk <ul> <li>Cost of equity</li> <li>Cost of debt/distress</li> </ul>"},{"location":"Finance_Electives/Corporate_Finance/05_Cost_of_Capital/#uses","title":"Uses","text":"<ol> <li>WACC is used to value the entire firm</li> <li>Evaluate return for projects</li> <li>Evaluate performance of firm</li> </ol>"},{"location":"Finance_Electives/Corporate_Finance/05_Cost_of_Capital/#some-notes","title":"Some Notes","text":"<ul> <li>Growing companies have high WACC, as they have risks associated with them</li> <li>It is better if WACC decreases over time</li> </ul>"},{"location":"Finance_Electives/Corporate_Finance/05_Cost_of_Capital/#calculation","title":"Calculation","text":"\\[ \\begin{aligned} \\text{WACC} = \\quad &amp; w_l  \\times k_l (1-\\tau) \\\\ + &amp; w_b  \\times k_b (1-\\tau) \\\\ + &amp; w_p \\times k_p \\\\ + &amp; w_c \\times k_c \\end{aligned} \\] Term Meaning Formula \\(w_d\\) Proportion of debt \\(\\frac{n_d}{n_d + n_p + n_c}\\) \\(w_p\\) Proportion of preference shares \\(\\frac{n_p}{n_d + n_p + n_c}\\) \\(w_c\\) Proportion of common shares \\(\\frac{n_c}{n_d + n_p + n_c}\\) \\(k_l\\) Pre-Tax Cost of Loan (Interest Rate) \\(k_l (1-\\tau)\\) Post-Tax Cost of Loan \\(k_b\\) Pre-Tax Cost of Bond (Yield to Maturity) \\(k_b (1-\\tau)\\) Post-Tax Cost of Bond \\(k_p\\) Cost of preference shares \\(\\frac{D_p}{P_p}\\) \\(k_c\\) Cost of common shares \\(\\tau\\) Tax rate Available <p>Interest is tax-deductable, hence it gives \u2018tax shield\u2019</p>"},{"location":"Finance_Electives/Corporate_Finance/05_Cost_of_Capital/#capm","title":"CAPM","text":"<p>Capital Asset Pricing Model</p> <p>Describes relation between systematic risk and expected rate of return of risky investments.</p> <p>Expected return on a risk investment depends on</p> <ul> <li>Risk-free rate (return rate of bond)</li> <li>Risk premium, depending on \\(\\beta\\), where \\(\\beta\\) is the sensitivity of the stock wrt the market</li> </ul> \\[ \\begin{aligned} k &amp;= r_\\text{min} \\\\ &amp;= r_f + \\beta \\Big[ R_m - r_f \\Big] \\end{aligned} \\] <p>where</p> <ul> <li>\\(r_\\text{min} =\\) Required return of investment</li> <li>\\(r_f =\\) Risk-Free rate</li> <li>\\(r_m =\\) Stock market return</li> <li>Take only recent data (say, 1 year or so)</li> </ul>"},{"location":"Finance_Electives/Corporate_Finance/06_Fixed_Income_Valuation/","title":"Valuation","text":""},{"location":"Finance_Electives/Corporate_Finance/06_Fixed_Income_Valuation/#debt","title":"Debt","text":"<p>It is the rate of return the firm\u2019s lenders demand when they loan money to the firm. </p>"},{"location":"Finance_Electives/Corporate_Finance/06_Fixed_Income_Valuation/#forms-of-borrowing","title":"Forms of Borrowing","text":"Type Private Bank Loan Public Bond/Debenture"},{"location":"Finance_Electives/Corporate_Finance/06_Fixed_Income_Valuation/#bond","title":"Bond","text":"<p>Certificate of </p> Term Fixed? Meaning Formula Unit Face/PAR/Book Value \u2705 Listing price of the security \\(\\text{PAR } = \\frac{\\text{Total Amount}}{\\text{No of bonds}}\\) Currency Coupon Rate \u2705 Interest rate % of face value Time to Maturity/Time to Expiry \u2705 Bounding time period by which face value will be repayed(at every payment instant, we only pay the coupon amount) Credit Rating Partially YTM(Yield-to-Maturity) IRR of the bondActual return for the buyer of the bond Bond Traded at Purchase Returns PAR Market Value = Face Value YTM = Coupon rate Premium Market Value &gt; Face Value YTM &lt; Coupon rate Discount Market Value &lt; Face Value YTM &gt; Coupon rate"},{"location":"Finance_Electives/Corporate_Finance/06_Fixed_Income_Valuation/#bond-price","title":"Bond Price","text":"\\[ \\text{Bond Price} = \\sum_{t=1}^T \\frac{\\text{Coupon } t}{(1+\\text{YTM})^t} + \\frac{\\text{PAR}}{(1+\\text{YTM})^T} \\] \\[ \\text{Bond Price } \\propto \\frac{1}{\\text{Interest Rate}} \\] <p>This is because, if interest rate increases, lenders will go to loan market, and everyone will sell their bonds.</p>"},{"location":"Finance_Electives/Corporate_Finance/06_Fixed_Income_Valuation/#misc","title":"Misc","text":""},{"location":"Finance_Electives/Corporate_Finance/06_Fixed_Income_Valuation/#run-on-the-bank","title":"Run-on-the-bank","text":"<p>Banks should have minimum liquidity, to ensure that</p> <ul> <li>If a private bank falls show on SLR, they can request from government, using Rapport</li> <li>If a govt bank falls show on SLR, they can request from government, using Reverse Rapport</li> </ul>"},{"location":"Finance_Electives/Corporate_Finance/06_Fixed_Income_Valuation/#rapport","title":"Rapport","text":"<p>Repurchase agreement</p>"},{"location":"Finance_Electives/Corporate_Finance/06_Fixed_Income_Valuation/#reverse-rapport","title":"Reverse Rapport","text":""},{"location":"Finance_Electives/Corporate_Finance/06_Fixed_Income_Valuation/#why-are-govt-bonds-risk-free","title":"Why are Govt Bonds Risk-Free?","text":"<p>Chance of default is lowest.</p>"},{"location":"Finance_Electives/Corporate_Finance/06_Fixed_Income_Valuation/#preference-shares","title":"Preference Shares","text":"<ul> <li>Hybrid of debt and common shares</li> <li>Fixed dividends</li> <li>Deferrable dividends</li> <li>They don\u2019t have voting rights</li> <li>There is no expiration date</li> <li>It is the only real example of perpetuity</li> <li>Usually higher return than bonds</li> </ul> \\[ k_p = \\frac{d_p}{p_p} \\quad \\left(\\frac{c}{r} \\text{ from Perpetuity} \\right) \\]"},{"location":"Finance_Electives/Corporate_Finance/07_Equity_Valuation/","title":"Equity Valuation","text":"<p>Common Shares</p> <p>Returns</p> <ul> <li>Dividends</li> <li>Capital Gains</li> </ul> <p>Difficult to estimate pricing, as there are so many variables in play</p> <ol> <li>Unsure cashflows</li> <li>Life of investment is infinite</li> <li>No way to calculate required rate of return</li> </ol> <p>It is frowned upon for a corporation to reduce dividends. Hence, if it increases dividends, it does so very carefully.</p>"},{"location":"Finance_Electives/Corporate_Finance/07_Equity_Valuation/#book-value-method","title":"Book Value Method","text":"<p>Most appropriate for established companies $$ \\begin{aligned} \\text{Value} &amp;= \\dfrac{\\text{Net Worth}}{\\text{No of Shares}} \\ \\text{Net Worth} &amp;= \\text{Assets} - \\text{Liabilities} \\end{aligned} $$ Assumption: book values are representative of true worth of company</p>"},{"location":"Finance_Electives/Corporate_Finance/07_Equity_Valuation/#dividend-capitalization-model","title":"Dividend Capitalization Model","text":"<p>Value of equity is the sum of discounted dividends $$ \\begin{aligned} P_t &amp;= \\sum_{t=1}^\\infty \\dfrac{D_t}{(1+k)^t} \\end{aligned} $$</p>"},{"location":"Finance_Electives/Corporate_Finance/07_Equity_Valuation/#constant-growthgordon-model","title":"Constant Growth/Gordon Model","text":"\\[ \\begin{aligned} D_t &amp;= D_{t-p} \\times (1+g)^p \\\\ \\implies P_t &amp;= \\frac{D_{t+1}}{k_\\text{CS} - g} \\quad \\cancel{+ \\frac{P_\\infty}{(1+r)^\\infty}} \\\\ g &amp;= \\text{ROE} \\times \\text{Retention Rate} \\end{aligned} \\] <p>where</p> <ul> <li>\\(g =\\) dividend growth rate</li> <li>non-zero constant percentage change of dividend from one year to next. If non-constant, we take average \\(g\\) over a few years</li> <li>Retention rate = Plowback rate<ul> <li>\\(= 1-\\text{Payout Rate}\\)</li> </ul> </li> <li>\\(k=\\) market discount rate</li> </ul> Dividend Growth \\(g\\) No \\(0\\) Perpetuity Constant \\(\\ge 0\\)"},{"location":"Finance_Electives/Corporate_Finance/07_Equity_Valuation/#pvgo","title":"PVGO","text":"<p>Present Value of Growth Opportunities</p> <p>Represents value in an equity from expected growth opportunities</p> \\[ \\begin{alignedat}{1} E[&amp;\\text{PVGO}] &amp;&amp;= E[\\text{Growth}] \\\\ &amp;&amp;&amp;= P_{\\text{Growth}} &amp;- P_{\\text{No Growth}} \\\\ &amp;\\text{PVGO}_\\text{Actual} &amp;&amp;= P_\\text{Actual} &amp;- P_{\\text{No Growth}} \\end{alignedat} \\]"},{"location":"Finance_Electives/Corporate_Finance/07_Equity_Valuation/#earnings","title":"Earnings","text":"<p>Earnings Multiplier $$ P_t = (\\text{P/E})\\text{Industry} \\times \\text{EPS}\\text{Firm} $$</p> <p>Shouldn\u2019t \\((\\text{P/E})_\\text{Industry}\\) exclude the company we are analyzing?</p>"},{"location":"Finance_Electives/Corporate_Finance/07_Equity_Valuation/#free-cash-flow-model","title":"Free Cash Flow Model","text":"<p>Principle: Free cash flows will be</p> <ul> <li>Distributed as dividend</li> <li>Reinvested leading to capital appreciation</li> </ul> \\[ \\begin{aligned} P_t = \\ &amp; \\dfrac{\\text{FCFE}}{k} \\\\ \\text{FCFE} = \\ &amp; \\text{Surplus of time period} \\\\ = \\ &amp; \\text{Net Income} + \\text{Non-Cash Exp} \\\\ + \\ &amp; \\text{Investments in Working Capital} \\\\ + \\ &amp;\\text{Net Investment} + \\text{Net Borrowing} \\end{aligned} \\] <p>Use the signs appropriately based on inflow/outflow</p> Net Income Inflow Depreciation and Amortization Inflow Investment in WC Outflow Net Investment Outflow Net Borrowing Outflow"},{"location":"Finance_Electives/Corporate_Finance/08_Capital_Structure/","title":"08 Capital Structure","text":""},{"location":"Finance_Electives/Corporate_Finance/08_Capital_Structure/#capital-re-structuring","title":"Capital Re-Structuring","text":"<p>Change in capital structure and leverage</p> <p>If we need to keep assets constant, then inc in debt should be accompanied with purchasing shares</p> <p>We can maximize shareholder wealth by decreasing WACC</p>"},{"location":"Finance_Electives/Corporate_Finance/08_Capital_Structure/#leverage","title":"Leverage","text":"Operating Leverage Financial Leverage Asset for which firm has to pay a fixed cost Source of funds for which firm has to pay a fixed return Shows ability of firm to Use fixed costs magnify effects of change in sales on its EBIT Relationship between Sales &amp; EBIT Formula \\(\\frac{\\Delta \\% \\text{EBIT}}{\\Delta \\% \\text{Sales}}\\) We employ assets with fixed costs in the hope that volume will provide revenues sufficient to cover all fixed &amp; variable costs Interpretation If you believe that you can increase your sales by investing, go for investing in fixed costs Risk Operating risk EBIT &amp; EPS Preferred for Companies with steady expansion Liquidity\u00a0\\(\\propto \\frac{1}{\\text{Operating Leverage}}\\) <p>Fixed cost is a cost that does not change with no of units.</p>"},{"location":"Finance_Electives/Corporate_Finance/08_Capital_Structure/#operating-risk","title":"Operating Risk","text":""},{"location":"Finance_Electives/DRM/","title":"Derivatives &amp; Risk Management","text":""},{"location":"Finance_Electives/DRM/#references","title":"References","text":"<ul> <li> MIT 18.S096 Topics in Mathematics w Applications in Finance</li> <li> IIT Roorkee | Financial Derivatives and Risk Management</li> </ul>"},{"location":"Finance_Electives/DRM/99_Risk_Neutral_Valuation/","title":"99 Risk Neutral Valuation","text":""},{"location":"Finance_Electives/DRM/99_Risk_Neutral_Valuation/#risk-neutral-valuation","title":"Risk Neutral Valuation","text":"<p>Suppose our economy includes stock \\(S\\), riskless money market account \\(B\\) with interest rate \\(r\\) and derivative claim \\(f\\)</p> <p>Assuming there\u2019s only 2 possible outcomes at time \\(dt\\)</p> <p></p>"},{"location":"Finance_Electives/DRM/99_Risk_Neutral_Valuation/#naive-approach","title":"Naive Approach","text":"<p>Current price of a derivative claim is determined by current price of portfolio which exactly replicates the payoff of the derivative at maturity</p> <p>Consider Forward contract with pays \\(S-K\\) at time \\(dt\\). One could think that its strike \\(K\\) should be defined by the \u201creal world\u201d transition probability \\(p\\) $$ p(S_1 - k) + (1-p) (S_2 - k) = p S_1 + (1-p) S_2 - k \\ p = \u00bd \\implies k_0 = (S_1 + S_2)/2 $$</p> <ol> <li>Borrow \\(S_0\\) to buy stock. Enter forward contract with strike \\(k_0\\)</li> <li> <p>In time \\(dt\\) deliver stock in exchange for \\(k_0\\) and repay \\(S_0 e^{r \\ dt}\\)</p> </li> <li> <p>If \\(k_0 &gt; S_0 e^{r \\ dt}\\), riskless profit</p> </li> <li>If \\(k_0 &lt; S_0 e^{r \\ dt}\\), definite loss</li> </ol> <p>Notes</p> <ul> <li>Given current price of the stock and assumptions on the dynamics of stock price, there is no uncertainty about the price of a derivative.</li> <li>Price is defined only by the price of the stock and not by the risk preferences of the market participants</li> <li>Mathematical apparatus allows us to compute current price of a derivative and its risks, given certain assumptions about the market</li> </ul>"},{"location":"Finance_Electives/DRM/99_Risk_Neutral_Valuation/#general-derivative-claim","title":"General derivative claim","text":"<p>For a claim \\(f\\),\u00a0find \\(a\\)\u00a0and \\(b\\) such that $$ \\begin{aligned} f_1 &amp;= a S_1 + b B_0 e^{r dt} \\ f_2 &amp;= a S_2 + b B_0 e^{r dt} \\ \\implies f_0 &amp;= a S_0 + b B_0 \\end{aligned} $$</p> \\[ \\begin{aligned} a &amp;= \\dfrac{f_1 - f_2}{S_1 - S_2} \\\\ b &amp;= \\dfrac{S_1 f_2 - S_2 f_1}{(S_1 - S_2) B_0 e^{r \\ dt}} \\end{aligned} \\] \\[ f_0 = e^{- r \\ dt} \\Big( f_1 q + f_2 (1-q) \\Big) \\\\ q = (S_0 e^{r \\ dt} - S_2)/(S_1 - S_2), &amp; q \\in (0, 1) \\\\ \\implies q S_1 + (1-q) S_2 = e^{r \\ dt} S_0 \\]"},{"location":"Finance_Electives/DRM/99_Risk_Neutral_Valuation/#black-scholes","title":"Black-Scholes","text":"<p>Assumes that stock has log-normal dynamics $$ dS = \\mu S dt + \\sigma S dw $$ where \\(W\\)\u00a0is a Brownian motion: \\(dW\\) is normally-distributed with mean 0 and standard deviation \\(\\sqrt{dt}\\) </p> <p>We want t find a replicating portfolio such that $$ df = a dS + b dB $$</p> \\[ (dS)^2 = \\sigma^2 S^2 dt \\] \\[ \\dfrac{\\partial f}{\\partial t} + \\dfrac{1}{2} \\dfrac{\\partial^2 f}{\\partial S^2} \\sigma^2 S^2 + \\dfrac{\\partial f}{\\partial S} r S - rf = 0 \\]"},{"location":"Finance_Electives/Development_Economics/","title":"Development Economics","text":""},{"location":"Finance_Electives/Development_Economics/#overview","title":"Overview","text":"<ol> <li>Why are some countries so poor and some so rich</li> <li>What are the particular economic problems faced by countries that are poor, and how do we model and understand these phenomena</li> <li>What could be done to help solve market failures in poor countries</li> <li>What went right</li> <li>What risk going wrong? Is the COVID-19 crisis reversal here to stay</li> <li>Poverty traps</li> <li>Human capital</li> <li>Source</li> <li>Heterogeneity</li> <li> <p>Why people are not investing optimally in</p> <ol> <li>Education</li> <li>Health</li> <li>Nutrition</li> </ol> </li> <li> <p>Capital</p> </li> <li>Heterogeneity</li> <li>Credit &amp; savings</li> <li> <p>Land</p> </li> <li> <p>Labor markets: Allocation of labor</p> </li> <li>Productivity<ol> <li>Technology</li> <li>Organization of firms</li> </ol> </li> </ol>"},{"location":"Finance_Electives/Development_Economics/#references","title":"References","text":"<ul> <li> https://www.youtube.com/playlist?list=PLUl4u3cNGP61kvh3caDts2R6LmkYbmzaG</li> <li> https://www.youtube.com/playlist?list=PLUl4u3cNGP620R91K4KP_fO4l3eeK5lDn</li> <li> IIT Roorkee | Health Economics</li> </ul>"},{"location":"Finance_Electives/Development_Economics/01_Introduction/","title":"Introduction","text":""},{"location":"Finance_Electives/Development_Economics/01_Introduction/#steady-state-ramsey-model","title":"Steady State (Ramsey Model)","text":"\\[ A \\frac{\\partial f}{\\partial k} = \\rho \\] <p>where </p> <ul> <li>LHS = Return on capital</li> <li>\\(\\rho\\) = discount state</li> </ul> <p>Otherwise, people would save more</p>"},{"location":"Finance_Electives/Development_Economics/01_Introduction/#aggregate-production-function","title":"Aggregate Production Function","text":"\\[ y = A \\times f(k, h) \\] <p>where</p> <ul> <li>\\(y\\) is output (real GDP)</li> <li>\\(A\\) is measure of productivity of economy</li> <li></li> <li>\\(k\\) is capital per-capita</li> <li>\\(h\\) is human capital per-capita (skills, knowledge, etc)</li> </ul> <p>For most of history, Development Economics was mostly macro-oriented: focusing on aggregate income difference</p> <p>If the most of per-capita income difference \\(y\\) was mostly \\(k\\), just give the developing countries more \\(k\\), and likewise for \\(h\\).</p> <p>But just giving \\(k\\) would reduce the saving in the country due to Steady State (Ramsey Model), as \\(k\\) responds to A in equilibrium. If you increase \\(k\\) beyond steady state level, returns will be low, and the economy will adjust endogenously and move back to its previous point in time.</p> <p>Variation in policies lead to variation in the growth rate in A.</p>"},{"location":"Finance_Electives/Development_Economics/01_Introduction/#assumption-of-this-model","title":"Assumption of this model","text":"<p>Aggregation works as we are assuming that capital and human capital are optimally distributed by the economy, so the marginal return of investment is equalized.</p> <p>However, in reality, this is not true: some sectors of the economy within the same location may have different return to capital, as \\(h\\) and \\(k\\) are not allocated optimally. For eg, even though it would be more effective to invest into a highly-qualified stranger, we would prefer to invest in an average known-person. This ties back to Behavioral Economics.</p>"},{"location":"Finance_Electives/Development_Economics/01_Introduction/#income","title":"Income","text":"<p>Income and development are not always related.</p> <p>Countries with higher GDP may have worse infant mortality.</p>"},{"location":"Finance_Electives/Development_Economics/01_Introduction/#imf-stimulus","title":"IMF Stimulus","text":"<p>The emergency \u201cspecial drawing rights\u201d is allocated based on the country\u2019s contribution to the IMF. Thereby, the rich countries get money easily compared to poor countries.</p>"},{"location":"Finance_Electives/Development_Economics/01_Introduction/#poverty","title":"Poverty","text":""},{"location":"Finance_Electives/Development_Economics/01_Introduction/#poverty-rate","title":"Poverty Rate","text":"<p>How many people can a baseline basket of goods</p>"},{"location":"Finance_Electives/Development_Economics/02_Poverty_Traps/","title":"Poverty Traps","text":"<p>Does being poor keep you poor?</p>"},{"location":"Finance_Electives/Development_Economics/02_Poverty_Traps/#the-s-curve","title":"The S-curve","text":"<p>This clearly shows that where you start affects where you end.</p> \\[ \\text{Economic Status}_\\text{present} = \\beta_0 + \\beta_1 \\text{Economic Status}_\\text{past} \\] <p>So, the poorest of the poorest need enough direct income transfers to push them up, else they will go back to their past state.</p> <p>The mistake in some models is that they assume upward concave curve rather than the S-curve, thereby missing the existence of the Poverty Trap.</p>"},{"location":"Finance_Electives/Development_Economics/02_Poverty_Traps/#capacity-curve","title":"Capacity Curve","text":"<p>Relationship between work capacity and income.</p> <p>Piece-rate, or piecemeal pay, means that employees are paid by the unit when completing a particular task, for instance, bushels harvested, trees pruned, or acres mowed. In this case, a worker is compensated by individual output and not by an hourly rate</p> <p></p> <p>Higher the income, more food calories. The first few calories are required as a minimum to function actively; however, after a certain point, there is decreased return.</p> <p>\\(v^*\\) represents the smallest possible wage at which people meaningful work. It is not an economy-wide parameter, but an individual parameter depending on the amount of non-labor income (I suppose passive income?)</p>"},{"location":"Finance_Electives/Development_Economics/02_Poverty_Traps/#aggregate-labor-supply","title":"Aggregate Labor Supply","text":"<p>Involuntary unemployment occurs when the economics forces maintain equilibrium due to balancing labor demand and labor supply.</p> <p></p> <p></p> <p></p> <p>The capacity curve changes whether or not the person had food the previous day. So, we can conclude that being today\u2019s employment helps in continued employment tomorrow.</p>"},{"location":"Finance_Electives/Development_Economics/02_Poverty_Traps/#distribution-of-land","title":"Distribution of Land","text":""},{"location":"Finance_Electives/Development_Economics/02_Poverty_Traps/#effective-reservation-wage","title":"Effective Reservation Wage","text":"<p>The thick curve is the effective reservation wage, as you need to be</p> <ul> <li>willing to work</li> <li>able to work</li> </ul>"},{"location":"Finance_Electives/Development_Economics/02_Poverty_Traps/#equilibria","title":"Equilibria","text":"<p>When the wage is very low, very few people can work</p>"},{"location":"Finance_Electives/Development_Economics/02_Poverty_Traps/#policies","title":"Policies","text":"Land reform - May improve production and employment- Can improve production without reducing involuntary unemployment Loans to poor This does not happen, as the poor will first focus on getting basic needs rather than investing in increasing their wealth sufficient to repay, and hence will default on their loans Minimum wage Direct income transfer - The poorest people get empowered by getting cash- Others would get lazy Investment in the poor As nutrition plays a big role in working productivity, employers of long-term contracts tend to invest in their workers. But this does not happen for short-term contracts.Due to this reasoning, slaves were better fed than poor workers. <p>An economy is Pareto efficient. Hence, it is not possible to improve the welfare of someone without decreasing the welfare of someone else.</p>"},{"location":"Finance_Electives/Development_Economics/02_Poverty_Traps/#intra-family-resource-allocation","title":"Intra-Family Resource Allocation","text":"<p>In a poor family, the most efficient way is to invest more resources into the person who is employed so that they can thereby bring in more income, rather than equally. Unfortunately, this may lead to malnourishment of the other person. Sometimes, this may be the only way possible for poor families.</p> <p>In most cases, this is how it goes</p> <pre><code>flowchart TB\n\nFather --&gt;\n|Remaining resources| Children --&gt;\n|Remaining resources| Mother --&gt;\n|Remaining resources| Elderly</code></pre>"},{"location":"Finance_Electives/Development_Economics/02_Poverty_Traps/#poverty-traps-characteristics","title":"Poverty Traps Characteristics","text":"<p>Strong relationships</p> \\[ \\begin{aligned} \\text{Income}_{t+1} &amp;\\propto \\text{Nutrition}_t \\\\ \\text{Productivity}_{t+1} &amp;\\propto \\text{Nutrition}_t \\end{aligned} \\] <p></p> \\[ y_{t+1} = (f \\circ g)(y_t) \\] <p>where</p> <ul> <li>\\(f =\\) Capacity Curve</li> <li>\\(g =\\) Decision to consume curve</li> </ul> <p>Multiple equilibrium points exist when the curve intersects the 45deg line at multiple points</p> <p>Intersection happens where slope = slope of 45deg line = \\(\\tan 45\\) = 1</p> <p>When \\(y_{t+1} = y_t\\), the convolution \\((f \\circ g)\\) has a slope &gt; 1. This happens when the product of the elasticity of \\(f\\) and \\(g\\) curve &gt; 1</p> <p>In some cases, food is affordable, so \\(g\\) won\u2019t be steep.</p>"},{"location":"Finance_Electives/Development_Economics/02_Poverty_Traps/#nutrition-based-poverty-trap","title":"Nutrition-based Poverty Trap","text":"<ul> <li>Not much basis for Nutrition-based Poverty Trap, as food is relatively cheap</li> <li>When people become richer, they tend to spend more on food</li> <li>Half of the increase into better food</li> <li>Half of the increase into more calories</li> <li>Clear relationship between per capita expenditure vs calorie consumption</li> <li>Relationship does not appear to be non-linear, at least in this range, despite the fact it is probably an over-estimate due to reverse-causality</li> <li>Strong log-linear relationship between price of calories and expenditures, indicating a lot of substitution towards more expensive calories</li> <li>Duration of study affects people behavior</li> <li>When subsidies are provided for short-period, people focus on eating \u201cbetter\u201d food than to improve nutritional status</li> <li>if this was a long-term, then the behavior may have been different</li> </ul>"},{"location":"Finance_Electives/Development_Economics/02_Poverty_Traps/#india","title":"India","text":"<p>As you can see, as people get richer, they eat more expensive calories. This means that they don\u2019t feel the pain of poverty trap, as they would rather than get more calories rather than expensive calories, with the same budget.</p> <p></p> <p></p> <p></p> <p>Even though the number of extremely poor people has gone down in India, the per capita consumption has not increased, as the Calorie Engel curve has moved rightward.</p>"},{"location":"Finance_Electives/Development_Economics/02_Poverty_Traps/#china","title":"China","text":"<p>Subsidize staple food in 2 regions fro randomly selected household</p> <p>In both regions, substitution towards more expensive calories</p> <p>Elasticity of calorie consumption with income is low</p>"},{"location":"Finance_Electives/Development_Economics/02_Poverty_Traps/#graduation-programs","title":"Graduation Programs","text":""},{"location":"Finance_Electives/Econometrics/","title":"Econometric Methods","text":""},{"location":"Finance_Electives/Econometrics/#structure-of-this-course","title":"Structure of this course","text":"<ol> <li>What kinds of data</li> <li>Techniques</li> <li>Inference    Explanation of results, by building a story</li> </ol>"},{"location":"Finance_Electives/Econometrics/#application-of-this-course","title":"Application of this course","text":"<ul> <li>Quantification of relationships</li> <li>Address questions like<ul> <li>Change in revenue when price changes</li> <li>Change in productivity of employee when wage rate is changed</li> <li>How does another year of education change earnings</li> </ul> </li> </ul>"},{"location":"Finance_Electives/Econometrics/#references","title":"References","text":"<ul> <li> Econometric Methods | Dr. Sartaj Rasool Rather</li> <li> Forecasting: Principles &amp; Practice</li> <li> MIT 18.S096 Topics in Mathematics w Applications in Finance</li> <li> Forecasting Principles &amp; Practice</li> <li> Website</li> <li> Course</li> <li> Slides</li> <li> Business Analytics Using Forecasting | Galit Shmueli</li> <li> Introduction to Econometrics - 2021 | Pedram Jahangiry</li> <li> Forecasting | Michael Sinkey</li> <li> MIT 6.262 Discrete Stochastic Processes, Spring 2011</li> <li> MIT 15.S50 Poker Theory and Analysis: Applications of poker analytics to investment management and trading</li> <li> NPTEL-NOC IITM | Introduction to Econometrics</li> <li> IIT Roorkee July 2018 | Econometric Modelling</li> <li> NPTEL-NOC IITM | Applied Econometrics</li> <li> IIT KANPUR-NPTEL | Applied Statistics and Econometrics</li> <li> Monte Carlo Simulations</li> <li> Monte Carlo methods &amp; simulations | Jan Dufek | KTH Royal Institute of Technology</li> <li> Monte Carlo Methods | UCLA</li> <li> Monte Carlo Simulation in Python</li> <li> Lancaster CMAF</li> <li> Business Forecasting Principles</li> <li> Forecasting Lectures</li> <li> AcademicEgg</li> <li> Econometrics 1</li> <li> Econometrics 2</li> </ul>"},{"location":"Finance_Electives/Econometrics/01_Intro/","title":"01 Intro","text":""},{"location":"Finance_Electives/Econometrics/01_Intro/#what-is-econometrics","title":"What is Econometrics?","text":"<p>How do we test economic theories are true using real-world data, using statistics.</p>"},{"location":"Finance_Electives/Econometrics/01_Intro/#types-of-data","title":"Types of Data","text":"Experimental Real-World Controlled Environments \u2705 \u274c"},{"location":"Finance_Electives/Econometrics/01_Intro/#why-econometrics","title":"Why Econometrics?","text":"Statistics Econometrics Analyze experimental data Real world data"},{"location":"Finance_Electives/Econometrics/01_Intro/#monetary-policy","title":"Monetary Policy","text":"<p>Policy that deals with finances of a country; most importantly, the money supply.</p>"},{"location":"Finance_Electives/Econometrics/01_Intro/#currency","title":"Currency","text":"<p>is just a medium of exchange.</p> <p>It eliminates the need for \u2018coincidence of wants\u2019- that both parties agree to trade the exact goods and services which the other party also wants.</p>"},{"location":"Finance_Electives/Econometrics/01_Intro/#rationality","title":"Rationality","text":"<p>Decisions taken in order to maximize self-interest and happiness.</p>"},{"location":"Finance_Electives/Econometrics/01_Intro/#banking","title":"Banking","text":"<p>Central bank regulates interest rates through rapport rate.</p> <p>In order to increase interest rate, (i think this missing part is central bank would increase rapport rate)</p>"},{"location":"Finance_Electives/Econometrics/01_Intro/#rapport-rate","title":"Rapport Rate","text":"<p>Rate at which central banks lends to commercial banks</p>"},{"location":"Finance_Electives/Econometrics/01_Intro/#interest-rate","title":"Interest Rate","text":"<p>Rate at which commercial banks lends to the public.</p>"},{"location":"Finance_Electives/Econometrics/01_Intro/#theory","title":"Theory","text":"<ul> <li>Inflation \\(\\propto\\) Money Supply \\(\\propto\\) \\(\\frac{1}{\\text{Interest Rate}}\\)</li> <li>Interest Rate \\(\\propto\\) Rapport Rate</li> </ul> <p>However, this may not always be true</p> <ol> <li>Interest Rate \\(\\not \\propto\\) Rapport Rate    Commericial banks may not follow the same as central bank, if they have high liquidity (lots of hot cash)</li> <li>Inflation \\(\\not \\propto \\frac{1}{\\text{Interest Rate}}\\)    People may not always spend more just because of higher money supply caused by lower interest rates</li> </ol>"},{"location":"Finance_Electives/Econometrics/01_Intro/#type-of-relationships","title":"Type of Relationships","text":"<ul> <li>One-way</li> <li>Simultaneous</li> </ul>"},{"location":"Finance_Electives/Econometrics/01_Intro/#problems","title":"Problems","text":""},{"location":"Finance_Electives/Econometrics/01_Intro/#noise","title":"Noise","text":"<p>Randomness due to nature of the real world.</p>"},{"location":"Finance_Electives/Econometrics/01_Intro/#heteroskedasticity","title":"Heteroskedasticity","text":"<p>Variance in distribution of possible outcomes will change over different time, location, settings.</p> <p>This is the biggest problem when using observational data, rather than experimental data.</p>"},{"location":"Finance_Electives/Econometrics/01_Intro/#auto-correlation","title":"Auto-Correlation","text":"<p>Tendency of a variable to be determined by its own past values.</p> <p>Mostly happens in time-series.</p>"},{"location":"Finance_Electives/Econometrics/01_Intro/#overnightcall-money-market","title":"Overnight/Call Money Market","text":"<p>Short-term lending from central bank to commercial banks.</p> <p>These interest rates are called call money rate. It will be different from the regular loans, and will change regularly.</p> <p>An interbank call money market is a short-term money market which allows for large financial institutions to borrow and lend money at interbank rates. The loans in the call money market are very short, usually lasting no longer than a week.</p>"},{"location":"Finance_Electives/Econometrics/01_Intro/#random-variable","title":"Random Variable","text":"<p>Numerical outcome of a random event</p> <p>There are multiple outcomes possible, and there exists an underlying probability distribution. Hence, for each possible outcome, there exists a corresponding probability.</p> <p>2 random variables cannot perfectly correlated; otherwise it has some cause.</p>"},{"location":"Finance_Electives/Econometrics/01_Intro/#systematic-event","title":"Systematic Event","text":"<p>only one outcome is possible</p> <p>\\(P(A)=1; P(A')=0\\)</p> <p>numerical outcome of a systematic event is systematic variable</p>"},{"location":"Finance_Electives/Econometrics/01_Intro/#africa-case-study","title":"Africa Case Study","text":"<p>People were dying due to malaria and dengue, as they were not having access proper shelter from the mosquitos.</p> <p>The first idea was to increase their income, by proving them direct income transfer, because they would use that money to take care of themselves.</p> <p>2 researchers at MIT studied these African Countries. These 2 researchers disagreed.</p> <p>There were 2 communities</p> <ul> <li>One community was given direct income transfers</li> <li>One community was given nets</li> </ul> <p>Giving nets was more effective, as the other community spent the money for other aspects such as food, baby requirements, purchasing books; they considered nets as a luxury.</p>"},{"location":"Finance_Electives/Econometrics/01_Intro/#psychological-theory-of-consumption","title":"Psychological Theory of Consumption","text":"<p>Proposed by John Maynard Keynes</p> <p>Assuming Consumption \\(\\propto\\) Income</p> \\[ C = c + kI \\] <ul> <li>\\(C\\) is the total consumption</li> <li>\\(c\\) is the basic consumption<ul> <li>\\(c \\ne 0\\)</li> </ul> </li> <li>\\(k\\) is called the MPC (Marginal Propensity to Consume)<ul> <li>\\(k \\in [0, 1]\\)</li> </ul> </li> <li>\\(I\\) is the income</li> </ul>"},{"location":"Finance_Electives/Econometrics/02_Methodology/","title":"02 Methodology","text":""},{"location":"Finance_Electives/Econometrics/02_Methodology/#methodology-for-measuring-relationships","title":"Methodology for measuring relationships","text":"<pre><code>flowchart LR\net[Hypothesis] --&gt;\nmm --&gt;\nem --&gt;\ndc[(Data&lt;br/&gt;Collection)] --&gt;\nEstimation --&gt;\nht[Hypothesis&lt;br/&gt;Testing] --&gt;\nForecasting --&gt;\np[Policy to&lt;br/&gt;affect behavior]\n\nsubgraph Model\n    mm[Mathematical]\n    em[Economic]\nend</code></pre>"},{"location":"Finance_Electives/Econometrics/02_Methodology/#theorylogic","title":"Theory/Logic","text":""},{"location":"Finance_Electives/Econometrics/02_Methodology/#hyphothesis","title":"Hyphothesis","text":"<p>Theoretical assertion whose truth can be tested</p> <p>Logical reasoning on how variables would be related, ie what could be the factors</p> <p>This is also called as specification of relationship. We need to make appropriate specification.</p> <p>An increase in \\(x\\) will cause an increase in \\(y\\)</p> <ul> <li>Null Hypthosis</li> <li>Alternate Hypthosis</li> </ul> <p>Problem with social sciences (business, economics, etc) is that there may be number of factors, but it is not feasible to incorporate all the features</p>"},{"location":"Finance_Electives/Econometrics/02_Methodology/#some-theories","title":"Some theories","text":""},{"location":"Finance_Electives/Econometrics/02_Methodology/#structural-changes","title":"Structural changes","text":"<p>Population shifting from primary sector (agriculture) to secondary sector(manufacturing, construction)</p> <p>Economic sectors are highly-interconnected.</p> <pre><code>flowchart LR\n\nPrimary --&gt;\n|Inflation of Raw Materials| Secondary --&gt;\n|Inflation of Tools| Primary</code></pre>"},{"location":"Finance_Electives/Econometrics/02_Methodology/#population-sentiment","title":"Population Sentiment","text":"<p>People will spend money because they feel secure.</p>"},{"location":"Finance_Electives/Econometrics/02_Methodology/#govt","title":"Govt","text":"<p>Govt gives out many schemes and development projects, in order to mitigate the effect of decreased private interest.</p>"},{"location":"Finance_Electives/Econometrics/02_Methodology/#moral-hazard","title":"Moral Hazard","text":"<p>Insurance will actually cause people to be less cautious.</p> <p>Check in India if the direct monetary support to infected bank agents actually increased the amount of cases.</p>"},{"location":"Finance_Electives/Econometrics/02_Methodology/#mathematical-model","title":"Mathematical Model","text":"<p>Expressing theory in terms of mathematical equations.</p> <p>\u274c Assumes that the relationship is perfect</p> Single Multi Simulateneous Dependency Direct Indirect Multiple Direct Direction Uni Uni Multi Uni-variate \\(y=f(x)\\) \\(y = f(x)\\)\\(x = g(z)\\) \\(y = f(x)\\)\\(x = g(y)\\) Multi-variate \\(y=f(x, z)\\) \\(y = f(x, a)\\)\\(x = g(z, b)\\) Example - Dubai economy depends on US, which depends on China- Risk &amp; Return - Attendance &amp; Performance- Demand &amp; Price- Basically all economic aspects The intermediary variable of multi-equation model is called as moderator <p>Let\u2019s say that, height of wife is a function of height of husband, but not vice-versa; it is a male-dominated society</p> <ul> <li>height of husband is independent</li> <li>height of wife is dependent</li> </ul> <p>Let\u2019s say that, height of wife is a function of height of husband, but vice-versa is also applicable, then it is a equal society.</p> <ul> <li>height of husband is independent</li> <li>height of wife is independent</li> </ul>"},{"location":"Finance_Electives/Econometrics/02_Methodology/#econometric-model","title":"Econometric Model","text":"<p>Similar to Mathematical Model, but understands that relationships are not perfect. There will remain some change unexplained by our mathematical model.</p> <p>The real world is complex and continuously changing, but human knowledge is limited.</p>"},{"location":"Finance_Electives/Econometrics/02_Methodology/#specifying-inexact-relationships","title":"Specifying inexact relationships","text":"\\[ y = \\beta_1 + \\beta_2 x + u \\] <ul> <li>\\(y\\) is actual value of the dependent variable</li> <li> <p>\\(\\beta_1 + \\beta_2 x\\) is the estimated/forecasted/predicted component of \\(y\\)</p> <ul> <li>\\(B_1\\) is the value of \\(y\\) even when \\(x=0\\) It is the value of \\(x\\), that is independent of \\(x\\)</li> <li> <p>For eg, consumption can be non-zero even if income is 0 (called as autonomous consumption spending)</p> </li> <li> <p>\\(B_2\\) is the change in \\(y\\) for a unit change in \\(x\\)</p> </li> </ul> </li> <li> <p>\\(u\\) is the residual/error/disturbance/unexplained component of \\(y\\)</p> <ul> <li>difference between estimated value and actual value</li> <li>component not explained by your initial mathematical model in terms of only \\(x\\)</li> <li>it is different for each point</li> </ul> </li> </ul> Nature of \\(u\\) Accepted? Note Random \u2705 Systematic \u274c \\(\\exists\\) some factor that can be used to better explain \\(y\\)Increase no of independent variables, until \\(u\\) becomes random"},{"location":"Finance_Electives/Econometrics/02_Methodology/#linear-vs-non-linear-model","title":"Linear vs Non-Linear Model","text":"<p>We have to decide based on theory and logic</p> <p>If you are not aware about the theory, then visualize and use trial-error</p>"},{"location":"Finance_Electives/Econometrics/02_Methodology/#types-of-regression","title":"Types of Regression","text":"Regression through Intercept Regression through Origin \\(y\\) has a minimum? \u2705 \u274c \\(B_1\\) required? \u2705 \u274cBasically no intercept Example Supply function"},{"location":"Finance_Electives/Econometrics/02_Methodology/#data","title":"Data","text":""},{"location":"Finance_Electives/Econometrics/02_Methodology/#why-do-we-need-data","title":"Why do we need data?","text":"<p>To estimate numerical values, we need data.</p>"},{"location":"Finance_Electives/Econometrics/02_Methodology/#types","title":"Types","text":"<p>Basics - Refer other notes</p> <ul> <li> <p>Cross Sectional Data</p> <ul> <li>Marks of all students in 1 year</li> </ul> </li> <li> <p>Time Series Data</p> <ul> <li>Marks of 1 student from all years</li> </ul> </li> <li> <p>Panel Data</p> <ul> <li>Marks of all students from all years</li> </ul> </li> <li> <p>Scale Data</p> <ul> <li>Qualitative data</li> <li>Ratings: Good-Poor</li> </ul> </li> </ul>"},{"location":"Finance_Electives/Econometrics/02_Methodology/#sources-of-data","title":"Sources of Data","text":""},{"location":"Finance_Electives/Econometrics/02_Methodology/#primary-data","title":"Primary Data","text":"<p>Data collected on your own, using sensors/surveys</p>"},{"location":"Finance_Electives/Econometrics/02_Methodology/#secondary-data","title":"Secondary Data","text":"<p>Data collected by someone else</p>"},{"location":"Finance_Electives/Econometrics/02_Methodology/#data-frequency","title":"Data Frequency","text":"<p>How often the data is collected</p> <ul> <li>High Frequency: Stock Prices (recorded every second)</li> <li>Low Frequency: GDP (recorded monthly)</li> </ul>"},{"location":"Finance_Electives/Econometrics/02_Methodology/#high-frequency-data-is-preferred-over-low-frequency","title":"High Frequency data is preferred over Low Frequency","text":"<p>This is because, monthly data does not capture small changes appearing between 2 time periods, those small changes may not even be visible if you collect low frequency data.</p>"},{"location":"Finance_Electives/Econometrics/02_Methodology/#data-quality","title":"Data Quality","text":"<p>We must check the following properties of the data</p> <ul> <li>Verify Characteristics<ul> <li>Mean</li> <li>Standard Deviation</li> <li>Skewness</li> </ul> </li> <li> <p>Ensure Reliability</p> <ul> <li>Sensors are working correctly</li> <li>Calculations were made correctly</li> </ul> </li> <li> <p>No Bias</p> <ul> <li>There should be no researcher bias</li> <li> <p>Picking a particular sample</p> </li> <li> <p>Ensuring participants of survey have been unbiased</p> </li> <li>For ex: Satisfaction of students in UAE</li> </ul> </li> </ul>"},{"location":"Finance_Electives/Econometrics/02_Methodology/#sample-estimation","title":"Sample Estimation","text":"<p>Obtain the values of parameters/coefficients, using a sample of data</p>"},{"location":"Finance_Electives/Econometrics/02_Methodology/#types_1","title":"Types","text":"<p>We have to choose a method based on</p> <ul> <li>Nature of relationship</li> <li>Distribution of variables</li> </ul>"},{"location":"Finance_Electives/Econometrics/02_Methodology/#ols","title":"OLS","text":"<p>Assumes normal distribution</p> <p>Finds the best fit to reduce error term</p>"},{"location":"Finance_Electives/Econometrics/02_Methodology/#maximum-likelihood","title":"Maximum Likelihood","text":"<p>Assumes normal or other distribution</p> <p>Finds the best fit to pick the data point corresponding to highest probability of occurance for each data point</p>"},{"location":"Finance_Electives/Econometrics/02_Methodology/#hypothesis-testing","title":"Hypothesis Testing","text":"<p>Testing if our hypothesis holds true</p> <p>Is our sample representative?</p>"},{"location":"Finance_Electives/Econometrics/02_Methodology/#sample-evidence-and-statistical-inference","title":"Sample Evidence and Statistical Inference","text":"<p>Is estimated value statistically closer to hypothesized/assume value?</p> <p>Here, we are only interested in the existence of a relationship.</p> <ul> <li>\\(H_0: B_1 = 0\\) (there exists a relationship)</li> <li>\\(H_1: B_1 \\ne 0\\) (there exists no relationship)</li> </ul>"},{"location":"Finance_Electives/Econometrics/02_Methodology/#localization-of-hypothesis","title":"Localization of Hypothesis","text":"Localization Meaning Local/Specific You cannot generalize a hypothesis for the entire test sample/population that only applies to a training sample. General Universally-applicable hypothesis"},{"location":"Finance_Electives/Econometrics/02_Methodology/#forecastprediction","title":"Forecast/Prediction","text":"<p>Using the estimated equation</p>"},{"location":"Finance_Electives/Econometrics/02_Methodology/#types_2","title":"Types","text":"Implementation type Model only learns from Static Initial training Dynamic latest data Sample type In-Sample Train and test on the same dataset Out-of-Sample Train on a datasetTest on a different dataset"},{"location":"Finance_Electives/Econometrics/02_Methodology/#evaluation","title":"Evaluation","text":"<p>Error is the deviation between predicted and actual value</p> Type Full Form Equation MAE Mean Absolute Error \\(\\sum_{i=1}^n \\vert  \\hat y_i - y_i  \\vert\\) MSE Mean Squared Error \\(\\sum_{i=1}^n (\\hat y_i - y_i)^2\\) RMSE Root Mean Squared Error \\(\\sqrt{ \\sum_{i=1}^n (\\hat y_i - y_i)^2 }\\)"},{"location":"Finance_Electives/Econometrics/02_Methodology/#policy-purposeimpact-analysis","title":"Policy Purpose/Impact Analysis","text":"<p>Understand the impact of a policy decision</p>"},{"location":"Finance_Electives/Econometrics/02_Methodology/#multiplier","title":"Multiplier","text":"\\[ M = \\frac{1}{1 - \\beta_2} \\] <p>An initial increase in income will increase the aggregate income to \\(M\\) times the initial aggregate income.</p>"},{"location":"Finance_Electives/Econometrics/02_Methodology/#disposal-income","title":"Disposal Income","text":"<p>Income ready for spending</p>"},{"location":"Finance_Electives/Econometrics/03_Basic_Regression_Analysis/","title":"03 Basic Regression Analysis","text":""},{"location":"Finance_Electives/Econometrics/03_Basic_Regression_Analysis/#regression","title":"Regression","text":"<p>Examine relationship between different variables</p> <p>Dependence of one variable on another variable</p> <p>Identify PRF, using SRF</p>"},{"location":"Finance_Electives/Econometrics/03_Basic_Regression_Analysis/#assumptions","title":"Assumptions","text":"<ul> <li>Dependent var \\(\\to\\) Random   Variable whose distribution changes for different variables</li> <li>Independent \\(\\to\\) Non-Random</li> </ul>"},{"location":"Finance_Electives/Econometrics/03_Basic_Regression_Analysis/#purpose","title":"Purpose","text":"<p>Derive a function that traces through the conditional means of \\(y\\) corresponding to different values of \\(x\\)</p> <p>Expected value = mean = average Same meaning</p> \\[ E(y|x_i) = \\beta_0 + \\beta_1 x_i \\]"},{"location":"Finance_Electives/Econometrics/03_Basic_Regression_Analysis/#population","title":"Population","text":"<p>not necessarily humans</p> <p>It refers to any set of data (universal); different from sample (will be covered later).</p>"},{"location":"Finance_Electives/Econometrics/03_Basic_Regression_Analysis/#prf","title":"PRF","text":"<p>Population Regression Function</p> <p>Also called as Conditional Expectation Function(CEF)</p> <p>It is theoretical; we almost never have access to this</p> <p>It is always linear wrt hyper-parameters, but may/may not be linear wrt variables</p>"},{"location":"Finance_Electives/Econometrics/03_Basic_Regression_Analysis/#linearity","title":"Linearity","text":"Linear wrt variables Non-Linear wrt variables Linear wrt parameters \\(\\beta_0 + \\beta_1 x_1\\) \\(\\beta_0 + \\beta_2 {x_i}^2\\) Non-Linear wrt parameters \\(\\beta_0 + {\\beta_1}^2 x_1\\) \\(\\beta_0 + {\\beta_1}^2 {x_1}^2\\)"},{"location":"Finance_Electives/Econometrics/03_Basic_Regression_Analysis/#transformation","title":"Transformation","text":"\\[ y_t = e^\\alpha x^\\beta_t e^{u_t} \\iff \\ln y_t = \\alpha + \\beta \\ln x_t + u_t \\] <p>One more thing in slide</p> <p>Some models cannot be changed;  they are intrinsically non-linear</p> \\[ y_t = \\alpha + x^{\\color{orange} \\beta}_t + u_t \\]"},{"location":"Finance_Electives/Econometrics/03_Basic_Regression_Analysis/#stochastic-specification-of-prf","title":"Stochastic Specification of PRF","text":"\\[ \\begin{aligned} y_i &amp;= E(y|x_i) + u_i \\\\ &amp; \\updownarrow \\\\ u_i &amp;= y_i - E(y|x_i) \\end{aligned} \\]"},{"location":"Finance_Electives/Econometrics/03_Basic_Regression_Analysis/#components","title":"Components","text":"<ul> <li>Systematic/Deterministic/Common/Explained component</li> <li>Non-Systematic/Random/Disturbance/Idiosyncratic component<ul> <li>effect of all omitted variables</li> <li>random effects</li> <li>effect of measurement error</li> </ul> </li> </ul>"},{"location":"Finance_Electives/Econometrics/03_Basic_Regression_Analysis/#equivalency-with-prf","title":"Equivalency with PRF","text":"<p>Stochastic Specification is equal to PRF, as long as \\(E(u_i|x_i) = 0\\); this does not mean that \\(u_i = 0 , \\forall i\\)</p> <p>Why? This is because only if it is so, the line passes through the expectations of \\(y\\) for different values of \\(x\\). It is mathematically possible only if so. (Draw graph and see)</p> \\[ E(y_i | x_i) = E(y|x_i) + E(u_i|x_i) \\]"},{"location":"Finance_Electives/Econometrics/03_Basic_Regression_Analysis/#why-do-we-need-stochastic-specification","title":"Why do we need Stochastic Specification?","text":"<ul> <li>Vagueness of theory<ul> <li>Social Sciences has no definite theory for any event</li> </ul> </li> <li>Randomness in human behavior</li> <li>Incorporates effect of missing data<ul> <li>Wealth data is not as easy to get as income data</li> </ul> </li> <li>More appropriate for inexact relatioships</li> <li>Captures effect of omitted variables<ul> <li>Some variables are not as important</li> </ul> </li> <li>Captures effect of poor proxy variables</li> <li>Principle of Parsimony   We usually try to limit to simple models</li> <li>Incorrect functional form<ul> <li>Unknown theory</li> <li>Linear/Non-Linear function</li> </ul> </li> <li>Incorporates measurement errors</li> </ul>"},{"location":"Finance_Electives/Econometrics/03_Basic_Regression_Analysis/#proxy-variable","title":"Proxy Variable","text":"<p>A variable that is closely-associated with the variable we want to use.</p> <p>We use proxy variables, when the main variable is not available</p> <p>eg:</p> <ul> <li>Age and Experience</li> <li>CPI and Inflation</li> </ul>"},{"location":"Finance_Electives/Econometrics/03_Basic_Regression_Analysis/#types-of-relationships","title":"Types of Relationships","text":"Statistical/Schochastic Deterministic Independent var Non-Random Non-Random Dependent var Random Non-Random eg Predicting Crop Yield Ohm\u2019s Law"},{"location":"Finance_Electives/Econometrics/03_Basic_Regression_Analysis/#terms","title":"Terms","text":"\\(y\\) \\(x\\) DependentExplainedPredictandRegressandResponseExogeneous IndependentExplanatoryPredictorRegressorStimulusEndogeneous"},{"location":"Finance_Electives/Econometrics/03_Basic_Regression_Analysis/#capital-flight","title":"Capital Flight","text":"<p>Capital moves from one country to another</p>"},{"location":"Finance_Electives/Econometrics/03_Basic_Regression_Analysis/#regression-ne-causation","title":"Regression \\(\\ne\\) Causation","text":"<p>Does not help understand the direction of causality</p> <p>We need to use domain knowledge, and impose restriction that \\(x\\) causes \\(y\\)</p>"},{"location":"Finance_Electives/Econometrics/03_Basic_Regression_Analysis/#regression-vs-correlation","title":"Regression vs Correlation","text":"Regression Correlation Understand Exact relationship Degree of linear association between 2 variables Assumption One Dependent variableOne/more independent variable Both \\(x\\) and \\(y\\) are random"},{"location":"Finance_Electives/Econometrics/03_Basic_Regression_Analysis/#exogeneous-vs-endogeneous","title":"Exogeneous vs Endogeneous","text":"<p>Exogeneous vs endogeneous depends on what you assume to be the system</p> Exogeneous Endogeneous In our control? \u274c \u2705 <ul> <li>Exo = out</li> <li>Endo = in</li> </ul>"},{"location":"Finance_Electives/Econometrics/03_Basic_Regression_Analysis/#basic-concepts-of-regression","title":"Basic Concepts of Regression","text":"<ol> <li>Derive Conditional values of \\(y\\) wrt \\(x\\)</li> <li>Calculate Conditional probabilities of \\(y\\) for different values of \\(x\\)</li> <li>Calculate conditional mean</li> <li>Calculate the weighted average using probality of occurance<ul> <li>This is different mean from arithmetic mean(simple average)</li> </ul> </li> </ol> <p>The expected value of unconditional random variable \\(y\\) is ???</p>"},{"location":"Finance_Electives/Econometrics/03_Basic_Regression_Analysis/#variability-of-y","title":"Variability of \\(y\\)","text":"<p>The variation of \\(y\\) for different values of \\(x\\)</p> <p>Higher variability is preferred</p>"},{"location":"Finance_Electives/Econometrics/04_Sample_Regression_Function/","title":"04 Sample Regression Function","text":""},{"location":"Finance_Electives/Econometrics/04_Sample_Regression_Function/#sample","title":"Sample","text":"<p>Subset of Population</p> <p>Useful, as we can never have access to the entire population</p> <p>We will only have limited values of \\(y\\) for given value(s) of \\(x\\); we don\u2019t have access to the true distribution of \\(y\\) for different values of \\(x\\)</p>"},{"location":"Finance_Electives/Econometrics/04_Sample_Regression_Function/#srf","title":"SRF","text":"<p>Sample Regression Function</p> \\[ \\begin{aligned} \\hat y_i &amp;= \\hat \\beta_0 + \\hat \\beta_1 x_i \\\\ y_i &amp;= \\hat \\beta_0 + \\hat \\beta_1 x_i + \\hat u_i \\end{aligned} \\] <p>We try to make each of these hyperparameters close to their PRF counter-parts</p> <ul> <li>\\(\\hat \\beta_0\\) is an estimator of \\(\\beta_0\\) from the PRF</li> <li>\\(\\hat \\beta_1\\) is an estimator of \\(\\beta_1\\) from the PRF</li> <li>\\(\\hat u_i\\) is an approximation of \\(u_i\\) from the PRF</li> <li>\\(\\hat y_i\\) is the predicted value of dependent variable from sample data</li> <li>\\(y_i\\) is the true value of dependent variable</li> </ul> <p>We can use the SRF to approximate a variable\u2019s distribution, by using statistical distributions, such as Poisson, \\(t\\), Normal, \\(\\chi^2\\)</p>"},{"location":"Finance_Electives/Econometrics/04_Sample_Regression_Function/#problem","title":"Problem","text":"<ul> <li>Over-estimation   SRF estimate may be higher than the PRF estimate</li> <li>Under-estimation   SRF estimate may be lower than the PRF estimate</li> </ul>"},{"location":"Finance_Electives/Econometrics/04_Sample_Regression_Function/#issue-with-sample-data","title":"Issue with Sample Data","text":"<p>We will get different SRF for each sample</p> <p>Perfect fit is impossible, due to Sample Fluctuations</p> <ul> <li>The tendancy of each sample to be different from each other</li> <li>It is basically impossible to avoid this</li> </ul> <p>There is no way to say what best represents the PRF</p> <p>Sample can be mis-representing You have to be careful about interpreting the results</p> <ul> <li>Nature of random sample may cause SRF to over-estimate/under-estimate</li> <li>Biased choosing of sample<ul> <li>Researcher chooses a particular sample</li> <li>For eg</li> <li>I chose Ronaldo for 2<sup>nd</sup> year study project</li> <li>Choosing students of high attendance only</li> </ul> </li> </ul>"},{"location":"Finance_Electives/Econometrics/04_Sample_Regression_Function/#sampling-distribution","title":"Sampling Distribution","text":"<p>The distribution of \\(\\hat \\beta_0, \\hat \\beta_1, \\hat u_i\\) for different samples</p>"},{"location":"Finance_Electives/Econometrics/04_Sample_Regression_Function/#why-is-it-important","title":"Why is it important?","text":"<p>Helps us understand which pair of \\(\\hat \\beta_0, \\hat \\beta_1\\) is the closest to the PRF \\(\\beta_0, \\beta_1\\)</p>"},{"location":"Finance_Electives/Econometrics/04_Sample_Regression_Function/#techniques-to-identify-srf","title":"Techniques to identify SRF","text":"<p>You don\u2019t have to memorize the formula, but you should know what is happening; that way you can debug any errors</p> <ul> <li>OLS</li> <li>Maximum Likelihood Estimation</li> </ul>"},{"location":"Finance_Electives/Econometrics/04_Sample_Regression_Function/#autocorrelation","title":"Autocorrelation","text":"<p>Correlation between values of the same variable</p> <p>Usually used for time series data</p> <p>We use ARIMA(AutoRegressive Integrated Moving Averages) Model It\u2019s just values based on lagged values</p>"},{"location":"Finance_Electives/Econometrics/05_OLS/","title":"OLS","text":"<p>Refer to 07_Regression.md </p>"},{"location":"Finance_Electives/Econometrics/06_Hypothesis_Testing/","title":"Hypothesis Testing","text":"<p>Hypothesis testing involves testing the following question</p> <p>Is the estimated value sufficiently close to stated value?</p>"},{"location":"Finance_Electives/Econometrics/06_Hypothesis_Testing/#hypothesis","title":"Hypothesis","text":"<ul> <li>Simple Hypothesis<ul> <li>Single Tailed</li> <li>\\(\\beta_2 &gt; 0.3\\)</li> </ul> </li> <li>Composite Hypothesis<ul> <li>2 Tailed</li> <li>Bi-Directional</li> <li>Useful when not sure about the direction</li> <li>\\(\\beta_2 \\ne 0.3\\)</li> </ul> </li> </ul>"},{"location":"Finance_Electives/Econometrics/06_Hypothesis_Testing/#null-hypothesis-h_0","title":"Null Hypothesis \\((H_0)\\)","text":"<p>Your initial statement</p>"},{"location":"Finance_Electives/Econometrics/06_Hypothesis_Testing/#alternative-hypothesis-h_1","title":"Alternative Hypothesis \\((H_1)\\)","text":"<p>[Usually] complement of your initial statement</p> <p>Also called as Maintained Hypothesis. It acts as the fallback in case null hypothesis is proven to be false.</p> <p>Then the value of \\(y\\) is taken to be the value obtained from the sample</p>"},{"location":"Finance_Electives/Econometrics/06_Hypothesis_Testing/#number-of-hypotheses","title":"Number of Hypotheses","text":"<p>If \\(n =\\) no of independent variables, the number of hypothesis is \\(2n + 1\\)</p> <p>\\(+1\\) is due to intercept(constant)</p>"},{"location":"Finance_Electives/Econometrics/06_Hypothesis_Testing/#steps","title":"Steps","text":"<ol> <li> <p>Formula hypotheses</p> </li> <li> <p>Determine if one/two tailed test</p> </li> <li> <p>Construct a \\(100(1-\\alpha) \\ \\%\\) confidence interval for \\(\\beta_2\\)</p> </li> <li> <p>Determine critical values</p> </li> <li> <p>Determine rules to accept/reject null hypothesis</p> </li> <li> <p>Compare estimate-value with critical region</p> </li> <li> <p>Conclusion</p> <ul> <li>If it lies within critical region, accept null hypothesis</li> <li>If it lies outside critical region, reject null hypothesis</li> <li>accept alternate hypothesis</li> <li>\\(\\beta_2\\) will take the sample value</li> </ul> </li> </ol>"},{"location":"Finance_Electives/Econometrics/06_Hypothesis_Testing/#confidence-interval","title":"Confidence Interval","text":"\\[ \\begin{aligned} (1-\\alpha) &amp;= P(- t_{\\alpha/2} \\le \\textcolor{hotpink}{t} \\le +t_{\\alpha/2}) \\\\ \\textcolor{hotpink}{t} &amp;= \\frac{\\hat \\beta_2 - \\beta_2}{\\sigma(\\hat \\beta_2)} \\end{aligned} \\] \\[ (1-\\alpha) = P(\\hat \\beta_2  t) \\] <ul> <li>\\(\\alpha =\\) level of significance</li> <li>\\((1-\\alpha) =\\) Confidence coefficient</li> </ul> <p>Construct the confidence interval for \\(t\\) distribution, with \\((n-2)\\) degrees of freedom. This is because we have 2 unknowns.</p> <p></p>"},{"location":"Finance_Electives/Econometrics/06_Hypothesis_Testing/#level-of-significance","title":"Level of Significance","text":"<p>Tolerance level for error</p> <p>This is</p> <ul> <li>probability of committing type 1 error</li> <li>probability of rejecting null hypothesis, and then getting sample value as the actual value just by chance</li> </ul> Field Conventional \\(\\alpha\\) Conventional \\((1 - \\alpha) \\%\\) Pure Sciences 0.01 99% Social Sciences 0.05 95% Psychology 0.10 90%"},{"location":"Finance_Electives/Econometrics/06_Hypothesis_Testing/#normal-distribution","title":"Normal Distribution","text":"<ul> <li>\\(95 \\%\\) values lies within 1 standard deviation on each side from the center</li> <li>\\(2.5 \\%\\) values lies outside 1 standard deviation on left side</li> <li>\\(2.5 \\%\\) values lies outside 1 standard deviation on right side</li> </ul>"},{"location":"Finance_Electives/Econometrics/06_Hypothesis_Testing/#errors","title":"Errors","text":"Type 1 Type 2 Error of Rejecting correct null hypothesis Accepting incorrect null hypothesis Meaning False Negative False Positive Measured by \\(\\alpha\\)(Level of Significance) Happens when Sample is not a good representation of population"},{"location":"Finance_Electives/Econometrics/06_Hypothesis_Testing/#statistical-equivalence","title":"Statistical Equivalence","text":"<p>0.5 can be statistically = 0, or not; depends on the context</p> \\[ \\begin{aligned} P(\\text{rejecting } H_0) &amp;\\propto |t| \\\\ &amp;\\propto \\text{Deviation of sample value from true value} \\end{aligned} \\] \\[ t = 0 \\implies \\hat \\beta_2 = \\beta_2 \\]"},{"location":"Finance_Electives/Econometrics/06_Hypothesis_Testing/#p-value","title":"p-Value","text":"<p>Observed level of significance</p> <p>\\(\\text{p value} \\le \\alpha \\implies\\) Reject null hypothesis</p>"},{"location":"Finance_Electives/Econometrics/06_Hypothesis_Testing/#t2-rule","title":"\\(t=2\\) Rule","text":"<p>For degree of freedom \\(\\ge 20\\)</p>"},{"location":"Finance_Electives/Econometrics/06_Hypothesis_Testing/#2-tailed","title":"2 Tailed","text":"<p>\\(H_0: \\beta_2 = 0, H_0: \\beta_2 \\ne 0\\) </p> <p>If \\(|t| &gt; 2 \\implies p \\le 0.05 \\implies\\) reject \\(H_0\\)</p>"},{"location":"Finance_Electives/Econometrics/06_Hypothesis_Testing/#1-tailed","title":"1 Tailed","text":"<p>\\(H_0: \\beta_2 = 0, H_0: \\beta_2 &gt; 0\\) or \\(H_0: \\beta_2 = 0, H_0: \\beta_2 &lt; 0\\) </p> <p>If \\(|t| &gt; 1.73 \\implies p \\le 0.05 \\implies\\) reject \\(H_0\\)</p>"},{"location":"Finance_Electives/Econometrics/06_Hypothesis_Testing/#why-t-distribution","title":"Why \\(t\\) distribution?","text":"<p>\\(t\\) distribution is a variant of \\(z\\) distribution</p> <ul> <li>For small samples, we use \\(t\\) dist</li> <li>For large sample, we use \\(z\\) dist</li> </ul>"},{"location":"Finance_Electives/Econometrics/07_Time_Continuous/","title":"Continuous","text":""},{"location":"Finance_Electives/Econometrics/07_Time_Continuous/#challenge","title":"Challenge","text":"<p>How to describe probability distribution</p>"},{"location":"Finance_Electives/Econometrics/07_Time_Continuous/#brownian-motionwiener-process","title":"Brownian Motion/Wiener Process","text":"<p>Basically a continuous version of simple RW: \u2018Limit\u2019 of simple RW</p> <p>Denoted using \\(y_t = B(t)\\)</p>"},{"location":"Finance_Electives/Econometrics/07_Time_Continuous/#properties","title":"Properties","text":"<ul> <li>Always starts at 0: \\(P( y_0 = 0 ) = 1\\)</li> <li>Stationary \\(\\forall s \\in [0, t)\\)</li> <li>\\(y_t - y_s \\sim N(0, t-s)\\)<ul> <li>where \\((t-s)\\) is the length of the interval</li> </ul> </li> <li>Independent increment: If intervals \\([s_i, t_i]\\) are non-overlapping, then \\(y_{t_i} - y_{s_i}\\) are independent</li> </ul>"},{"location":"Finance_Electives/Econometrics/07_Time_Continuous/#characteristics","title":"Characteristics","text":"<ul> <li>Cross the independent axis indefinitely-often</li> <li>Does not deviate too much from \\(y_t = \\sqrt{t}\\)</li> <li>Not differentiable</li> <li>Standard calculus cannot be applied</li> <li>Requires Ito calculus</li> <li>Max series </li> </ul> \\[ \\begin{aligned} &amp; M_t = \\max_{s \\le t} (y_s) \\\\ \\implies &amp;P(M_t &gt; a) = 2 \\cdot P (y_t &gt; a) \\\\ &amp; \\forall \\ t, a &gt; 0 \\end{aligned} \\] <ul> <li>Quadratic variation</li> </ul> <p>$$ t = \\frac{i}{n} T \\ \\implies \\lim_{n \\to \\infty} \\sum_{i=1}^n (y_{t} - y_{t-1})^2 = T \\ \\forall T &gt; 0 \\</p> <p>\\implies (dB)^2 = dt $$</p>"},{"location":"Finance_Electives/Econometrics/07_Time_Continuous/#implications","title":"Implications","text":"\\[ \\dfrac{d y_t}{y_t} = d B_t \\\\ d y_t \\ne \\dfrac{d B_t}{dt} \\cdot dt \\\\ \\implies y_t \\ne e^{B_t} \\] <p>This is because \\(\\dfrac{d B_t}{dt}\\) is not defined since \\(B_t\\) is not differentiable</p>"},{"location":"Finance_Electives/Econometrics/07_Time_Continuous/#itos-lemma","title":"Ito\u2019s Lemma","text":"<p>Consider \\(y_t = f(B_t)\\), where \\(f\\) is a smooth function $$ \\begin{aligned} y_t &amp;= f(B_t) \\ \\implies df &amp; \\ne f'(B_t) \\cdot d B_t \\quad [\\because (dB)^2 = dt] \\ \\implies df &amp;= f'(B_t) \\cdot d B_t + \\dfrac{1}{2} f''(B_t) \\cdot dt \\end{aligned} $$</p>"},{"location":"Finance_Electives/Econometrics/07_Time_Continuous/#idk","title":"IDK","text":"<p>Assuming \\(\\mu, \\sigma\\) are constant $$ \\begin{aligned} dy_t &amp;= \\underbrace{\\mu dt}_\\text{Drift} + \\sigma d B_t \\ \\implies y_t &amp;= \\mu t + \\sigma B_t \\end{aligned} $$ Using Ito\u2019s Lemma (Basically Taylor\u2019s expansion) $$ d f(t, x) = \\dfrac{\\partial f}{\\partial t} + \\mu \\dfrac{\\partial f}{\\partial x} + \\dfrac{1}{2} \\sigma^2 \\dfrac{\\partial^2 f}{\\partial x^2} + \\dfrac{\\partial f}{\\partial x} d B_t $$</p>"},{"location":"Finance_Electives/Econometrics/07_Time_Continuous/#integration","title":"Integration","text":"\\[ \\begin{aligned} F(t, B_t) &amp;= \\int f(t, B_t) d B_t + \\int g(t, B_t) dt \\\\ dF &amp;= f dB_t + g dt \\end{aligned} \\] <p>Ito integral is the limit of Riemanian sums when we always take leftmost point of each integral</p> <p>Intuitively, it only uses the data you have seen so far</p>"},{"location":"Finance_Electives/Econometrics/07_Time_Continuous/#adapted-process","title":"Adapted Process","text":"<p>A strategy/decision \\(D_t\\) is said to be adapted to \\(y_t\\), if \\(D_t\\)\u00a0only depends on \\(y_s, s \\le t, \\forall t\\)</p> <p>If \\(D_t\\) only depends on \\(t\\) and not on \\(B_t\\), then \\(y_t = \\int D_t \\cdot d B_t\\) is normally-distributed at all times</p>"},{"location":"Finance_Electives/Econometrics/07_Time_Continuous/#ito-isometry","title":"Ito Isometry","text":"<p>Used to calculate variance of Brownian motion $$ \\begin{aligned} D_t &amp;\\text{ adapted to } B_t \\ \\implies V(B_t) &amp;= E \\left[ (\\int_0^t D_s \\cdot dB_s)^2 \\right] \\ &amp;= E \\left[ \\int_0^t D^2_t \\cdot ds \\right] \\end{aligned} $$ Due to quadratic variance</p>"},{"location":"Finance_Electives/Econometrics/07_Time_Continuous/#martingale","title":"Martingale","text":"<p>If \\(g(t, B_t)\\) is adapted to \\(B_t\\) then \\(\\int g(t, B_t) \\cdot dB_t\\),\u00a0as long as \\(g\\)\u00a0is \u201creasonable\u201d</p> <p>\\(g\\) is reasonable if \\(\\int \\int g^2 \\cdot dt \\cdot dB_t &lt; \\infty\\)</p> <p>If a stochastic differential equation does not have a drift term, then it is a martingale $$ d y_t = \\sigma \\cdot dB_t \\qquad [\\mu = 0] $$ Defining stock price as brownian motion, as it is a martingale process $$ \\begin{aligned} S_t &amp;= \\exp(\\frac{-\\sigma^2 t}{2} + \\sigma B_t) \\ \\implies \\dfrac{d S_t}{S_t} &amp;= \\sigma \\cdot d B_t \\end{aligned} $$</p>"},{"location":"Finance_Electives/Econometrics/07_Time_Continuous/#stochastic-differential-equation","title":"Stochastic Differential Equation","text":"\\[ d y_t = \\mu \\cdot dt + \\sigma \\cdot dB_t \\]"},{"location":"Finance_Electives/Econometrics/07_Time_Continuous/#change-of-measure","title":"Change of measure","text":"<p>Consider </p> <ul> <li>\\(B\\) is brownian process w/ drift and pdf \\(P\\)</li> <li>\\(\\tilde B\\) is brownian process w/o drift and pdf \\(\\tilde P\\)</li> </ul> \\[ \\exists z, \\text{ such that } P(t) = z(t) \\cdot \\tilde P(t) \\iff P \\equiv \\tilde P \\] \\[ P \\equiv P \\text{ if } \\\\ P (A) &gt; 0 \\iff \\tilde P (A) &gt; 0, \\quad \\forall A \\subseteq \\Omega \\] <p>\\(z\\)\u00a0is called the Radon-Nikodym derivative</p>"},{"location":"Finance_Electives/Econometrics/07_Time_Continuous/#girsanov-theorem","title":"Girsanov theorem","text":"\\[ z(t) = \\dfrac{d \\tilde P}{dP} (t) = e^{-\\mu t T - \\frac{\\mu^2 T}{2}} \\] \\[ E[y_t] = \\tilde E[\\tilde z_t y_t] \\\\ \\tilde E[y_t] = E[z_t y_t] \\]"},{"location":"Finance_Electives/Econometrics/07_Time_Series_Processes/","title":"Time Series Processes","text":""},{"location":"Finance_Electives/Econometrics/07_Time_Series_Processes/#time-series","title":"Time Series","text":"<p>Observation of random variable ordered by time</p> <p>Time series variable can be</p> <ul> <li>Time series at level (absolute value)</li> <li>Difference series (relative value)<ul> <li>First order difference \\(\\Delta y_t = y_t - y_{t-1}\\)</li> <li>Called as \u2018returns\u2019 in finance</li> <li>Second order difference \\((\\Delta y_t)_2 = \\Delta y_t - \\Delta y_{t-1}\\)</li> </ul> </li> </ul>"},{"location":"Finance_Electives/Econometrics/07_Time_Series_Processes/#univariate-time-series","title":"Univariate Time Series","text":"<p>Basic model only using a variable\u2019s own properties like lagged values, trend, seasonality, etc</p>"},{"location":"Finance_Electives/Econometrics/07_Time_Series_Processes/#why-do-we-use-different-techniques-for-time-series","title":"Why do we use different techniques for time series?","text":"<p>This is due to</p> <ul> <li>behavioral effect</li> <li>history/memory effect<ul> <li>Medical industry always looks at the records of your medical history</li> </ul> </li> <li>Inertia of change</li> <li>Limited data</li> </ul>"},{"location":"Finance_Electives/Econometrics/07_Time_Series_Processes/#components-of-time-series-processes","title":"Components of Time Series Processes","text":"Characteristic Frequency Example Level Average value of series Constant Trend Gradual Low Drift Exogeneous Constant Cycles &gt; 1 year Economy cycle Seasonality Daily, Weekly, Monthly Structural Break Holidays Eid, Christmas Auto-Correlation Relationship with past Shocks Power outage Noise Random High"},{"location":"Finance_Electives/Econometrics/07_Time_Series_Processes/#auto-correlation","title":"Auto-correlation","text":"<p>Sometimes just auto-correlation is enough to learn the values of a value</p> \\[ y_t = \\beta_0 + \\beta_1 y_{t-1} + e_t \\] <p>If we take \\(j\\) lags,</p> \\[ y_t = \\beta_0 + \\sum_{i=1}^j \\beta_i y_{t-i} + e_t \\] <p>Generally, \\(i&gt;j \\implies \\beta_i &lt; \\beta_j\\)</p> <p>Impact of earlier lags is lower than impact of recent lags</p>"},{"location":"Finance_Electives/Econometrics/07_Time_Series_Processes/#shock","title":"Shock","text":"<p>\u2018Shock\u2019 is an abrupt/unexpected deviation(inc/dec) of the value of a variable from its expected value</p> <p>This incorporates influence of previous disturbance</p> <p>They cause a structural change in our model equation. Hence, we need to incorporate their effect.</p> \\[ \\text{Shock}_t = y_t - E(y_t) \\] <p>Basically, shock is basically \\(u_t\\) but it is fancily called as a shock, because they are large \\(u\\)</p> <p>Can be</p> Temporary Permanent Duration Short-term Long-Term Causes structural change Examples Change in financial activity due to Covid Change in financial activity due to 2008 Financial Crisis Heart rate change due to minor strokeHeart rate change due to playing Heart rate change due to major strokeHeart rate change due to big injury Goals scored change due to small fever Goals scored change due to big injury <p>Model becomes</p> \\[ y_t = \\beta_0 + \\beta_1 y_{t-1} + \\beta_2 u_{t-1} \\]"},{"location":"Finance_Electives/Econometrics/07_Time_Series_Processes/#structural-breaks","title":"Structural Breaks","text":"<p>Permanent change in the variable causes permanent change in relationship</p> <p>We can either use</p> <ul> <li>different models before and after structural break</li> <li>binary \u2018structural dummy variable\u2019 to capture this effect</li> </ul> <p>For eg, long-term injury</p> \\[ y_t = \\beta_0 + \\beta_1 y_{t-1} + \\beta_2 B_t \\]"},{"location":"Finance_Electives/Econometrics/07_Time_Series_Processes/#trend","title":"Trend","text":"<p>Tendency of time series to change at a certain expected rate.</p> <p>Trend can be</p> <ul> <li>deterministic/systematic (measurable)</li> <li>random/stochastic (not measurable \\(\\implies\\) cannot be incorporated)</li> </ul> <p>For eg: as age increases, humans have a trend of</p> <ul> <li>growing at a certain till the age of 20 or so</li> <li>reducing heart rate</li> </ul> <p>Model becomes</p> \\[ y_t = \\beta_0 + \\beta_1 y_{t-1} + \\beta_2 t \\]"},{"location":"Finance_Electives/Econometrics/07_Time_Series_Processes/#seasonalityperiodicity","title":"Seasonality/Periodicity","text":"\\[ y_t = \\beta_0 + \\beta_1 y_{t-1} + \\beta_2 \\textcolor{hotpink}{S} + \\beta_3 \\textcolor{hotpink}{t S} \\] <p>Tendency of a variable to change in a certain manner at regular intervals.</p> <p>For eg</p> <ul> <li>demand for woolen clothes is high every winter</li> <li>demand for ice cream is high every summer</li> </ul> <p>Finance industry has \u2018anomalies\u2019</p> <p>Two ways to encode</p> Type Advantage Disadvantage Example \\(S\\) Binary Simple Unrealistic Dummy \\(\\{0, 1\\}\\) Continuous Realistic Complex Cyclic Linear Basis \\(\\exp{\\left[\\frac{- 1}{2 \\alpha^2} (x_i - \\text{pivot})^2\\right]}\\)- Pivot is the center of the curve Preferred, as more control over amplitude and bandwidth Fourier series \\(\\alpha \\cos \\left(\\frac{2 \\pi}{\\nu} + \\phi \\right) + \\beta \\sin \\left( \\frac{2 \\pi}{\\nu} + \\phi\\right)\\), where \\(\\nu =\\) Frequency of seasonality, and \\(\\phi\\) is the offset- Quarterly = 4- Monthly = 12"},{"location":"Finance_Electives/Econometrics/07_Time_Series_Processes/#volatility","title":"Volatility","text":"<p>Annualized standard deviation of change of a random variable</p> <p>Measure of variation of a variable from its expected value</p> <p>If the variance is heteroskedastic (changes over time), the variable is volatile</p> \\[ \\begin{aligned} \\sigma^2_{y_t} &amp;= E \\Big [\\Big(y_t - E(u_t) \\Big)^2 \\Big] \\\\ &amp;= E \\Big [\\Big(y_t - \\textcolor{hotpink}{0} \\Big)^2 \\Big] \\\\ &amp;= E [y_t^2 ] \\\\ &amp;= y_t^2 \\\\ \\end{aligned} \\] \\[ y_t = \\beta_0 + \\beta_1 y_{t-1} + \\beta_2 \\sigma^2_{t-1} \\]"},{"location":"Finance_Electives/Econometrics/07_Time_Series_Processes/#lag-terms","title":"Lag Terms","text":"\\[ \\begin{aligned} \\text{Let's say} \\to y_t &amp;= f(y_{t-2}) \\\\ y_t &amp;= f(y_{t-1}) \\\\ y_{t-1} &amp;= f(y_{t-2}) \\end{aligned} \\] \\[ y_t = \\rho_1 y_{t-1} + \\rho_2 y_{t-2} + u_t \\] <p>Here, \\(\\rho_1\\) and \\(\\rho_2\\) are partial-autocorrelation coefficient of \\(y_{t-1}\\) and \\(y_{t-2}\\) on \\(y_t\\)</p> \\[ y_t = \\rho_1 y_{t-2} + u_t \\] <p>Here, \\(\\rho_1\\) is total autocorrelation coefficient of \\(y_{t-2}\\) on \\(y_t\\)</p> <p>We choose the number of lags by trial-and-error and checking which coefficients are significant (\\(\\ne 0\\))</p>"},{"location":"Finance_Electives/Econometrics/07_Time_Series_Processes/#stochastic-data-generating-processes","title":"Stochastic Data-Generating Processes","text":"<p>Stochastic process is a sequence of random observations indexed by time</p>"},{"location":"Finance_Electives/Econometrics/07_Time_Series_Processes/#markov-chain","title":"Markov Chain","text":"<p>Stochastic process where effect of past on future is summarized only by current state $$ P(y_{t+1} = a \\vert x_0, x_1, \\dots x_t) = P(x_{t+1} = a \\vert x_t) $$ If possible values of \\(x_i\\) is a finite set, MC can be represented as a transition probability matrix</p>"},{"location":"Finance_Electives/Econometrics/07_Time_Series_Processes/#martingale","title":"Martingale","text":"<p>Stochastic processes which are a \u201cfair\u201d game $$ E[y_{t+1} \\vert y_t] = y_t $$ Follow optimal stopping theorem</p>"},{"location":"Finance_Electives/Econometrics/07_Time_Series_Processes/#subordinated","title":"Subordinated","text":""},{"location":"Finance_Electives/Econometrics/07_Time_Series_Processes/#stationarity","title":"Stationarity","text":"Type Meaning Stationary Constant mean: \\(E(y_t) = \\mu\\)Constant variance: \\(\\text{Var}(y_t) = \\sigma^2\\) Covariance Stationary Constant mean: \\(E(y_t) = \\mu\\)Constant variance: \\(\\text{Var}(y_t) = \\sigma^2\\)Constant auto-covariance: \\(\\text{Cov}(y_{t+h}, y_t) = \\gamma(\\tau)\\) Non-Stationary Will have either one/both of the following- Mean at each time period is different across all time periods  - Mean of distribution of possible outcomes corresponding to each time period is different- Variance at each time period is different across all time periods  - Variance of distribution of possible outcomes corresponding to each time period is differentWe need to transform this somehow, as OLS and GMM cannot be used for non-stationary processes, because the properties of OLS are violated - heteroskedastic variance of error term"},{"location":"Finance_Electives/Econometrics/07_Time_Series_Processes/#types-of-stochastic-processes","title":"Types of Stochastic Processes","text":"<p>Consider \\(u_t = N(0, \\sigma^2)\\)</p> Process Characteristics \\(y_t\\) Comments Mean Variance Memory Example White Noise Stationary \\(u_t\\) PAC &amp; TAC for each lag = 0 0 \\(\\sigma^2\\) None If a financial series is a white noise series, then we say that the \u2018market is efficient\u2019 Ornstein Uhlenbeck Process/Vasicek Model StationaryMarkov chain \\(\\beta_1 y_{t-1} + u_t; \\ 0 &lt; \\vert \\beta_1 \\vert &lt; 1\\) Series has Mean-reverting Earlier past is less important compared to recent past.Less susceptible to permanent shockSeries oscilates 0/non-zero \\(\\sigma^2\\) Short GDP growthInterest rate spreadsReal exchange ratesValuation ratios (divides-price, earnings-price) Covariance Stationary \\(y_t = V_t + S_t\\) (Wold Representation Theorem)\\(V_t\\) is a linear combination of past values of \\(V_t\\) with constant coefficients\\(S_t = \\sum \\psi_i u_{t-i}\\) is an infinite moving-average process of error terms, where \\(\\psi_0=1, \\sum \\psi_i^2 &lt; \\infty\\); \\(\\eta_t\\) is linearly-unpredictable white noise and \\(u_t\\) is uncorrelated with \\(V_t\\) Simple Random Walk Non-StationaryMarkov chainMartingale \\(\\begin{aligned} &amp;= y_{t-1} + u_t \\\\ &amp;= y_0 + \\sum_{i=0}^t u_i \\end{aligned}\\) PAC &amp; TAC for each lag = 0\\(y_{t+h} - y_t\\) has the same dist as \\(y_h\\) \\(y_0\\) \\(t \\sigma^2\\) Long Explosive Process Non-Stationary \\(\\beta_1 y_{t-1} + u_t; \\  \\vert \\beta_1 \\vert &gt; 1\\) Random Walk w/ drift Non-Stationary \\(\\begin{aligned} &amp;= \\beta_0 + y_{t-1} + u_t \\\\ &amp;= t\\beta_0 + y_0 + \\sum_{i=0}^t u_i \\end{aligned}\\) \\(t \\beta_0 + y_0\\) \\(t \\sigma^2\\) Long Random Walk w/ drift and deterministic trend Non-Stationary \\(\\begin{aligned} &amp;= \\beta_0 + \\beta_1 t + y_{t-1} + u_t \\\\ &amp;= y_0 + t \\beta_0 + \\beta_1 \\sum_{i=1}^t i + \\sum_{i=1}^t u_t \\end{aligned}\\) \\(t \\beta_0 + \\beta_1 \\sum_{i=1}^t i + y_0\\) \\(t \\sigma^2\\) Long Random Walk w/ drift and non-deterministic trend Non-Stationary Same as above, but \\(\\beta_1\\) is non-deterministic <p>Impulse Response Function of covariance stationary process \\(y_t\\) is $$ \\begin{aligned} \\text{IR}(j) &amp;= \\dfrac{\\partial y_t}{\\partial \\eta_{t-j}} \\ &amp;= \\psi_j \\ \\implies \\sum \\text{IR}(j) &amp;= \\phi(L), \\text{with L=}1 \\ &amp;\\text{ (L is lag operator)} \\end{aligned} $$</p>"},{"location":"Finance_Electives/Econometrics/07_Time_Series_Processes/#differentiation","title":"Differentiation","text":"<p>When converting a non-stationary series \\(y_t\\) into a stationary series \\(y'_t\\), we want</p> <ul> <li>Obtain stationarity: ADF Stat at 95% CL as \\(-2.8623\\)</li> <li>Retain memory: Similarity to original series; High correlation b/w original series and differentiated series</li> </ul> \\[ y'_t = y_t - d y_{t-1} \\\\ d \\in [0, 1] \\\\ d_\\text{usual} \\in [0.3, 0.5] \\] \\(d\\) Stationarity Memory 0 \u274c \u2705 \\((0, 1)\\)(Fractional differentiation) \u2705 \u2705 1 \u2705 \u274c <p></p>"},{"location":"Finance_Electives/Econometrics/07_Time_Series_Processes/#integratedds-process","title":"Integrated/DS Process","text":"<p>Difference Stationary Process</p> <p>A non-stationary series is said to be integrated of order \\(k\\), if mean and variance of \\(k^\\text{th}\\)-difference are time-invariant</p> <p>If the first-difference is non-stationary, we take second-difference, and so on</p>"},{"location":"Finance_Electives/Econometrics/07_Time_Series_Processes/#pure-random-walk-is-ds","title":"Pure random walk is DS","text":"\\[ \\begin{aligned} y_t &amp;= y_{t-1} + u_t \\\\ \\implies \\Delta y_t &amp;= \\Delta y_{t-1} + u_t \\quad \\text{(White Noise Process = Stationary)} \\end{aligned} \\]"},{"location":"Finance_Electives/Econometrics/07_Time_Series_Processes/#random-walk-w-drift-is-ds","title":"Random walk w/ drift is DS","text":"\\[ \\begin{aligned} y_t &amp;= \\beta_0 + y_{t-1} + u_t \\\\ \\implies \\Delta y_t &amp;= \\beta_0 + \\Delta y_{t-1} + u_t \\quad \\text{(Stationary)} \\end{aligned} \\]"},{"location":"Finance_Electives/Econometrics/07_Time_Series_Processes/#ts-process","title":"TS Process","text":"<p>Trend Stationary Process</p> <p>A non-stationary series is said to be \u2026, if mean and variance of de-trended series are time-invariant</p> <p>Assume a process is given by</p> \\[ y_t = \\beta_0 + \\beta_1 t + y_{t-1} + u_t \\] <p>where trend is deterministic/stochastic</p> <p>Then</p> <ul> <li>Time-varying mean</li> <li>Constant variance ???</li> </ul> <p>We perform de-trending \\(\\implies\\) subtract \\((\\beta_0 + \\beta_1 t)\\) from \\(y_t\\)</p> \\[ (y_t - \\beta_0 - \\beta_1 t) = y_{t-1} + u_t \\] <p>If</p> <ul> <li>\\(\\beta_2 = 0\\), the de-trended series is white noise process</li> <li>\\(\\beta_2 \\ne 0\\), the de-trended series is a stationary process</li> </ul> <p>Note Let\u2019s say \\(y_t = f(x_t)\\)</p> <p>If both \\(x_t\\) and \\(y_t\\) have equal trends, then no need to de-trend, as both the trends will cancel each other</p>"},{"location":"Finance_Electives/Econometrics/07_Time_Series_Processes/#unit-root-test-for-process-identification","title":"Unit Root Test for Process Identification","text":"\\[ y_t = \\textcolor{hotpink}{\\beta_1} y_{t-1} + u_t \\] \\(\\textcolor{hotpink}{\\beta_1}\\) \\(\\gamma\\) Process \\(0\\) White Noise \\((0, 1)\\) Stationary \\([1, \\infty)\\) Non-Stationary"},{"location":"Finance_Electives/Econometrics/07_Time_Series_Processes/#augmented-dicky-fuller-test","title":"Augmented Dicky-Fuller Test","text":"<ul> <li>\\(H_0: \\beta_1=1\\)</li> <li>\\(H_0: \\beta_1 \\ne 1\\)</li> </ul> <p>Alternatively, subtract \\(y_{t-1}\\) on both sides of main equation</p> \\[ \\begin{aligned} y_t - y_{t-1} &amp;= \\beta_1 y_{t-1} - y_{t-1} + u_t \\\\ y_t - y_{t-1} &amp;= (\\beta_1-1) y_{t-1} + u_t \\\\ \\Delta y_t &amp;= \\gamma y_{t-1} + u_t &amp; (\\gamma = \\beta_1 - 1) \\end{aligned} \\] <ul> <li>\\(H_0: \\gamma=1\\) (Non-Stationary)</li> <li>\\(H_1: \\gamma \\ne 1\\) (Stationary)</li> </ul> <p>If p value \\(\\le 0.05\\)</p> <ul> <li>we reject null hypothesis and accept alternate hypothesis</li> <li>Hence, process is stationary</li> </ul> <p>We test the hypothesis using Dicky-Fuller distribution, to generate the critical region</p> Model Hypotheses \\(H_0\\) Test Statistic \\(\\Delta y_t =\\)"},{"location":"Finance_Electives/Econometrics/07_Time_Series_Processes/#long-memory-series","title":"Long memory series","text":"<p>Earlier past is as important as recent past</p>"},{"location":"Finance_Electives/Econometrics/07_Time_Series_Processes/#q-statistic","title":"Q Statistic","text":"<p>Test statistic like \\(z\\) and \\(t\\) distribution, which is used to test \u2018joint hypothesis\u2019</p>"},{"location":"Finance_Electives/Econometrics/07_Time_Series_Processes/#inertia-of-time-series-variable","title":"Inertia of Time Series Variable","text":"<p>Persistance of value due to Autocorrelation</p> <p>Today\u2019s exchange rate is basically yesterday\u2019s exchange rate, plus-minus something</p>"},{"location":"Finance_Electives/Econometrics/08_Autocorrelation/","title":"Autocorrelation","text":""},{"location":"Finance_Electives/Econometrics/08_Autocorrelation/#durbin-watson-test","title":"Durbin-Watson Test","text":""},{"location":"Finance_Electives/Econometrics/08_Autocorrelation/#positive","title":"Positive","text":"<ul> <li>\\(H_0: \\rho = 0\\)</li> <li>\\(H_1: \\rho &gt; 0\\) (not negative)</li> </ul> \\[ \\begin{aligned} D &amp;= \\dfrac{ \\sum_{i=p}^n (u_i - u_{i-p})^2 }{ \\sum_{i=1}^n (u_i)^2 } \\\\ &amp; \\approx 2(1-\\rho) \\end{aligned} \\] <p>where \\(p=\\) lag being tested</p> <ul> <li>If \\(D&gt;D_h\\), cannot reject null hypothesis</li> <li>If \\(D&lt;D_l\\), reject null hypothesis</li> <li> <p>If \\(D_l &lt; D&lt;D_h\\), inconclusive</p> </li> <li> <p>\\(D \\ge 2 \\implies\\) no autocorrelation</p> </li> <li>\\(D \\to 0 \\implies\\) perfect autocorrelation</li> </ul>"},{"location":"Finance_Electives/Econometrics/08_Autocorrelation/#negative","title":"Negative","text":"<p>\\(D' = 4-D\\)</p>"},{"location":"Finance_Electives/Econometrics/08_Autocorrelation/#runs-test","title":"Runs Test","text":"<p>Run: any sequence on the same side of 0</p> <p>Usually one-tailed to test for +ve correlation</p> <ul> <li>+ve correlation: bounces less frequently</li> <li>-ve correlation: bounces very frequently; (not very common in data)</li> </ul> \\[ \\begin{aligned} \\bar R &amp;= \\dfrac{2 n_+ n_-}{n} + 1 \\\\ s^2_R &amp;= \\dfrac{2 n_+ n_- (2 n_+ n_- - n)}{n^2 (n-1)} \\\\ \\implies Z_R &amp;= \\dfrac{R-\\bar R}{s_R} \\sim N(0, 1) \\end{aligned} \\] <p>where</p> <ul> <li>\\(R=\\) number of runs in data</li> <li>\\(n_+=\\) number of +ve residuals</li> <li>\\(n_-=\\) number of -ve residuals</li> <li>\\(n=\\) total number of residuals</li> </ul>"},{"location":"Finance_Electives/Econometrics/08_Cointegrating_Process/","title":"Cointegrating Processes","text":"<p>Tendency of 2 variables (that are theoretically at equilibrium) to be related to each other</p> <p>2 processes that are integrated of order 1, but \\(\\exists\\) linear combination of the 2 variables that is stationary.</p> <p>If there is divergence, it will only be temporary, as there is bound to be error correction</p> <p>The coefficient associated with the 2 variables will be non-zero</p> <p>Usually happens with highly connected variables</p> <p>If there are \\(n\\) cointegrating variables, then there can be</p> <ul> <li>\\([1, n-1]\\) independent cointegrating relationships (not lesser or greater than this range)</li> <li>\\([1, n]\\) error correction relationships</li> </ul> <p>eg:</p> <ul> <li>Demand and Supply for a commodity</li> <li>US interest rate and UAE interest rate<ul> <li>US is leading market</li> <li>UAE is following market</li> </ul> </li> <li>Dubai and Sharjah rent</li> <li>GCC stock markets</li> </ul> <p>Consider \\(x, z\\) which are both \\(I(1)\\) processes; \\(x_t\\) and \\(z_t\\) are cointegrated processes \\(\\iff u_t\\) is stationary process,</p> \\[ \\begin{aligned} z_t &amp;= \\alpha_1 x + u_t &amp; \\text{(Long-Term Specification)} \\\\ \\implies u_t &amp;= z_t - \\alpha_1 x_t &amp; \\text{(Short-Term Specification)} \\\\ z_t - z_{t-1} &amp;= \\textcolor{hotpink}{-}\\alpha_D(z_{t-1} - \\alpha_1 x_{t-1}) + v_t \\\\ \\Delta z_t &amp;= \\textcolor{hotpink}{-}\\alpha_D(u_{t-1}) + v_t \\\\ &amp; \\text{if } x \\text{ also has correcting tendancy,} \\\\ \\implies \\Delta x_t &amp;= \\textcolor{orange}{+} \\alpha_G(u_{t-1}) + w_t \\end{aligned} \\] <ul> <li>\\(\\alpha_D\\)<ul> <li>Speed of adjustment parameter, or error correction coefficient</li> <li>\\(\\alpha_D \\in (0, 1)\\)</li> </ul> </li> <li>\\(u_t=\\)\u00a0Disequilibrium error/Cointegration residual</li> </ul>"},{"location":"Finance_Electives/Econometrics/08_Cointegrating_Process/#parts","title":"Parts","text":"<ul> <li>Attractor/Leader</li> <li>Attracted/Follower</li> </ul>"},{"location":"Finance_Electives/Econometrics/08_Cointegrating_Process/#correlation-vs-co-integration","title":"Correlation vs Co-integration","text":"<p>Co-integration  \\(\\  \\not \\!\\!\\!\\!\\! \\iff\\) Correlation</p> Correlation Co-Integration Co-movementDuration short-term long-term"},{"location":"Finance_Electives/Econometrics/08_Cointegrating_Process/#drunk-couple-and-dog","title":"Drunk Couple and Dog","text":""},{"location":"Finance_Electives/Econometrics/09_Time_Series_Decomposition/","title":"Time Series Decomposition","text":"Advantages Disadvantages Classical Easy to understand &amp; interpret 1. Estimate of trend is unavailable in the first few and last few observations2. Assumes that seasonal component repeats3. Not robust to outliers due to usage of means X-11 1. Relatively robust to outliers2. Completely automated choices for trend and seasonal changes3. Tried &amp; tested method 1. No prediction/confidence intervals2. Ad hoc method with no underlying model3. Only for quarterly &amp; monthly data X-12-ARIMA/X-13-ARIMA 1. Allow adjustments for trading days and explanatory variables2. Known outliers can be omitted3. Level shifts &amp; ramp effects can be modelled4. Missing values estimated and replaced5. Holiday factors can be estimated X-13-ARIMA-SEATS 1. Model-based2. Smooth trend estimate3. Allows estimates at end-points4. Incorporates changing seasonality STLSeasonal &amp; Trend Decomposition using Loess - Iterative alogirthm- Starts with \\(\\hat T = 0\\)- Uses mixture of loess and moving averages to successively refine trend &amp; seasonal estimates- Trend window controls loess bandwidth applied to de-seasonalized values- Season window controls loess bandwidth applied to detrended subseries- Seasonal component allowed to change over time; Rate of change controlled by analyst- Smoothness of trend controlled by analyst - Versatile- Robust- Handle any type of seasonality - Only additive (Use log/Box-Cox transformations for other)- No training day/calendar adjustments"},{"location":"Finance_Electives/Econometrics/09_Time_Series_Decomposition/#classical-decomposition","title":"Classical Decomposition","text":"\\(y_t\\) Appropriate when Magnitude of seasonal fluctuations proportional to level of series Addititive \\(S_t + T_t + R_t\\) \u274c Multiplicative \\(S_t \\times T_t \\times R_t\\) \u2705 <p>Alternatively, use Box-Cox transformation, and then use additive decomposition. Logs turn multiplicative relationship into additive</p> \\[ \\begin{aligned} y_t &amp;= S_t \\times T_t \\times R_t \\\\  \\implies \\ln y_t &amp;= \\ln S_t + \\ln T_t + \\ln R_t \\end{aligned} \\]"},{"location":"Finance_Electives/Econometrics/09_Time_Series_Decomposition/#trend-estimation","title":"Trend Estimation","text":"<p>Centered moving averages, to combat odd order</p> \\[ \\begin{aligned} \\hat T_t &amp;= \\dfrac{1}{2m} \\left( \\sum_{i = -(k+1)}^k y_{t+i} + \\sum_{i = -k}^{k+1} y_{t+i} \\right) \\\\ \\text{where } k &amp;= \\dfrac{m-1}{2} \\end{aligned} \\] Order (\\(m\\)) Curve Data Retention Larger Smoother, flatter Less(end points are lost) Smaller Noisy More <p>Moving average of the same length of a season/cycle removes its pattern</p>"},{"location":"Finance_Electives/Econometrics/09_Time_Series_Decomposition/#seasonal-adjusted-data","title":"Seasonal Adjusted Data","text":"<p>Component excluding the seasonal component</p>"},{"location":"Finance_Electives/Econometrics/09_Time_Series_Decomposition/#detrended-series","title":"Detrended Series","text":"\\[ \\begin{aligned} y_t - \\hat T_t &amp;= \\hat S_t + \\hat R_t \\\\ \\frac{y_t}{\\hat T_t} &amp;= \\hat S_t \\times \\hat R_t \\end{aligned} \\]"},{"location":"Finance_Electives/Econometrics/09_Time_Series_Decomposition/#seasonal-component","title":"Seasonal component","text":"<p>Average of de-trended series for that season. For eg, average of all values in Januaries</p> <p>You can constraint the seasonal components such that</p> \\[ \\hat S_1 + \\hat S_2 + \\dots + \\hat S_{n} = 0 \\\\ \\hat S_1 \\times \\hat S_2 \\times \\dots \\times \\hat S_{n} = m \\]"},{"location":"Finance_Electives/Econometrics/09_Time_Series_Decomposition/#remainder-component","title":"Remainder Component","text":"\\[ \\begin{aligned} \\hat R_t &amp;= y_t - (\\hat T_t + \\hat S_t) \\\\ \\hat R_t &amp;= \\dfrac{y_t}{\\hat T_t \\hat S_t} \\end{aligned} \\]"},{"location":"Finance_Electives/Econometrics/09_Time_Series_Decomposition/#fourier-transforms","title":"Fourier Transforms","text":"<p>FT\u2019s limitation: FT is completely blind to time, in accordance with Heisenberg\u2019s Uncertainty principle. There\u2019s a tradeoff between correctly estimating the value of function in the frequency &amp; time domain.</p> <p>It is 1D representation</p>"},{"location":"Finance_Electives/Econometrics/09_Time_Series_Decomposition/#types-of-fourier-transforms","title":"Types of Fourier Transforms","text":"Type Continuous Time &amp; Frequency Functional form of time series is known analytically \\(\\hat x(f) = \\int \\limits_{-\\infty}^\\infty x(t) e^{-2\\pi i f t} dt\\) Continuous Time, Discrete Frequency(Fourier Series) \\(\\hat x(f_n) = \\dfrac{1}{T} \\int \\limits_{0}^T x(t) e^{-2\\pi i f_n t} dt; f_n = \\dfrac{n}{T}\\) Discrete Time &amp; Frequency(Fourier Frequencies) \\(\\hat x(f_n) = \\sum \\limits_{k=0}^{N-1} x_t e^{- 2 \\pi i f_n (k \\Delta t)} \\Delta t; f_n = \\dfrac{n}{N \\Delta t}; \\hat x_n = \\hat x^*_{-n}\\) FFT(Fast Fourier Transform)"},{"location":"Finance_Electives/Econometrics/09_Time_Series_Decomposition/#denoising-using-fft","title":"Denoising using FFT","text":"<ol> <li>Apply FFT</li> <li>Filter it to only the frequencies with the highest amplitude</li> <li>Take inverse FFT</li> </ol>"},{"location":"Finance_Electives/Econometrics/09_Time_Series_Decomposition/#wavelet-transform","title":"Wavelet Transform","text":"<p>Overcomes FT\u2019s limitation: FT is completely blind to time, by obtaining an optimal balance between accuracy in frequency &amp; time domain</p>"},{"location":"Finance_Electives/Econometrics/09_Time_Series_Decomposition/#wavelet","title":"Wavelet","text":"<p>Short-lived oscillation, localized in time</p> <ul> <li>Zero mean: \\(E[\\phi(t)]=0; \\int \\phi(t) \\cdot dt = 0\\) (Admissibility condition)</li> <li>Finite energy: \\(\\int [\\phi(t)]^2 \\cdot dt = k, k &lt; \\infty\\)</li> <li></li> </ul> Type \\(\\phi(t)\\) Daubechies Coiflet Symlet Haar Morlet \\(k_0 \\cdot e^{i w_0 t} \\cdot e^{-t^2/2}\\) Gaussian Shannon Meyer Mexican Hat"},{"location":"Finance_Electives/Econometrics/09_Time_Series_Decomposition/#idk","title":"IDK","text":"<p>2D representation: \\(y(t) \\to T(t, f)\\) represents the contribution of frequency \\(f\\) at time \\(t\\)</p> <p>Scaled Wavelet \\(\\phi(t, a, b) = \\phi \\left(\\dfrac{t-b}{a} \\right)\\)</p> <p>The value of \\(T(a, b) =\\) contribution of \\(\\phi(t, a, b)\\) to comprising the signal $$ T(a, b) = \\int y(t) \\cdot \\phi(t, a, b) \\cdot dt $$ Demonstrates the goodness of fit: local similarity</p>"},{"location":"Finance_Electives/Econometrics/09_Time_Series_Decomposition/#signals","title":"Signals","text":"Time Resolution Frequency Resolution Raw Time Series High \\(\\approx 0\\) Fourier Transform \\(\\approx 0\\) High Wavelet Transform Low for small frequenciesHigh for high frequenciesThis is intuitive, as high freq signals are usually short-lived, and small freq signals are usually long-lived"},{"location":"Finance_Electives/Econometrics/10_Time_Series_Modelling/","title":"Time Series Modelling","text":"<p>For all the following models</p> <ul> <li>The variable has to be stationary model<ul> <li>Else, use non-stationary \\(\\to\\) stationary transformation</li> </ul> </li> <li>We drop parameters if they are significantly equal to 0</li> </ul> <p>Difficulty</p> <ul> <li>The underlying data-generating process may change; give higher sample weight to recent past</li> </ul>"},{"location":"Finance_Electives/Econometrics/10_Time_Series_Modelling/#forecasting-types","title":"Forecasting Types","text":""},{"location":"Finance_Electives/Econometrics/10_Time_Series_Modelling/#single-step-forecasting","title":"Single-Step Forecasting","text":""},{"location":"Finance_Electives/Econometrics/10_Time_Series_Modelling/#multi-step-forecasting","title":"Multi-Step Forecasting","text":"<p>Rather than building a model for each step, you can define the model as</p> \\[ \\Delta^d y_{t+h} = f(h) + \\sum_{i=1}^p \\alpha_i \\Delta^d y_{t-1} + \\sum_{i=1}^q \\beta_i u_{t-1} + u_t \\] <p>where</p> <ul> <li>\\(h\\) is the horizon</li> <li>\\(f(h)\\) is the captured mapping for \\(h\\). You may have to perform binary encoding (such as one-hot, etc).</li> </ul>"},{"location":"Finance_Electives/Econometrics/10_Time_Series_Modelling/#forecast-confidence-interval","title":"Forecast Confidence Interval","text":"<p>It shows the range upto which the forecast is expected to deviate</p> \\[ \\text{CI }{y_{t+h}} = \\hat y_{t+h} \\pm h \\sigma_{y+h} \\] <p>If standard deviation remains constant across all time points, \\(\\sigma_{y+h} = \\sigma_y\\)</p>"},{"location":"Finance_Electives/Econometrics/10_Time_Series_Modelling/#correlogram","title":"Correlogram","text":"If the correlogram of error term wrt previous lags has Accepted? Reason all bars inside the marked lines \u2705 \\(u_t\\) has no auto-correlation one/more bars outside marked lines \u274c \\(u_t\\) has auto-correlation"},{"location":"Finance_Electives/Econometrics/10_Time_Series_Modelling/#simplebaseline-models","title":"Simple/Baseline Models","text":"Method \\(\\hat y_{t+h}, \\ h&gt;=0\\) Appropriate for Average Average of past values \\(\\overline{ \\{ y_{t-k} \\} }\\) Naive Last value \\(y_{t-1}\\) Random walk process(Consequence of efficient market hypothesis) Seasonal Naive Last seasonal value \\({\\large y}_{t+h-mk}\\)where \\(m=\\) seasonal period Drift Method Last value plus average changeEquivalent to extrapolating line between first and last point \\({\\large y}_{t-1} + \\overline{ \\{ y_t - y_{t-1} \\} }\\) <p>Where \\(k &gt; 0\\)</p>"},{"location":"Finance_Electives/Econometrics/10_Time_Series_Modelling/#simulation-models","title":"Simulation Models","text":"<p>We do not use the observed values of the process as inputs</p> <p>Preferred for long-term forecasts</p>"},{"location":"Finance_Electives/Econometrics/10_Time_Series_Modelling/#advantages","title":"Advantages","text":"<ol> <li>Simple &amp; Intuitive</li> <li>Non-parametric</li> <li>Easy to aggregate</li> </ol>"},{"location":"Finance_Electives/Econometrics/10_Time_Series_Modelling/#disadvantages","title":"Disadvantages","text":"<ol> <li>Needs lots of data for good sample</li> <li>Assumption required for new products</li> <li>Assumes stationarity</li> </ol>"},{"location":"Finance_Electives/Econometrics/10_Time_Series_Modelling/#synthetic-data-generation-using-gaussian-copula","title":"Synthetic Data Generation using Gaussian Copula","text":"<p>You can use the below property to generate data similar to your original data $$ R \\Alpha^{\u00bd} E \\sim N(0, \\Sigma) $$</p> <ul> <li>\\(R\\) is an \\(n \\times 1\\) random normal vector</li> <li>\\(\\Alpha^{1/2}\\) is an \\(n \\times n\\) diagonal matrix with square roots of eigen values</li> <li>\\(E\\) is matrix of Eigen vectors</li> <li>\\(\\Sigma\\) is covariance matrix of \\(X\\)</li> </ul>"},{"location":"Finance_Electives/Econometrics/10_Time_Series_Modelling/#ets-model","title":"ETS Model","text":"<p>Errors, Trend, Seasonality</p> \\[ \\hat y_t = f(t, S, u_t) \\]"},{"location":"Finance_Electives/Econometrics/10_Time_Series_Modelling/#monte-carlo-simulation","title":"Monte-Carlo Simulation","text":"<p>Allows us to model the random component of a process; can be used along with an existing model for systematic component</p> <p>System needs to describable in terms of pdf</p> \\[ \\hat y_t = f(\\hat y_{t-1}, u_t) \\]"},{"location":"Finance_Electives/Econometrics/10_Time_Series_Modelling/#fir-model","title":"FIR Model","text":"<p>Only using input features</p> \\[ \\hat y_t = f(X_{t-k}, u_t) \\] <p>\\(k\\) is the no of lagged input features</p>"},{"location":"Finance_Electives/Econometrics/10_Time_Series_Modelling/#output-error-modelrecursive-forecasting","title":"Output Error Model/Recursive Forecasting","text":"<p>FIR model using past estimations also. Ideally you should develop a model for this (infinite-step forecasting), and then work on using the same model for multi-step forecasting.</p> \\[ y_t = \\sum_{i=p} \\hat y_{t-i} +\\sum_{i=\\textcolor{hotpink}{0}} \\hat X_{t-k} + u_t \\]"},{"location":"Finance_Electives/Econometrics/10_Time_Series_Modelling/#state-space-models","title":"State Space Models","text":""},{"location":"Finance_Electives/Econometrics/10_Time_Series_Modelling/#kalman-filter","title":"Kalman Filter","text":""},{"location":"Finance_Electives/Econometrics/10_Time_Series_Modelling/#gmm","title":"GMM","text":"<p>Generalized method of moments</p> <p>Find relationship b/w moments of random variables</p> <p>Yule-Walker estimates</p>"},{"location":"Finance_Electives/Econometrics/10_Time_Series_Modelling/#types-of-errors","title":"Types of Errors","text":"Error Type Amplitude Phase"},{"location":"Finance_Electives/Econometrics/11_Autoregressive_Modelling/","title":"Auto-Regressive Models","text":""},{"location":"Finance_Electives/Econometrics/11_Autoregressive_Modelling/#limitations","title":"Limitations","text":"<ul> <li>Assumes that factors will affect in the same manner throughout</li> <li>Temporal confounding: Makes learning of exogenous effects harder</li> </ul> <pre><code>flowchart TB\nxt1[\"x_t-1\"] --&gt;\nyt1[\"y_t-1\"]\n\nyt1 -.-&gt; yt\n\nxt1[\"x_t-1\"] ----&gt;\nyt[\"y_t\"]</code></pre>"},{"location":"Finance_Electives/Econometrics/11_Autoregressive_Modelling/#ar-modelprocess","title":"AR Model/Process","text":"<pre><code>flowchart LR\nyt1[\"y&lt;sub&gt;t-1&lt;/sub&gt;\"] &amp; ut[\"u&lt;sub&gt;t&lt;/sub&gt;\"] --&gt; yt[\"y&lt;sub&gt;t&lt;/sub&gt;\"]</code></pre> <p>AutoRegressive Model</p> <p>Variable is regressed using its own lagged values; we assume \\(y_t\\) depends only on its own lagged values</p> <p>More lags \\(\\implies\\) we lose more date points \\(\\implies\\) low degree of freedom</p>"},{"location":"Finance_Electives/Econometrics/11_Autoregressive_Modelling/#types","title":"Types","text":"<p>AR\\((p)\\) model means that there are \\(p\\) lags involved in the AR model</p> \\[ \\text{AR}(p) = \\sum_{i=1}^{p} \\alpha_i y_{t-i} + u_t \\] Model Order(No of lags involved) Example AR\\((1)\\) 1There is only \\(1\\) particular lag (not necessarily \\(y_{t-1}\\)) \\(y_t = \\beta_1 y_{t-\\textcolor{hotpink}{1}} + u_t \\\\ \\text{or} \\\\ y_t = \\beta_1 y_{t-\\textcolor{hotpink}{2}} + u_t \\\\ \\text{or} \\\\ \\dots \\\\ y_t = \\beta_1 y_{t-\\textcolor{hotpink}{100}} + u_t\\) AR\\((2)\\) 2 \\(y_t = \\beta_1 y_{t-\\textcolor{hotpink}{1}} + u_t,  y_{t-\\textcolor{hotpink}{2}} + u_t \\\\ \\text{or} \\\\ y_t = \\beta_1 y_{t-\\textcolor{hotpink}{1}} + u_t,  y_{t-\\textcolor{hotpink}{100}} + u_t\\)"},{"location":"Finance_Electives/Econometrics/11_Autoregressive_Modelling/#ma-model","title":"MA Model","text":"<pre><code>flowchart LR\nyt1[\"u&lt;sub&gt;t-1&lt;/sub&gt;\"] &amp; ut[\"u&lt;sub&gt;t&lt;/sub&gt;\"] --&gt; yt[\"y&lt;sub&gt;t&lt;/sub&gt;\"]</code></pre> <p>Moving Averages Model</p> <p>MA\\((q)\\) model means that there are \\(q\\) lagged error differences involved in the MA model</p> \\[ \\text{MA}(q) = \\sum_{i=1}^{q} \\beta_i u_{t-i} + u_t \\] <p>\\(u_{t-i}\\) is a multiple regression with past errors as predictors. Don\u2019t confuse this with moving average smoothing!</p> Model Order(No of lags involved) Example MA\\((1)\\) 1There is only \\(1\\) particular lag (not necessarily \\(u_{t-1}\\)) \\(y_t = \\beta_1 u_{t-\\textcolor{hotpink}{1}} + u_t \\\\ \\Big(\\text{ie, } y_t = \\beta_1 (y_{t-\\textcolor{hotpink}{1}}-E[y_{t-\\textcolor{hotpink}{1}}]) + u_t \\Big)\\) MA\\((2)\\) 2 \\(y_t = \\beta_1 u_{t-\\textcolor{hotpink}{1}} + \\beta_2 u_{t-\\textcolor{hotpink}{2}} + u_t\\)"},{"location":"Finance_Electives/Econometrics/11_Autoregressive_Modelling/#arma","title":"ARMA","text":"<p>Autoregressive Moving Average Model</p> <pre><code>flowchart LR\nyt1[\"y&lt;sub&gt;t-1&lt;/sub&gt;\"] &amp; ut1[\"u&lt;sub&gt;t-1&lt;/sub&gt;\"] &amp; ut[\"u&lt;sub&gt;t&lt;/sub&gt;\"] --&gt; yt[\"y&lt;sub&gt;t&lt;/sub&gt;\"]</code></pre> <p>ARMA\\((p, q)\\) model means that there are __ involved in the ARMA model</p> <ul> <li>\\(p\\) autoregressive lags</li> <li>\\(q\\) moving averages lags</li> </ul> \\[ \\text{ARMA}(p, q) = \\sum_{i=1}^{p} \\alpha_i y_{t-i} + \\sum_{i=1}^{q} \\beta_i u_{t-i} + u_t \\]"},{"location":"Finance_Electives/Econometrics/11_Autoregressive_Modelling/#arima-process","title":"ARIMA Process","text":"<p>ARIMA\\((p, d, q)\\) model means</p> <ul> <li>\\(p\\)</li> <li>\\(d\\)</li> <li>\\(q\\)</li> </ul> \\[ \\Delta^d y_t = \\sum_{i=1}^p \\alpha_i \\Delta^d y_{t-1} + \\sum_{i=1}^q \\beta_i u_{t-1} + u_t \\] <p>If \\(y_t\\) is an integrated series of order(\\(\\textcolor{hotpink}{1}\\)), then we can use ARIMA\\((1, \\textcolor{hotpink}{1}, 1)\\)</p> \\[ \\Delta y_t = \\alpha_1 y_{t-1} + \\beta_1 u_{t-1} + u_t \\]"},{"location":"Finance_Electives/Econometrics/11_Autoregressive_Modelling/#box-jenkins-decision-tree","title":"Box-Jenkins Decision Tree","text":"<p>for ARIMA Model Building</p> <pre><code>flowchart LR\nim[Identify Model] --&gt;\nep[Estimate Paramaters] --&gt;\nd --&gt;\nForecast\n\nd --&gt; rm[Revise Model] --&gt; ep\n\nsubgraph d[Diagnostics]\n    r2[R_adj^2]\n    RMSE\nend</code></pre> ACF Correlogram PACF Correlogram -&gt; Conclusion Model No significant spikes No significant spikes White Noise Damps out Spikes cut off at lag \\(p\\) Stationary AR\\((p)\\) Spikes cut off at lag \\(q\\) Damps out Stationary MA\\((q)\\) Damps out Damps out Stationary ARMA\\((p, q)\\) Spikes damp out very slowly Spikes cut off at lag \\(p\\) Random WalkNon-Stationary Monte-Carlo SimulationTake difference"},{"location":"Finance_Electives/Econometrics/11_Autoregressive_Modelling/#var","title":"VAR","text":"<p>Vector AutoRegressive Model</p> <p>Each input variable time series should also be stationary</p> <p>\\(\\text{VAR}(p) \\equiv \\text{VAR}(1)\\) where $$ \\begin{aligned} z_t &amp;= { X_t, X_{t-1}, \\dots, X_{t-p+1} } \\ z_{t-1} &amp;= { X_{t-1}, X_{t-2}, \\dots, X_{t-p} } \\ D &amp;= \\begin{bmatrix} c \\ 0_m \\ \\vdots \\ 0_m \\end{bmatrix}, A = \\begin{bmatrix} \\phi_1  &amp; \\phi_2    &amp; \\cdots &amp; \\phi_p \\ I_m         &amp; 0             &amp; \\cdots &amp; 0 \\ \\vdots  &amp; \\ddots    &amp; \\ddots &amp; \\vdots \\ I_m         &amp; 0             &amp; I_m &amp; 0 \\end{bmatrix}, F = \\begin{bmatrix} u_t \\ 0_m \\ \\vdots \\ 0_m \\end{bmatrix} \\ \\implies z_t &amp;= D + A y_{t-1} + F \\end{aligned} $$</p>"},{"location":"Finance_Electives/Econometrics/11_Autoregressive_Modelling/#stationary-varp","title":"Stationary VAR(p)","text":"<p>A VAR(p) model is stationary if one/both of the following</p> <ul> <li>All eigen values of the companion matrix \\(A\\) have modulus less than 1</li> <li>All roots of \\(\\text{det} ( \\ I_m - \\sum_{i=1}^p \\phi_i z^p \\ ) = 0\\) as a function of the complex variable \\(z\\) are outside the complex unit circle \\(\\vert z \\vert \\le 1\\)</li> </ul> <p>Mean: $$ \\begin{aligned} C &amp;= (I - \\sum_i^p \\phi_i) \\mu \\ E[y_t] \\implies \\mu &amp;= (I - \\sum_i^p \\phi_i)^{-1} C \\ y_t - \\mu &amp;= \\sum_i^p \\phi_i[y_{t-i} - \\mu]  + u_t \\end{aligned} $$</p>"},{"location":"Finance_Electives/Econometrics/11_Autoregressive_Modelling/#optimality","title":"Optimality","text":"<p>Component-wise OLS estimates are equal to the GLS estimates accounting for the general case of innovation covariance matrix with possibly unequal comment variance and non-zero correlations </p>"},{"location":"Finance_Electives/Econometrics/11_Autoregressive_Modelling/#varma","title":"VARMA","text":"<p>Vector AutoRegressive Moving Averages</p> <p>Simultaneous equations</p> <p>Consider the following regression</p> \\[ y_t = \\alpha_1 {x_1}_t + \\alpha_2 {x_2}_t + u_t \\]"},{"location":"Finance_Electives/Econometrics/11_Autoregressive_Modelling/#vecm","title":"VECM","text":"<p>Vector Error-Correction Model</p> <p>Useful when you want to perform VARMA without losing the \u201cstructure\u201d associated with differencing to enforce stationarity</p>"},{"location":"Finance_Electives/Econometrics/12_Volatility_Modelling/","title":"Volatility Modelling","text":""},{"location":"Finance_Electives/Econometrics/12_Volatility_Modelling/#historicalsample-volatility-measures","title":"Historical/Sample Volatility Measures","text":"<p>Annualized Vol $$ \\text{Annualized Vol} = \\begin{cases} s \\sqrt{365.25} &amp; \\text{Daily data} \\ s \\sqrt{52} &amp; \\text{Weekly data} \\ s \\sqrt{12} &amp; \\text{Monthly data} \\end{cases} $$</p> <ul> <li>Simple Moving average of volatility</li> <li>Exponential Moving average of volatility</li> <li>Simple regression of volatility</li> </ul>"},{"location":"Finance_Electives/Econometrics/12_Volatility_Modelling/#geometric-brownian-motion-model","title":"Geometric Brownian Motion Model","text":"<p>Assumes that volatility over time is stationary $$ \\begin{aligned} dy_t &amp;= \\mu y_t  dt + \\sigma y_t   dW_t \\ \\implies \\dfrac{dy_t}{y_t} &amp;= \\mu  dt + \\sigma  dW_t \\end{aligned} $$</p> <ul> <li>\\(dy_t\\) is infinitesimal increment in series</li> <li>\\(\\mu\\) is mean difference (per unit team) [drift term]</li> <li>\\(\\sigma\\) is volatility of series</li> <li>\\(dW_t\\) is infinitesimal increment of standard Brownian motion/Wiener process</li> <li>\\(dW_t = N(0, t'-t)\\)</li> <li>\\(dW_i\\) and \\(dW_j\\) are independent</li> </ul> \\[ \\begin{aligned} \\text{Let } R_j &amp;= \\log(y_j/y_{j-1}) \\\\ R_j &amp;\\sim N(\\mu \\Delta_j, \\sigma^2 \\Delta_j), \\Delta_j = t_j - t_{j-1} \\end{aligned} \\] <p>This is general case, to incorporate the possibility that the observations are at equal or unequal time intervals</p> <ul> <li>If \\(\\Delta j = 1\\)</li> <li>\\(\\hat \\mu = \\bar R\\)</li> <li>\\(\\hat \\sigma^2 = E[(R_t - \\bar R)^2]\\)</li> <li>else</li> <li></li> </ul>"},{"location":"Finance_Electives/Econometrics/12_Volatility_Modelling/#garman-klass-estimator","title":"Garman-Klass Estimator","text":"<p>More information than just closing</p> <p>Sample information of</p> <ul> <li>Period-close (last)</li> <li>Period-high (max)</li> <li>Period-low (min)</li> <li>Period-open (first)</li> </ul> <p>Assume that</p> <ul> <li>\\(\\mu=0, \\Delta_j = 1\\)</li> <li>\\(f \\in (0, 1)\\) denote the fraction of the day prior to the market open</li> <li>\\(f \\times T\\) between the previous day\u2019s closing and the current day\u2019s opening, and an interval of length</li> <li>\\((1-f) \\times T\\) between the current opening and the current closing, during which the market is open for trading</li> <li>\\(T\\) is the time frame between two consecutive closing prices</li> </ul> \\[ \\begin{aligned} C_j &amp;= \\log \\vert y_{t_j} \\vert \\\\ O_j &amp;= \\log \\vert y_{t_{j-1}+f} \\vert \\\\ H_j &amp;= \\max_{t_{j-1} + f \\le t \\le t_j} \\log \\vert y_t \\vert \\\\ L_j &amp;= \\min_{t_{j-1} + f \\le t \\le t_j} \\log \\vert y_t \\vert \\end{aligned} \\] Return Series Denotation Estimate E[Estimate] Var(Estimate) Efficiency of estimate compared to close-close Close-Close \\(\\hat \\sigma_{cc}^2\\) \\((C_1 - C_0)^2\\) \\(\\sigma^2\\) \\(2 \\sigma^4\\) Close-Open \\(\\hat \\sigma_{co}^2\\) \\(\\dfrac{(O_1 - C_0)^2}{f}\\) \\(\\sigma^2\\) \\(2 \\sigma^4\\) 1 Open-Close \\(\\hat \\sigma_{oc}^2\\) \\(\\dfrac{(C_1 - O_1)^2}{1-f}\\) \\(\\sigma^2\\) \\(2 \\sigma^4\\) 1 Combining close-open &amp; open-close \\(\\hat \\sigma_*^2\\) \\(\\frac{1}{2}(\\hat \\sigma^2_{co} + \\hat \\sigma^2_{oc})\\) \\(\\sigma^2\\) \\(\\sigma^4\\) 2 Parkinson \\(\\hat \\sigma^2_{p}\\) \\(\\dfrac{(H_1-L_1)^2}{4 \\cdot \\log 2}\\)\\(f=0\\) 5.2 Garman &amp; Klass w/ \\(\\hat \\sigma^2_p\\) \\(\\hat \\sigma^2_{gkp}\\) \\(a \\hat \\sigma^2_{co} + (1-a) \\hat \\sigma^2_p\\)\\(a \\approx 0.17; 0 &lt; f &lt; 1\\) 6.2 \u201cBest Analytic Scale-Invariant Estimator\u201d of Volatility \\(\\hat \\sigma^2_{**}\\) \\(0.511(u_1-d_1)^2 - 0.019[ \\ c_1(u_1+d_1) - 2 u_1 d_1 \\ ] - 0.383 c_1^2\\)\\(u_j = H_j - O_j; d_j = L_j-O_j; c_j = C_j - O_j\\) are the normalized high/low/close values 7.4 Garman &amp; Klass w/ \\(\\hat \\sigma^2_{**}\\) \\(\\hat \\sigma^2_{gk**}\\) \\(a \\dfrac{(O_1 - C_0)^2}{f} + (1-a)\\dfrac{\\sigma^2_{**}}{1-f}\\)\\(a \\approx 0.12; 0&lt;f&lt;1\\) 8.4 \\[ \\hat \\sigma^2_{co} \\text{ and } \\hat \\sigma^2_{oc} \\text{ are independent} \\] <p>These expectations &amp; variances of volatility estimate are obtained as standard deviation follows \\(\\chi^2\\) distribution</p>"},{"location":"Finance_Electives/Econometrics/12_Volatility_Modelling/#poisson-jump-diffusion-model","title":"Poisson Jump Diffusion Model","text":"<p>Assumes that volatility over time is stationary</p> <p>Over time, a Brownian motion process is fully-continuous. It experiences shocks according to a Poisson process.</p> <ul> <li>Model is a Poisson mixture of Gaussian distributions</li> <li>Moment-Generating function derived as that of random sum of independent random values</li> <li>EM Algorithm expressible in closed form</li> <li>Shocks treated as latent variables which simplify computations</li> <li>Algo provides posterior estimates of number of shocks per time period</li> </ul> \\[ \\dfrac{dy_t}{t} = \\mu dt + \\sigma dW_t + \\gamma \\sigma Z_t d \\Pi_t \\] <ul> <li>\\(dy_t\\) = infinitesimal increment in series</li> <li>\\(\\mu =\\) mean return (per unit time)</li> <li>\\(\\sigma =\\) diffusion volatility of process</li> <li>\\(dW_t =\\) increment in standard Wiener process</li> <li>\\(d \\Pi_t =\\) increment of a Poisson process with rate \\(\\lambda\\) modelling the shock process</li> <li>\\((\\gamma \\sigma) \\times Z_t\\) is the magnitude of a return shock</li> <li>\\(Z_t = N(0, 1)\\)</li> <li>\\(\\gamma =\\) scale (\\(\\sigma\\) units) of shock magnitudes</li> </ul>"},{"location":"Finance_Electives/Econometrics/12_Volatility_Modelling/#arch","title":"ARCH","text":"<p>AutoRegressive Conditional Heteroskedacity models</p> <ul> <li>Handles time-dependent volatility</li> <li>Specifies relative to discrete-time process for time series</li> <li>Implies that \\((u_t)^2\\) is an AR process</li> </ul> <p>Basically AR model of variance</p> <p>Type of Volatility Clustering</p> <pre><code>flowchart LR\nyt1[\"y&lt;sub&gt;t-1&lt;/sub&gt;\"] --&gt; sigmat[\"&amp;sigma;&lt;sub&gt;t&lt;/sub&gt;\"]\n\nsigmat &amp; ut[\"u&lt;sub&gt;t&lt;/sub&gt;\"] --&gt; yt[\"y&lt;sub&gt;t&lt;/sub&gt;\"]</code></pre> \\[ \\begin{aligned} y_t &amp;= \\sigma_t u_t \\\\ \\implies R_t &amp;= \\log \\vert y_t/y_{t-1} \\vert \\\\ &amp;= \\mu_t + u_t \\\\ u_t &amp;= Z_t \\times \\sigma_t; Z_t = N(0, 1) \\\\ \\implies \\sigma^2_t &amp;= \\text{Var}(R_t | F_{t-1}) \\\\ &amp;= \\alpha_0 + \\sum_{i=1}^p \\alpha_i (u_{t-i})^2 &amp; (\\alpha_i \\ge 0; \\sum_{i=1}^p \\alpha_i &lt; 1) \\end{aligned} \\] <p>where</p> <ul> <li>\\(\\mu_t\\) is the mean return conditional on \\(F_{t-1}\\)</li> <li>\\(F_{t-1}\\) is the information available till time \\(t-1\\)</li> </ul> <p>Adding \\((u_t)^2 - \\sigma^2_t\\) to both sides of ARCH model $$ (u_t)^2 = \\alpha_0 + \\sum_{i=1}^p \\alpha_i (u_{t-i})^2 + (u_t)^2 - \\sigma^2_t $$ where</p> <ul> <li>\\(E[(u_t)^2 - \\sigma^2_t \\vert F_t] = 0\\)</li> <li>\\(\\text{Var}[(u_t)^2 - \\sigma^2_t \\vert F_t] = \\text{Var}[{(u_t)^2}] = 2 \\sigma^4_t\\)</li> </ul>"},{"location":"Finance_Electives/Econometrics/12_Volatility_Modelling/#lagrange-multiplier-test","title":"Lagrange Multiplier Test","text":"\\[ H_0: \\alpha_i = 0 , \\forall i \\in [1, p] \\] <ol> <li>Fit \\(AR(p)\\) on \\((u_t)^2\\)</li> </ol> <p>The \\(AR(p)\\) estimates of parameters are not MLEs under gaussian assumptions; they correspond to quasi-MLE</p> <ol> <li> <p>LM test statistic = \\(nR^2\\), where \\(R^2=\\) R-Squared of fitted \\(AR(p)\\)</p> </li> <li> <p>Under \\(H_0\\), \\(n R^2 \\approx {\\chi^2}_p\\)</p> </li> </ol>"},{"location":"Finance_Electives/Econometrics/12_Volatility_Modelling/#limitation","title":"Limitation","text":"<p>It can be \u201cbursty\u201d: Sudden jumps in volatility rather than smooth ones. Hence, it cannot effectively model persistent volatility</p>"},{"location":"Finance_Electives/Econometrics/12_Volatility_Modelling/#garch","title":"GARCH","text":"<p>Generalized AutoRegressive Conditional Heteroskedacity models</p> <p>Basically ARMA model of variance</p> <p>Type of Volatility Clustering</p> <pre><code>flowchart LR\nyt1[\"y&lt;sub&gt;t-1&lt;/sub&gt;\"] &amp; sigmat1[\"&amp;sigma;&lt;sub&gt;t-1&lt;/sub&gt;\"] --&gt; sigmat[\"&amp;sigma;&lt;sub&gt;t&lt;/sub&gt;\"]\nsigmat &amp; ut[\"u&lt;sub&gt;t&lt;/sub&gt;\"] --&gt; yt[\"y&lt;sub&gt;t&lt;/sub&gt;\"]</code></pre> \\[ \\begin{aligned} y_t &amp;= \\sigma_t u_t \\\\ \\sigma^2_t &amp;= \\alpha_0 + \\sum_{i=1}^p \\alpha_i (u_{t-i})^2 + \\sum_{j=1}^q \\beta_j \\sigma^2_{t-j} \\\\ &amp; (\\alpha_i \\ge 0; \\beta_j \\ge 0) \\end{aligned} \\] \\[ \\text{Garch}(p, q) = \\text{ARMA}( \\ \\max(p, q) \\ , q  \\ ) \\]"},{"location":"Finance_Electives/Econometrics/12_Volatility_Modelling/#garch1-1","title":"GARCH(1, 1)","text":"\\[ \\sigma^2_t = \\alpha_0 + \\alpha_1 (u_{t-1})^2 + \\beta_1 \\sigma^2_{t-1} \\] <ul> <li>Fits most financial time-series</li> <li>Implies a ARMA(1, 1) with \\((u_t)^2 - \\sigma^2_t \\sim WN(0, 2 \\sigma^4)\\)</li> </ul>"},{"location":"Finance_Electives/Econometrics/12_Volatility_Modelling/#diagonal-vectorization","title":"Diagonal Vectorization","text":"<p>\\(\\Sigma_t\\) is conditional covariance</p> <p></p> <p>Advantage</p> <ul> <li>Simple element-wise GARCH model</li> </ul> <p>Disadvantage</p> <ul> <li>\\(\\Sigma_t\\) not guaranteed to be positive-definite</li> <li>Prone to overfitting, so dimensionality reduction required</li> </ul>"},{"location":"Finance_Electives/Econometrics/12_Volatility_Modelling/#stochastic-volatility-models","title":"Stochastic Volatility Models","text":""},{"location":"Finance_Electives/Econometrics/12_Volatility_Modelling/#implied-volatility","title":"Implied Volatility","text":"<p>From options/derivatives</p>"},{"location":"Finance_Electives/Econometrics/12_Volatility_Modelling/#volatility-clustering","title":"Volatility Clustering","text":"<ul> <li>Large \\(u_t^2\\) follow large \\(u_{t-1}^2\\)</li> <li>Small \\(u_t^2\\) follow large \\(u_{t-1}^2\\)</li> </ul> <p>GARCH Models can prescribe</p> <ul> <li>Large \\(\\sigma_t^2\\) follow large \\(\\sigma_{t-1}^2\\)</li> <li>Small \\(\\sigma_t^2\\) follow small \\(\\sigma_{t-1}^2\\)</li> </ul>"},{"location":"Finance_Electives/Econometrics/12_Volatility_Modelling/#extended-garch","title":"Extended GARCH","text":"<ul> <li>EGARCH</li> <li>TGARCH</li> <li>PGARCH</li> <li>GARCH-In-Mean</li> </ul>"},{"location":"Finance_Electives/Econometrics/13_Cointegration_Modelling/","title":"Cointegration Modelling","text":""},{"location":"Finance_Electives/Econometrics/13_Cointegration_Modelling/#error-correction-models","title":"Error Correction Models","text":"\\[ \\begin{aligned} \\Delta m_t &amp;= \\lambda_m (u_{t-1}) + \\epsilon_{mt} \\\\ &amp;= \\lambda_m (blah blah) + \\epsilon_{mt} \\\\ \\Delta p_t &amp;= \\\\ \\Delta y_t &amp;=  \\end{aligned} \\] <ul> <li>\\(\\epsilon\\) is white noise error</li> <li>\\(\\lambda\\) are velocity of adjustment parameters</li> </ul> <p>Atleast one of the \\(\\lambda\\) should be significant, otherwise there is no error correction \\(\\implies\\) no cointegration</p> <p>Cointegration and error correction are equivalent representation</p> <p>~ Granger representation theorem</p>"},{"location":"Finance_Electives/Econometrics/13_Cointegration_Modelling/#vector-regressionstructural-estimation-model","title":"Vector regression/Structural estimation model","text":"<p>Used when there is no cointegration</p> <p>I missed this</p> <p>If the values of \\(\\lambda\\) are zero, then it is a simple VAR model and there is no cointegration. </p> <p></p>"},{"location":"Finance_Electives/Econometrics/13_Cointegration_Modelling/#example-quantity-theory-of-money","title":"Example: Quantity Theory of Money","text":"\\[ MV = \\underbrace{PY}_{\\text{GDP}} \\] Integrated of order \\(M\\) Total quantity of money \\(I(1)\\) \\(V\\) Velocity of money Number of times a unit of currency is transferred in a year N/A(Constant value) \\(P\\) Price \\(I(1)\\) \\(Y\\) Real quantity of Output \\(I(1)\\) <p>As they are \\(I(1)\\), they are not mean-reverting variables. Hence, taking log on both sides of equation, and then transposing</p> \\[ \\beta_0 + \\beta_1 m_t - \\beta_2 p_t - \\beta_3 y_t = u_t \\] <p>Velocity is a constant, which is an intercept. Here it is represented by \\(\\beta_0\\), but can also represented by \\(1\\cdot V\\)</p> <p>If \\(u_t\\) is \\(I(0) \\implies M, V, P, Y\\) are cointegrating</p>"},{"location":"Finance_Electives/Econometrics/13_Cointegration_Modelling/#notes","title":"Notes","text":"<ol> <li>There can be multiple cointegrating vectors \\(\\{\\beta_0, \\beta_1, \\beta_2, \\beta_3 \\} = \\{\\lambda \\beta_0, \\lambda \\beta_1, \\lambda \\beta_2, \\lambda \\beta_3 \\} \\iff \\lambda \\ne 0\\)</li> <li>If \\(m\\) and \\(p\\) are \\(I(2)\\) whereas \\(y\\) is \\(I(1)\\). The linear combination of these three variables will be \\(I(2)\\), hence the 3 are not cointegrated</li> <li>However, if a linear combination \\(\\beta_1 m + \\beta_2 p\\) is \\(I(1)\\), and this is cointegrated with y which is \\(I(1)\\), then we say there is multi-cointegration</li> <li>if monetary policy folows feedback rule that changes money supply based on inflation, then inflation will be another cointegrated variable</li> </ol>"},{"location":"Finance_Electives/Econometrics/13_Cointegration_Modelling/#granger-causality","title":"Granger Causality","text":"<p>Let\u2019s say we have 2 variables \\(x, y\\). We can check if \\(x\\) granger causes \\(y\\)</p> \\[ y_t = \\beta_1 y_{t-1} + \\beta_2 x_{t-1} + u_t \\]"},{"location":"Finance_Electives/Econometrics/13_Cointegration_Modelling/#hypotheses","title":"Hypotheses","text":"<ul> <li>\\(H_0: \\beta_2 = 0\\) </li> <li>\\(y\\) is independent of \\(x\\)</li> <li>\\(x\\) does not granger cause \\(y\\)</li> <li>\\(H_1: \\beta_2 \\ne 0\\)</li> <li>\\(x \\to y\\)</li> <li>\\(x\\) granger causes \\(y\\)</li> </ul>"},{"location":"Finance_Electives/Econometrics/13_Cointegration_Modelling/#procedure","title":"Procedure","text":"<ol> <li>We check if the \\(R_{adj}^2\\) has increased by incorporating \\(x_{t-1}\\), when compared to without it \\((y_t = \\beta_1 y_{t-1} + u_t)\\)</li> <li>Do a hypothesis test</li> <li>If \\(p \\le 0.05,\\) reject null hypothesis, and hence conclude that \\(x \\to y\\)</li> </ol>"},{"location":"Finance_Electives/Econometrics/13_Cointegration_Modelling/#spread","title":"Spread","text":"<p> $$ \\begin{aligned} y_{1t} &amp;= x_t + u_{1t} \\ y_{2t} &amp;= \\gamma x_t + u_{2t} \\ z_t &amp;= y_{1t} - y_{2t} \\quad \\text{(Spread)}\\ &amp;= y_{1t} - \\gamma y_{2t} \\ &amp;= u_{1t} - \\gamma u_{2t} \\ y_1, y_2 &amp;\\text{ are co-integrating} \\iff z_t \\to \\text{Stationary Process} \\end{aligned} $$</p> <p>This mean-reverting tendency of the spread can be used for \u201cpairs trading\u201d/\u201cstatistical arbitrage\u201d</p> <p></p>"},{"location":"Finance_Electives/Econometrics/13_Cointegration_Modelling/#estimating-gamma","title":"Estimating \\(\\gamma\\)","text":""},{"location":"Finance_Electives/Econometrics/13_Cointegration_Modelling/#simple","title":"Simple","text":"\\[ \\begin{aligned} z_t \\implies y_{1t} - \\gamma y_{2t} &amp;= \\mu + u_t \\\\ y_{1t} &amp;= \\mu + \\gamma y_{2t} + u_t \\end{aligned} \\] <p>Perform linear regression for \\(\\gamma\\)</p>"},{"location":"Finance_Electives/Econometrics/13_Cointegration_Modelling/#kalman","title":"Kalman","text":"\\[ \\begin{aligned} z_t \\implies y_{1t} - \\gamma_\\textcolor{hotpink}{t} y_{2t} &amp;= \\mu_\\textcolor{hotpink}{t} + u_t \\\\ y_{1t} &amp;= \\mu_\\textcolor{hotpink}{t} + \\gamma_\\textcolor{hotpink}{t} y_{2t} + u_t \\\\ \\mu_{t+1} &amp;= \\mu_t + \\eta_{1t} \\\\ \\gamma_{t+1} &amp;= \\gamma_t + \\eta_{2t} \\end{aligned} \\]"},{"location":"Finance_Electives/Econometrics/14_Value_Modelling/","title":"Value Modelling","text":"VAR VAG Meaning Value at Risk Value at Gain \\(p_x = x \\%\\) VAR/VAG is values for __ of distribution Bottom \\(x \\%\\) Top \\(x \\%\\)Bottom \\((1-x) \\%\\) Probability of __ given level Losses &lt; Gains &gt; Preferred for Lending (concerned about receiving repayment) Investing (interested in gain) Example <p>Note: Both are one-sided tails</p>"},{"location":"Finance_Electives/Econometrics/14_Value_Modelling/#target-curve","title":"Target Curve","text":"<p>Cumulative Distribution of outcomes (rarely frequency distribution)</p> <p>Goes from VAR % to VAG %</p> <p></p>"},{"location":"Finance_Electives/Econometrics/14_Value_Modelling/#dominance","title":"Dominance","text":"<p>If target curve 1 always to right of another, it dominates</p> <p>But it is not necessary that one alternative always performs better than other in all situations, as best case for one situation may be bad for another situation</p>"},{"location":"Finance_Electives/Econometrics/14_Value_Modelling/#evaluation-methods","title":"Evaluation Methods","text":"Method Historical Percentile of historical values Parametric/Variance-Covariance 1. Calculate covariance matrix of all securities2. Annualize them3. Calculate portfolio standard deviation: \\(\\sigma_p = \\sqrt{w' \\Sigma w}\\) Monte Carlo Simulation 1. Obtain dist statistics: Mean, Variance, \u20262. Run simulation3. Get the required percentiles"},{"location":"Finance_Electives/Econometrics/15_Time_Series_Filters/","title":"Filters","text":"<p>Algorithms that use uncertain measurements from sensors to predict unknown variable with acceptable accuracy, to estimate the current state.</p> <p>Help identify underlying trends, by smoothing time series and hence filtering out noise</p> <p>They are not for prediction</p>"},{"location":"Finance_Electives/Econometrics/15_Time_Series_Filters/#working","title":"Working","text":"\\[ \\begin{aligned} y_t &amp;= y^*_t + u_{t, \\text{PN}} + u_{t, \\text{MN}} \\\\ \\implies {\\tilde y}_t &amp;= E[y_t] \\\\ &amp;= E[y^*_t] + E[u_{t, \\text{PN}}] + E[u_{t, \\text{MN}}] \\\\ &amp;= y^*_t + \\mu_\\text{PN} + \\mu_\\text{MN} \\\\ &amp;= y^*_t + 0 + 0 &amp; (\\mu_\\text{MN} = \\mu_\\text{PN} = 0) \\\\ &amp; \\approx y^*_t \\end{aligned} \\] <p>where</p> <ul> <li>\\(\\tilde y_t=\\) smoothed/filtered value</li> <li>\\(y_t=\\) observed value</li> <li>\\(y^*_t=\\) true value</li> <li>\\(u_{t, \\text{PN}} =\\) Process noise</li> <li>\\(u_{t, \\text{MN}} =\\) Measurement noise</li> </ul>"},{"location":"Finance_Electives/Econometrics/15_Time_Series_Filters/#filter-design","title":"Filter Design","text":"<ol> <li>Define problem which consists of the state</li> <li>Define the motion model</li> <li>Define how the state will be measured</li> <li>Define uncertainty in system\u2019s dynamic model</li> <li>Implement &amp; test the filter in controlled environment with known inputs &amp; outputs</li> <li>Tune the filter</li> </ol>"},{"location":"Finance_Electives/Econometrics/15_Time_Series_Filters/#notes","title":"Notes","text":"<ul> <li>Window size = Duration of Seasonality will remove it</li> <li>Be wary of over-smoothing</li> <li>Never go live without tuning filter</li> </ul>"},{"location":"Finance_Electives/Econometrics/15_Time_Series_Filters/#types-wrt-time-dependence","title":"Types wrt Time Dependence","text":"<ul> <li>Centered</li> <li>Lagging</li> </ul>"},{"location":"Finance_Electives/Econometrics/15_Time_Series_Filters/#filters-vs-rolling-statistics","title":"Filters vs Rolling Statistics","text":"<p>The Kalman filter is better suited for estimating things that change over time. The Kalman Filter lets you add more information about how the system you're filtering works. In other words, you can use a signal model to improve the output of the filter.</p> <p>Sure, a moving average filter can give very good results when you're expecting a close-to-constant output. But as soon as the signal you're modelling is dynamic (think speech or position measurements), then the simple moving average filter will not change quickly enough (or at all) compared with what the Kalman Filter will do.</p> <p>The Kalman filter uses the signal model, which captures your knowledge of how the signal changes, to improve its output in terms of the variance from \"truth\".</p>"},{"location":"Finance_Electives/Econometrics/15_Time_Series_Filters/#concepts","title":"Concepts","text":"Denotation Measurement \\(y_{t, t}\\) Estimation Current state \\(\\hat y_{t, t}\\) Prediction Future state \\(\\hat y_{t, t-1}\\)"},{"location":"Finance_Electives/Econometrics/15_Time_Series_Filters/#applications","title":"Applications","text":"<p>These can be used in any field, but a few examples</p> <ul> <li>Guidance</li> <li>Navigation</li> <li>Control of vehicles</li> </ul>"},{"location":"Finance_Electives/Econometrics/15_Time_Series_Filters/#batch-vs-recursive","title":"Batch vs Recursive","text":"Batch Recursive Complexity \\(O(n)\\) \\(O(1)\\) Preferred"},{"location":"Finance_Electives/Econometrics/15_Time_Series_Filters/#uncertainty","title":"Uncertainty","text":"<p>Variance of measurement errors provided by scale vendor/derived through calibration; Even though we can\u2019t accurately know the estimate error, we can estimate the uncertainty in the estimates</p> <p>Most modern systems are equipped with multiple sensors that provide estimation of hidden/unknown variables based on series of measurements</p> <p>One of the biggest challenges of tracking and control systems is to provide accurate and precise estimation of the hidden variables in the presence of uncertainty.</p>"},{"location":"Finance_Electives/Econometrics/15_Time_Series_Filters/#bias-variance","title":"Bias &amp; Variance","text":"<p>Very similar to Machine Learning Prediction Bias &amp; Variance </p>"},{"location":"Finance_Electives/Econometrics/15_Time_Series_Filters/#measurement-filters","title":"Measurement Filters","text":"System Alpha Static Beta Dynamic Adaptive Dynamic"},{"location":"Finance_Electives/Econometrics/15_Time_Series_Filters/#alpha-filter","title":"Alpha Filter","text":""},{"location":"Finance_Electives/Econometrics/15_Time_Series_Filters/#average-filter","title":"Average Filter","text":"\\[ \\begin{aligned} &amp;\\text{Estimated current state} \\\\ &amp;= \\text{Mean of all measurements} \\\\ &amp;=\\text{Predicted current state} \\\\ &amp;\\quad + \\text{Factor} \\times (\\text{Measurement - Predicted current state}) \\end{aligned} \\] \\[ \\begin{aligned} \\hat y_{t, t} &amp;= \\hat y_{t, t-1} + \\alpha_t (y_t - \\hat y_{t, t-1}) \\\\ &amp;= \\alpha_t y_t + (1-\\alpha_t) \\hat y_{t, t-1}\\\\ \\hat y_{1, 0} &amp;= 0 \\\\ \\alpha_t &amp;= \\dfrac{1}{t} \\end{aligned} \\] <p>The \\(\\alpha\\) factor is called as Gain, and is taken as \\(\\alpha_t = \\dfrac{1}{t}\\). As number of measurements increase, each successive measurement has less weight in estimation, as \\(t \\uparrow \\implies \\alpha \\downarrow\\)</p> <p>Gives the less weight to recent data compared to past data</p> <p>Kalman filter requires an initial guess as a preset; it may be approximate. </p> <p>\\((y_{t, t} - \\hat y_{t, t-1})\\) is called the measurement residual</p>"},{"location":"Finance_Electives/Econometrics/15_Time_Series_Filters/#beta-filter","title":"Beta Filter","text":"\\(\\tilde y_{t+h}\\)(Recursive Additive) \\(\\tilde y_{t+h}\\)(Batch Additive) Weightage to history Parameter SMASimple Moving Average \\(\\tilde L_t\\)\\(\\tilde L_t = \\dfrac{1}{w} (y_t - y_{t-w}) + \\hat y_{t, t-1}\\) \\(\\dfrac{1}{w} \\sum_{i=0}^{w-1} y_{t-i}\\) Equally for recent and past history \\(w\\): Rolling Window Size EMAExponential Moving Average \\(\\tilde L_t\\)\\(\\tilde L_t = \\alpha_t y_t + (1-\\alpha_t) \\hat y_{t, t-1}\\) \\(\\sum_{i=0}^{w-1} \\alpha^{i+1} y_{t-i}\\) More weight to recent history Assumes that whole history is encapsulated in \\({\\tilde y}_{t-1}\\) Recommended \\(\\alpha=\\dfrac{2}{\\text{Window Size}}\\) Double EMA(Holt) \\(\\tilde L_t + h \\tilde T_t\\)\\(\\tilde T_t = \\beta(\\tilde L_t - \\tilde L_{t-1}) + (1-\\beta) \\tilde T_{t-1}\\)\\(\\tilde L_t = \\alpha L_t + (1-\\alpha) (\\tilde L_{t-1} + \\tilde T_{t-1})\\) Triple EMA(Holt-Winters) \\((\\tilde L_t + h \\tilde T_t) + \\tilde s_{t+h-s}\\)\\(\\tilde L_t = \\alpha (y_t-\\tilde S_{t-s}) + (1-\\alpha)({\\tilde L}_{t-1} + \\tilde T_{t-1})\\)\\(\\tilde T_t = \\beta(\\tilde L_t-\\tilde L_{t-1}) + (1-\\beta) {\\tilde T}_{t-1}\\)\\(\\tilde S_t = \\gamma (y_t - \\tilde L_{t-1} - \\tilde T_{t-1}) + (1-\\gamma) \\tilde S_{t-s}\\) <p>where</p> <ul> <li>\\(s=\\) seasonality duration</li> </ul>"},{"location":"Finance_Electives/Econometrics/15_Time_Series_Filters/#complex","title":"Complex","text":""},{"location":"Finance_Electives/Econometrics/15_Time_Series_Filters/#parameters","title":"Parameters","text":"\\(w, \\alpha\\) Curve Delay High Noisy Low Low Smooth High"},{"location":"Finance_Electives/Econometrics/15_Time_Series_Filters/#dynamic-system","title":"Dynamic System","text":"<p>State(s) change over time</p> <p>Let\u2019s take the case of 2 states: position &amp; velocity $$ \\begin{align} \\hat y_{t, t} &amp;= \\hat y_{t, t-1} + \\Delta t  \\hat {\\dot x}{t, t-1} \\tag{1} \\ \\implies \\hat {\\dot x}} &amp;= \\hat {\\dot x{t, t-1} + \\beta \\left( \\dfrac{y_t - \\hat y \\ \\hat y_{1, 0} &amp;= 0; \\hat {\\dot x}_{1, 0} = 0 \\end{align} $$ However, if we assume constant velocity, but measurement residual in }}{\\Delta t} \\right) \\tag{2\\(x \\ne 0,\\) then it could be due to 2 reasons</p> More likely when Measurement error Sensor has low precision Velocity is not constant Sensor has high precision <ul> <li>Value of \\(\\beta = \\text{const}\\), unlike \\(\\alpha_t\\)</li> <li>\\(\\beta \\propto \\text{Precision} \\propto \\dfrac{1}{\\sigma_\\text{measurement}}\\)</li> </ul>"},{"location":"Finance_Electives/Econometrics/15_Time_Series_Filters/#adaptive-filter","title":"Adaptive Filter","text":""},{"location":"Finance_Electives/Econometrics/15_Time_Series_Filters/#kalman-filter","title":"Kalman Filter","text":"<p>A low-pass filter with dynamically-changing \\(\\alpha\\)</p> <p>Assumes that the following are normally-distributed</p> <ul> <li>measurements</li> <li>current state estimates</li> <li>next state estimates</li> </ul> <p>Also quantifies the uncertainties associated with the estimates</p> <p>Optimal filter that combines the prior state estimate with the measurement, such that uncertainty of current state estimate is minimized</p> <pre><code>flowchart LR\n\nsubgraph i[Inputs]\n    direction LR\n    mp[Measured&lt;br/&gt;Parameter]\n    mu[Measurement&lt;br/&gt;Uncertainty]\nend\n\nsubgraph kf[Kalman Filter]\n    direction LR\n    u[Update] --&gt; p[Predict] --&gt; ud[Unit&lt;br/&gt;Delay] --&gt; u\nend\n\nsubgraph o[Outputs]\n    direction LR\n    sse[System State&lt;br /&gt;Estimate]\n    eu[Estimate&lt;br /&gt;Uncertainty]\nend\n\nsubgraph ii[Initial Inputs]\n    direction LR\n    is[Initial State]\n    isu[Initial State&lt;br /&gt;Uncertainty]\nend\n\nii --&gt; p\ni ---&gt; sf[Sensor&lt;br /&gt;Fusion] --&gt; u --&gt; o</code></pre> <p>where</p> <ul> <li>\\(r\\) is the measurement uncertainty in variance</li> <li>\\(p\\) is the estimate uncertainty in variance</li> </ul> <p></p>"},{"location":"Finance_Electives/Econometrics/15_Time_Series_Filters/#equations","title":"Equations","text":"Purpose Equation name Static System Dynamic System Comments State Update State UpdateFiltering Equation $\\hat y_{t, t} = \\hat y_{t, t-1} + K_t (y_t - \\hat y_{t, t-1}) $ \ud83d\udc48 Covariance gainCorrector Equation \\(p_{t, t} = (1-K_t) p_{t, t-1}\\) \ud83d\udc48 Kalman GainWeight Equation \\(K_t = \\dfrac{p_{t, t-1}}{p_{t, t-1} + r_t}; \\in [0, 1]\\) \ud83d\udc48 When measurement uncertainty is large and estimate uncertainty estimate is small, \\(K_n \\approx 0\\), the new measurement is given low weightage State Prediction State ExtrapolationPrediction EquationTransition EquationDynamic ModelState Space Model \\(\\hat y_{t+1, t} = \\hat y_{t, t}\\) \\(\\hat y_{t+1, t} = \\hat y_{t, t} + \\Delta t \\ \\hat {\\dot x}_{t, t}\\)\\(\\hat {\\dot x}_{t+1, t} = \\hat {\\dot x}_{t, t}\\) Covariance ExtrapolationPredictor Covariance Equation \\(p_{t+1, t} = p_{t, t}\\) \\(p^y_{t+1, t} = p^y_{t, t} + \\Delta t^2 p^v_{t, t}\\)\\(p^v_{t+1, t} = p^v_{t, t}\\) Kalman Gain \\(\\approx 0\\) \\(\\approx 1\\)"},{"location":"Finance_Electives/Econometrics/15_Time_Series_Filters/#advantages","title":"Advantages","text":"<ul> <li>Handles noise in initial estimate, state transitions, and observations </li> <li>Removes need to store historical data</li> <li>Can handle asynchronous measurements, ie measurements recorded by multiple sensors at different time points</li> <li>Computationally-efficient</li> <li>Suitable for real-time applications</li> </ul>"},{"location":"Finance_Electives/Econometrics/15_Time_Series_Filters/#disadvantages","title":"Disadvantages","text":"<ul> <li>Only for linear gaussian state space models.</li> <li>In practice, state transitions may be non-linear/noise and may be non-gaussian.</li> <li>Use other types of Kalman filters for this</li> </ul>"},{"location":"Finance_Electives/Econometrics/15_Time_Series_Filters/#kalman-smoother","title":"Kalman Smoother","text":""},{"location":"Finance_Electives/Econometrics/15_Time_Series_Filters/#extended-kalman-filter","title":"Extended Kalman Filter","text":"<p>Developed for non-linear dynamics</p> <p>Performs analytic linearization of the model at each point</p>"},{"location":"Finance_Electives/Econometrics/15_Time_Series_Filters/#unscented-kalman-filter","title":"Unscented Kalman Filter","text":""},{"location":"Finance_Electives/Econometrics/15_Time_Series_Filters/#without-model-based-approach","title":"Without Model-Based Approach","text":"\\(\\mu'\\) \\({\\sigma^2}'\\) Estimation uncertainty Update Parameter/MeasurementUses Bayes\u2019 rule \\(\\left( \\dfrac{\\mu}{\\sigma^2} + \\dfrac{\\nu}{r^2} \\right) {\\sigma^2}'\\) \\(\\dfrac{1}{\\dfrac{1}{r^2} + \\dfrac{1}{\\sigma^2}}\\) decreases Predict Motion \\(\\mu + u\\) \\({\\sigma^2} + r^2\\) increases"},{"location":"Finance_Electives/Economics/","title":"Economics","text":"Class Instructor Lecture Dr. Sartaj Rasool Rather <p>objective is to maximize profit with given investment every rational person does everything to maximize self-benefit</p> <p>deals with</p> <ol> <li>what to produce</li> <li>how to produce</li> <li>for whom to produce</li> </ol>"},{"location":"Finance_Electives/Economics/#aspects","title":"Aspects","text":"<ul> <li>scarcity of resources</li> <li>forecasting</li> <li>factors of production</li> <li>proportion of input and output</li> <li>allocation of resources</li> </ul>"},{"location":"Finance_Electives/Economics/#micro","title":"Micro","text":"<p>study individual units (producers, consumers, etc) and how they interact with each other</p>"},{"location":"Finance_Electives/Economics/#macro","title":"Macro","text":"<p>studies effects of large scale economic decisions on entire society</p> <p>eg: output level of economy, inflation, unemployment</p>"},{"location":"Finance_Electives/Economics/#movies","title":"Movies","text":"<ul> <li> The Big Short</li> <li> the good the bad the ugly</li> <li> coach carter movie</li> <li> the beautiful mind</li> </ul>"},{"location":"Finance_Electives/Economics/#references","title":"References","text":"<ul> <li> Dr. Sartaj Principles of Economics</li> <li> Managerial Economics | IIT Madras</li> <li> Ashley Hodgson</li> <li> Intermediate Microeconomics | Ben Zamzow</li> <li> Game Theory 101 Full Course | William Spaniel</li> <li> 60 Second Adventures in Economics | The Open University</li> <li> Game Theory with Ben Polak | YaleCourses</li> <li> Game Theory and Economics | IIT Guwahati</li> </ul>"},{"location":"Finance_Electives/Economics/01_Intro/","title":"01 Intro","text":""},{"location":"Finance_Electives/Economics/01_Intro/#assumptions-of-classical-economics","title":"Assumptions of Classical Economics","text":"<ol> <li>stable &amp; well-defined preferences</li> <li>rational behavior</li> <li>maximize utility function (happiness, etc)</li> <li>only have self-interest</li> <li>People don\u2019t care about others\u2019 interests</li> <li> <p>No peer-pressure</p> </li> <li> <p>risk-averse</p> </li> <li>Perfect Bayesian information processors</li> <li>process information optimally</li> <li>pay perfect attention to all details</li> <li> <p>don\u2019t forget information</p> </li> <li> <p>No biases</p> </li> <li>preferences over final outcomes, not changes</li> <li>no \u201ctaste\u201d for beliefs/information; purely objective</li> </ol> <p>These may seem like we are making very incorrect assumptions. But these assumptions are required to understand the main concepts. For more accurate modelling, we use Behavioral Economics</p>"},{"location":"Finance_Electives/Economics/01_Intro/#trade","title":"Trade","text":"<p>Voluntary transaction between parties, resulting in their betterment.</p> <p>Trade offs are there as everything has pros/cons, gains/costs; other options could have been chosen, but did not as the customer takes the best option</p>"},{"location":"Finance_Electives/Economics/01_Intro/#factors-of-trade","title":"Factors of Trade","text":""},{"location":"Finance_Electives/Economics/01_Intro/#consumer","title":"Consumer","text":"<ol> <li>Preferences/Tastes</li> <li>Cost (what is the amount expected of the customer)</li> <li>Budget/Income (what is available to the customer)</li> <li>Threshold/Willingness to make the trade</li> </ol>"},{"location":"Finance_Electives/Economics/01_Intro/#manager","title":"Manager","text":"<ol> <li>Price of factors</li> <li>Cost of production</li> <li>Revenue/profits</li> </ol>"},{"location":"Finance_Electives/Economics/01_Intro/#handling-recession","title":"Handling Recession","text":"Incentivize Through Investments Lowering interest rateProviding subsidies Consumer spending Universal Basic Income/Direct income transfers to people (only effective till a limit - excessive causes inflation)Reducing taxes"},{"location":"Finance_Electives/Economics/01_Intro/#budget-deficit","title":"Budget Deficit","text":"<p>Scenario where Govt Expenditure &gt; Revenue. This happens a lot in developing countries, as they are trying to develop as rapidly as possible.</p>"},{"location":"Finance_Electives/Economics/01_Intro/#opportunity-cost","title":"Opportunity Cost","text":"\\[ \\text{OC} = \\frac{\\text{What you sacrifice}}{\\text{What you receive}} \\] <p>Fraction that shows what is to be sacrificed to pick an option A instead of B.</p> <p>This can be incorporated into daily life instead of regular pros/cons list, by adding a cost/benefit associated with every factor for a decision.</p>"},{"location":"Finance_Electives/Economics/02_10_Principles/","title":"10 Principles of Economics","text":""},{"location":"Finance_Electives/Economics/02_10_Principles/#theres-always-trade-offs","title":"There's always Trade-offs","text":"<p>Refers to the fact that when picking an option, you're always giving up on something else; ie there is always an opportunity cost</p> <p>It is due to scarcity of resources; every resource is finite; nothing comes free</p> <p>Eg: when i want to study, i'm giving up sleep</p>"},{"location":"Finance_Electives/Economics/02_10_Principles/#real-cost","title":"Real Cost","text":"<p>Cost of something is what you sacrifice to get it Real Cost = relative price = opportunity cost</p> <p>It's not just monetary cost</p>"},{"location":"Finance_Electives/Economics/02_10_Principles/#rational-people-think-at-margin","title":"Rational people think at Margin","text":"<p>Rationality: making a decision considering all possibilities to obtain best outcome with minimal input and losses.</p> <p>Marginality: making decisions considering incremental benefits/costs associated with an action; not at total benefit/cost</p> <p>marginal profit (net benefit) = marginal benefit - marginal cost Proceed incrementing production as long as marginal net benefit \\(\\ge\\) 0</p>"},{"location":"Finance_Electives/Economics/02_10_Principles/#people-respond-to-incentives","title":"People respond to incentives","text":"<p>Incentive: instrument to induce an individual to act/react tool to change behavior</p> <p>Subsidies = +ve incentive Taxes = -ve incentive</p>"},{"location":"Finance_Electives/Economics/02_10_Principles/#trade-benefits-everyone","title":"Trade benefits everyone","text":"<ol> <li>Reduces cost of production through specialization    Specialists require lower resources for production, because they are really good at it; hence they can increase output</li> <li>Both parties gain a greater variety of goods and services</li> </ol> <p>Overall trade gains depends on</p> <ol> <li>choice set</li> <li>cost minimization</li> </ol> <p>Individual gains depends on</p> <ol> <li>Terms of trade    basically means the exchange rate</li> <li>Bargaining power</li> <li>nature of the product (Shelf life / perishability)       The one with the greater shelf life product will have greater bargaining power because they don't have to sell in a hurry</li> <li>elasticity of demand</li> </ol>"},{"location":"Finance_Electives/Economics/02_10_Principles/#benefits-of-scale","title":"Benefits of scale","text":"<p>You get larger benefits with lower costs if you focus on specialization at large scale</p> <p>Larger systems are more productive per unit input</p> <p>The average cost reduces when the volume of production increases</p> <p>eg: college mess cooking $$ \\text{Cost}(y) = y^\\alpha $$ Where \\(y =\\) output quantity</p> <p>Benefits of scale exists if \\(\\alpha &lt; 1\\)</p>"},{"location":"Finance_Electives/Economics/02_10_Principles/#elasticity-of-demand","title":"Elasticity of demand","text":""},{"location":"Finance_Electives/Economics/02_10_Principles/#elastic-demand","title":"Elastic demand","text":"<p>Demand for commodity is highly sensitive to income of consumers</p> <p>eg: smartphones</p>"},{"location":"Finance_Electives/Economics/02_10_Principles/#inelastic-demand","title":"Inelastic demand","text":"<p>Demand for commodity is nearly independent of income of consumers.</p> <p>Eg: coffee, rice, food Even if someone gets richer, they won't be eating more food</p>"},{"location":"Finance_Electives/Economics/02_10_Principles/#markets-are-usually-good","title":"Markets are usually good","text":""},{"location":"Finance_Electives/Economics/02_10_Principles/#economic-systems","title":"Economic Systems","text":"<p>An economic system is an arrangement of economic activity</p> Type Description Regulation Competition Variety in Products Example Private/Market-Based/Capitalist Dynamically affected by what the producers produce and consumers consumePure capitalism means that the market is free from govt intervention Low/None High (encourages people to perform better; everyone tries to optimize) High USA Centralized/Govt Central authority dictates High Low Low North Korea Public Communism Type Hybrid Moderate Moderate Moderate India (Centralized railways, while others are market-based)"},{"location":"Finance_Electives/Economics/02_10_Principles/#functions-of-market","title":"Functions of Market","text":"<ol> <li> <p>organizing economic activity</p> </li> <li> <p>determining the price    supply reflects the availability/abundance of the commodity    demand reflects the willingness for purchasing commodity    price for a product without a market(demand/supply) for it is undefined</p> </li> <li> <p>Signaling and allocation of resources</p> </li> </ol> Price Reaction of Production Reaction of Demand Inc Inc Dec Dec Dec Inc <p>Not all market signals are pure market signals    Distortion of market signal: when govt creates policies to change pricing in a free market    Eg: subsidies</p>"},{"location":"Finance_Electives/Economics/02_10_Principles/#government","title":"Government","text":"<p>Govt can help improve market outcomes</p> <ol> <li>Protect property rights</li> <li> <p>Generate efficient outcomes, such as prevent </p> <ol> <li>Monopoly</li> <li> <p>Tragedy of Commons for CPR(Common Property Resources)</p> <p>For community resources(like fishing ponds, forests), the govt comes in to prevent over-utilization of the resources; supports sustainability</p> </li> </ol> </li> <li> <p>Market failure</p> <ol> <li>Market fails when it comes to non-excludable commodity</li> <li>Govt interferes in fields where private investors are not interested</li> </ol> <p>Examples</p> <ul> <li>national defence</li> <li>community services</li> <li>landscapes</li> <li>fresh air</li> </ul> </li> <li> <p>Promote equality/equity</p> <ol> <li>prevention of trade of whatever is socially harmful (alcohol, drugs)</li> <li>keeps market's obsession with efficiency at the expensive of equity in check</li> <li>reduces economic inequality</li> </ol> </li> <li> <p>Socially desirable outcomes</p> </li> </ol>"},{"location":"Finance_Electives/Economics/02_10_Principles/#monopoly","title":"Monopoly","text":"<p>private monopoly: unfair situation for consumers where there is only a sole producer of a product natural monopoly: situation where there is a single producer (like govt), but not necessarily bad; eg: Railways</p> <p>monopoly can be addressed through govt subsidies for the monopoly the monopoly company will tend to increase output to attain the new maximum marginal net benefit hence, the level of optimal output will be greater after subsidies than without it, and hence the producers increase the supply this will reduce the price and benefit the consumers</p>"},{"location":"Finance_Electives/Economics/02_10_Principles/#excludability","title":"Excludability","text":""},{"location":"Finance_Electives/Economics/02_10_Principles/#excludable","title":"Excludable","text":"<p>possible to exclude whoever did not pay for it</p> <p>Eg: netflix subscription</p>"},{"location":"Finance_Electives/Economics/02_10_Principles/#non-excludable-commodity","title":"Non-excludable commodity","text":"<p>where it is impossible to exclude those who did not pay for it</p> <p>eg: national defence, community parks</p>"},{"location":"Finance_Electives/Economics/02_10_Principles/#idk","title":"Idk","text":"<p>There is always a conflict b/w efficiency vs equity.</p> Term Meaning Efficiency optimization to maximize output with minimum input doing things right Effectiveness Impact of output doing the right things Equity Upholding morals, social norms, benefitting society/the world"},{"location":"Finance_Electives/Economics/02_10_Principles/#rivalry-of-consumption","title":"Rivalry of consumption","text":"Consumption Rivalled Availability of a product is dependent of its consumption Non-rivalled Availability of a product is independent of its consumptionThat's one of the best things of digital revolution: it has enabled non-rivalryEg: national defence, fresh air, lighthouses, netflix subscription"},{"location":"Finance_Electives/Economics/02_10_Principles/#goods","title":"Goods","text":"<p>Something that generates pleasure</p> <p>You pay for it</p> Public Club CPR Private Rivalled N N Y Y Excludable N Y N Y Example national defence netflix forests, natural goods food prone to exploitation"},{"location":"Finance_Electives/Economics/02_10_Principles/#bads","title":"Bads","text":"<p>generates displeasure</p> <p>you receive payment</p>"},{"location":"Finance_Electives/Economics/02_10_Principles/#standard-of-living","title":"Standard of living","text":"<p>Standard of living of a country depends on its production level</p> <p>Measures for standard of living</p> <ul> <li>for individuals   comparing real personal income</li> <li>for countries<ul> <li>comparing real GDP shows the employment opportunities in the country</li> <li>comparing real GNP (Gross National Product) shows the status of citizens of the country</li> <li>per capita income and output per capita income \\(= \\frac{income}{population}\\), per capita output \\(= \\frac{output}{population}\\) shows the average contribution of people; doesn't show income inequality though</li> </ul> </li> </ul> <p>Disparities in standard of living is caused due to difference in productivity Productivity: amount of goods and services produced by a worker during an hour of work</p> <p>i don\u2019t exactly agree with this cuz women have a history of earning less for the same productivity</p>"},{"location":"Finance_Electives/Economics/02_10_Principles/#income","title":"Income","text":"Nominal income monetary income Real income \\(\\dfrac{\\text{Nominal income}}{\\text{Avg cost of purchases}}\\)Reflects purchasing power: the quantity of goods and services you can buy IDKisn\u2019t this the best? \\(\\text{Value of savings wrt exchange rate}\\)\\(\\text{Savings} = \\text{Nominal income}-\\text{Avg cost of purchases}\\)"},{"location":"Finance_Electives/Economics/02_10_Principles/#gdp-gnp","title":"GDP &amp; GNP","text":"GDP GNP Full Form Gross Domestic Product Gross National Product Total value of final goods and services produced within the borders of a country (regardless of nationality) by nationals/citizens of a country (regardless of location) From the consumer's perspective, it is the same as national incomeie, sum of incomes of all individuals is mathematically = GDP"},{"location":"Finance_Electives/Economics/02_10_Principles/#nominal-gdp-gnp","title":"Nominal GDP &amp; GNP","text":"<p>production quantity x prices of the current year</p>"},{"location":"Finance_Electives/Economics/02_10_Principles/#real-gdp-gnp","title":"Real GDP &amp; GNP","text":"<p>production quantity x prices of base year</p>"},{"location":"Finance_Electives/Economics/02_10_Principles/#paradox-of-value","title":"Paradox of value","text":"<p>The paradox of value is the contradiction that, although water is more necessary than diamonds for survival, diamonds are costlier</p>"},{"location":"Finance_Electives/Economics/02_10_Principles/#prices-rise-when-govt-prints-excess-money","title":"Prices rise when govt prints excess money","text":"<p>4 functions of money</p> <ol> <li>Facilitates trade    prevents the need for coincidence of wants; ie money prevents the need for both parties to want to exchange products that they want from the other person</li> <li>mode of deferred payment</li> <li>unit of value</li> <li>store of value</li> </ol> <p>If you print money without them producing anything, money loses its value and inflation occurs However, if you print money due to increase in the country's output, then it's fine</p> <p>When prices of commodity increases, value of money reduces</p> <p>value of money \\(\\propto \\frac{1}{\\text{price of commodities} }\\)</p>"},{"location":"Finance_Electives/Economics/02_10_Principles/#inflation","title":"Inflation","text":"<p>Rate at which prices of commodities increase</p> <p>happens when economy is doing well</p> <p>upto 2-5% inflation is acceptable</p>"},{"location":"Finance_Electives/Economics/02_10_Principles/#inflation-models","title":"Inflation Models","text":""},{"location":"Finance_Electives/Economics/02_10_Principles/#taylor-rule","title":"Taylor Rule","text":"<p>Inflation rate something</p> \\[ I_t = \\beta_1 + \\beta_2 (\\pi_t - \\pi_t^*) + e_t \\]"},{"location":"Finance_Electives/Economics/02_10_Principles/#money-supply","title":"Money Supply","text":"\\[ I_t = \\beta_1 + \\beta_2 M_t + e_t \\]"},{"location":"Finance_Electives/Economics/02_10_Principles/#inflation-welfare-cost-relation","title":"Inflation-Welfare Cost Relation","text":"Optimal Inflation Rate Developed Countries 2% Developing Countries 4-6%"},{"location":"Finance_Electives/Economics/02_10_Principles/#micro-effects","title":"Micro Effects","text":"<p>Money-givers gain</p> <p>Fixed income money-receivers lose eg: regular employees</p> <p>Variable income money-receivers not affected business people, freelancers</p>"},{"location":"Finance_Electives/Economics/02_10_Principles/#macro-effects","title":"Macro Effects","text":"<p>Imports increase: demand for domestic products decreases, as they are costlier Exports decrease: as other countries do not want a more expensive product</p> <p>Hence, high inflation is not good</p>"},{"location":"Finance_Electives/Economics/02_10_Principles/#cost-push-inflation","title":"Cost-Push inflation","text":"<p>Prices increase due to increase in the cost for supply </p> <ul> <li>cost of production</li> <li>prices of raw materials</li> <li>oil prices (transportation)</li> </ul> <p>for the name, think suppliers push the product to the consumers</p>"},{"location":"Finance_Electives/Economics/02_10_Principles/#demand-pull-inflation","title":"Demand-pull inflation","text":"<p>Prices increase due to increase in demand</p> <p>for the name, think consumers pull the product from the sellers</p>"},{"location":"Finance_Electives/Economics/02_10_Principles/#structural-inflation","title":"Structural inflation","text":"<p>The suppliers are not able to keep up with the demand due to lack of infrastructure; ie, demand increases and producers want to increase output, but aren't able to do so</p> <p>idk point: Agricultural inflation affects non-agricultural sectors, as agricultural products are used as raw materials also, agricultural sector depends on the manufacturing sector and vice-versa</p>"},{"location":"Finance_Electives/Economics/02_10_Principles/#deflation","title":"Deflation","text":"<p>Rate at which prices decreases</p> <p>in other words, it is negative inflation</p> <p>more dangerous than high inflation</p>"},{"location":"Finance_Electives/Economics/02_10_Principles/#welfare-cost","title":"Welfare Cost","text":"<p>The cost associated with any action that has macro-level consequences and affects entire society eg: taxes, interest rates have welfare costs associated with them</p>"},{"location":"Finance_Electives/Economics/02_10_Principles/#inflationunemployment","title":"Inflation/Unemployment","text":"<p>Economy faces a short-run trade-off between inflation and unemployment</p> <p>Short run: period where contracts cannot be renegotiated (monthly, quarterly) Lon run: a long period (annually) which contains multiple renegotiations of contracts</p>"},{"location":"Finance_Electives/Economics/02_10_Principles/#unemployment","title":"Unemployment","text":"<p>the fraction/proportion of people seeking jobs but cannot get</p> <p>does not include people who aren't seeking jobs</p>"},{"location":"Finance_Electives/Economics/02_10_Principles/#natural-rate","title":"Natural Rate","text":"<p>Unemployment rate which exists regardless of whatever we do</p> <p>Depends on the country's</p> <ol> <li>resources</li> <li>technology</li> <li>production capacity/no of factories</li> <li>population/size of the country</li> </ol>"},{"location":"Finance_Electives/Economics/02_10_Principles/#actual-rate","title":"Actual Rate","text":"<p>The current rate of unemployment </p> <p>it fluctuates a lot</p> <ul> <li>during depression, actual &gt; natural</li> <li>during boom, actual &lt; natural</li> </ul> <p>However, if you take the average of 20yrs or so, the average actual rate tends to the natural rate</p>"},{"location":"Finance_Electives/Economics/02_10_Principles/#output","title":"Output","text":""},{"location":"Finance_Electives/Economics/02_10_Principles/#potential-output","title":"Potential output","text":"<p>predicted output an economy can ideally produce by using all its resources</p>"},{"location":"Finance_Electives/Economics/02_10_Principles/#actual-output","title":"Actual output","text":"<p>What an economy actually produces</p>"},{"location":"Finance_Electives/Economics/02_10_Principles/#scenarios","title":"Scenarios","text":"<ul> <li>during recession, actual &lt; potential</li> <li>during boom, actual &gt; potential</li> </ul>"},{"location":"Finance_Electives/Economics/02_10_Principles/#phases-of-economy","title":"Phases of economy","text":"<pre><code>flowchart LR\nDepression --&gt; Recovery --&gt; Boom --&gt; Recession --&gt; Depression</code></pre>"},{"location":"Finance_Electives/Economics/02_10_Principles/#phillips-curve-relation","title":"Phillips Curve Relation","text":"<ul> <li>y-axis = inflation</li> <li>x-axis = unemployment</li> </ul> \\[ \\text{Inflation} \\propto \\frac{1}{\\text{Unemployment}} \\] \\[ \\begin{aligned} \\pi_t &amp;= \\alpha - \\beta U_t &amp;  (\\pi_t = - \\beta U_t + \\alpha, \\ y = mx+c) \\\\ \\text{Taking derivative wrt } \\pi_t \\implies \\beta &amp;= - \\frac{d \\pi_t}{dt} \\end{aligned} \\] <ul> <li>\\(\\pi_t =\\) Inflation</li> <li>\\(\\alpha =\\) inflation when there is no unemploment</li> <li>\\(\\beta =\\) cost for reducing unemployment by a unit</li> <li>\\(U_t =\\) actual rate of unemployment</li> </ul> <p>This relation is only short-run for long run, whatever is the inflation, unemployment tends to natural unemployment the graph will be a straigth line parallel to the y-axis</p> <p>During shortrun, the contracts for raw materials, employees is fixed but prices for commodity increases therefore, producers increase production to maximize profit (misperception by producers); this is done by increasing employees Unemployment rate decreases</p> <p>Moreover, workers suffer money illusion (only focus on the nominal income increase;  don\u2019t realise that the real income is the same)</p> <p>Then in the long run, few months later, the employees will renegotiate for higher wages; then the producers will hesitate as they no longer see the attraction for producing at such large volume and paying such wages; so they fire employees; therefore, the unemployment rate will increase again</p>"},{"location":"Finance_Electives/Economics/02_10_Principles/#inflation-employment-output","title":"Inflation, Employment, Output","text":"\\[ \\text{Output} \\propto \\text{Employment} \\propto \\text{Inflation} \\]"},{"location":"Finance_Electives/Economics/03_Sectors/","title":"03 Sectors","text":""},{"location":"Finance_Electives/Economics/03_Sectors/#trickle-down-effect","title":"Trickle-down Effect","text":"<p>The benefits of improving a particular country/sector will improve others also eventually</p>"},{"location":"Finance_Electives/Economics/03_Sectors/#health-of-an-economy","title":"Health of an economy","text":"<ol> <li>Real GDP/National Income (not GNP)</li> <li>Inflation</li> <li>Unemployment</li> </ol>"},{"location":"Finance_Electives/Economics/03_Sectors/#what-is-a-model","title":"What is a Model?","text":"<p>Simplified representation of a system with assumptions</p>"},{"location":"Finance_Electives/Economics/03_Sectors/#purpose","title":"Purpose","text":"<ul> <li>simplify economic decisions</li> <li>how to change outcomes</li> <li>focus on what's important</li> <li>built with assumptions</li> <li>omits details</li> </ul>"},{"location":"Finance_Electives/Economics/03_Sectors/#characteristics-of-a-good-model","title":"Characteristics of a good model","text":"<ol> <li>Simple</li> <li>Easy to work with</li> <li>Insightful</li> <li>Generalizable</li> <li>Easy to test, and thereby accept/reject</li> <li>Empirically consistent</li> <li>Predictive precision</li> </ol>"},{"location":"Finance_Electives/Economics/03_Sectors/#basic-economic-models","title":"Basic Economic Models","text":"<p>we are gonna learn 2 models</p> <ol> <li>circular flow</li> <li>production possibility frontier</li> </ol>"},{"location":"Finance_Electives/Economics/03_Sectors/#circular-flow-model","title":"Circular Flow Model","text":"<p>More cycles \\(\\implies\\)higher output</p> <p>Cashless economy allows to complete more cycles Credit card basically provides preponed salary; instigates flow from consumer sector to production sector</p>"},{"location":"Finance_Electives/Economics/03_Sectors/#sectors-of-economy","title":"Sectors of Economy","text":"<ol> <li>consumer</li> <li>business/production</li> <li>financial</li> <li>Govt</li> <li>external/foreign/international</li> </ol> <p>All of these sectors are inter-linked and anything that affects one sector affects the others too</p>"},{"location":"Finance_Electives/Economics/03_Sectors/#2-sector","title":"2 sector","text":"<p>Consumer and production</p> <p>Production sector produces goods and services; consumers buy them</p> <p>Production sector provides jobs; consumers work</p>"},{"location":"Finance_Electives/Economics/03_Sectors/#3-sector","title":"3 sector","text":"<p>Financial sector provides investments to the producers</p> <p>Consumers deposit the savings into the financial sector, and they gain more or assets go through depreciation</p>"},{"location":"Finance_Electives/Economics/03_Sectors/#4-sector","title":"4 Sector","text":"<p>International sector allows</p> <ul> <li>imports/exports</li> <li>remittance(overseas transfers) - consumer the money sent back to the family in another country</li> <li>net lending overseas - financial sector gives out loans to other countries</li> <li>overseas income from foreign to all other sectors</li> </ul>"},{"location":"Finance_Electives/Economics/03_Sectors/#5-sector","title":"5 sector","text":"<p>Govt</p> <ul> <li>consumers<ul> <li>collect taxes</li> <li>provide direct income transfers (like pensions)</li> </ul> </li> <li>business<ul> <li>collects taxes</li> <li>govt purchases</li> <li>transfer payments (tax holidays)</li> </ul> </li> <li> <p>financial sector</p> <ul> <li>loans in times of govt deficit</li> <li>liquidity injection Hot money/cash for the banks</li> <li>foreign sector</li> <li>import duties/taxes</li> </ul> </li> </ul>"},{"location":"Finance_Electives/Economics/03_Sectors/#budget-deficit-leads-to-high-inflation","title":"Budget deficit leads to high inflation","text":"<p>When the govt spends a lot of money in development, then the demand for investment and labor will increase</p> <p>Developing countries always try to spend more than their revenue, because they're trying to develop as quickly as possible</p>"},{"location":"Finance_Electives/Economics/03_Sectors/#monetization-of-budget-deficit","title":"Monetization of budget deficit","text":"<p>When the govt prints money/borrows from banks, instead of selling assets</p> <p>This leads to inflation as there is more money in the system</p>"},{"location":"Finance_Electives/Economics/03_Sectors/#duopoly","title":"Duopoly","text":"<p>A situation with multiple buyers, but only 2 sellers</p> <p>Nash equilibrium (by John Nash) says that</p> <p>An agent who has a competitor will always design the best policy, with the assumption that the competitor will also design its own best policy</p>"},{"location":"Finance_Electives/Economics/03_Sectors/#prisoners-dilemma","title":"Prisoner's dilemma","text":"<p>(not in portion; just extra)</p>"},{"location":"Finance_Electives/Economics/04_PPF/","title":"04 PPF","text":""},{"location":"Finance_Electives/Economics/04_PPF/#production-function","title":"Production function","text":"<p>Function that defines output as a function of input(s)</p> <p>\\(y = f_1(l, k), x = f_2(l, k)\\)</p>"},{"location":"Finance_Electives/Economics/04_PPF/#marginal-productivity","title":"Marginal Productivity","text":"<ul> <li>\\(\\text{MP}_l^y = \\frac{\\partial y}{\\partial l}\\)</li> <li>\\(\\text{MP}_k^{y} = \\frac{\\partial y}{\\partial k}\\)</li> </ul>"},{"location":"Finance_Electives/Economics/04_PPF/#law-of-diminishing-marginal-productivity","title":"Law of Diminishing Marginal Productivity","text":"<p>states that: With increase in input</p> <ul> <li>total productivity increases</li> <li>marginal productivity decreases</li> </ul>"},{"location":"Finance_Electives/Economics/04_PPF/#graph","title":"Graph","text":"<ul> <li>y = MP</li> <li>x = input (l,k)</li> </ul> <p>The graph will be a straight line with a downward slope</p>"},{"location":"Finance_Electives/Economics/04_PPF/#production-possibility-frontier","title":"Production Possibility Frontier","text":"<p>deals with the production capacity of an economy</p> <ul> <li>analyzes available resources</li> <li>predicts what can be produced with those resources, with the assumption that time and technology are constant</li> <li>analyzes various output mixes (combinations of different commodities)</li> </ul>"},{"location":"Finance_Electives/Economics/04_PPF/#production-possibility-curve","title":"Production Possibility curve","text":"<p>Curve that shows the optimal level of production for different combinations of commodities</p> <p>what is the maximum output that can be produced of various commodities</p>"},{"location":"Finance_Electives/Economics/04_PPF/#graph_1","title":"Graph","text":"<ul> <li>\\(y =\\) commodity 2</li> <li>\\(x =\\) commodity 1</li> </ul> <p>The graph will be a curve</p>"},{"location":"Finance_Electives/Economics/04_PPF/#mrt","title":"MRT","text":"<p>Marginal rate of transformation</p> <p>Slope shows the real/opportunity cost of \\(x\\) (how much we're sacrificing \\(y\\)) MRT of producing \\(x\\) wrt \\(y\\) shows no of units of \\(y\\) to be sacrificed to increase the output of \\(x\\) by one unit</p> \\[ \\begin{aligned} y &amp;= f(x) \\\\ \\text{MRT}_{x,y} &amp;= -\\frac{\\mathrm{d} y}{\\mathrm{d} x} \\end{aligned} \\] <p>When we go from left to right, the slope increases from left to right, ie, cost of production of \\(x\\) increases</p> <ul> <li>\\(\\text{MP}_k^y, \\text{MP}_l^y\\) increases</li> <li>\\(\\text{MP}_k^x, \\text{MP}_l^x\\) decreases</li> </ul> <p>Example: if mango\u2019s MP is 2 and apple\u2019s MP is 4, then 4 units of apples are to be sacrificed to produce 2 units of mangoes</p> \\[ \\begin{aligned} x &amp;= l \\cdot MP_l + k \\cdot MP_k \\\\ \\mathrm{d} y &amp;= \\mathrm{d} l_{y}(MP_l^y) + \\mathrm{d} k_y(MP_k^y) \\\\ \\mathrm{d} x &amp;= \\mathrm{d} l_{x}(MP_l^x) + \\mathrm{d} k_x(MP_k^x) \\\\ \\frac{\\mathrm{d} y}{\\mathrm{d} x} &amp;= \\frac{MP_k^y}{MP_k^x} = \\frac{MP_l^y}{MP_l^x} \\end{aligned} \\]"},{"location":"Finance_Electives/Economics/04_PPF/#points-on-the-curve","title":"Points on the curve","text":"<ul> <li>inside are inefficient input mix - wasting profit</li> <li>on the curve are optimal input mix</li> <li>outside is impossible input mix</li> </ul>"},{"location":"Finance_Electives/Economics/04_PPF/#shift-of-ppc","title":"Shift of PPC","text":"<p>It depends on type of technological change that occurs</p> <p>also depends on change in the quantity and productivity of labor and capital</p>"},{"location":"Finance_Electives/Economics/04_PPF/#technological-change","title":"Technological change","text":""},{"location":"Finance_Electives/Economics/04_PPF/#capital-intensive","title":"Capital-intensive","text":"<p>increases productivity of capital more than productivity of labor</p> <p>motivates producers to increase machines rather than increasing  labor</p> <p>Curve expands just for capital-intensive good side</p>"},{"location":"Finance_Electives/Economics/04_PPF/#labor-intensive","title":"Labor-intensive","text":"<p>increases productivity of labor more than productivity of capital</p> <p>motivates producers to increase labor rather than increasing machines</p> <p>Curve expands just for labor-intensive good side</p>"},{"location":"Finance_Electives/Economics/04_PPF/#neutral","title":"Neutral","text":"<p>increases productivity of labor and productivity of capital</p> <p>does not change the proportion in which labor and capital are used</p> <p>causes parallel shift (equal increase/decrease)</p>"},{"location":"Finance_Electives/Economics/04_PPF/#efficient-output-mix","title":"Efficient output mix","text":"<p>optimal points on the curve</p> <p>it is impossible to increase the output of one commodity without sacrificing the output of another</p> <p>shows efficient performance</p> <p>developed economies</p>"},{"location":"Finance_Electives/Economics/04_PPF/#inefficient-output-mix","title":"Inefficient output mix","text":"<p>points inside the curve</p> <p>it is possible to increase the output of one commodity without decreasing the output of another</p> <p>shows underperformance</p> <p>Developing economies</p>"},{"location":"Finance_Electives/Economics/04_PPF/#unattainable-output-mix","title":"Unattainable output mix","text":"<p>points outside the curve</p> <p>it is not possible to reach that output, even by using all resources</p> <p>that level of output may be achieved in the future with the help of development, but not with the current resources</p>"},{"location":"Finance_Electives/Economics/04_PPF/#idk","title":"IDK","text":"<p>Proportional ratio = \\(\\frac{\\text{capital}}{\\text{labor}}\\) Greater the ratio, higher the productivity of the country</p> <p>circular model is more sustainable</p>"},{"location":"Finance_Electives/Economics/05_Trade/","title":"05 Trade","text":""},{"location":"Finance_Electives/Economics/05_Trade/#trade","title":"Trade","text":"<p>We can live without trade, but the standard of living will be lower</p> <p>Trade happens only when it is cheaper to import than to produce it yourself</p> <p>we have already covered all this in principle of economics trade point</p> <p>When we trade, the total availability of goods and services increases, ie the post-trade PPC exceeds the pre-trade PPC</p> <p>Before trade, the production and consumption points will be the same After trade, the consumption point exceeds the production point</p>"},{"location":"Finance_Electives/Economics/05_Trade/#bases-for-trade","title":"Bases for Trade","text":"<p>shows what a country should import/export</p> <p>these minimize the opportunity costs</p>"},{"location":"Finance_Electives/Economics/05_Trade/#absolute-advantage-theory","title":"Absolute Advantage Theory","text":"<p>A country must specialize in which it has absolute advantage, ie when it can</p> <ul> <li>produce more at the same cost as others</li> <li>produce same at a lower cost than others</li> </ul>"},{"location":"Finance_Electives/Economics/05_Trade/#comparative-advantage-theory","title":"Comparative Advantage Theory","text":"<p>Also called as Ricardian Model</p> <p>Countries should specialize in goods in which they have a higher relative advantage; ie goods where they have a lower opportunity cost relative to the trading partner</p> <p>seems very obvious considering marginal net benefit for investment, and opportunity cost</p> <ol> <li>Calculate OC of all commodities in both countries \\(= \\frac{\\text{commodity production you  lose out on}}{\\text{what you chose to produce}}\\)</li> <li>Compare OC of each commodity b/w the countries</li> <li>The country with the lower OC for every commodity will produce that commodity</li> </ol>"},{"location":"Finance_Electives/Economics/05_Trade/#factors-of-comparative-advantage","title":"Factors of comparative advantage","text":"<p>endowment factors ie the relative abundance/scarcity/productivity of labor and capital</p> <ol> <li> <p>comp advantage in labor</p> <ul> <li> <p>lower labor costs</p> </li> <li> <p>higher labor productivity</p> </li> </ul> </li> <li> <p>comp advantage in capital goods</p> <ul> <li>more available capital (infrastructure, machinery)</li> <li>higher capital productivity</li> </ul> </li> </ol>"},{"location":"Finance_Electives/Economics/05_Trade/#exchange-rate","title":"Exchange rate","text":""},{"location":"Finance_Electives/Economics/05_Trade/#absolute-exchange-rate","title":"Absolute exchange rate","text":"<p>is the ratio of nominal value of 2 currencies</p>"},{"location":"Finance_Electives/Economics/05_Trade/#real-exchange-rate","title":"Real exchange rate","text":"<p>also called as terms of trade</p> <p>Rate at which one commodity is exchanged for another</p> <p>is the ratio of the goods/services you can buy with 2 currencies</p>"},{"location":"Finance_Electives/Economics/05_Trade/#pegging","title":"Pegging","text":"<p>the absolute exchange rate is fixed against another currency</p> <p>isn\u2019t natural it is due to govt intervention</p> <p>for eg, in UAE Dirham, the Central Bank of UAE</p> <ul> <li>buys dollars when the value of dollars reduces<ul> <li>in order to create a fake shortage</li> <li>and hence increase value</li> </ul> </li> <li>sells dollars when the value of dollars decreases</li> </ul>"},{"location":"Finance_Electives/Economics/05_Trade/#empirical-tests-of-the-ricardian-model","title":"Empirical Tests of the Ricardian Model","text":"<p>cost \\(\\propto \\frac{1}{\\text{productivity}}\\)</p> <p>eg: AC ka efficiency</p>"},{"location":"Finance_Electives/Economics/05_Trade/#us-vs-uk-productivity","title":"US vs UK productivity","text":"<p>Relative exports of 2 countries \\(\\propto\\) Relative output/productivity</p>"},{"location":"Finance_Electives/Economics/05_Trade/#us-vs-japanese-costs","title":"US vs Japanese costs","text":"<p>Relative exports of 2 countries \\(\\propto \\frac{1}{\\text{Relative labor cost}}\\)</p>"},{"location":"Finance_Electives/Economics/06_Elements_of_Market/","title":"06 Elements of Market","text":""},{"location":"Finance_Electives/Economics/06_Elements_of_Market/#elements-of-market","title":"Elements of Market","text":"<ol> <li>Demand</li> <li>Supply</li> <li>Equilibrium</li> </ol>"},{"location":"Finance_Electives/Economics/06_Elements_of_Market/#demand","title":"Demand","text":"<p>indicates optimal quantity of commodities which consumers are willing and able to buy at a particular price, in a particular period</p> <p>if prices change, demand also changes</p>"},{"location":"Finance_Electives/Economics/06_Elements_of_Market/#law-of-demand","title":"Law of Demand","text":"<p>Assuming every factor (like preferences, current state affairs) remains constant, \\(\\text{demand} \\propto \\frac{1}{\\text{price}}\\) this goes other way round too</p>"},{"location":"Finance_Electives/Economics/06_Elements_of_Market/#individual-demand-curve","title":"Individual Demand Curve","text":"<p>can vary from one individual to another even for the same commodity, due to Factors of Trade</p> <p>graph is a straight line with a negative slope</p> <ul> <li>x = Demand</li> <li>y = price</li> </ul> <p>Be careful of the slope, cuz the slope formula is for the inversed graph of the demand Curve Slope \\(= \\frac 1 {\\alpha_2}\\)</p> \\[ \\text{Demand as a function of price}\\\\ \\begin{aligned} D &amp;= f(P) \\\\ &amp;= \\alpha_1 - \\alpha_2 P \\\\ \\alpha_2 &amp;= -\\frac{\\partial D}{\\partial P} \\end{aligned} \\] Term Meaning \\(x\\) Demand \\(P\\) Price \\(\\alpha_2\\) Sensitivity of demand wrt priceThe no of units of demand decreases by when the price increases by 1 unit - Necessities have low sensitivity - Luxury goods have high sensitivity \\(\\alpha_1\\) Demand even when commodity is freeCaptures impact of all other factors that affect the demand (Income of consumers, advertising, etc)"},{"location":"Finance_Electives/Economics/06_Elements_of_Market/#graph-characteristics","title":"Graph Characteristics","text":"Horizontal Graph Vertical Graph Slope of graph \\(\\to 0\\) \\(\\to \\infty\\) \\(\\alpha_2\\) \\(\\to \\infty\\) \\(\\to 0\\) sensitivity High Low even a small change in price will cause variation in demand even large changes in price cause negligible change in demand Example when there are too many sellers and buyers; and only one seller changes the price medicines, food"},{"location":"Finance_Electives/Economics/06_Elements_of_Market/#market-demand","title":"Market Demand","text":"<p>total demand for a commodity in a market at a particular price</p> <p>summation of individual demands for commodity at particular prices</p>"},{"location":"Finance_Electives/Economics/06_Elements_of_Market/#giffen-goods","title":"Giffen Goods","text":"<p>Law of demand not applicable for them</p> <p>eg: BW TVs, Nokia Phone</p> <p>\\(\\text{demand} \\propto \\text{price}\\)</p>"},{"location":"Finance_Electives/Economics/06_Elements_of_Market/#factors-of-demand","title":"Factors of Demand","text":"<p>Out of the following factors, economic policies mainly target the expectations factor</p> \\[ x_1 =  \\alpha - \\alpha_1 p \\pm \\alpha_2 M \\pm \\alpha_3 W \\pm \\alpha_4 M^e \\pm \\alpha_5 p^e + \\alpha_6 A \\]"},{"location":"Finance_Electives/Economics/06_Elements_of_Market/#income-and-wealth","title":"Income and Wealth","text":"<p>More income and wealth means more spending and hence, higher demand</p> <ul> <li>Income is flow of money currently</li> <li>Wealth is what we have accumulated over time</li> </ul> RelationshipType ElasticDemand? Shift inindividual demand curve Consumption atsame price Example +ve \u2705 Rightward Greater Luxury Items Neutral \u274c None Same Staple foods -ve \u2705 Leftward Lower Inferior and Giffen goods"},{"location":"Finance_Electives/Economics/06_Elements_of_Market/#types-of-goods-based-on-income-elasticity","title":"Types of Goods based on Income Elasticity","text":"Type Income Elasticity Superior +ve Smartphones, LED TVs, Cars Necessities 0 Staple foods Inferior -ve B/W TV, tungsten bulbs, public transport"},{"location":"Finance_Electives/Economics/06_Elements_of_Market/#price-of-other-goods","title":"Price of Other goods","text":"<p>Cross Price is measured by \\(\\alpha_3\\)</p> <p>when price of complimentary good increases, the demand of main commodity decreases when price of substitute good increases, the demand of main commodity increases</p> <p>hence, if</p> <ul> <li>\\(\\alpha_3 &gt; 0\\) substitute</li> <li>\\(\\alpha_3 &lt; 0\\) complimentary</li> </ul>"},{"location":"Finance_Electives/Economics/06_Elements_of_Market/#types-of-goods-based-on-income-elasticity_1","title":"Types of Goods based on Income Elasticity","text":"Example Complimentary goods Goods that are consumed together Car &amp; Petrol Substitute Goods Goods that are alternatives of each other Pepsi &amp; Coke"},{"location":"Finance_Electives/Economics/06_Elements_of_Market/#tastespreferences","title":"Tastes/Preferences","text":"<p>idk how to write this</p>"},{"location":"Finance_Electives/Economics/06_Elements_of_Market/#customer-expectations","title":"Customer Expectations","text":"Expectation Meaning Explanation Expected Price What I predict to be the price of the commodity in the future If expected price &gt; current price, then demand increases, which ends up increasing the price; whether or not it would\u2019ve happened naturally, nobody will know \ud83d\ude06; here, our expectations clearly affects the actual outcomeIf expected price &lt; current price, then demand decreases Expected What I predict to be my income in the future"},{"location":"Finance_Electives/Economics/06_Elements_of_Market/#market-size","title":"Market Size","text":"<p>No of buyers in the market</p> \\[ \\text{Demand} \\propto \\text{Market Size} \\]"},{"location":"Finance_Electives/Economics/06_Elements_of_Market/#advertising-expenditure","title":"Advertising Expenditure","text":"<p>Does not affect the product, but changes the perception of the product in consumers\u2019 heads</p> \\[ \\text{Demand} \\propto \\text{Advertising Expenditure} \\]"},{"location":"Finance_Electives/Economics/06_Elements_of_Market/#seasontime-of-the-year","title":"Season/Time of the Year","text":"<ul> <li>demand for cotton is greater in summer</li> <li>demand for wool is greater in winter</li> </ul>"},{"location":"Finance_Electives/Economics/06_Elements_of_Market/#supply","title":"Supply","text":"<p>is the optimal quantity which sellers are willing and able to sell at a given price, in a particular period</p>"},{"location":"Finance_Electives/Economics/06_Elements_of_Market/#law-of-supply","title":"Law of Supply","text":"<p>Assuming that factors are same, supply \\(\\propto\\) selling price of commodity</p>"},{"location":"Finance_Electives/Economics/06_Elements_of_Market/#contemporary-relation","title":"Contemporary Relation","text":"<p>\\(S = \\beta_1 + \\beta_2 P_{t}\\)</p> <ul> <li>land</li> <li>eggs</li> <li>milk</li> </ul>"},{"location":"Finance_Electives/Economics/06_Elements_of_Market/#lagged-relation","title":"Lagged Relation","text":"<p>\\(S = \\beta_1 + \\beta_2 P_{t-1}\\)</p> <p>For commodities with large gestation period, such as agricultural</p>"},{"location":"Finance_Electives/Economics/06_Elements_of_Market/#terms","title":"Terms","text":"<ul> <li>\\(\\beta_1 =\\) minimum selling price for which suppliers are willing to produce commodity</li> <li>\\(\\beta_2 =\\) sensitivity of supply wrt price</li> </ul>"},{"location":"Finance_Electives/Economics/06_Elements_of_Market/#graph","title":"Graph","text":"<ul> <li>y = P</li> <li>x = S</li> </ul> Shift Supply for the same price Outward Greater Inward Lower <p>All points on the supply curve show the optimal supplies Any point inside/outside the supply curve will not provide maximum profit</p>"},{"location":"Finance_Electives/Economics/06_Elements_of_Market/#individual-supply","title":"Individual Supply","text":"<p>Every firm has different supply curve due to difference in cost structures</p> \\[ \\text{Cost} \\propto \\frac{1}{\\text{Scale}} \\quad (\\because \\text{Benefits of Scale}) \\]"},{"location":"Finance_Electives/Economics/06_Elements_of_Market/#market-supply","title":"Market Supply","text":"<p>Total supply for a commodity in a market at a particular price</p> <p>Summation of supplies of commodity by different firms at particular prices</p>"},{"location":"Finance_Electives/Economics/06_Elements_of_Market/#factors-of-supply","title":"Factors of Supply","text":"\\[ S = \\beta_1 + \\beta_2 P + \\beta_3 F + \\beta_4 T + \\beta_5 E \\]"},{"location":"Finance_Electives/Economics/06_Elements_of_Market/#cost-of-production","title":"Cost of Production","text":"<p>input prices, technology</p> <p>if cost inc, supply dec</p>"},{"location":"Finance_Electives/Economics/06_Elements_of_Market/#expectations","title":"Expectations","text":"<ul> <li>if expected price/returns &gt; current price, then supply increases</li> <li>else supply decreases</li> </ul>"},{"location":"Finance_Electives/Economics/06_Elements_of_Market/#price-of-related-products","title":"Price of related products","text":"<p>\\(S \\propto \\frac{1}{p_r}\\)</p> <p>if price of raw materials, transport and complimentary goods like oil increase, then supply decreases</p>"},{"location":"Finance_Electives/Economics/06_Elements_of_Market/#number-of-sellers","title":"Number of Sellers","text":"<p>\\(S \\propto n\\)</p> <p>greater the no of sellers, greater the market supply</p>"},{"location":"Finance_Electives/Economics/06_Elements_of_Market/#producer-sentiments","title":"Producer Sentiments","text":"<p>mindset of producers</p> <ul> <li>if it is positive, then supply increases</li> <li>else supply decreases</li> </ul>"},{"location":"Finance_Electives/Economics/06_Elements_of_Market/#market-equilibrium","title":"Market Equilibrium","text":"<p>market situation when price has reached the level where quantity supplied = quantity demand</p> <p>no tendency for change in price/decisions, as both producers and consumers are satisfied</p> <p>in the long run, excess demand and supply tends to 0</p> <p>Auctions take place until equilibrium is reached</p>"},{"location":"Finance_Electives/Economics/06_Elements_of_Market/#evaluating-equilibrium-pricequantity","title":"Evaluating Equilibrium Price/Quantity","text":"<ol> <li>Get \\(P\\) by equating demand and supply functions and solving them</li> <li>Get \\(Q\\) by substituting \\(P\\) in either function    (both gives the same answer)</li> </ol> <p>we can also draw a graph and obtain the point of interception of the supply and demand curves</p> <p></p>"},{"location":"Finance_Electives/Economics/06_Elements_of_Market/#dis-equilibrium","title":"Dis-equilibrium","text":"<p>Whenever there is a disequilibrium, the market automatically adjusts the price</p> Surplus Shortage Characteristic Excess Supply/Low Demand Excess Demand/Low Supply Buyers not willing to buy Sellers not willing to sell Price Actual &gt; Equilibrium Actual &lt; equilibrium AutomaticCorrection Actual price will reduce to equilibrium price automatically, as there is low demand actual price will increase to equilibrium price automatically, as there is low supply <p>Automatic market correction mechanism always occurs, given that</p> <ol> <li>Prices are flexible</li> <li>Market free from government intervention</li> <li>Both buyers and sellers are equally-informed about the market (internet has helped with making information symmetric); otherwise there will be one or more of the following</li> <li>sellers will manipulate buyers</li> <li>all the sellers will be selling bad products, as it more profitable to do so</li> <li>company recruiters will only get bad candidates as the salary they provide will be amazing for bad candidates, but too low for good candidates      so only the bad candidates will end up accepting the job</li> <li>buyers will be hesitant to pay higher price, even when the seller is justified to ask that much</li> </ol>"},{"location":"Finance_Electives/Economics/06_Elements_of_Market/#govt","title":"Govt","text":"<p>Whenever there is govt intervention in market dis-equilibrium, there will be always be a tendency for the existence of a parallel black market. Black market (not the illegal market one) is a market that sells commodity at price cheaper than govt-issued price.</p>"},{"location":"Finance_Electives/Economics/06_Elements_of_Market/#events","title":"Events","text":"<ol> <li>Determine if the event changes the demand/supply</li> <li>Determine the shift in the curves</li> <li>Find how the equilibrium price and quantity are affected</li> </ol>"},{"location":"Finance_Electives/Economics/06_Elements_of_Market/#change-in-equilibrium","title":"Change in Equilibrium","text":"<p>Equilibrium Geogebra</p> Direction of Shift in Demand &amp; Supply Curve Cause Meaning Right/Left Change in constant Change in factors other than price Angular upward/downward Change in coefficient of price Change in price <p>Imagine an auction</p> Supply const Supply inc Supply dec demand const P sameQ same P \u2b07Q \u2b06 P \u2b06Q \u2b07 demand inc P \u2b06Q \u2b06 P ambiguousQ \u2b06 P \u2b06Q ambiguous demand dec P \u2b07Q \u2b07 P \u2b07Q ambiguous P ambiguousQ \u2b07 <p>Ambiguous \u2013&gt; it depends on the relative change bw the supply and demand</p>"},{"location":"Finance_Electives/Economics/06_Elements_of_Market/#misc","title":"Misc","text":""},{"location":"Finance_Electives/Economics/06_Elements_of_Market/#black-money","title":"Black Money","text":"<p>The money isn\u2019t necessarily illegal, the problem is that the transaction is unofficial, to avoid paying taxes for the transaction.</p>"},{"location":"Finance_Electives/Economics/06_Elements_of_Market/#demonetization","title":"Demonetization","text":"<p>They assumed that all black wealth is in the form of cash, but if you look at data - 0.0002% of total wealth is in the form of cash - only ~1% of black wealth is in cash</p> <p>Demerits were the cost of - Printing the notes - Time of people in queues wasted and hence they did not perform productively - Informal sector shops lost everything, due to the above</p> <p>It mainly did one thing: introduction of a new commodity: old currency</p>"},{"location":"Finance_Electives/Economics/06_Elements_of_Market/#fake-currency","title":"Fake Currency","text":"<p>Fake currency is only an issue if it facilitates illegal activities. However, as long as fake currency is used for legal activities, then there is no problem from the perspective of economics. However, if the proportion of fake money is too large, then there is risk of inflation</p>"},{"location":"Finance_Electives/Economics/06_Elements_of_Market/#monopoly","title":"Monopoly","text":"<p>Monopoly supplier produces at the quantity where Marginal Revenue &amp; Marginal Cost intersect</p>"},{"location":"Finance_Electives/Economics/07_Elasticity/","title":"07 Elasticity","text":""},{"location":"Finance_Electives/Economics/07_Elasticity/#elasticity","title":"Elasticity","text":"<p>Partial derivatives are not a good way for comparing sensitivity, as unit of measurement is different and hence cannot be used</p> <ol> <li>for different commodities, or</li> <li>in different locations</li> </ol> <p>Elasticity is better</p> <ol> <li>unit-free</li> <li>different goods</li> <li>different locations</li> </ol>"},{"location":"Finance_Electives/Economics/07_Elasticity/#price-elasticity-of-demand","title":"Price Elasticity of Demand","text":"<p>Proportional change in demand of commodity wrt proportional change in price of that commodity</p> <p>measure/responsives of the demand of a commodity to a given change in price, in terms of %</p> <p>\\(e\\) is generally negative (demand is generally negatively-related) but for giffen goods, \\(e\\) is positive</p> \\[ \\begin{aligned} e_x^p &amp;= \\frac{(x' - x)/x}{(p' - p)/p} \\\\ &amp;= \\frac{\\% \\Delta x}{\\% \\Delta p} \\\\ &amp;= \\frac{\\color{orange} \\Delta x/x}{\\color{orange} \\Delta p/p} \\\\ &amp;= \\frac{\\color{orange} \\Delta x}{\\color{orange} \\Delta p} \\frac{\\color{orange} p}{\\color{orange}{x}} \\end{aligned} \\] <p>if price of commodity changes by some \\(k \\%\\), demand for it changes by \\(k \\times e \\%\\)</p>"},{"location":"Finance_Electives/Economics/07_Elasticity/#types","title":"Types","text":"<p>Note that the following shows the magnitude only</p> \\(\\vert e_x^p \\vert\\) Demand 0 perfectly inelastic(demand curve vertical - parallel to price axis) \\(0 &lt; \\vert e \\vert &lt; 1\\) inelastic (low sensitivity) 1 unitary elastic \\(1 &lt; \\vert e \\vert &lt; \\infty\\) elastic \\(\\infty\\) perfectly elastic(demand curve horizontal - parallel to demand axis)happens in perfectly-competitive market <p></p>"},{"location":"Finance_Electives/Economics/07_Elasticity/#cases","title":"Cases","text":"<p>Consider profit functions \\(\\pi_1 = f_1(R, \\dots), \\pi_2 = f_2(R, \\dots),\\) where</p> <ul> <li>\\(\\pi =\\) profit of company</li> <li>R = revenue (incorporates pricing of both companies)</li> </ul> <p>Now - if \\(\\frac{\\delta \\pi_2}{\\delta \\pi_1} = 0,\\) perfectly competitive market   the profits of one company is independent of the profits of another company     - eg: agricultural farmers - else imperfect market     - eg: aviation industry</p>"},{"location":"Finance_Electives/Economics/07_Elasticity/#cross-price-elasticity-of-demand","title":"Cross Price Elasticity of Demand","text":"<p>Proportional change in demand of commodity wrt proportional change in price of another commodity</p> <p>proportional change may be % change</p> \\[ \\begin{aligned} e_{x_1}^{p_2} \\% &amp;= \\frac{(x' - x)/x}{(p' - p)/p} \\\\ &amp;= \\frac{\\% \\Delta x_1}{\\% \\Delta p_2} \\\\ &amp;= \\frac{\\color{orange} \\Delta x_1/x_1}{\\color{hotpink} \\Delta p_2/p_2} \\\\ &amp;= \\frac{\\color{orange} \\Delta x_1}{\\color{hotpink} \\Delta p_2} \\frac{\\color{hotpink} p_2}{\\color{orange}{x_1}} \\end{aligned} \\] <p>if price of another commodity changes by some \\(k \\%\\), demand of our main commodity changes by \\(k \\times e \\%\\)</p>"},{"location":"Finance_Electives/Economics/07_Elasticity/#sign","title":"Sign","text":"<ul> <li>-ve for complimentary goods</li> <li>0 for insensitive</li> <li>+ve for substitute goods</li> </ul>"},{"location":"Finance_Electives/Economics/07_Elasticity/#magnitude","title":"Magnitude","text":"<p>shows the degree of complimentarity/substitutability</p>"},{"location":"Finance_Electives/Economics/07_Elasticity/#determinants-of-elasticity-of-demand","title":"Determinants of Elasticity of Demand","text":"<p>i\u2019m using \\(|e|\\) to highlight only the magnitude</p>"},{"location":"Finance_Electives/Economics/07_Elasticity/#availability-of-substitute","title":"Availability of substitute","text":"\\[ |e| \\propto n_s \\] <ul> <li>more substitutes, more elastic<ul> <li>coke, pepsi</li> <li>rice, wheat</li> </ul> </li> <li>no substitutes, inelastic<ul> <li>petrol</li> <li>water</li> </ul> </li> </ul>"},{"location":"Finance_Electives/Economics/07_Elasticity/#type-of-commodity","title":"Type of commodity","text":"<ul> <li>Luxury goods - elastic \\(|e| \\to \\infty\\)</li> <li>Necessities - inelastic \\(|e| \\to 0\\)</li> </ul>"},{"location":"Finance_Electives/Economics/07_Elasticity/#no-of-purposesuses-for-commodity","title":"No of purposes/uses for commodity","text":"\\[ |e| \\propto u \\] <p>more uses, more elastic</p> <ul> <li>eg: milk, steel</li> <li>if price increases, you will stop using for less important stuff</li> </ul> <p>fewer purposes, less elastic</p> <ul> <li>eg: petrol</li> <li>even if prices change, nothing really changes, you can only use petrol for car</li> </ul>"},{"location":"Finance_Electives/Economics/07_Elasticity/#proportion-of-income-spent-on-commodity","title":"Proportion of income spent on commodity","text":"\\[ |e| \\propto p \\] <p>more prop of income required, price elastic</p> <ul> <li>what you buy very frequently</li> <li>milk, petrol</li> </ul> <p>less prop of income required, price inelastic</p> <ul> <li>what you buy less frequently</li> <li>eg: flight tickets, junk food, noodles for me</li> </ul>"},{"location":"Finance_Electives/Economics/07_Elasticity/#time-period","title":"Time Period","text":"\\[ |e| \\propto T \\] <ul> <li>In the shortrun, demand is inelastic</li> <li>In the longrun, demand is elastic</li> </ul> <p>There is more time for consumers to reconsider their decision, to see if it is actually necessary to do so</p>"},{"location":"Finance_Electives/Economics/07_Elasticity/#addictiveness","title":"Addictiveness","text":"\\[ |e| \\propto \\frac 1 a \\] <ul> <li>More addictive product is inelastic</li> <li>less addictive product is elastic</li> </ul>"},{"location":"Finance_Electives/Economics/07_Elasticity/#income-elasticity","title":"Income Elasticity","text":""},{"location":"Finance_Electives/Economics/07_Elasticity/#total-revenue","title":"Total Revenue","text":"<p>basically finding the relationship bw TR and demand</p>"},{"location":"Finance_Electives/Economics/07_Elasticity/#graph","title":"Graph","text":"<ul> <li>y = price</li> <li>x = Total Revenue</li> </ul> \\[ \\begin{aligned} TR &amp;= xp \\\\ \\frac{\\partial (TR)}{\\partial p} &amp;= x[e_x^{p} + 1] \\end{aligned} \\] <p>Changes in total revenue depends on</p> <ol> <li>quantity demand</li> <li>elasticity of demand</li> </ol> <p>The change is based on which factor is of greater magnitude</p>"},{"location":"Finance_Electives/Economics/07_Elasticity/#inelastic","title":"Inelastic","text":"\\[ |e| &lt; 1 \\] <p>For necessities, when price increases, total revenue increases</p> <p>reduction in price is pointless, and increase in price is good for profits</p>"},{"location":"Finance_Electives/Economics/07_Elasticity/#unitary-elastic","title":"Unitary Elastic","text":"\\[ |e| = 1 \\to \\frac{\\partial (xp)}{\\partial p} = 0 \\] <p>graph is vertical line parallel to price</p> <p>proportion of consumers\u2019 income spend does not change with change in price</p>"},{"location":"Finance_Electives/Economics/07_Elasticity/#elastic","title":"Elastic","text":"\\[ |e| &gt; 1 \\] <p>For luxury goods, when price increases, total revenue decreases</p> <p>discounts and offers is always a good policy, cuz the inc in profit due to inc in demand &gt; dec in profits due to dec in price</p>"},{"location":"Finance_Electives/Economics/07_Elasticity/#tr-using-graph","title":"TR using Graph","text":"<p>normal price-demand graph</p> <p>TR = quantity x price = area under graph</p> <p>with change in price</p> <ul> <li>For inelastic demand, the area of graph has minimal change</li> <li>For elastic demand, the area of graph has significant change</li> </ul>"},{"location":"Finance_Electives/Economics/07_Elasticity/#various-points-of-graph","title":"Various Points of Graph","text":"<p>draw geogebra graph</p> <ul> <li>At greater price, demand is elastic</li> <li>At center point, demand is unitary elastic</li> <li>At lower price, demand is inelastic</li> </ul> <p>Revenue is greatest at center point</p> <p></p> <p>This is due to Point Elasticity</p> <p>eg: if price of car goes from 100k to 50k, so many more people will buy but if price of car goes from 10k to 5k, not that many more people will buy</p>"},{"location":"Finance_Electives/Economics/07_Elasticity/#income-elasticity_1","title":"Income Elasticity","text":"<p>Proportional change in demand of commodity wrt proportional change in income of consumers</p> \\[ \\begin{aligned} e_x^m \\% &amp;= \\frac{(x' - x)/x}{(p' - p)/p} \\\\ &amp;= \\frac{\\% \\Delta x}{\\% \\Delta m} \\\\ &amp;= \\frac{\\color{hotpink} \\Delta x/x}{\\color{orange} \\Delta m/m} \\\\ &amp;= \\frac{\\color{hotpink} \\Delta x}{\\color{orange} \\Delta m} \\frac{\\color{orange} m}{\\color{hotpink}{x}} \\end{aligned} \\] <p>For inferior goods, \\(- \\infty &lt; e &lt; 0\\)</p> <p>For normal goods, \\(0 &lt; e &lt; \\infty\\)</p> <ul> <li>For necessities \\(e \\to 0\\)<ul> <li>income inelastic</li> </ul> </li> <li>For luxury goods, \\(e \\to \\infty\\)</li> </ul>"},{"location":"Finance_Electives/Economics/07_Elasticity/#relation-bw-income-elasticity-and-price-elasticity","title":"Relation bw Income Elasticity and Price Elasticity","text":"<p>\\(e^p \\propto e^m\\) High Income Elasticity \\(\\implies\\) High Price Elasticity</p> <p>This is because of</p> <ol> <li> <p>substitution effect</p> <ol> <li>when price decreases, you start buying more</li> <li>when price increases, you just don\u2019t buy this product and start buying alternatives</li> </ol> </li> <li> <p>income effect</p> <p>the real income gets changed</p> <ol> <li>when price decreases, you feel richer, cuz you can now buy more</li> <li>when price increases, you feel poorer, cuz you can now buy less</li> </ol> </li> </ol>"},{"location":"Finance_Electives/Economics/07_Elasticity/#midpoint-formula-for-change","title":"Midpoint Formula for % change","text":"<p>Good as it gives the same answer regardless of direction of change</p> <p>helpful when we don\u2019t know what the initial value, ie we don\u2019t know what \\(\\frac p x\\) is</p> <p>let \\(v\\) be the value</p> \\[ \\begin{aligned} \\% \\text{ change} &amp;= \\frac{\\Delta v}{v_\\text{avg}} \\\\ v'&amp;= \\frac{|\\Delta v|}{(v_1 + v_2)/2} \\end{aligned} \\]"},{"location":"Finance_Electives/Economics/07_Elasticity/#elasticity-of-supply","title":"Elasticity of Supply","text":"<p>Proportional change in the supply of a commodity wrt proportional change of its price</p> <p>Always \\(0 &lt; e &lt; \\infty\\)</p>"},{"location":"Finance_Electives/Economics/07_Elasticity/#perfectly-inelastic","title":"perfectly inelastic","text":"<ul> <li> <p>\\(e \\propto T\\)</p> <ul> <li>sellers have small time to revise their decisions</li> <li>Real Estate in short run</li> </ul> </li> <li> <p>\\(e \\propto \\frac 1 G\\)</p> <ul> <li>or where gestation period is long(time taken to convert raw materials into final good)</li> <li>eg: agrigultural, large machinery</li> </ul> </li> </ul>"},{"location":"Finance_Electives/Economics/07_Elasticity/#perfectly-elastic","title":"Perfectly elastic","text":"<p>perfectly competitive market</p> <p></p>"},{"location":"Finance_Electives/Economics/07_Elasticity/#factors-of-elasticity-of-supply","title":"Factors of Elasticity of Supply","text":""},{"location":"Finance_Electives/Economics/07_Elasticity/#nature-of-commodity","title":"Nature of Commodity","text":"<p>supply is elastic if it is possible to change the amount produced</p> <ul> <li>it is not possible for real estate</li> <li>it is possible for books, cars, manufactured goods</li> </ul>"},{"location":"Finance_Electives/Economics/07_Elasticity/#time","title":"Time","text":"<p>supply is more elastic if suppliers have time to respond</p> <p>supply is more elastic in long run, as there is time to find alternatives</p> <ul> <li>oil was nearly inelastic before cuz there were no other alternatives</li> <li>but now, suppliers have to make decision on production quantity based on how much they think the consumers will buy, as there are other alternatives</li> </ul>"},{"location":"Finance_Electives/Economics/07_Elasticity/#interesting-question-on-farmer","title":"Interesting question on farmer","text":"<p>based on total revenue</p> <p>in slides</p> <ul> <li>the relation for eggs is lagged; the supplier wouldn\u2019t know the future price</li> <li>but for oil, they can do it instantly</li> </ul> <p>but the breaking eggs and reducing oil production only works in the short run, cuz consumers will find other alternatives</p> <p>for medicines, technological change is disliked by pharmaceutical companies</p> <p>but for luxury goods and computer manufacturing it is opposite, cuz demand for computers is elastic; so decrease in price would increase the total revenue </p>"},{"location":"Finance_Electives/Economics/07_Elasticity/#applications","title":"Applications","text":""},{"location":"Finance_Electives/Economics/07_Elasticity/#opec","title":"OPEC","text":"<p>(above)</p>"},{"location":"Finance_Electives/Economics/07_Elasticity/#drugs","title":"Drugs","text":"<p>2 options to address</p> <ul> <li>Interdiction: cracking down on suppliers and restrict the supply for drugs</li> <li>Education: reduce the demand for drugs</li> </ul> <p>Keeping in mind that drugs are price-inelastic, education is more effective cuz</p> Interdiction Education supply dec same demand same dec price inc dec surviving cartels richer poorer addicts poorer better off crime inc dec"},{"location":"Finance_Electives/Economics/07_Elasticity/#immigration","title":"Immigration","text":"<p>the increase in price for luxury housing will be greater than that for cars</p> supply demand luxury housing inelastic elastic cars elastic elastic"},{"location":"Finance_Electives/Economics/07_Elasticity/#point-elasticity","title":"Point Elasticity","text":"<p>This wasn\u2019t taught in class, but I came across during Study Project research.</p> <p>Point elasticity is the elasticity at a point, duh.</p> <p>We know that elasticity is \\(e_x^p = \\dfrac{\\Delta x}{\\Delta p} \\dfrac{p}{{x}}\\). But for a single point, we do not have \\(\\Delta x\\) and \\(\\Delta p\\). So what we do is we take the slope of the graph instead.</p> <p>So the point elasticity of any point is</p> \\[ e_\\text{point} = \\frac{1}{\\text{Slope}} \\times \\frac{p}{{x}} \\]"},{"location":"Finance_Electives/Economics/08_Policies/","title":"Policies","text":"<p>Method to affect consumer behavior</p>"},{"location":"Finance_Electives/Economics/08_Policies/#the-impossible-trinity","title":"The Impossible Trinity","text":"<pre><code>flowchart LR\nfer[Fixed Exchange Rate] &lt;--&gt; fcf[Free Capital Flow] &lt;--&gt; smp[Sovereign Money Policy] &lt;--&gt; fer</code></pre> <p>It is not possible to achieve all 3 simultaneously</p>"},{"location":"Finance_Electives/Economics/08_Policies/#common-types","title":"Common Types","text":"<ol> <li>Price control</li> <li> <p>Incentives</p> </li> <li> <p>Taxes</p> </li> <li> <p>Subsidies</p> <p>Many companies give lower prices for online booking, to reduce waiting times at queues   this is kinda like a subsidy. eg: in global village</p> </li> <li> <p>Laissez-faire (no intervention)</p> </li> <li>Educational intervention</li> </ol> <p>Not very effective, as just providing information to humans does not guarantee that they will act on this information. According to classical economic assumption, this is perfect; refer Behavioral Economics</p>"},{"location":"Finance_Electives/Economics/08_Policies/#government-intervention","title":"Government Intervention","text":"<p>Sometimes, govt may not be satisfied with the market outcome</p> <p>great where market is not perfectly competitive</p>"},{"location":"Finance_Electives/Economics/08_Policies/#price-control","title":"Price Control","text":"<p>when govt believes that market price is unfair to buyers/sellers</p> Price Ceiling Price Floor price limit legal maximum legal minimum govt believes price is high low to protect buyers sellers binding policy below equi price above equi price effects of binding 1. shortage2. non-price rationing3. creation of black market :(4. discrimination by sellers surplus"},{"location":"Finance_Electives/Economics/08_Policies/#price-ceiling","title":"Price Ceiling","text":"<p>sometimes, price ceiling may be 0 for illegal transactions such as prostitution, sale of organs</p>"},{"location":"Finance_Electives/Economics/08_Policies/#non-price-rationing","title":"Non-price rationing","text":"<p>restore equilibrium through imposing limits on buying</p> <p>help solve shortages</p> <p>eg:</p> <ol> <li>purchasing limit for gas cylinders</li> <li>issue tokens</li> <li>long wait times cause the buyers to rethink if they need to buy</li> </ol>"},{"location":"Finance_Electives/Economics/08_Policies/#oil-ceiling","title":"Oil Ceiling","text":"<p>case study</p>"},{"location":"Finance_Electives/Economics/08_Policies/#rent-ceiling","title":"Rent Ceiling","text":"<ul> <li>in short run, it is fine</li> <li>in the long run, causes a huge shortage in houses</li> </ul> <p>\u201cthe best way to destory a city, other than bombing\u201d \ud83e\udd2d</p> <ol> <li>people pay lower rents</li> <li>both rich and poor tenants gain</li> <li>landlords lose</li> <li>maintainence worsens</li> <li>(more points)</li> </ol>"},{"location":"Finance_Electives/Economics/08_Policies/#alternative-to-rent-control","title":"Alternative to Rent Control","text":"<p>Housing subsidies</p> <p>effects</p> <ol> <li>no shortage</li> <li>equilibrium does not change</li> <li>helps only those in need</li> <li>however, the problem is funding the subsidies; which ends up increasing taxes</li> </ol>"},{"location":"Finance_Electives/Economics/08_Policies/#price-floor","title":"Price Floor","text":"<p>if the surplus is not taken care of, the sellers will be unsatisfied</p> <ol> <li>this will cause a black market</li> <li>and consumers will buy from there</li> </ol> <p>The surplus is taken care of through non-price rationing by creating</p> <ol> <li>artificial demand</li> <li>the govt purchases huge stock of agricultural produce and store in times of natural disasters</li> <li>limitation on production quantity</li> </ol> <p>eg:</p> <ul> <li>MSP for agriculture</li> <li>minimum wage<ul> <li>pro</li> <li>child labor dec</li> <li>income of workers inc</li> <li>cons</li> <li>causes unemployment</li> <li>students drop out cuz they get tempted by the salary</li> <li>on-job training reduces, as the companies are paying a lot, so cannot afford to waste time</li> <li>some benefit goes to teens from rich families</li> </ul> </li> </ul>"},{"location":"Finance_Electives/Economics/08_Policies/#alternative-to-minimum-wage","title":"Alternative to minimum wage","text":"<p>wage subsidies for low earners</p> <p>the funding comes from taxing the rich</p> <p>but taxes de-incentives the rich, cuz they don\u2019t like it</p> <p>(i zoned out for this part \ud83d\ude1e)</p>"},{"location":"Finance_Electives/Economics/08_Policies/#case-study-sugar","title":"Case Study: Sugar","text":"<p>US Sugar sellers faced a problem with repaying loans</p> <p>govt created a artificial demand for sugar</p> <p>purchased sugar at high price and sold to ethanol producers at lower price</p> <p>govt faced huge losses</p> <p>but helped farmers pay their loans</p>"},{"location":"Finance_Electives/Economics/08_Policies/#taxes","title":"Taxes","text":"<p>compulsory payment citizens give the govt</p> <ol> <li>raise revenue/funds</li> <li>public projects, infrastucture, police</li> <li>discourage harmful activities</li> <li>(like sugary items)</li> <li>to make society less unfair and reduce income inequality</li> </ol>"},{"location":"Finance_Electives/Economics/08_Policies/#implementation","title":"Implementation","text":"<p>Taxes can be imposed on</p> <ul> <li>buyers</li> <li>sellers</li> <li>both</li> </ul> <p>in all 3 cases of taxation, the money buyers pay and sellers receive is the same</p> <p>Taxes = consumer payment - seller receiving</p> <p>\\(P_b = P_s + T\\)</p>"},{"location":"Finance_Electives/Economics/08_Policies/#types-of-taxes","title":"Types of Taxes","text":""},{"location":"Finance_Electives/Economics/08_Policies/#direct-tax","title":"Direct Tax","text":"<p>imposed on direct income of consumers</p> <p>eg: Wealth, Income Tax</p>"},{"location":"Finance_Electives/Economics/08_Policies/#indirect-tax","title":"Indirect Tax","text":"<p>imposed on purchases/transactions</p>"},{"location":"Finance_Electives/Economics/08_Policies/#specific-tax","title":"Specific Tax","text":"<p>imposed on quantity/volume of commodity</p> <p>eg: </p>"},{"location":"Finance_Electives/Economics/08_Policies/#ad-volremsales-tax","title":"Ad volrem/Sales Tax","text":"<p>imposed on price of commodity</p> <p>eg: GST, VAT</p>"},{"location":"Finance_Electives/Economics/08_Policies/#vat","title":"VAT","text":"<p>Tax imposed on the added value</p>"},{"location":"Finance_Electives/Economics/08_Policies/#effects-of-taxes","title":"Effects of Taxes","text":"<ol> <li>quantity bought and sold reduces</li> <li>both buyers and sellers are affected adversely, regardless of who\u2019s taxed</li> <li>govt earns revenue</li> <li>price that buyers pay \\(\\ne\\) the amount the sellers get</li> </ol>"},{"location":"Finance_Electives/Economics/08_Policies/#effects-on-equi","title":"Effects on equi","text":"<p>equilibrium is maintained, but the equi quantity reduces \\(Q' &lt; Q\\)</p> <p>geogebra</p> <ol> <li>Find the quantity at which    demand-supply = tax (on the left side)</li> <li>Find new points on the supply and demand curve</li> </ol>"},{"location":"Finance_Electives/Economics/08_Policies/#tax-shifting","title":"Tax Shifting","text":"<p>the situation when burden of taxation is shifted from supplier to consumer, or vice-versa</p>"},{"location":"Finance_Electives/Economics/08_Policies/#tax-burdenincidence","title":"Tax Burden/Incidence","text":"<p>burden \\(\\Delta P = |P' - P|\\)</p> <p>loss for supplier = \\(\\Delta Q \\times \\Delta P\\)</p> <p>\\(B \\propto \\frac 1 {|e|}\\)</p> <p>the relative burden depends on elasticity of supply and demand burden is heavier on inelastic side</p>"},{"location":"Finance_Electives/Economics/08_Policies/#cases","title":"Cases","text":"Elasticity Elastic Inelastic Who\u2019s affected more \\(\\vert  e_d \\vert  = \\vert  e_s  \\vert\\) - - both equally \\(\\vert  e_d \\vert  &gt; \\vert  e_s  \\vert\\) demand supply sellers \\(\\vert  e_d \\vert  &lt; \\vert  e_s  \\vert\\) supply demand consumer <p>this is why basic necessities are not taxed, as</p> <ol> <li>demand is inelastic</li> <li>supply is elastic</li> <li>the burden of the taxes is borne by the farmers</li> <li>this would cause a shortage</li> </ol>"},{"location":"Finance_Electives/Economics/08_Policies/#case-study","title":"Case Study","text":"<p>in 1990, US imposed a new luxury tax, such as on yachts, with the objective of raising funds from the rich</p> <p>didn\u2019t work and repealed after a few years, because</p> <ol> <li>demand for yachts is elastic</li> <li>firstly, it is a luxury good</li> <li>there are many substitutes and \\(|e| \\propto n_s\\)</li> <li>supply for yachts is inelastic, or atleast in the short run</li> <li>hence, the producers felt the burden of the tax, rather than the rich consumers</li> </ol>"},{"location":"Finance_Electives/Economics/08_Policies/#subsidies","title":"Subsidies","text":"<p>compulsory payment citizens receive from the govt</p> <p>it is the exact opposite of tax</p> <p>uses</p> <ol> <li>help people in need</li> <li>+ve incentive, used to encourage certain actions</li> </ol> <p>Subsidies can be given to</p> <ol> <li>buyers</li> <li>sellers</li> <li>both</li> </ol> <p>price received by sellers = price paid by buyers + subsidies</p> <p>\\(P_s = P_b + S\\)</p>"},{"location":"Finance_Electives/Economics/08_Policies/#effects","title":"Effects","text":"<ol> <li>Price received by \\(\\ne\\) Price paid by buyers</li> <li>Buyers pay less, sellers receive more</li> <li>Govt spending and investment increases</li> </ol>"},{"location":"Finance_Electives/Economics/08_Policies/#effects-on-equilibrium","title":"Effects on Equilibrium","text":"<p>equilibrium is maintained, but the equi quantity increases \\(Q' &gt; Q\\)</p> <p>Geogebra</p> <ol> <li>Find the quantity at which    supply-demand = tax (on the right side)</li> <li>Find new points on the supply and demand curve</li> </ol>"},{"location":"Finance_Electives/Economics/08_Policies/#benefit","title":"Benefit","text":"<p>depends on the elasticity</p> <p>\\(B \\propto \\frac 1 {|e|}\\) (same like taxes)</p> <p>benefit is more for inelastic side</p> <p>\\(B_b + B_s = S\\)</p> <p>if both are inelastic, the benefits depends on the ratio of elasticity</p> <ul> <li>eg: For agricultural subsidies, it depends</li> <li>eg: For oil,</li> </ul>"},{"location":"Finance_Electives/Economics/08_Policies/#best-policies","title":"Best Policies","text":"<ul> <li>subsidies for basic necessities</li> <li>taxes for luxuries</li> </ul>"},{"location":"Finance_Electives/Economics/08_Policies/#types-of-policies","title":"Types of Policies","text":"Policy Incorporates Monetary Central Bank Interest Rates Fiscal policy Ministry of Finance (Govt) Budget, Tax"},{"location":"Finance_Electives/Economics/09_International_Trade/","title":"09 International Trade","text":""},{"location":"Finance_Electives/Economics/09_International_Trade/#benefit","title":"Benefit","text":"<p>Graphs</p>"},{"location":"Finance_Electives/Economics/09_International_Trade/#consumer-surplus","title":"Consumer Surplus","text":"<p>Benefit that consumers enjoy when they purchase goods and services</p> \\[ \\begin{aligned} &amp;= \\text{area under demand curve and above equilibrium price line}  \\\\ &amp;= \\text{willingness to pay - total payment} \\\\ &amp;= \\int \\limits_0^{Q} Q_d - \\int \\limits_0^{P} P_d \\end{aligned} \\] <p>if the value associated with a commodity is </p> <ul> <li>high, surplus is high</li> <li>low, surplus is low</li> </ul> <p>\\(CS \\propto \\frac 1 {P_{eq}}\\)</p> <p>\\(CS \\propto \\frac{1}{|e|}\\)</p> P \\(Q_d\\) \\(P_{eq} - P\\) 1 8 6 2 7 4 3 6 2 4 5 0 5 4 -2 <p>Let\u2019s say that 4 is the equi price</p> <p>The Consumer surplus = 6 + 4 + 2 = 12</p>"},{"location":"Finance_Electives/Economics/09_International_Trade/#supplier-surplus","title":"Supplier Surplus","text":"<p>It is basically the profit</p> \\[ \\begin{aligned} &amp;= TR - \\text{Cost} \\\\ &amp;= \\text{area above supply curve and below price line} \\\\ &amp;= \\int \\end{aligned} \\] P \\(Q_s\\) \\(P_{eq} - P\\) 1 1 3 2 2 2 3 3 1 4 4 0 5 5 -1 <p>PS = 3 + 2 + 1 = 6</p> <p>\\(PS \\propto P_{eq}\\)</p> <p>\\(PS \\propto |e|\\)</p>"},{"location":"Finance_Electives/Economics/09_International_Trade/#total-benefit","title":"Total Benefit","text":"<p>sum of producer and consumer surplus</p>"},{"location":"Finance_Electives/Economics/09_International_Trade/#trade","title":"Trade","text":"<p>When you open for trade, the domestic price tends to the international price.</p> <p>country will</p> <ul> <li>export if domestic price &lt; international price</li> <li>import if domestic price &gt; international price</li> </ul>"},{"location":"Finance_Electives/Economics/09_International_Trade/#export","title":"Export","text":"<ul> <li>the supply in domestic market decreases</li> <li>domestic price increases</li> <li>domestic demand decreases</li> </ul> <p>Then the export = domestic supply - domestic demand</p>"},{"location":"Finance_Electives/Economics/09_International_Trade/#effects","title":"Effects","text":"<p>graph</p> <p>CS decreases</p> <p>PS increases</p> <p>However, there is a net benefit from trade (small triangle)</p>"},{"location":"Finance_Electives/Economics/09_International_Trade/#import","title":"Import","text":"<ul> <li>the supply in domestic market inc</li> <li>domestic price dec</li> <li>domestic demand inc</li> </ul> <p>Then the import = domestic demand - domestic supply</p>"},{"location":"Finance_Electives/Economics/09_International_Trade/#effects_1","title":"Effects","text":"<p>CS increases</p> <p>PS decreases</p>"},{"location":"Finance_Electives/Economics/09_International_Trade/#idk","title":"IDK","text":"<p>The total surplus is always +ve in free trade, due to specialization and benefits of scale.</p> <p>T</p>"},{"location":"Finance_Electives/Economics/09_International_Trade/#pareto-optimal-policy","title":"Pareto optimal policy","text":"<p>any govt policy, where some people benefit and some lose out, is optimal if those who benefit can compensate for the people who lose out</p>"},{"location":"Finance_Electives/Economics/09_International_Trade/#tariffs","title":"Tariffs","text":"<p>tax on imports</p> <p>it is economically bad, the losses of the consumers is not compensated but it is socially good</p> <p>Tariffs are better than quota, because it generates revenue for the govt Quota - govt restricts the quantity of commodity imported/exported</p>"},{"location":"Finance_Electives/Economics/09_International_Trade/#effects_2","title":"Effects","text":"<p>Assuming that the country is small, such that tariffs does not affect world price</p> <ul> <li>imports and total surplus decreases (closer to no-trade equi), but</li> <li>consumer surplus dec</li> <li>producer surplus inc</li> <li>domestic price increases</li> <li>domestic suppliers are protected</li> <li>unemployment doesn\u2019t rise</li> <li>Govt earns tariff revenue = quantity imported x tariff rate</li> </ul> <p>the reduction in total surplus due to a govt policy is called deadweight loss of the policy</p>"},{"location":"Finance_Electives/Economics/09_International_Trade/#benefits-of-free-trade","title":"Benefits of Free Trade","text":"<ol> <li>increased variety of goods</li> <li>lower costs of production</li> <li>increased competition</li> <li>higher incentives</li> <li>better ideas</li> </ol>"},{"location":"Finance_Electives/Economics/09_International_Trade/#why-do-tariffs-exist","title":"Why do Tariffs exist","text":"<p>we need tariffs because</p> <p>without tariffs</p> <ol> <li>jobs are shipped abroad</li> <li>national security endangered</li> <li>highly dependent on another country</li> <li>this is why US is able to interfere in other countries policies, as other countries are dependent on them</li> <li>infant industries protected</li> <li>avoids unfair competition</li> <li>cheap labor</li> <li>environmental standards</li> <li>tariffs as a political tool</li> <li>reduces inequality</li> </ol>"},{"location":"Finance_Electives/Economics/10_Costs/","title":"10 Costs","text":""},{"location":"Finance_Electives/Economics/10_Costs/#market-power","title":"Market Power","text":"<p>It is the power of firms to control their selling price. It allows them to determine their own profits. It depends on</p> <ul> <li>type and intensity of competition</li> <li>costs of production<ul> <li>this influences the type of competition in market, because it is cheaper for one firm to produce at a larger scale</li> <li>big companies threaten small by lowering their price, to make them run out of business; otherwise acquire their company</li> <li>eg: Facebook-Instagram</li> </ul> </li> </ul> <p>Sometimes, consumer may have greater market power. This occurs when there is only a sole consumer, but many sellers eg: Dubai Metro</p>"},{"location":"Finance_Electives/Economics/10_Costs/#competition-in-markets","title":"Competition in markets","text":""},{"location":"Finance_Electives/Economics/10_Costs/#monopoly","title":"Monopoly","text":"<p>1 seller</p>"},{"location":"Finance_Electives/Economics/10_Costs/#duopoly","title":"Duopoly","text":"<p>2 sellers</p>"},{"location":"Finance_Electives/Economics/10_Costs/#oligopoly","title":"Oligopoly","text":"<p>few sellers have control, but not as much as monopoly</p> <p>eg: telecom</p>"},{"location":"Finance_Electives/Economics/10_Costs/#monopolistic","title":"Monopolistic","text":"<p>there are large no of sellers</p> <p>but no of sellers is small enough that if one seller changes their price, other sellers will tend to change their price also in reaction</p> <p>eg: aviation industry, snacks</p>"},{"location":"Finance_Electives/Economics/10_Costs/#perfectly-competitive-market","title":"Perfectly-competitive Market","text":"<p>best for consumers</p> <p>when there are large no of sellers and buyers</p> <ul> <li>no of sellers large enough such that change in price by one seller should not affect other sellers\u2019 decision</li> <li>no of buyers large enough such that decision by one buyer should not affect other buyers\u2019 decision</li> </ul> <p>if sellers inc price, then the demand for their product becomes 0, as no one will be willing to buy from them.</p> <p>eg</p> <ul> <li>agricultural industry<ul> <li>one farmer stops producing rice does not affect other rice farmers</li> </ul> </li> <li>food consumption<ul> <li>one buyer stops eating rice does not affect other buyers of rice</li> </ul> </li> </ul>"},{"location":"Finance_Electives/Economics/10_Costs/#profit-maximization","title":"Profit Maximization","text":"<p>Every company tries to maximize its profit.</p> <p>Profits can be maximized by cost minimization through optimization of input mix. The input mix depends on</p> <ol> <li>productivity of input factor (high is preffered)</li> <li>price of input factor (low is preffered)</li> </ol> <p>Labor is preferred due to lower price; capital is preferred due to higher productivity</p> <p>$$ \\text{Profit}_\\max = \\max_p \\Bigg{ p q_m - c \\Big( q_m(p) \\Big) \\Bigg} \\ \\implies c'(q_m) = p_m + \\dfrac{q_m}{q'_m(p_m)} $$ where </p> <ul> <li>\\(p\\) is unit sale price</li> <li>\\(q(p)\\) is units sold</li> <li>\\(c\u00a0\\Big( q(p) \\Big)\\) is cost</li> <li>\\(c' \\Big( q(p) \\Big)\\) is the marginal cost</li> </ul>"},{"location":"Finance_Electives/Economics/10_Costs/#problem-with-capitalism","title":"Problem with Capitalism","text":"<p>humans are getting replaced by machines. This is causing consumers to run out of money. Wealth is heavily-skewed, to the point that consumers cannot buy anymore. Most recessions have been due to low demand, not low supply</p> <p>Producers produce for themselves, by themselves</p> <p>Karl Marx said</p> <p>Capitalism</p>"},{"location":"Finance_Electives/Economics/10_Costs/#efficiency","title":"Efficiency","text":"<p>input mix is said to be</p> <ul> <li>Economically efficient   if the costs associated with an input mix is minimal </li> <li>Technically efficient   if it is impossible to maintain the same output, without keeping the same inputs   if any input decreases, the output also decreases<ul> <li>in india, we have a technically inefficient agricultural sector. This is proved by the fact that:   even though many people moved to tertiary sector, it did not affect agricultural output</li> </ul> </li> </ul>"},{"location":"Finance_Electives/Economics/10_Costs/#costs","title":"Costs","text":"<p>Total cost = explicit + implicit</p> <p>Explicit costs are direct. eg: raw materials, labor costs</p> <p>Implicit costs are indirect. eg:</p> <ol> <li>i work in my own restaurant. I lost my time, which could\u2019ve been used somewhere else like some other restaurant/company</li> <li>investing in your company. I could\u2019ve invested it somewhere else, which could\u2019ve earned me more money</li> </ol>"},{"location":"Finance_Electives/Financial_Forensics/","title":"Financial Forensics","text":""},{"location":"Finance_Electives/Financial_Forensics/#references","title":"References","text":"<ul> <li> Finacial Forensics | IIT Madras</li> </ul>"},{"location":"Finance_Electives/Fintech/","title":"Fintech","text":""},{"location":"Finance_Electives/Fintech/#references","title":"References","text":"<ul> <li> https://www.youtube.com/watch?v=90JWoR9MfYU&amp;list=PLUl4u3cNGP61Q_RVDn6srWbLV_zFnd9n0</li> </ul>"},{"location":"Finance_Electives/Lean%20Six%20Sigma/","title":"Lean Six Sigma","text":""},{"location":"Finance_Electives/Lean%20Six%20Sigma/#references","title":"References","text":"<ul> <li>https://www.youtube.com/playlist?list=PLUl4u3cNGP60dsF8ICyGEiSJcTYzQWSVZ</li> </ul>"},{"location":"Finance_Electives/MBFM/","title":"Money, Banking &amp; Financial Markets","text":""},{"location":"Finance_Electives/MBFM/#references","title":"References","text":""},{"location":"Finance_Electives/MBFM/#recommended-reading","title":"Recommended Reading","text":"<ul> <li> Trading and Exchanges: Market Microstructure for Practitioners</li> </ul>"},{"location":"Finance_Electives/MBFM/01_Introduction/","title":"Introduction","text":""},{"location":"Finance_Electives/MBFM/01_Introduction/#financial-system","title":"Financial System","text":""},{"location":"Finance_Electives/MBFM/01_Introduction/#trade-types","title":"Trade Types","text":"Goal Related variables Hedging Already have exposureNot proactively adding risk to what you have Currency exchange rateInterest rate Market Making Earn from bid offer Proprietary Trading/Portfolio Management Maximize returns Directional: Long/ShortArbitrage: Find relationship between prices &amp; profit from mispricingValue/Relative ValueSystematic modellingFundamental analysis"},{"location":"Finance_Electives/MBFM/01_Introduction/#parties-of-financial-markets","title":"Parties of Financial Markets","text":"Parties Individual/Retail Investors Dealers Trade with 1 one interested party when there is no market Take principal risks Brokers Intermediary between 2 trade parties Don\u2019t take principal risks Mutual Funds Manage public-investors\u2019 money in a long-only format Insurance Companies Pension Funds Asset Managers Sovereign Wealth Funds Hedge Funds Find opportunities from inefficient market positioning/pricing Private Equity Invest in companies Governments Policy MakersIntervene in certain cases Corporate Hedgers Liability Managers"},{"location":"Finance_Electives/MBFM/01_Introduction/#financing-types","title":"Financing Types","text":"Advantages Disadvantages Direct - Information asymmetry- Risk Indirect - Information asymmetry alleviation- Risk alleviation"},{"location":"Finance_Electives/MBFM/02_Financial_Markets/","title":"Financial Markets","text":"<p>Mechanism that facilitates trade of financial securities between</p> <ul> <li>savers/investors/lenders (have money, need return)</li> </ul> <p>and</p> <ul> <li>borrowers (need money, have risks)</li> </ul> <p>Zero-sum game</p>"},{"location":"Finance_Electives/MBFM/02_Financial_Markets/#functions","title":"Functions","text":"<ul> <li> <p>Provide liquidity (key function)</p> </li> <li> <p>Avoids need for coincidence of wants</p> </li> <li> <p>Price discovery</p> </li> <li> <p>Reduce total costs due to benefits of scale</p> </li> <li> <p>Base for capital formation</p> </li> <li> <p>Economic stability</p> </li> <li> <p>Innovation</p> </li> <li> <p>Helps in continuous flow of money</p> </li> </ul>"},{"location":"Finance_Electives/MBFM/02_Financial_Markets/#type","title":"Type","text":""},{"location":"Finance_Electives/MBFM/02_Financial_Markets/#idk","title":"Idk","text":"Bond Market Stock Market Derivative Market Forex Market"},{"location":"Finance_Electives/MBFM/02_Financial_Markets/#idk_1","title":"Idk","text":"Money Market Capital Market Maturity Duration Short-term(&lt; 1 yr) Long-term(&gt;= 1 yr) Volume High Low/High Regulator Central Bank Capital Market Regulator/SEC(except for Govt Bonds)Central bank (for cross-border transactions) Cost of capital Cheaper(lower risk) Expensive Participants Central bankLarge corporations Commercial BanksRetail investors/borrowers Purpose Borrowers: Working capitalLenders: Investing temporary overage Borrowers: ExpenditureLenders: Long-term investments Comment Interest-bearing instruments are usually zero-coupon bonds Comment Money doesn\u2019t actually flow, it\u2019s just recorded on financial statements <p>Why are bonds preferred over equity</p> <ul> <li>Retaining ownership &amp; control</li> <li>Equity may cause noise in valuation, due to large number of players</li> <li>Tax deductibility</li> </ul> <p>Call money</p>"},{"location":"Finance_Electives/MBFM/02_Financial_Markets/#money-market-instruments","title":"Money Market Instruments","text":"Instrument Participant Comment Call &amp; Notice Commercial banks Central bank audits every 2 weeks to check for reserves, so commercial banks take short-term loans to maintain liquidity for the inspection Treasury Bill Central govtExercised by Central bank Short-term bondHelps maintain short-term liquidity Commercial bills 2 businesses Unpaid invoice that can be traded Commercial Paper 2 businesses Promissory note Money-Market Mutual Funds Public Synthetically-createdPool funds to take different positions in money market Repo/Reverse-Repo Central bankCommercial bank Short-term loanRe-purchase agreement"},{"location":"Finance_Electives/MBFM/02_Financial_Markets/#idk_2","title":"IDK","text":""},{"location":"Finance_Electives/MBFM/02_Financial_Markets/#sources-of-corporate-debt","title":"Sources of Corporate Debt","text":"<ul> <li>Sorted in order of least risky to more risky</li> <li>Also, Sorted in order of least return to highest return</li> </ul> Source Duration Market Treasury Bill(T-Bill) Short term Money Govt Bond Long term Corporate Bond (Zero Coupon)(More traded) Long-term Corporate Bond (Coupon) Long-term Commercial Paper Short-term CD (Certificate of Deposit) Rapport"},{"location":"Finance_Electives/MBFM/02_Financial_Markets/#idk_3","title":"IDK","text":"Primary Market Secondary Market(Stock change) Investors trade with Company directly Each other"},{"location":"Finance_Electives/MBFM/02_Financial_Markets/#primary-market","title":"Primary Market","text":"<ul> <li>Initial Public Offering</li> <li>Follow-up public offering</li> </ul> Instrument Participants Requirements Advantages Limitations Public Issue General public High transparencyProspectus (detailed document of company)Under-writer No filteringHigh costsLarge number of people to convincePotential under-subscription Private placement Selected subset of new investors Overcomes limitations of public issue Right issue Existing shareholders preferredThen only general public Prefer dedicated investors Bonus issue Existing shareholders Give instead of dividends Maintain share priceBroadening of shares through distributing bonus shares makes it harder for Hostile Takeover Base Rate Central bankCommercial Bank"},{"location":"Finance_Electives/MBFM/02_Financial_Markets/#secondary-market-instruments","title":"Secondary Market Instruments","text":"<p>Exchanges may be order-driven and quote-driven</p> Order-Driven Quote-Driven Direct Indirect Party Buyer &amp; Seller Buyer-Dealer-Seller Settlement credited \\(t+1\\) basis Advantages Example NSE NASDAQ"},{"location":"Finance_Electives/MBFM/02_Financial_Markets/#indian-stock-exchanges","title":"Indian Stock Exchanges","text":"<ul> <li>Volume of trades: 92.7%, 7.3% BSE</li> <li>Volume of trades market: 97.2% NSE, 1.8% BSE</li> </ul> <pre><code>---\ntitle: Stocks Trade Volume Share in India\n---\npie\n\"NSE\": 92.7\n\"BSE\":  7.3\n\"Others\": 0</code></pre> <pre><code>---\ntitle: Derivatives Trade Volume Share in India\n---\npie\n\"NSE\":      97.2\n\"BSE\":       1.8\n\"Others\":  1.0</code></pre>"},{"location":"Finance_Electives/MBFM/02_Financial_Markets/#international","title":"International","text":"Value of Stocks(not volume)(Trillions) NYSE 26.64 NASDAQ 23.46 Shanghai 7.63 Euronext 7.33 Japan 6.79 Hong Kong 6.13 Shenzhen 5.74 London 4.05 BSE 3.96 NSE 3.77"},{"location":"Finance_Electives/MBFM/02_Financial_Markets/#us-stock-market","title":"US Stock Market","text":"<ul> <li>Stock Exchanges</li> <li>New York Stock Exchange (Blue Chip Industrial Companies, 2800, hybrid-both broker and dealer market)</li> <li>Nasdaq (Technology-driven Companies, 3300, totally electronic, dealer market)</li> <li>Stock Indices</li> <li>Dow Jones Industrial Average (DJIA, Top 30 Comp from NYSE and Nasdaq, Price Weighted)</li> <li>S&amp;P 500 (Top 500 Comp headquartered in the US, Market Weighted)</li> <li>Nasdaq Composite (Top 2500 Comp of Nasdaq, be it headquartered in the US or Outside, Market Weighted)</li> <li>Russell 2000 (Small Cap Companies)</li> </ul>"},{"location":"Finance_Electives/MBFM/02_Financial_Markets/#advantages-of-us-market","title":"Advantages of US Market","text":"<ul> <li>Fractional investment: Mutual funds are basically from this idea</li> <li>Selection effect: The fact that these companies are on the US stock market means they have already been successful on their local domestic market</li> <li>Geographical diversification</li> <li>US and US-listed companies tend to have global operations</li> <li>US market has outperformed other markets</li> <li>Exposure to high tech companies</li> <li>Avoid currency depreciation wrt the global principal reserve currency (which is currently US Dollars)</li> </ul>"},{"location":"Finance_Electives/MBFM/02_Financial_Markets/#disadvantages-of-us-market","title":"Disadvantages of US Market","text":"<ul> <li>Set</li> <li>High charges</li> <li>Bank\u2019s fixed remittance charges     (1500 - 2000)</li> <li>GST</li> <li>Exchange Rates</li> <li>High brokerage and maintenance charges</li> <li>Tax Issues (DTAA)</li> <li>STCG within 24 months: 30%</li> <li>LTCG after 24 months: 20%.</li> </ul>"},{"location":"Finance_Electives/MBFM/02_Financial_Markets/#uae-stock-market","title":"UAE Stock Market","text":"<ul> <li>Abu Dhabi Securities Exchange (ADX) (2000)</li> <li>ADX General Index is a capitalization-weighted index which represents the performance of all the listed companies (92 securities) on the exchange (A base value of 1000 as of June 2001)</li> <li>Regulated by the Securities and Commodities Authority (SCA)</li> <li>Dubai Financial Market (DFM) (2000)</li> <li>Dubai Financial Market General Index (DFMGI) (119 securities)</li> <li>Modified capitalization-weighted index (maximum capitalization cap of 20%)</li> <li>NASDAQ Dubai (2005)</li> <li>Middle East's international financial exchange (70 securities)</li> <li>Dubai Financial Market holds two-thirds and Borse Dubai holds one-third of the shares in NASDAQ Dubai</li> <li>Regulated by Dubai Financial Services Authority (DFSA)</li> </ul>"},{"location":"Finance_Electives/MBFM/02_Financial_Markets/#sukuk","title":"Sukuk","text":"<p>\"sharia compliant\" bonds</p> <p>Sukuk is an Islamic instrument that provides the same commercial equivalent to a conventional bond, the difference being that it is structured in a sharia compliant manner and represents proportionate undivided ownership in the underlying asset or investment.</p> <p></p> <pre><code>sequenceDiagram\n\nparticipant i as Investor&lt;br/&gt;(Lender)\nparticipant s as SPV&lt;br/&gt;(Special Purpose Vehicle)\nparticipant b as Originator&lt;br/&gt;(Borrower)\n\ni -&gt;&gt; s: Investment\ns -&gt;&gt; b: Loan&lt;br/&gt;(Investment Proceeds)\nb -&gt;&gt; s: Sale of Asset(s)\nb -&gt;&gt; s: Lease &amp; Principal Payments\ns -&gt;&gt; i: Returns&lt;br/&gt;(Lease &amp; Principal Payments proceeds)\ns -&gt;&gt; b: Repurchase of assets</code></pre>"},{"location":"Finance_Electives/MBFM/02_Financial_Markets/#trading-platforms","title":"Trading Platforms","text":"<ul> <li>IND</li> <li>Money</li> <li>Vested</li> <li>Groww</li> <li>Stockal</li> <li>Winvesta</li> </ul>"},{"location":"Finance_Electives/MBFM/02_Financial_Markets/#capital-market-participants","title":"Capital Market Participants","text":"Meaning Ownership in India Daily Trading Volume in India Daily Trading Volume in Developed Countries Retail Investors Individuals 18% 42-45% 18-20% Mutual Funds Institutional Investors Professional institutions whose career are investing (such as Investment Banks)Corporates that invest (Shark Tank) FIIs(Foreign Institution Investor) Hedge Funds"},{"location":"Finance_Electives/MBFM/02_Financial_Markets/#retail-investment-in-developed-nations-in-india-but-trade-volume-is-lower","title":"Retail investment in Developed nations &gt; in India, but trade volume is lower","text":"<p>Indian stocks have lower return than risk-free returns, so Indian retail investors actually incur losses. India is still a developing country, and hence stock investing is being popular.</p> <p>In developed nations, they prefer investing indirectly through mutual funds.</p>"},{"location":"Finance_Electives/MBFM/02_Financial_Markets/#participant-objectives","title":"Participant Objectives","text":"Run Duration Focus Look for Investors Long Business ValueCapital appreciation Under-Valued stocks Speculators Short Volatility Arbitrageurs Mis-pricing <p>Arbitrage is the \u2018invisible hand of market\u2019 that corrects the </p>"},{"location":"Finance_Electives/MBFM/02_Financial_Markets/#bond-market","title":"Bond Market","text":"G-Secs Market Maturity: 1-30yrs PSU MarketPublic Sector Undertakings State Govt/Municipal Bonds Corporate Debentures/Bonds <p>Terms of the bond are called as \u2018bond indenture\u2019</p> <p>Thinking point: The share of corporate bonds in India in bond market is very low compared to other countries.</p>"},{"location":"Finance_Electives/MBFM/02_Financial_Markets/#indian-stock-markets","title":"Indian Stock Markets","text":"Index BSE(Bombay Stock Exchange) Sensex NSE(National Stock Exchange) Nifty <p>LPG: Liberalization, Privatization, Globalized</p>"},{"location":"Finance_Electives/MBFM/02_Financial_Markets/#ponzi-schemes","title":"Ponzi Schemes","text":"<p>Financial schemes </p>"},{"location":"Finance_Electives/MBFM/02_Financial_Markets/#why-are-us-markets-institutions-the-best","title":"Why are US markets &amp; institutions the best","text":"<ul> <li>Entrepreneurial ambitions; no societal discouraging like in India</li> <li>Supportive financial system, such as supportive IBC</li> </ul>"},{"location":"Finance_Electives/MBFM/02_Financial_Markets/#ibc","title":"IBC","text":"<p>IBC: Insolvency &amp; Bankruptcy Code</p> <p>Defines the liberty for corporates</p> <p>If a company is insolvent and you are not able to repay loans, within a period of 90 days, the lenders can request for the company\u2019s liquidation.</p>"},{"location":"Finance_Electives/MBFM/02_Financial_Markets/#stock-market-indices","title":"Stock Market Indices","text":"Types Preferred for Limitations Market Value-Weighted Company Performance Price-Weighted Price Performance Can be manipulated by changing number of stocks"},{"location":"Finance_Electives/MBFM/02_Financial_Markets/#indian","title":"Indian","text":"Number Characteristic BSE Sensex 30 Largest and most-actively traded stocks NSE Nifty 50 Largest and most liquid <p>Both are Market Value-Weighted</p>"},{"location":"Finance_Electives/MBFM/02_Financial_Markets/#trading-schedules","title":"Trading Schedules","text":"09:00-09:08 Request Bulk Trades 09:08-09:12 Clear Bulk Trades 09:12-09:15 Buffer 09:15-15:30 Trading Session"},{"location":"Finance_Electives/MBFM/02_Financial_Markets/#transaction-process","title":"Transaction Process","text":"<ul> <li>Trading</li> <li>Post Trading Activities: Clearing &amp; Settlement</li> </ul> <p>Agencies</p> NSCCLNational Securities Clearing Corporation Ltd NSE ICCLIndian Clearing Corporation Ltd BSE"},{"location":"Finance_Electives/MBFM/02_Financial_Markets/#orders","title":"Orders","text":"Bids Buy Offers Sell"},{"location":"Finance_Electives/MBFM/02_Financial_Markets/#order-types","title":"Order Types","text":"Limit Orders Specifies price- Sell Limit Order- Buy Limit Order Market Orders Does not specify price Stop Orders - Sell\u2013stop order- Buy\u2013stop order"},{"location":"Finance_Electives/MBFM/02_Financial_Markets/#depository","title":"Depository","text":"<ul> <li>Demat Account</li> <li>Depository Services</li> <li>NSDL: National Securities     Depository Ltd</li> <li>CDSL: Central Depository Services     (India) Ltd</li> </ul>"},{"location":"Finance_Electives/MBFM/02_Financial_Markets/#bear-vs-bull","title":"Bear vs Bull","text":"Bull Bear Stock prices are rising falling Market Momentum Effect Stronger Weaker <p>The origin of these expressions is unclear, but one reason could be that bulls attack by bringing their horns upward, while bears attack by swiping their paws downward</p>"},{"location":"Finance_Electives/MBFM/02_Financial_Markets/#broker-vs-dealer-market","title":"Broker vs Dealer Market","text":"Dealer(Over-the-counter) Broker Exchanges Counterparty for both buyers and sellers Finds a counterparty to both buyers and sellers Most automated Dealer sets bid and asks prices for the security in question, and will trade with any investor willing to accept those prices The advantage of the exchange is the provision of a central location for buyers and sellers to find their own counterparties Intermediary Ownership \u2705 \u274c Intermediary Risk \u2705 \u274c Barriers to entry High Low dealer provides liquidity in the market at the cost of a small premium often set bid prices lower than the market and ask prices higher"},{"location":"Finance_Electives/MBFM/02_Financial_Markets/#idk_4","title":"IDK","text":"Example Adverse Selection Sub-optimal selection of borrower by lenders, which may lead to default Moral Hazard Incurred Satyam IT Debt collections?"},{"location":"Finance_Electives/MBFM/02_Financial_Markets/#financial-year","title":"Financial Year","text":"<p>Why does financial year in India start in April: This is to follow the agricultural cycles</p>"},{"location":"Finance_Electives/MBFM/03_Financial_Institutions/","title":"Financial Institutions","text":"<p>Non-individuals/organizations that</p> <ul> <li>Intermediary</li> <li>acts as mobilizers</li> <li>depositors of savings</li> <li> <p>dealers of credit/finance</p> </li> <li> <p>Non-Intermediary</p> </li> <li>Monitor</li> <li>Coordinate</li> <li>Protect rights</li> <li>Intervene (if required)</li> </ul>"},{"location":"Finance_Electives/MBFM/03_Financial_Institutions/#types","title":"Types","text":"Type Regulated by Primary Function Secondary Function India US Intermediary Banking Financial institution licensed to accept deposits &amp; provide loans Central Bank Accept depositsLending loans Agency functionGeneral utility services Non-Banking Cannot accept \u2018loans\u2019 Forgot Non-Intermediary Banking Regulators RBI Insurance Regulators IRTA Securities Regulators SAVEE?SEBI SEC(Security Exchange Committee) <p>Universal bank that is involved in banking &amp; non-banking activities</p>"},{"location":"Finance_Electives/MBFM/03_Financial_Institutions/#types-of-banks","title":"Types of Banks","text":"Type Owners Goal Size Commercial Private Profit Large Cooperative Community(Owners = Customers) Non-Profit Small Regional Rural Government (Finance Ministry) Non-ProfitProvide financing in communities where not available Small Investment Fixed IncomeEquityIBDCorporate FinanceAsset ManagementWealth Management"},{"location":"Finance_Electives/MBFM/03_Financial_Institutions/#non-banking-institutions","title":"Non-Banking Institutions","text":"Purpose Non-Banking Financial Companies Bajaj Finance LtdMahindra Finance ServicesMuthoot FinanceCholamandalam Development Financial Institutions IFCI: Industrial Finance Corporation of IndiaICICI: Industrial Credit and Investment Corporation of IndiaIDBIIRCISIDBIHDFC Insurance Companies Protection against contingent/uncertain loss LICICICI Prudential Life Insurance CompanyGeneral Insurance CompanyThe New India Assurance Company Mutual Funds Pool funds of any amount invested by Asset Management Company with Custodians to inspect UTISBI Mutual FundAxis Bank Mutual Fund Index Funds Pooled investments that passively aim to replicate the returns of market indexes <p>Number of issues shares controls who can invest</p> <ul> <li>Stability: Decrease number of shares to increase share price, and exclude retail investors</li> <li>High Returns: Increase number of shares to decrease share price, and include retail investors</li> </ul>"},{"location":"Finance_Electives/MBFM/04_Financial_Services/","title":"Financial Service","text":"<p>The process of holding shares in electronically is Dematerialisation</p>"},{"location":"Finance_Electives/MBFM/04_Financial_Services/#factoring","title":"Factoring","text":"<p>Service for a business redeem its accounts receivable from a 3<sup>rd</sup> party at a discount, to improve current liquidity</p> <pre><code>flowchart LR\nCompany --&gt;\n|Sale of&lt;br/&gt;goods| Debtor --&gt;\n|Payment&lt;br/&gt;after expiry of credit period| Factor --&gt;\n|Payment&lt;br/&gt;at discount| Company\nCompany --&gt; |Receivables&lt;br/&gt;Transfer| Factor</code></pre>"},{"location":"Finance_Electives/MBFM/04_Financial_Services/#types","title":"Types","text":"Risk Bearer Full-Recourse Factoring Company Non-Recourse Factoring Factor"},{"location":"Finance_Electives/MBFM/04_Financial_Services/#foreign-exchange-services","title":"Foreign Exchange Services","text":"<ul> <li>Currency exchange</li> <li>Wire transfer</li> <li>Remittance</li> </ul>"},{"location":"Finance_Electives/MBFM/05_Interest_Rate/","title":"Interest Rate","text":"Entity Interpretation of Interest Rat Lender Income from fixed-income securities Borrower Cost of borrowing Economist Base/Rapport rate at which central banks lend to commercial banks Misc Discount Rate/Compounding Rate/Hurdle Rate <pre><code>flowchart LR\nCredit --&gt; Borrowing --&gt; Production --&gt; Employment --&gt; Income --&gt; pp[Purchasing Power] --&gt; Demand\nBorrowing --&gt; pp\n\nProduction -----&gt; Supply\n\nDemand &amp; Supply --&gt; Credit</code></pre>"},{"location":"Finance_Electives/MBFM/05_Interest_Rate/#factors-that-affect-interest-rate","title":"Factors that affect interest Rate","text":"<ul> <li>Inflation</li> <li>Time</li> <li>Risk</li> </ul>"},{"location":"Finance_Electives/MBFM/05_Interest_Rate/#interest-based-instruments","title":"Interest-Based Instruments","text":"<ul> <li>Simple loan</li> <li>Fully amortized loan: Fixed payment loan</li> <li>Coupon bond</li> <li>Discount bond: Zero-Coupon Bond</li> </ul>"},{"location":"Finance_Electives/MBFM/05_Interest_Rate/#ytm","title":"YTM","text":"<p>Yield Till Maturity</p> <ul> <li>Coupon</li> <li>Capital Gains</li> <li>Reinvestment income</li> </ul> <p>Opportunity of </p>"},{"location":"Finance_Electives/MBFM/05_Interest_Rate/#tvm","title":"TVM","text":"<p>Time Value of Money</p>"},{"location":"Finance_Electives/MBFM/05_Interest_Rate/#determinants","title":"Determinants","text":"<p>Supply and change in bond market</p>"},{"location":"Finance_Electives/MBFM/05_Interest_Rate/#change-in-equilibrium","title":"Change in equilibrium","text":"<ul> <li>Change in Price of bond: Movements along a demand/supply curve</li> <li>Change due to other factors: Shifts in a demand/supply curve</li> </ul>"},{"location":"Finance_Electives/MBFM/05_Interest_Rate/#shift-in-demand","title":"Shift in Demand","text":"Increase in Shift in Demand Curve Bond Price Interest Rate Wealth/Purchasing Power Right Inc Dec Expected returns on bonds relative to alternative assets Right Inc Dec Risk of bonds relative to alternative assets Left Dec Inc Liquidity of bonds relative to alternative assets Right Inc Dec Expected Inflation Left Dec Inc"},{"location":"Finance_Electives/MBFM/05_Interest_Rate/#shift-in-supply","title":"Shift in Supply","text":"Increase in Shift in Supply Curve Bond Price Interest Rate Wealth Left Inc Dec Profitability of project for which funds required Right(Higher cost of capital is tolerated) Dec Inc Expected Inflation Right Dec Inc Govt Budget"},{"location":"Finance_Electives/MBFM/05_Interest_Rate/#fisher-effect","title":"Fisher Effect","text":"\\[ r = i - \\pi_e \\] <ul> <li>\\(r =\\) Real Interest Rate</li> <li>\\(i=\\) Nominal interest rate</li> <li>\\(\\pi_e =\\) Expected inflation rate</li> </ul>"},{"location":"Finance_Electives/MBFM/05_Interest_Rate/#structure-of-interest","title":"Structure of Interest","text":"Interest rate of Risk Structure Different bonds at same time point Term Structure Same bond over time"},{"location":"Finance_Electives/MBFM/05_Interest_Rate/#risk-structure","title":"Risk Structure","text":"Risk Type Shift in Demand Curve Bond Price Interest Rate of Bond Default Risk Bond Rating Left Dec Inc Liquidity Right Inc Dec Income tax exemption Right Inc Dec <p>High spread means </p> <p>Increase in interest rate of bond1 causes a decrease in another bond, due to its change in demand curve</p>"},{"location":"Finance_Electives/MBFM/06_Yield/","title":"Yield Curve","text":"<p>Curve that shows the interest rate at different time horizons of holding a fixed-income security</p>"},{"location":"Finance_Electives/MBFM/06_Yield/#purpose","title":"Purpose","text":"<ul> <li>Indicator of future yield levels, and help in setting the yield for all debt market instruments</li> <li>Measure and compare returns across maturity spectrum</li> <li>One more point</li> </ul>"},{"location":"Finance_Electives/MBFM/06_Yield/#types","title":"Types","text":"Causes Upward Sloping Flat Humped Downward Sloping/Inverted Present Future interest rate &lt; Present Short-term interest ratePresent Future interest rate &lt; Future Short-term interest rate <ul> <li>Spot rate is the instantaneous rate</li> <li>Forward rate is the contract on a future rate</li> <li>Future rate is the spot rate in the future</li> </ul>"},{"location":"Finance_Electives/MBFM/06_Yield/#theories-behind-yield-curves","title":"Theories behind Yield Curves","text":""},{"location":"Finance_Electives/MBFM/06_Yield/#expectations-theory","title":"Expectations Theory","text":"<p>(photo in gallery)</p> <p>Risk Neutrality</p> <p>Consequence: Shape of yield curve will depend on the expected returns</p> Theory Assumption Advantages Limitation Expectations Investors are risk-neutral Incorrect to assume that bonds with different maturities do not have different expected return Bond-investors are risk-averse, not risk-neutral Liquidity-Premium Investors are risk-averseBonds are partial substitutes for each Investors demand risk premium for- Maturity Risk Premium Premium- Liquidity Premium Does not explain downward sloping/inverted curves Market Segmentation <p>Maturity Risk Premium: With maturity, risk increases</p> <p>Liquidity premium: </p>"},{"location":"Finance_Electives/MBFM/07_Market_Efficiency/","title":"Efficient Market Hypothesis","text":"<ul> <li>Market corrects itself when new info becomes   available, through quick analysis and   necessary price adjustments</li> <li>Prices fully reflect all   available information</li> </ul>"},{"location":"Finance_Electives/MBFM/07_Market_Efficiency/#market-efficiency","title":"Market Efficiency","text":"<p>Represents only information-processing efficiency</p> <p>No mis-pricings</p> <ul> <li>Good for investors</li> <li>Bad for speculators (Only mispricings will provide large gains. Just value investing provides nominal returns)</li> </ul>"},{"location":"Finance_Electives/MBFM/07_Market_Efficiency/#reactions","title":"Reactions","text":"Type Under-reaction Delayed Efficient Over-reaction"},{"location":"Finance_Electives/MBFM/07_Market_Efficiency/#forms-of-efficiency","title":"Forms of Efficiency","text":"<p>According to Fama, markets are neither purely inefficient nor purely efficient</p> Inefficient Weak Moderate Strong Market reflects __ info None historical historicalpublic historicalpublicprivate Action Type None Overreaction Pre-action Time Before/at the time of announcement Prices Characteristic Random Walk Implication No relation between past prices and future pricesNo trading rule that depends on past can predict futurePast prices do no provide any info to outperform the market Incorporates WFHIf an Technical Analysis Applicable? \u2705 \u274c \u274c \u274c Fundamental Analysis Applicable? \u2705 \u2705 \u274c Serial Correlation \u274c Momentum \u2705 Studies - Serial Correlation- Momentum/overreaction hypothesis - Dividend declaration- Stock splits- Earnings announcement- Announcement of half-yearly results- Acquisitions &amp; mergers- Announcements related to taxes - Studies of corporate insiders- Specialists in stock exchanges"},{"location":"Finance_Electives/MBFM/07_Market_Efficiency/#causes-of-efficiency","title":"Causes of Efficiency","text":"<ul> <li>Sufficiently-large no of knowledgeable investors, who diffuse info</li> <li>Intense competition among investors</li> <li>Rapid transmission of information</li> <li>High speed of transactions</li> <li>Low cost of transactions</li> </ul>"},{"location":"Finance_Electives/MBFM/07_Market_Efficiency/#causes-of-inefficiency","title":"Causes of Inefficiency","text":"<p>Probability of mispricings in asset market \\(\\propto\\)</p> <ul> <li>\\(\\dfrac{1}{\\text{Ease of trading}}\\)</li> <li>cost of information and transactions for exploiting inefficiency</li> </ul>"},{"location":"Finance_Electives/MBFM/07_Market_Efficiency/#anomalies","title":"Anomalies","text":"<p>That prove that no market is purely efficient</p> Effect Size Small stocks tend to give higher returns January Stock prices are higher in January Small P/E Book Value to Market Value Ratio Reversal Market Momentum"},{"location":"Finance_Electives/MBFM/08_Financial_Crises/","title":"Financial Crises","text":""},{"location":"Finance_Electives/MBFM/08_Financial_Crises/#stage-1-buildup","title":"Stage 1: Buildup","text":""},{"location":"Finance_Electives/MBFM/08_Financial_Crises/#credit-boom-bust","title":"Credit boom &amp; bust","text":"<ol> <li>Boom</li> <li>Financial liberalization: Reduced restrictions</li> <li>Financial innovation: New financial product</li> <li>Bust</li> <li>Lenders may lack expertise and/or incentives to manage risk appropriately</li> </ol>"},{"location":"Finance_Electives/MBFM/08_Financial_Crises/#asset-price-boom-post","title":"Asset-price boom &amp; post","text":"<p>Herding</p>"},{"location":"Finance_Electives/MBFM/08_Financial_Crises/#increased-uncertainty","title":"Increased uncertainty","text":"<ul> <li>Hard to obtain information</li> <li>Reduced lending</li> <li>Reduced economic activity</li> </ul>"},{"location":"Finance_Electives/MBFM/08_Financial_Crises/#stage-2-banking-crisis","title":"Stage 2: Banking Crisis","text":""},{"location":"Finance_Electives/MBFM/08_Financial_Crises/#stage-3-debt-deflation","title":"Stage 3: Debt Deflation","text":"<p>Underwater mortgage problem</p> <p>Deflationary spiral: Deflation increase defaults, and that causes more deflation</p>"},{"location":"Finance_Electives/MBFM/08_Financial_Crises/#case-studies","title":"Case Studies","text":""},{"location":"Finance_Electives/MBFM/08_Financial_Crises/#great-depression","title":"Great Depression","text":""},{"location":"Finance_Electives/MBFM/08_Financial_Crises/#asian-tigers","title":"Asian Tigers","text":""},{"location":"Finance_Electives/MBFM/08_Financial_Crises/#dot-com-bubble","title":"Dot-Com Bubble","text":"<p>Tech bubble/Internet bubble</p>"},{"location":"Finance_Electives/MBFM/08_Financial_Crises/#2008-financial-bubble","title":"2008 Financial Bubble","text":"<ul> <li>Housing prices</li> <li>Stock prices</li> </ul> <p>Causes</p> <ul> <li>Failure for default prediction algorithms</li> <li>Used FICO (Fair Isaac Corporation) Score</li> <li>The models were trained on dataset where housing prices always increased</li> <li>Financial innovations</li> <li>Securitization: Process of building small loans (like mortgages) into standard debt securities</li> <li>CDO: Collateralized Debt Obligations</li> <li>CDS: Credit Default Swap</li> <li>Agency problems in mortgage markets</li> <li>Asymmetric information</li> <li>Credit-Rating Services</li> </ul>"},{"location":"Finance_Electives/MBFM/08_Financial_Crises/#ai-bubble-coming-in-2030s","title":"AI Bubble coming in 2030s?","text":""},{"location":"Finance_Electives/MBFM/09_Central_Bank/","title":"Central Bank","text":""},{"location":"Finance_Electives/MBFM/09_Central_Bank/#monetary-policy","title":"Monetary Policy","text":"<p>What is the difference between SLR and CRR? Ans. Cash Reserve Ratio (CRR) is the percentage of money, which a bank has to keep with RBI in the form of cash. Whereas, Statutory Liquidity Ratio (SLR) is the proportion of liquid assets to time and demand liabilities.</p> <p>Central bank checks fortnightly</p> <p>Usually commercial banks have</p> <ul> <li>Short-term sources</li> <li>Long-term lending</li> </ul> <p>Balance of payment</p>"},{"location":"Finance_Electives/MBFM/09_Central_Bank/#unsterilized-intervention","title":"Unsterilized Intervention","text":"<p>Intervention without worrying about side-effects</p> <p>Domestic currency is bought and foreign assets </p> <p>Outflow of foreign assets</p>"},{"location":"Finance_Electives/MBFM/10_Bond_Market/","title":"Bond Market","text":"<ul> <li>Government-dated securities</li> <li>Public sector bonds</li> <li>Corporate bonds</li> </ul>"},{"location":"Finance_Electives/MBFM/10_Bond_Market/#govt-securities-market","title":"Govt Securities Market","text":"<p>Acts as benchmark for rest of market</p>"},{"location":"Finance_Electives/MBFM/11_UAE/","title":"UAE Financial Markets","text":"<ul> <li>Comparatively new</li> <li>Doesn\u2019t have that \u2018depth\u2019?</li> <li>What does that mean?</li> </ul> <p>Money market is very small in UAE</p> <p>The Royal family of Abu Dhabi- Al Nahyan family\u2014 is known as the richest family in the world</p> <ul> <li>Central Bank: CBUAE</li> <li>SCA: Securities &amp; Commodities Authority</li> </ul>"},{"location":"Finance_Electives/MBFM/11_UAE/#uae-stock-markets","title":"UAE Stock Markets","text":"<ul> <li>DFM: Dubai Financial Market</li> <li>ADX: Abu Dhabi Security Exchange</li> <li>NASDAQ Dubai </li> </ul> <p>Trading Time</p> <ul> <li>Monday-Friday</li> <li>9:30-3:00</li> </ul> DFM ADX NASDAQ Dubai Founded 2000-Mar-26 2000-Nov-15 2005-Sep-05 Market Cap AED 582 B AED 2.85 T Owned Govt of Dubai till 2006 Govt of Abu Dhabi 20% shares sold to public EquitiesSukukBondsFundsETFsREIT EquitiesDebt InstrumentsDerivativesETFs Settlement T+2 T+2"},{"location":"Finance_Electives/MBFM/11_UAE/#market-indices","title":"Market Indices","text":"<p>DFMGI</p> <p>Dubai Financial Market General Index (30 DFM-listed equities)</p> <p>Secondary listed companies have a free float adjusted market cap &gt; AED 5 B</p> DFMGI ADX General Weightage Modified capitalization-weighted index (maximum cap of 20%) Capitalization-weighted Base Date 2003-12-31 Base Currency AED 1000 AED 1000 New listing are added"},{"location":"Finance_Electives/MBFM/11_UAE/#how-to-invest-in-uae-markets","title":"How to invest in UAE Markets","text":"<ol> <li>Open trading account with licensed broker</li> <li>Apply for National Investor Number with SCA</li> <li>Perform trading</li> </ol>"},{"location":"Finance_Electives/MBFM/11_UAE/#idk","title":"IDK","text":"<ul> <li>Full-service</li> <li>Discount-service</li> </ul>"},{"location":"Finance_Electives/MBFM/11_UAE/#idk_1","title":"IDK","text":"<ul> <li>Onboarding fees</li> <li>Commissions: Borker chanrges + Market charges + VAT</li> </ul>"},{"location":"Finance_Electives/MBFM/11_UAE/#tabadul","title":"Tabadul","text":"<p>Mutual market access model</p> <p>Enables trading between member exchanges</p> <ul> <li>Abu Dhabi</li> <li>Bahrain</li> <li>Muscat</li> </ul> <p>Traded in local currency of the exchange</p> <p>Service providers</p>"},{"location":"Finance_Electives/MBFM/11_UAE/#-","title":"-","text":""},{"location":"Finance_Electives/New_Venture_Creation/","title":"New Venture Creation","text":"<p>How to plan and execute a new venture</p>"},{"location":"Finance_Electives/New_Venture_Creation/#references","title":"References","text":"<ul> <li> Nuts and Bolts of Business Plans | MIT</li> <li> How to Start a Startup | YCombinator | Stanford</li> <li> New Enterprises | MIT</li> <li> MIT Bootcamps</li> </ul>"},{"location":"Finance_Electives/New_Venture_Creation/01_Introduction/","title":"Introduction","text":""},{"location":"Finance_Electives/New_Venture_Creation/01_Introduction/#entrepreneurship-types","title":"Entrepreneurship Types","text":"SME IDE Meaning Small-Medium Entrepreneurship Innovation Driven Entrepreneurship Focus Market Local Global Risk Lower Higher Clustered(meaning??) \u274c \u2705 Usually Service-Based Annual cashflow with time Stable and then levels off"},{"location":"Finance_Electives/New_Venture_Creation/01_Introduction/#innovation","title":"Innovation","text":"<p>Innovation \\(\\ne\\) Invention</p> <ul> <li>Innovation generates value</li> <li>Invention = any new idea</li> </ul> \\[ \\text{Innovation} = \\text{Invention} \\times \\text{Commercialization} \\]"},{"location":"Finance_Electives/New_Venture_Creation/01_Introduction/#types","title":"Types","text":"<ul> <li>Technology</li> <li>Process</li> <li>Business Model</li> <li>Position</li> <li>Other</li> </ul>"},{"location":"Finance_Electives/New_Venture_Creation/01_Introduction/#categories","title":"Categories","text":"<ul> <li>Disruptive</li> <li>Incremental</li> <li>Lateral</li> </ul>"},{"location":"Finance_Electives/New_Venture_Creation/01_Introduction/#ways-to-start-a-company","title":"Ways to Start a Company","text":"Tech push Market Pull Passion Possess Technological advantage Problem Solution looking for a market Market looking for a solution Not enoughYou need to go to other 2"},{"location":"Finance_Electives/New_Venture_Creation/01_Introduction/#myths-of-entrepreneurship","title":"Myths of Entrepreneurship","text":"Myth Truth Entrepreneurs are the most successful and highest achievers Usually good at only few things Entrepreneurs are individualist Good team is required Entrepreneurs are born Entrepreneurs are made Entrepreneurs love risk Entrepreneurs take calculated risk in advantageous areas Entrepreneurs are charismatic Entrepreneurs create change through leadership- Vision- Sense-making capability- Network/Relationships- Innovation engineering- Personal signature Entrepreneurs are undisciplined Entrepreneurs are disciplined- Explorative spirit of a pirate- Execution discipline of a Navy Seal Team Six"},{"location":"Finance_Electives/New_Venture_Creation/01_Introduction/#idk","title":"IDK","text":"<ol> <li>Identify the problem</li> <li>5 whys</li> </ol>"},{"location":"Finance_Electives/New_Venture_Creation/01_Introduction/#market-segmentation","title":"Market Segmentation","text":"<ol> <li> <p>Primary market research</p> </li> <li> <p>What problem</p> </li> <li> <p>What they think of idea</p> </li> <li> <p>Understand needs of customer</p> <ul> <li>Rational</li> <li>Emotional</li> <li>Social</li> </ul> </li> <li> <p>Limit to a few consumer segments</p> </li> <li> <p>Secondary research</p> </li> <li> <p>Select a Beachhead Market: market that can be captured easily, and also facilitates growth into other markets</p> </li> </ol> <p>Do not trust online, always do your own research</p>"},{"location":"Finance_Electives/New_Venture_Creation/01_Introduction/#requirements-of-starting","title":"Requirements of Starting","text":"<ol> <li>Knowledge</li> <li>Capability/Skills</li> <li>Connections</li> <li>Financial Assets</li> <li>Name Recognition</li> <li>Credibility, from past work experience</li> <li>Passion</li> <li>Commitment</li> </ol>"},{"location":"Finance_Electives/New_Venture_Creation/01_Introduction/#what-makes-a-business","title":"What makes a business","text":"<p>Until you have paying customers, you do not have a business</p>"},{"location":"Finance_Electives/New_Venture_Creation/02_Business_Plan/","title":"Business Plan","text":"<p>Dynamic &amp; shared vision with team on where we are, where we\u2019re going, how we\u2019re going there</p> <p></p>"},{"location":"Finance_Electives/New_Venture_Creation/02_Business_Plan/#elements","title":"Elements","text":"<ol> <li>Executive Summary</li> <li>Opportunity and the Company &amp; its Services/Products</li> <li>Market Research/Analysis</li> <li>Economics of the Business</li> <li>Marketing Plan</li> <li>Design &amp; Development Plan</li> <li>Manufacturing &amp; Operations Plan</li> <li>Management Team</li> <li>Schedule</li> <li>Critical Risks, Problems &amp; Assumptions</li> <li>Financial Plan</li> <li>Assumptions</li> </ol> <p>Technology isn\u2019t a section, as it is a means to the end. The real point is the value produced.</p> <p>Cover Page</p> <ul> <li>Name of Venture</li> <li>Address, Contact Details</li> <li>Confidentiality Legend</li> <li>Securities Law Legend</li> <li>Control numbering of copies</li> </ul>"},{"location":"Finance_Electives/New_Venture_Creation/02_Business_Plan/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Put one in</li> <li>Include page numbers</li> </ol>"},{"location":"Finance_Electives/New_Venture_Creation/02_Business_Plan/#executive-summary","title":"Executive Summary","text":"<p>Resume for the full plan</p> <p>Goal is to get the interview to give the pitch, and answer What do investors look for?</p>"},{"location":"Finance_Electives/New_Venture_Creation/02_Business_Plan/#elements_1","title":"Elements","text":"<ol> <li>Description of Business Concept</li> <li>Opportunity &amp; Strategy</li> <li>Target Market &amp; Projections</li> <li>Competitive Advantages</li> <li>Economics, Profitability, Harvest Potential</li> <li>The Team</li> </ol>"},{"location":"Finance_Electives/New_Venture_Creation/02_Business_Plan/#characteristics","title":"Characteristics","text":"<ul> <li>2 - 5 pages long</li> <li>Should be Logical, Clear, Interesting/Exciting</li> </ul>"},{"location":"Finance_Electives/New_Venture_Creation/02_Business_Plan/#body","title":"Body","text":"<ol> <li>IDK Plan</li> <li>Marketing Strategy</li> <li>Pricing &amp; Distribution</li> <li>Sales Tactics</li> <li>Advertising &amp; Promotion</li> <li>Opportunity</li> <li>How big is it now?</li> <li>Trends</li> <li>Market Analysis</li> <li>Existing competitors</li> <li>Who loses if you win and what will be their response?</li> <li>Are customers interested?</li> <li>Development Plan</li> <li>Current Product Status</li> <li>Further steps (Time &amp; Resources) required</li> <li>Difficulty &amp; Risks</li> <li>Product Pipeline Plans</li> <li>Action Plan</li> <li>What will you do and when?</li> <li>Identification of \u2018Credibility Testers\u2019</li> <li>Sequencing to build value</li> <li>Eliminate/Reduce Dependencies</li> <li>Coordination of Schedule, Value Recognition Events, and Financing Requirements</li> <li>Appendix</li> </ol>"},{"location":"Finance_Electives/New_Venture_Creation/02_Business_Plan/#business-plan-as-a-financing-document","title":"Business Plan as a Financing Document","text":""},{"location":"Finance_Electives/New_Venture_Creation/02_Business_Plan/#first-reading-like-a-resume","title":"First reading: Like a resume","text":"<ol> <li>Excellent Idea</li> <li>Excellent financial promise</li> <li>Excellent team</li> <li>Action plan is credible and focused</li> <li>Details that give assurance of insight, commitment, and follow-through</li> </ol>"},{"location":"Finance_Electives/New_Venture_Creation/02_Business_Plan/#why-some-plans-fail-here","title":"Why some plans fail here","text":"<ul> <li>Insufficient market</li> <li>Non-Credible technology</li> <li>Too large investment</li> <li>Failure to understand market</li> <li>Overly optimistic action plan</li> <li>Non-Credible team</li> <li>Poor business plan submission (misspellings, poor grammar)</li> </ul>"},{"location":"Finance_Electives/New_Venture_Creation/02_Business_Plan/#second-reading-justify-the-investment","title":"Second reading: Justify the investment","text":""},{"location":"Finance_Electives/New_Venture_Creation/02_Business_Plan/#third-reading-commit-to-a-plan","title":"Third reading: Commit to a plan","text":""},{"location":"Finance_Electives/New_Venture_Creation/02_Business_Plan/#what-do-investors-look-for","title":"What do investors look for?","text":"<ol> <li>Why this? Why should I be interested?</li> <li>Why now?</li> <li>Why this team?</li> <li>Why will this succeed?</li> </ol>"},{"location":"Finance_Electives/New_Venture_Creation/02_Business_Plan/#business-model-canvas","title":"Business Model Canvas","text":"<ul> <li>Value proposition</li> <li>Customer segment</li> <li>Distribution channel</li> <li>Customer relationship: Feedback mechanism</li> <li>Key activities</li> <li>Key resources</li> <li>Key partnerships</li> <li>Cost structure</li> <li>Revenue stream</li> </ul>"},{"location":"Finance_Electives/New_Venture_Creation/02_Idea/","title":"Refining and Presenting your Venture Idea","text":"<pre><code>---\ntitle: Success Rate of Startups\n---\npie\n\"Succeed\": 1\n\"Moderate\": 3\n\"Fail\": 6</code></pre> <ul> <li>1/10 startups succeed</li> <li>6/10 startups</li> </ul>"},{"location":"Finance_Electives/New_Venture_Creation/02_Idea/#startup","title":"Startup","text":"<p>Long term plan: Exit Strategy</p> Going public Get acquired - In technology, this is not the right way to pitch.- It gives a message to investors that you are not interested in long-term growth. You\u2019re just interested in build, sell and run <p>Why would anyone want to acquire a company anyways?</p> <ul> <li>Startups are faster at innovating than large companies. </li> </ul> Startup Corporate Employees\u2019 Motivation Get rich by stocks Stable lifestyle Same goal \u2705 \u274c No internal competition \u2705 \u274cOther employees will try to make you look bad so that they can get ahead of you"},{"location":"Finance_Electives/New_Venture_Creation/02_Idea/#elevator-pitch","title":"Elevator Pitch","text":"<p>For target customers who are dissatisfied with current solution, our product is a new product category that provides key problem solving opportunity. Unlike a competitive substitute, we have assembled key whole product features.</p> <ul> <li>It has to be spot-on since investors get thousands of pitches. They can easily determine if your pitch is valuable or not.</li> <li>Induce greed &amp; urgency in the investors</li> </ul>"},{"location":"Finance_Electives/New_Venture_Creation/02_Idea/#the-pitch-kiss","title":"The Pitch: KISS","text":""},{"location":"Finance_Electives/New_Venture_Creation/02_Idea/#structure","title":"Structure","text":"<ol> <li>Hook</li> <li>&lt; 7 seconds to engage audience </li> <li> <p>Phrases</p> <ol> <li>\u201cImagine \u2026 \u201d</li> <li>\u201cHave you ever felt \u2026\u201d</li> <li>\u201cDo you get \u201d</li> </ol> </li> <li> <p>Solution</p> </li> <li>Technology</li> <li>Business Model</li> <li>Marketing &amp; Sales</li> <li>Competition</li> <li>Management</li> <li>Financials</li> <li>Status &amp; Timeline</li> <li>The Offer</li> </ol>"},{"location":"Finance_Electives/New_Venture_Creation/02_Idea/#characteristics","title":"Characteristics","text":"<ul> <li> <p>Focus should be on customer benefits, not on the technology</p> </li> <li> <p>Simple - No technical jargon</p> </li> <li> <p>Tailor the pitch for the Target Audience</p> </li> <li> <p>Lowest Common Denominator: Explain keeping in mind the least experienced/knowledgeable audience member</p> </li> <li> <p>Short (10/20/30): Audience questions will always take it to an hour, so prepare accordingly</p> </li> <li> <p>Pictures</p> </li> <li> <p>Answer details/curveballs quickly. 2 options:</p> </li> <li> <p>\u201cNo, that\u2019s not the case. From our research, this is why short reason. I\u2019ll get back to you in more detail later.\u201d</p> </li> <li> <p>\u201cThat\u2019s a very good point. We are not very familiar with. Let\u2019s discuss this in more detail later.\u201d</p> </li> </ul> <p>Make sure to shut off them quickly because you have an agenda to complete.</p> <ul> <li> <p>Presentation</p> </li> <li> <p>Passion and confidence</p> </li> <li>Engage the audience</li> <li>Slow and calm speech</li> <li>Get everyone nodding along with what you are saying</li> </ul>"},{"location":"Finance_Electives/New_Venture_Creation/02_Idea/#style","title":"Style","text":"<ul> <li>Best presenter should be presenting</li> <li>Be/bring a highly knowledgeable teammate in the room</li> <li>Give firm answers</li> <li>Look at everyone in the room</li> <li>Do your research on your audience</li> <li>Pull questions</li> </ul>"},{"location":"Finance_Electives/New_Venture_Creation/02_Idea/#when-to-raise-money","title":"When to Raise Money","text":"<ul> <li>Finish closing money at least 6 months before you need it</li> <li>Make pitches to investors simultaneously. This way investors are not influenced by other investors</li> <li>Be \u201cintroduced\u201d/use testimonials</li> <li>If you have 2 offers, prefer the quality of investor over the amount they\u2019re offering</li> </ul>"},{"location":"Finance_Electives/New_Venture_Creation/02_Idea/#business-basics","title":"Business Basics","text":"<ul> <li>Profit &amp; Cash Flow</li> <li>Business PLan</li> <li>Be flexible (Lean Startup methodology)</li> <li>Initial vs Final Product/Service may not necessarily be the same</li> <li>Speed of progress is more important than Patents/Intellectual Property</li> <li>Mission</li> <li>The Team</li> <li>Chemistry</li> <li>Should be able to handle shocks</li> <li>Convey that you are</li> <li>Be willing to persist after failure</li> <li>Experienced to do a startup</li> <li>Speed of Execution</li> <li>Focused about your value proposition. Pivot; You don\u2019t always end up finishing with the idea you started with, but don\u2019t keep changing your project unnecessarily</li> </ul>"},{"location":"Finance_Electives/New_Venture_Creation/03_Marketing_Sales/","title":"Marketing &amp; Sales","text":""},{"location":"Finance_Electives/New_Venture_Creation/03_Marketing_Sales/#2-success-strategies","title":"2 Success Strategies","text":"<ul> <li>Cost Leader: Aggressively cut costs</li> <li>Innovator</li> </ul>"},{"location":"Finance_Electives/New_Venture_Creation/03_Marketing_Sales/#idk","title":"IDK","text":"<ul> <li>Identify your customers correctly</li> <li>\u201cProduct for everyone\u201d isn\u2019t for anyone</li> <li>Perform market segmentation</li> <li>Identify your segments of customers to target</li> <li>Some customers won\u2019t be interested even with the best of marketing</li> <li>Survey your customers</li> <li>Understand your consumer\u2019s behaviors</li> <li>Sell the benefits, not the product</li> <li>Leverage influencers (educators, doctors, etc)</li> <li>Use networking effect</li> </ul>"},{"location":"Finance_Electives/New_Venture_Creation/03_Marketing_Sales/#lean-methodology","title":"Lean Methodology","text":"<ol> <li> <p>Develop Idea</p> </li> <li> <p>Identify unmet needs in market</p> </li> <li>Interview potential consumers, influencers, retailers</li> <li> <p>Focus on one unmet need</p> </li> <li> <p>MVP</p> </li> </ol> <p>Goal: Identify consumers for launch</p> <ol> <li>Efficacy first</li> <li>Minimal packaging</li> <li> <p>Minimal attention to fancy features</p> </li> <li> <p>Create customers</p> </li> <li> <p>Branding, packaging</p> </li> <li>Go to market</li> <li>Ramp up hiring &amp; spending</li> <li> <p>Confirm: re-orders</p> </li> <li> <p>Grow company</p> </li> <li> <p>Build staff</p> </li> <li>Become profitable &amp; self-sustaining</li> <li>Position for acquisition</li> </ol>"},{"location":"Finance_Electives/New_Venture_Creation/04_Business_Models/","title":"Business Models","text":"<ul> <li>Value Creation</li> <li>Marketing</li> <li>Sales</li> <li>Value Delivery</li> <li>Finance</li> </ul> <p>Business model \\(\\ne\\) business.</p> <p>Business model innovation is critical to develop a quality business, attack new markets and drive profitability</p>"},{"location":"Finance_Electives/New_Venture_Creation/04_Business_Models/#components","title":"Components","text":"<ul> <li>Value Proposition</li> <li>Market Segment</li> <li>Value Chain Structure</li> <li>Position in the Value Network</li> <li>Revenue Generation and Margins</li> <li>High volume/Low margin</li> <li>Low volume/High margin</li> <li>Competitive Strategy</li> <li>Stage of Development</li> </ul>"},{"location":"Finance_Electives/New_Venture_Creation/05_Financial_Projections/","title":"Financial Projections","text":"<p>Failure to plan is planning to fail</p>"},{"location":"Finance_Electives/New_Venture_Creation/05_Financial_Projections/#startup-ceo-role","title":"Startup CEO Role","text":"<p>Even if you don't understand accounting/finance, you need to know</p> <ul> <li>Average Selling Price</li> <li>Gross margins</li> <li>Cost of R&amp;D</li> <li>Sales &amp; Marketing Strategy &amp; Expenses</li> <li>Start-up and/or Capital Expenses</li> </ul>"},{"location":"Finance_Electives/New_Venture_Creation/05_Financial_Projections/#venture-capitalist-wants","title":"Venture Capitalist Wants","text":"<ul> <li>3-5x absolute returns</li> <li>5-7yr investment horizon</li> <li>4x in 5yrs = 32% IRR</li> <li>Get a significant amount of $ invested</li> <li>Own a significant ownership % (50% +)</li> </ul> \\[ \\text{VC} \\% = \\dfrac{\\$ \\text{Invested}}{\\$ \\text{Invested} + \\text{Pre-VC Valuation}} \\]"},{"location":"Finance_Electives/New_Venture_Creation/05_Financial_Projections/#tech-model","title":"Tech Model","text":"<ul> <li>COGS (Cost of Goods/Services): Unit costs + manufacturing O/H + Support</li> <li>R&amp;D should be 10-20%</li> <li>G&amp;A (General and administrative expensive) should be 5-15%</li> <li>Target operating profit of 15-20%</li> <li>Distribution strategy</li> </ul>"},{"location":"Finance_Electives/New_Venture_Creation/05_Financial_Projections/#idk","title":"IDK","text":"<ul> <li>Do not use business planning software</li> <li>Do not project best-case/worst-case</li> <li>Build sales projection from the bottom-up</li> </ul>"},{"location":"Finance_Electives/New_Venture_Creation/05_Financial_Projections/#tech-companies-rules-of-thumb","title":"Tech companies rules-of-thumb","text":"<ul> <li>Staffing drives departmental expenses</li> <li>Employee benefits add 15%</li> <li>Salaries wil be 60-70% of total expenses</li> <li>Remainder will be rent, utilities, travel</li> </ul>"},{"location":"Finance_Electives/New_Venture_Creation/05_Financial_Projections/#cashflow-projections","title":"Cashflow Projections","text":"<ul> <li>Burn rate</li> <li>Monthly operating loss + capital expenditures</li> <li>Cash flow projection</li> <li>Cumulative operating losses excluding depreciation</li> <li>Plus cumulative capital expenses</li> <li>Determining total cash required</li> <li>Cumulative operating loss + Cumulative capital expenses loss</li> <li>On the month you turn cash positive</li> </ul>"},{"location":"Finance_Electives/New_Venture_Creation/05_Financial_Projections/#presentation","title":"Presentation","text":"<p>Financial Summary</p> <ul> <li>Always show % next to absolute values</li> <li>Show pre-tax only</li> <li>Don\u2019t allocate G&amp;A</li> </ul> <p>Executive summmary</p> <ul> <li>Annual P&amp;L for 4-5yrs</li> <li>Data to justify revenue projectison</li> <li>Unit sales</li> <li>Average Selling Price (ASP)</li> <li>When will you be profitable</li> <li>Total cash requirement </li> </ul> <p>Full business plan</p> <ul> <li>Annual P&amp;L for 4years</li> <li>Quarterly P&amp;L for 4years</li> <li>Quarterly staffing plan for 4years</li> <li>Quarterly cash flow for 4years</li> </ul>"},{"location":"Finance_Electives/New_Venture_Creation/05_Financial_Projections/#equity-distribution","title":"Equity Distribution","text":"<p>Compensate for</p> <ul> <li>Ownership of IP</li> <li>Commitment</li> <li>Risk</li> <li>Sacrifice</li> <li>Past &amp; future contributions</li> </ul> <p>Work to be done &gt; Work already done. Incentivize people to do things in the future</p> <p>Everyone should vest</p> <p>You can always grant later</p>"},{"location":"Finance_Electives/New_Venture_Creation/05_Financial_Projections/#common-ownership-breakdown","title":"Common ownership breakdown","text":"<ul> <li>CEO: 5%</li> <li>VP: 1-2.5%</li> <li>Sr Manager: 0.25%</li> <li> <p>Sr Ind Contributor: 0.1%</p> </li> <li> <p>Founding management gets 2x to 3x</p> </li> <li>Founding employees gets 5x to 10x</li> </ul>"},{"location":"Finance_Electives/New_Venture_Creation/06_Legal_Issues/","title":"Legal Issues","text":""},{"location":"Finance_Electives/New_Venture_Creation/06_Legal_Issues/#lifecycle","title":"Lifecycle","text":""},{"location":"Finance_Electives/New_Venture_Creation/06_Legal_Issues/#idk","title":"IDK","text":"<ul> <li>IP</li> <li>Secrecy</li> <li>Speed</li> <li> <p>Become the industry-standard, lock-in customers, making it costly to switch</p> </li> <li> <p>You need to make sure you have correct owning rights. If you have an employed coder, you implicitly own the code; if you have an independent coder paid on a contract basis, you don\u2019t implicitly own it.</p> </li> <li>When you use open-source tools, make sure you\u2019re complying with its license</li> </ul>"},{"location":"Finance_Electives/New_Venture_Creation/06_Legal_Issues/#ip","title":"IP","text":"<p>Intellectual Property</p> <p>Valuable asset to raise capital</p>"},{"location":"Finance_Electives/New_Venture_Creation/06_Legal_Issues/#types","title":"Types","text":"Meaning Duration Example Cost Enhance value Protect expression Protect function Trademark Mark under which you sell goods and servicesDeveloping a name for yourselfDo not pick descriptive words/phrases such as \u201cStorage Technology, Analog Devices\u201dwww.uspto.gov Unlimited as long as continued renewal Apple, IBM, ThinkPad Cheap \u2705 \u274c \u274c Copyright Right to make copiesFederal registration helpsGood for music, not for software 75years \u2705 \u2705 \u274c Trade Secret Secrets to give yourself an advantage in the market Lasts as long as you can keep it secret with NDAs Formula for Coke \u274c \u2705 \u2705 Patent Limited time monopolyFederally-granted right to any system/method that is new, non-obvious and usefulOwnership \\(\\ne\\) Right to use 20years Expensive \u2705 \u2705 \u2705 Provisional Patent Requires meaningful description of invention (claims not required)FastNothing happens at the PTOWhatever not mentioned might not covered when filing patent) 1year Cheap(<code>$130</code> for small entity, <code>$65</code> for micro) University Licensing (Bayh-Dole Act) Sponsored researchShare royalties with inventors Combination"},{"location":"Finance_Electives/New_Venture_Creation/06_Legal_Issues/#patent","title":"Patent","text":""},{"location":"Finance_Electives/New_Venture_Creation/06_Legal_Issues/#requirements-to-get-a-patent","title":"Requirements to get a patent","text":"<ul> <li>Novel</li> <li>Something new: Prior art must be cited</li> <li>Useful</li> <li>Patentable subject matter</li> <li>Not previously sold/publicly described</li> <li>Enabling disclosure</li> <li>1 year window in US only</li> <li>Not obvious \u201cto one of ordinary skill in the art\u201d</li> <li>Prior art \u201cteaches against\u201d</li> <li>Commercial success can show non-obviousness</li> </ul>"},{"location":"Finance_Electives/New_Venture_Creation/06_Legal_Issues/#obtaining-patent","title":"Obtaining Patent","text":"<ul> <li>Determine what to patent</li> <li>Determine when to patent</li> <li>Prepare one/more applications</li> <li>Prosecuting applications</li> </ul>"},{"location":"Finance_Electives/New_Venture_Creation/06_Legal_Issues/#when-to-file","title":"When to file","text":"<ul> <li>Before you lose rights</li> <li>Before a public disclosure</li> <li>Before an \u201con-sale\u201d bar</li> <li>First to file wins under AIA</li> </ul>"},{"location":"Finance_Electives/New_Venture_Creation/06_Legal_Issues/#parts","title":"Parts","text":"<ul> <li>Field of invention</li> <li>Background of invention</li> <li>\u201cPrior Art\u201d</li> <li>List of advantages compared to prior art</li> <li>Summary of invention</li> <li>Detailed description</li> <li>Examples of use</li> <li>Best mode: what is the best way to implement your invention</li> <li>Claims</li> <li>What exactly is your invention</li> </ul> <p>If you ever want to enforce the patent, claims is the part that matters. Hence this is where you should spend money for someone else to do, not the other parts.</p>"},{"location":"Finance_Electives/New_Venture_Creation/06_Legal_Issues/#typical-ip-terms","title":"Typical IP Terms","text":"W/o Equity W/ Equity Issue fees $ 50-150k $ 5-50k Maintenance fees ~50% of expected RR ~50% of expected RR Diligence Can\u2019t leave on shelf Can\u2019t leave on shelf Royalty as % of Sales 3-5% 2-4% Patent costs $ 25-200k $ 25-200k Research Sponsorship Not required Not required"},{"location":"Finance_Electives/New_Venture_Creation/06_Legal_Issues/#corporation-registration","title":"Corporation Registration","text":"<ul> <li>Delaware, US is the best place to register a company</li> <li>Why do it soon?</li> <li>Avoid personal liability</li> <li>Avoid \u201cpartner\u201d liability</li> <li>Minimize personal taxes</li> <li>Section 83<ul> <li>Rule</li> <li>If you receive \u201cproperty in connection with providing services\u201d</li> <li>You have ordinary income (taxed unto 35%) equal to fair market value of property minus What you paid</li> <li>How to overcome</li> <li>Separate the time when stock is issued to you from the investment by others, ie incorporate earlier, issue stock &amp; make 83(b) election</li> </ul> </li> <li>S corporation is preferable over a regular corporation, to avoid double taxation</li> <li></li> </ul>"},{"location":"Finance_Electives/New_Venture_Creation/06_Legal_Issues/#forms-of-equity-compensation","title":"Forms of Equity Compensation","text":"<ul> <li>Restricted stock</li> <li>Stock sold/granted outright</li> <li>Starts capital gains/SEC holding periods running</li> <li>Subject to vesting and buyback by company</li> <li>If 83(b) election timely:<ul> <li>modest/zero income at grant</li> <li>No further income unto stock sold, then capital gain</li> <li>No employer tax deduction for increase in value</li> </ul> </li> <li>ISOs (Incentive Stock Options) tax-qualified stock options</li> <li>Options compelling with tax requirements</li> <li>Only for employees of corporation</li> <li>Exercise price = FMV on date of grant</li> <li>Typical exercise vesting over time</li> <li>No tax on grant/vesting</li> <li>Possible alternative minimum tax on exercise</li> <li>Taxation upon stock sale - capital grain if holding period requirements met (&gt; 1yr from exercise and &gt;2yers from grant data); no employer deduction</li> <li>NQOs (Non-Qualified Stock Options)</li> <li>Complete tax freedom in design, but there may be accounting issues<ul> <li>Discounted options</li> <li>Repricing</li> <li>Performance vesting</li> </ul> </li> <li>No tax on grant/vesting</li> <li>Ordinary income (and employer deduction) upon exercise</li> </ul>"},{"location":"Finance_Electives/New_Venture_Creation/06_Legal_Issues/#vesting","title":"Vesting","text":"<p>Conditions on keeping what seems to have been awarded to you</p> <ul> <li>Time-based vesting</li> <li>3-5yrs</li> <li>Monthly, quarterly, annual</li> <li>Performance vesting</li> <li>Design issues</li> <li>Accounting issues</li> <li>Accelerated vesting on change in control? IPO?</li> </ul>"},{"location":"Finance_Electives/New_Venture_Creation/06_Legal_Issues/#forfeiture-expiration-of-vesting-rights","title":"Forfeiture &amp; Expiration of Vesting Rights","text":"<ul> <li>How long after employment ends may vested options be exercised (ISO rules generally limit to 90days)</li> <li>Forfeiture if \u201cbad boy\u201d provision violated</li> <li>Consequences of violation of non-competition, non-solicitation agreements</li> </ul>"},{"location":"Finance_Electives/New_Venture_Creation/06_Legal_Issues/#buyback-issues","title":"Buyback Issues","text":"<p>Can company repurchase vested equity for fair value?</p> <ul> <li>Always</li> <li>When employment ends?</li> <li>If covenants violated?</li> <li>Never?</li> </ul>"},{"location":"Finance_Electives/New_Venture_Creation/06_Legal_Issues/#securities","title":"Securities","text":"<ul> <li>Don\u2019t offer to sell securities in a business plan</li> <li>Using the business plan to \u201csell securities\u201d presents problems beyonds VC\u2019s, corporate investors</li> <li>Properly paper all deals</li> <li>even friendly deals with friends/relatives to avoid misunderstandings</li> <li>Under federal law, all offers of securities must be registered with the SEC (very expensive), unless there is an exemption (eg: private placement)</li> <li>Avoid public pronouncements</li> <li>Blue sky (state securities) laws differ from state to state</li> <li>Non-compliance may trigger a recision offer, stop order, personal liability</li> </ul> <p>Private placements</p> <ul> <li>Use an experienced securities law attorney</li> <li>Avoid dealing with individual investors who are not \u201caccredited\u201d</li> <li>Include \u201crisk factors\u201d in disclosure materials</li> <li>Provide a capitalization table</li> <li>SEC 10b-5: don\u2019t make material misstatements of fact/omit to state material facts</li> <li>Legends/control number of documents</li> <li>Regulation D: Form D filing with SEC</li> </ul>"},{"location":"Finance_Electives/New_Venture_Creation/idk/","title":"Idk","text":""},{"location":"Finance_Electives/New_Venture_Creation/idk/#startup","title":"Startup","text":""},{"location":"Finance_Electives/New_Venture_Creation/idk/#startup-learning-loop","title":"Startup Learning Loop","text":"<pre><code>---\ntitle: Minimize total time through loop\n---\nflowchart LR\ni[Ideas] --&gt;\n|Build| Product --&gt;\n|Measure| Data --&gt;\n|Learn &amp; Improve| i</code></pre>"},{"location":"Finance_Electives/New_Venture_Creation/idk/#corporate-structure","title":"Corporate Structure","text":"<pre><code>flowchart TB\n\nbod[Board of Directors] --&gt;\nCEO --&gt;\nChiefs --&gt;\nPresident --&gt;\nSVP --&gt;\nVP --&gt;\nDirector --&gt;\nManager --&gt;\nExecutives\n\nsubgraph Chiefs\ndirection TB\nCTO[\"CTO&lt;br /&gt;Tech\"]\nCRDO[\"CMO&lt;br /&gt;Reseach &amp; Development\"]\nCOO[\"COO&lt;br /&gt;Operations\"]\nCMO[\"CMO&lt;br /&gt;Marketing\"]\nCFO[\"CFO&lt;br /&gt;Finance\"]\nCSO[\"CSO&lt;br /&gt;Security\"]\nCHRO[\"CMO&lt;br /&gt;Human Resources\"]\nCGO[\"CGO&lt;br /&gt;Green \"]\nend</code></pre>"},{"location":"Finance_Electives/Operations_Management/","title":"Operations Management","text":""},{"location":"Finance_Electives/Operations_Management/#references","title":"References","text":""},{"location":"Finance_Electives/Revenue_Optimization/","title":"Revenue Optimization","text":""},{"location":"Finance_Electives/Revenue_Optimization/#references","title":"References","text":"<ul> <li>https://ocw.mit.edu/courses/15-071-the-analytics-edge-spring-2017/</li> <li>Airline Planning &amp; Optimisation | TUD-AE4423</li> </ul>"},{"location":"Finance_Electives/Revenue_Optimization/01_Introduction/","title":"01 Introduction","text":""},{"location":"Finance_Electives/Revenue_Optimization/01_Introduction/#goal","title":"Goal","text":"<ul> <li>Sell a minimum number of seats without selling every seat at discount prices, such that it is enough to cover fixed operating costs</li> <li>Sell remaining seats at higher rates to maximize revenue</li> </ul>"},{"location":"Finance_Electives/Revenue_Optimization/01_Introduction/#profit","title":"Profit","text":"\\[ \\begin{aligned} \\text{Profit} &amp;= \\text{Income} - \\text{Expenses} \\\\ &amp;= \\text{Sale Price} \\times \\min(\\text{Demand}, \\text{Quantity}) - \\text{Cost} \\times \\text{Quantity} \\end{aligned} \\]"},{"location":"Finance_Electives/Revenue_Optimization/01_Introduction/#passengers","title":"Passengers","text":"<p>Passengers have different valuations</p> Business people Others Keen on Flexibility \u2705 \u274c Booking Time Late Early Keen on refunds \u2705 \u274c Price Elasticity Low High Purchasing Power High Low"},{"location":"Finance_Electives/Revenue_Optimization/01_Introduction/#selling-cases","title":"Selling Cases","text":"Sell too many discounted seats Not enough seats for high-paying passengers Sell too many discounted seats Empty seats at takeoff <p>Lost revenue in both scenarios</p>"},{"location":"Finance_Electives/Revenue_Optimization/01_Introduction/#optimization","title":"Optimization","text":"<p>We can formulate using Optimization </p> <ul> <li>Objective Function: Maximize Total Revenue</li> <li>Constraints</li> <li>Seats sold \\(&gt;=\\) 0</li> <li>Seats sold \\(&lt;=\\) Capacity</li> <li>Seats sold \\(&lt;=\\) Demand</li> </ul>"},{"location":"Finance_Electives/Revenue_Optimization/01_Introduction/#shadow-price","title":"Shadow Price","text":"<p>Marginal revenue for unit increase in demand of regular seats</p>"},{"location":"Finance_Electives/Revenue_Optimization/02_Airline_Industry/","title":"Airline Industry","text":""},{"location":"Finance_Electives/Revenue_Optimization/02_Airline_Industry/#types-of-optimization","title":"Types of Optimization","text":"Point-to-Point Network Entire trip"},{"location":"Finance_Electives/Revenue_Optimization/02_Airline_Industry/#single-leg","title":"Single-Leg","text":""},{"location":"Finance_Electives/Revenue_Optimization/02_Airline_Industry/#marketing-focus","title":"Marketing Focus","text":"<p>Marginal revenue analysis</p> Initial Increasing discounted fare demand through marketing Increasing regular fare demand through marketing <p></p>"},{"location":"Finance_Electives/Revenue_Optimization/02_Airline_Industry/#capacity-allocation","title":"Capacity Allocation","text":"Initial Increasing capacity(assuming demand remains constant)"},{"location":"Finance_Electives/Revenue_Optimization/02_Airline_Industry/#multi-leg","title":"Multi-Leg","text":"<p>Passengers on multiple legs must be considered for all the legs that they are a part of.</p>"},{"location":"Finance_Electives/SAPM/","title":"Security Analysis &amp; Portfolio Management","text":"<p>Course will focus on investing, not on trading/speculationCourse is not going to give tips to earn super-normal profits</p>"},{"location":"Finance_Electives/SAPM/#tools","title":"Tools","text":"<ul> <li>https://finance.yahoo.com/screener/</li> <li>https://wallmine.com/screener</li> <li>http://alphavantage.co/</li> <li>https://iexcloud.io/</li> <li>https://finnhub.io/</li> <li>https://alpaca.markets/</li> </ul>"},{"location":"Finance_Electives/SAPM/#recommended-readings","title":"Recommended Readings","text":"<ul> <li> Investment Fables</li> <li> Stocks for the long run</li> <li> The Intelligent Investor (Value investing)</li> <li> Technical Analysis of Stock Trends</li> <li> Irrational Exuberance (Psychological aspects of investing)</li> <li> The Black Swan</li> <li> Fooled by Randomness</li> <li> Flash Boys</li> </ul>"},{"location":"Finance_Electives/SAPM/#recommended-movies","title":"Recommended Movies","text":"<ul> <li> Big Short</li> <li> Inside Job</li> <li> Trillion Dollar Bet</li> <li> Floored</li> <li> Wall Street</li> <li> Barbarians at the Gate</li> <li> Rogue Trader</li> <li> Margin Call</li> <li> Too Big to Fail</li> </ul>"},{"location":"Finance_Electives/SAPM/#references","title":"References","text":"<ul> <li> MIT 18.S096 Topics in Mathematics w Applications in Finance</li> <li> Aswath Damodaran | Investment Philosophies</li> <li> ML in Finance| Hudson &amp; Thames</li> <li> Marcos Lopez de Prado Talks</li> <li> IIT Roorkee | Quantitative Investment Management</li> <li> Security Analysis &amp; Portfolio Management | IIT Roorkee</li> <li> Mathematical Portfolio Theory | IIT Guwahati</li> </ul>"},{"location":"Finance_Electives/SAPM/01_Introduction/","title":"Introduction","text":""},{"location":"Finance_Electives/SAPM/01_Introduction/#investment","title":"Investment","text":"<p>Sacrificing current resources with the expectation of future gains</p> <ul> <li>Sacrificing current resources is certain</li> <li>Future returns has risk &amp; uncertainty</li> </ul> <p>It is nearly impossible to \u201cbeat the market\u201d consistently</p>"},{"location":"Finance_Electives/SAPM/01_Introduction/#investment-vs-speculation","title":"Investment vs Speculation","text":"Investment Speculation Buying undervalued, holding for a long time and selling high, hence making a largecapital gain Buying and selling of high-risk securities with anticipation of earning higher returns in the short-term Horizon Long Short"},{"location":"Finance_Electives/SAPM/01_Introduction/#return","title":"Return","text":"<p>Total income an investor gets from their investment every year</p> <p>Compensation for</p> <ul> <li>Time</li> <li>Inflation</li> <li>Risk</li> <li>Opportunity cost (Compensation for postponing consumption)</li> </ul>"},{"location":"Finance_Electives/SAPM/01_Introduction/#investment-amount-factors","title":"Investment Amount Factors","text":"<ul> <li>Income</li> <li>Expenses (Necessary/Optional)</li> <li>Time Horizon</li> <li>Expected Return</li> <li>Risk tolerance</li> </ul>"},{"location":"Finance_Electives/SAPM/01_Introduction/#investment-steps","title":"Investment Steps","text":"<ol> <li>Set investment objectives (Factors)</li> <li>Major asset allocation</li> <li> <p>Portfolio generation</p> </li> <li> <p>Asset/Security selection</p> </li> <li> <p>Proportion</p> </li> <li> <p>Execution</p> </li> <li>Performance Review</li> <li>Portfolio Revision: Inclusion/exclusion of assets in an    existing portfolio or changing the ratio of funds invested</li> </ol> <p>Compare portfolio with benchmark returns and revise portfolio 7. Go to step 1</p>"},{"location":"Finance_Electives/SAPM/01_Introduction/#investment-objectives","title":"Investment Objectives","text":"<ul> <li>Returns</li> <li>Regular Income<ul> <li>Stock Dividends</li> <li>Bond Coupon</li> <li>Zero-coupon bond maturity repayment should not be taken as capital appreciation; it is interest income</li> </ul> </li> <li>Capital Appreciation<ul> <li>Stock Value</li> <li>Bond Value increment due to change in market interest rate</li> </ul> </li> <li>Safety/Risk</li> <li>Liquidity</li> <li> <p>Tax factors: Govt security bonds are free from tax </p> </li> <li> <p>Ease of management</p> </li> <li>Legal &amp; regulatory factors</li> <li>Unique needs &amp; preferences</li> <li>Duration of investment</li> <li>Frequency of performance evaluation</li> </ul>"},{"location":"Finance_Electives/SAPM/01_Introduction/#asset-allocation","title":"Asset Allocation","text":"Strategic Tactical Approach buy-and-hold Duration Long-term Short-term"},{"location":"Finance_Electives/SAPM/01_Introduction/#security-selection","title":"Security Selection","text":"Value Stock Growth Stock Valuation Under-valued Overvalued Price to earnings Low High Volatility Low High Dividends High Low/No Source of Return Dividends Expected capital gain Cyclical Stock Defensive Stock Volatility High Low Sensitivity to market trends High Low Company usually deals with Luxury goods Necessities Comment Follow all cycles of economy: expansion, peak, and recession, recovery Outperform market during economic slowdown"},{"location":"Finance_Electives/SAPM/01_Introduction/#security","title":"Security","text":"<p>Always invest in business, not stocks</p> <p>Stocks don\u2019t always \u2018win\u2019 in the long-run. Index funds are better, as they keep revising the portfolio. Easier to invest in index funds for passive income rather than evaluating yourself.</p> <p>Yield on bond market actually is more volatile than stock market, due to fluctuations in the interest rate</p>"},{"location":"Finance_Electives/SAPM/01_Introduction/#taxes","title":"Taxes","text":""},{"location":"Finance_Electives/SAPM/01_Introduction/#dividends","title":"Dividends","text":"<p>Fully-taxable regardless of the dividend amount</p> <p>Exception: if you are below the taxable income slab</p>"},{"location":"Finance_Electives/SAPM/01_Introduction/#capital-gains","title":"Capital Gains","text":"Tax in India Taxable when STCGShort-Term Capital Gains 15% Any gain LTCGLong-Term Capital Gains 10% Only if gain &gt; 1 lakh"},{"location":"Finance_Electives/SAPM/01_Introduction/#note","title":"Note","text":"<ul> <li>When comparing investments, remember about Survivorship Bias</li> </ul>"},{"location":"Finance_Electives/SAPM/02_Risk_Returns/","title":"Risk and Returns","text":"<p>Note: Horizon need not always be \\(h=1\\)</p>"},{"location":"Finance_Electives/SAPM/02_Risk_Returns/#return","title":"Return","text":"<p>\u201cReturn is backward-looking\u201d $$ r(t, h) = y_t - y_{t-h} $$</p>"},{"location":"Finance_Electives/SAPM/02_Risk_Returns/#roi","title":"ROI","text":"<p>% change in series</p> <p>Return on investment is in percentage relative to original investment</p> ROI \\(R_t\\) Time Additive? Multi-Period Return is __ sum of individual returns Simple \\(\\dfrac{y_t - y_h}{y_h}\\) \u274c Geometric Continuous(Preferred) \\(\\ln \\left \\vert \\dfrac{y_t}{y_h} \\right \\vert = \\ln \\vert y_t \\vert - \\ln \\vert y_{t_h} \\vert\\) \u2705 Arithmetic \\[ \\text{CR} = \\ln \\vert 1 + \\text{SR} \\vert \\]"},{"location":"Finance_Electives/SAPM/02_Risk_Returns/#re-investment-benefit","title":"Re-Investment Benefit","text":"\\[ \\text{Re-Investment Benefit} = \\text{IRR} - \\text{ROI} \\] <p>Benefit that could be obtained by investing all intermediate inflows at the same ROI</p>"},{"location":"Finance_Electives/SAPM/02_Risk_Returns/#yield","title":"Yield","text":"<p>\u201cYield is forward-looking\u201d $$ Y_t = \\dfrac{y_t - y_h}{y_t} $$</p>"},{"location":"Finance_Electives/SAPM/02_Risk_Returns/#dividends","title":"Dividends","text":"<p>Dividend rate are relative to face value, not your investment</p>"},{"location":"Finance_Electives/SAPM/02_Risk_Returns/#dates","title":"Dates","text":"Dividend Declaration Date Ex-Dividend Date Record Date Payment Date"},{"location":"Finance_Electives/SAPM/02_Risk_Returns/#return-series","title":"Return Series","text":"<p>Assumed to be a random walk</p>"},{"location":"Finance_Electives/SAPM/02_Risk_Returns/#expected-returns","title":"Expected Returns","text":"\\[ E(R) = \\sum_i r_i \\cdot P(r_i) \\]"},{"location":"Finance_Electives/SAPM/02_Risk_Returns/#risk","title":"Risk","text":"<p>Chance of actual return differing from expected return</p> <p>Statistically quantified through variance/standard deviation of returns\u2019 PDF</p>"},{"location":"Finance_Electives/SAPM/02_Risk_Returns/#types-of-unknowns","title":"Types of Unknowns","text":"Systematic risk Unsystematic risk Uncertainty Meaning Sensitivity to market fluctuations Personal factors Unknown effects Type ExternalMacro InternalMicro External Minimizable \u274c \u2705through diversification (portfolio) \u274c Risk Compensation expected \u2705 \u2705 \u274c \\[ \\begin{aligned} \\text{Risk: } \\sigma^2 &amp;= \\text{SR} + \\text{UR} \\\\ \\text{SR} &amp;= \\beta^2 \\cdot \\sigma^2 (R_m) \\end{aligned} \\]"},{"location":"Finance_Electives/SAPM/02_Risk_Returns/#risk-measures","title":"Risk Measures","text":"Standard Deviation \\(\\sigma (R_p)\\) Beta(Market sensitivity) \\(\\dfrac{\\text{cov} (R_p, R_m)}{\\sigma^2_{m}}\\) Semi Deviation \\(\\sigma (\\text{Loss}_p)\\)\\(\\text{Loss}_t = \\arg \\max(R_t, 0)\\) <p>where \\(p=\\) portfolio and \\(m=\\) market</p>"},{"location":"Finance_Electives/SAPM/02_Risk_Returns/#risk-return-tradeoff","title":"Risk-Return Tradeoff","text":"<ul> <li>Investors are rational and risk-averse: prefer less risk investments</li> <li>Investors expect risk premium: Investors are ready to take risk only with the expectation of higher return</li> </ul> \\[ R_\\min = R_f + \\underbrace{\\left ( \\dfrac{R_m - R_f}{\\sigma_m} \\right )}_\\text{Market Price of Risk} \\sigma \\]"},{"location":"Finance_Electives/SAPM/02_Risk_Returns/#jensens-inequality","title":"Jensen\u2019s Inequality","text":"<p>Using Jensen\u2019s Inequality $$ E[f(x)] \\ne f(E[x])  \\ \\implies E[u(R)] &gt; u(E[R]) $$ where</p> <ul> <li>\\(R\\) is the return obtained</li> <li>\\(u(R)\\) is the utility obtained from the return</li> </ul>"},{"location":"Finance_Electives/SAPM/02_Risk_Returns/#effect-of-frequency-on-volatility","title":"Effect of Frequency on Volatility","text":"\\[ V \\propto \\nu \\]"},{"location":"Finance_Electives/SAPM/02_Risk_Returns/#trading-days","title":"Trading Days","text":"Trading Days Fixed-Income 365.25 Variable-Income 252"},{"location":"Finance_Electives/SAPM/02_Risk_Returns/#annualization","title":"Annualization","text":"\\[ \\begin{aligned} \\text{Annual } E(R) &amp;= 252 \\times E(R) \\\\ \\text{Annual } \\sigma(R) &amp;= \\sqrt{252} \\times \\sigma(R) \\end{aligned} \\] <p>There are 252 trading days in a year</p>"},{"location":"Finance_Electives/SAPM/02_Risk_Returns/#idk","title":"IDK","text":"<p>Fixed-income securities are also very volatile</p>"},{"location":"Finance_Electives/SAPM/02_Risk_Returns/#ytm","title":"YTM","text":"<p>Yield to Maturity = IRR of security if held until maturity</p>"},{"location":"Finance_Electives/SAPM/03_Trading_Styles/","title":"Trading Styles","text":""},{"location":"Finance_Electives/SAPM/03_Trading_Styles/#margin-trading","title":"Margin Trading","text":"<p>Allows investors to leverage positions</p> <p>A margin account provides you the resources to buy more quantities of a stock than you can afford at any point of time. For this purpose, the broker would lend the money to buy shares and keep them as collateral.</p> <p>You need to maintain</p> <ul> <li>Initial margin</li> <li>Maintenance margin</li> </ul> <p>Margin calls occur when an investor's equity falls below a certain threshold, and the broker requires the investor to deposit more funds into their account.</p>"},{"location":"Finance_Electives/SAPM/03_Trading_Styles/#short-selling","title":"Short Selling","text":"<p>Trader borrows shares from a broker and immediately sells them with the expectation that the share price will fall shortly after. If it does, the trader can buy the shares back at the lower price, return them to the broker, and keep the difference, minus any loan interest, as profit</p> <p>Being short in an asset means investing in such a way that the investor will profit if the value of the asset falls. This is the opposite of a more conventional \"long\" position, where the investor will profit if the value of the asset rises</p> <p>It is riskier than going \u201clong\u201d</p> Profit Bound Loss Bound Long Unlimited Limited: Cost of stock Short Limited: Cost of stock Unlimited <p>Security Lending and Borrowing Scheme \u2013 1997 (SLB):</p> <ul> <li>SLB is a scheme that has been launched to enable the   settlement of securities sold short.</li> <li>SLB enables lending of idle securities by the investors   through the clearing corporation/clearing house of   stock exchanges to earn a return through the same.</li> <li>Securities in the F&amp;O segment are eligible for SLB</li> </ul>"},{"location":"Finance_Electives/SAPM/03_Trading_Styles/#day-trading","title":"Day Trading","text":"<p>Practice of buying and selling financial instruments within the same trading day, such that all positions are usually closed before the market close for the trading day</p> <p>Square-off position intra-day</p> <p>Institutional traders are not permitted to do day trading in India</p>"},{"location":"Finance_Electives/SAPM/04_Fundamental_Analysis/","title":"Fundamental Analysis","text":"<p>Examines various factors affecting supply and demand conditions, thereby influencing future income from and value of an investment</p> <p>Studies fundamental factors that determine earnings and risks associated with a share</p> <p>Suitable for</p> <ul> <li>Long-term investing: value of share that should prevail in capital market</li> <li>Determining intrinsic worth of share</li> <li>Explaining price-behavior of shares in terms of underlying fundamental factors</li> </ul>"},{"location":"Finance_Electives/SAPM/04_Fundamental_Analysis/#why-invest-in-assets-with-low-pe-ratio","title":"Why invest in assets with low P/E ratio?","text":"<ul> <li>Store of value</li> <li>Expected future earnings</li> </ul>"},{"location":"Finance_Electives/SAPM/04_Fundamental_Analysis/#geic-model","title":"GEIC Model","text":"<ul> <li>**G**lobal economy</li> <li>Domestic **e**conomy</li> <li>**I**ndustry</li> <li>**C**ompany</li> </ul> <pre><code>flowchart LR\n\nsubgraph ee[Economic&lt;br/&gt;Environment]\n    direction LR\n    ge[Global&lt;br/&gt;Economy]\n    e[Domestic&lt;br/&gt;Economy]\n    i[Industry] \nend\n\nee --&gt; Company --&gt; Performance --&gt; v[Value&lt;br/&gt;of share]</code></pre>"},{"location":"Finance_Electives/SAPM/04_Fundamental_Analysis/#global-economy","title":"Global Economy","text":"<ul> <li>Prospects for Exports</li> <li>Price competition</li> <li>Cost of foreign inputs</li> <li>Profits through foreign investments</li> <li>Exchange rate fluctuations</li> <li>Risk of changing political environment in world</li> <li>Increasing/decreasing peace in international scenario</li> </ul> <p>Stronger globalization and international collaboration, higher the importance of monitoring global economy. Eg: sub-prime crisis</p>"},{"location":"Finance_Electives/SAPM/04_Fundamental_Analysis/#economy","title":"Economy","text":"<p>Shocks: Change in macro-economic vars provide a force that goes against inertia inherent in the performance of firms, and hence the share prices</p> <p>Types of shocks</p> <ul> <li>Supply</li> <li>Demand</li> <li>Financial market</li> </ul>"},{"location":"Finance_Electives/SAPM/04_Fundamental_Analysis/#pest","title":"PEST","text":"Dimension Factors Political Political stabilityTax PolicyEmployment lawsEnvironment regulationsTrade restrictions &amp; tariffs Economic Economic growthInterest ratesExchange ratesInflation rateUnemployment Social Health consciousnessPopulation growth rateAge distributionCareer attitudesEmphasis on safetyCultural dimensions of society Technological R&amp;D ActivityAutomationTechnological incentivesRate of technological change"},{"location":"Finance_Electives/SAPM/04_Fundamental_Analysis/#swot","title":"SWOT","text":"<p>Strengths, Weaknesses, Opportunities, Threats</p>"},{"location":"Finance_Electives/SAPM/04_Fundamental_Analysis/#industry","title":"Industry","text":"<p>An industry is set of companies that serves a particular niche of consumers</p>"},{"location":"Finance_Electives/SAPM/04_Fundamental_Analysis/#industry-phase","title":"Industry Phase","text":"<p>Identify which phase of the life cycle the industry belongs to</p> Phase Pioneering Trial phasePotential to be commercialized Expansion Starting to be commercialized Stabilized/Mature Fully commercialized Decay Being abandoned"},{"location":"Finance_Electives/SAPM/04_Fundamental_Analysis/#structural-analysis","title":"Structural Analysis","text":"<p>Intensity of competition among firms in the same industry determines its profitability</p> <p>Michael Porter\u2019s Model: Rivalry among existing competitors depends on</p> <ul> <li>Threat of entry: New supplier</li> <li>Threat of substitution: New industry</li> <li>Bargaining powers of consumers</li> <li>Bargaining powers of suppliers</li> </ul>"},{"location":"Finance_Electives/SAPM/04_Fundamental_Analysis/#company","title":"Company","text":"<ul> <li>Quantitative: Financial</li> <li>Qualitative: Non-Financial</li> <li>Quality of mgmt</li> <li>Product portfolio/range</li> <li>Collaborations</li> <li>Shareholders pattern and listing</li> <li>R&amp;D, Innovation</li> <li>Diversification</li> <li>Does this company fall under strict Govt regulations</li> <li>Disputes &amp; contingent liabilities</li> <li>Availability of inputs</li> <li>Industrial relations</li> </ul>"},{"location":"Finance_Electives/SAPM/04_Fundamental_Analysis/#equity-valuation","title":"Equity Valuation","text":"<p>07_Equity_Valuation.md </p> <p>Check if stock is under-valued/over-valued</p>"},{"location":"Finance_Electives/SAPM/04_Fundamental_Analysis/#ratios","title":"Ratios","text":""},{"location":"Finance_Electives/SAPM/04_Fundamental_Analysis/#pe-ratio","title":"P/E Ratio","text":"<p>Price that market is willing to pay for one unit of earning</p> <p>Lower is better; extreme P/E ratios are not desirable</p> <p>Provides a benchmark in determining the value of a share and hence the value of shareholders</p> <p>There is a strong connection between P/E, dividend discount models, and fundamentals $$ \\begin{aligned} \\text{P/E} &amp;= \\dfrac{P_\\text{Dividend Growth Model}}{\\text{Earnings}} \\ &amp;= \\dfrac{1}{\\text{Earnings}} \\times \\dfrac{D_{t+1}}{k-g} \\ &amp;= \\dfrac{1}{\\text{Earnings}} \\times \\dfrac{D_t (1+g)}{k-g} \\ &amp;= \\dfrac{D_t}{\\text{Earnings}} \\times \\dfrac{1+g}{k-g} \\ &amp;= \\text{Dividend Payout Ratio} \\times \\dfrac{1+g}{k-g}  \\end{aligned} $$ </p> <p>How to determine if P/E ratio is good</p> <ul> <li>Company growth rate</li> <li>How fast has company been growing in the past?</li> <li>Are these rates expected to increase in the future</li> <li>Industry</li> </ul>"},{"location":"Finance_Electives/SAPM/04_Fundamental_Analysis/#peg-ratio","title":"PEG Ratio","text":"<p>Good way to decide if P/E ratio of company is high/low $$ \\text{PEG} = \\dfrac{\\text{P/E}}{\\text{Earnings Growth}} $$</p> PEG Ratio Interpretation Action \\((0, 0.50)\\) Undervalued Buy \\([0.50, 0.65)\\) Consider buying \\([0.65, 1.00)\\) Watch/Hold \\([1.00, 1.30)\\) Consider selling \\([1.30, 1.70)\\) Consider shorting \\([1.70, \\infty)\\) Overvalued Short"},{"location":"Finance_Electives/SAPM/04_Fundamental_Analysis/#preferred-values","title":"Preferred Values","text":"Preferred PV of dividends High P/E Low P/Dividends Low P/Book Value Low P/Sales Low PEG Low Capitalization Low"},{"location":"Finance_Electives/SAPM/05_Technical_Analysis/","title":"Technical Analysis","text":"<ol> <li>Identify trend changes at early stage through predicting stock price patterns, using historical data (volume/price)</li> <li>Identify when to buy/sell</li> <li>Maintain investment position until evidence indicates that trend has reversed</li> </ol>"},{"location":"Finance_Electives/SAPM/05_Technical_Analysis/#notes","title":"Notes","text":"<ol> <li>Probabilistic modelling; always uncertain, never deterministic</li> <li>Continued success is dependent on keeping successful strategies known only to a few</li> </ol>"},{"location":"Finance_Electives/SAPM/05_Technical_Analysis/#premises","title":"Premises","text":"<p>Market action determines everything</p> <ol> <li>Everything will continue in state of rest/uniform motion unless compelled by external force</li> <li>Market price is solely dependent on forces of demand and supply</li> <li>Prices have a tendency to move in trends that persist for appreciable duration</li> <li>Reversals of trends are caused by shifts in demand &amp; supply</li> <li>There is a time gap b/w technicians perceiving a change and when investors assesses the change</li> </ol>"},{"location":"Finance_Electives/SAPM/05_Technical_Analysis/#tools","title":"Tools","text":"<ul> <li>Typical</li> <li>Variables<ul> <li>Price</li> <li>Volume</li> <li>Rate of change</li> </ul> </li> <li>Charts &amp; graphs<ul> <li>Line charts</li> <li>Linear scale</li> <li>Log scale</li> <li>Bar charts: OHLC - Open High Low Close</li> <li>Candle chart: </li> <li>Point &amp; figure chart</li> </ul> </li> <li>Dow Theory Measures</li> <li>Moving averages</li> <li>Momentum and oscillators</li> <li>Breadth</li> <li>Market indicators</li> <li>Investors\u2019 sentiments</li> <li>Contrary opinion</li> <li>Professional investors\u2019 behavior</li> <li>Economic indicators</li> </ul>"},{"location":"Finance_Electives/SAPM/05_Technical_Analysis/#typical","title":"Typical","text":"OHLC Candlestick chart Candle color:Red/black: close&gt;openGreen/Blue: close&gt;open"},{"location":"Finance_Electives/SAPM/05_Technical_Analysis/#dow-theory","title":"Dow Theory","text":"<p>Assumes that most shares follow the trend of the market most of the time</p> <p>It intends to show the general trend/direction of the market as a whole and does not predict the direction of change in a particular security</p> <p>In order to measure the \u201cmarket\u201d, two indices are used</p> <ul> <li>Industrial average: combination of blue-chip shares from industry</li> <li>Transportation average: shares of transport companies</li> <li>To reinforce the conclusions obtained from Industrial Average</li> </ul>"},{"location":"Finance_Electives/SAPM/05_Technical_Analysis/#trends","title":"Trends","text":"Primary Secondary Tertiary Overall trend- Business cycles- Intrinsic value Reactions that interrupt the progress of prices in primary trendMay give wrong signals &amp; confuse market Random movementsBuilding blocks to secondary trends Duration Few years Few months Day-to-day Direction - bullish: upward- bearish: downward Opposing primary trend: Technical reaction- Technical corrections: upward -&gt; downward- Technical rally: downward -&gt; upward Concerns Long-term investors Weak holdersTraders High-frequency traders"},{"location":"Finance_Electives/SAPM/05_Technical_Analysis/#principle-of-confirmation","title":"Principle of Confirmation","text":"<p>Whatever trends emerge in Industrial average must be confirmed by Transportation average</p> <p>If trends in share prices are contradictory to industrial production/transportation of goods, then one should not design a trading strategy in shares and must wait until one gets confirmation of trends</p>"},{"location":"Finance_Electives/SAPM/05_Technical_Analysis/#price-volume","title":"Price-Volume","text":"<p>Volume is the \u2018fuel\u2019 to move prices</p> <p>Usually, \\(\\text{Volume} \\propto \\text{Price}\\) , ie volume</p> <ul> <li>contracts on decline</li> <li>expands on rallies/advances</li> </ul> <p>If it is against this normal relationship, it is an indicator of an upcoming trend reversal. However, it should only be used as background information, since the actual reversals would be signaled by averages</p>"},{"location":"Finance_Electives/SAPM/05_Technical_Analysis/#price-actions","title":"Price Actions","text":"<p>Price actions determine the trend</p> <ul> <li>Bullish indications: successive rallies penetrate peaks while the trough of an intervening decline is above the preceding trough</li> <li>Bearish indications: series of declining peaks and troughs</li> </ul>"},{"location":"Finance_Electives/SAPM/05_Technical_Analysis/#averages","title":"Averages","text":"<p>Time Series Filters </p> <p>Usually uses the closing prices</p> <p>Convention is to use 2 averages</p> <ul> <li>Slower average: larger window</li> <li>Faster average: smaller window</li> </ul> <p>Notes</p> <ul> <li> <p>Normally, an average moves along with a trend; but a reversal in trend may be captured by a crossover of 2 averages</p> </li> <li> <p>Signals to buy/sell are generated when the</p> </li> <li> <p>price crosses the moving average</p> <p>or</p> </li> <li> <p>one MA crosses the another</p> </li> <li> <p>Doubtful about this</p> </li> <li> <p>Buy</p> <ul> <li>price &gt; moving average</li> <li>Faster average &gt; Slower average</li> </ul> </li> <li> <p>Sell</p> <ul> <li>price &lt; moving average</li> <li>Faster average &lt; Slower average</li> </ul> </li> <li> <p>MA is a lagging indicator \u2013&gt; crossover will usually signal a trend reversal well after new trend has begin and is used mainly for confirmation</p> </li> </ul>"},{"location":"Finance_Electives/SAPM/05_Technical_Analysis/#trend-channels","title":"Trend Channels","text":"<p>Trends have to be bounded</p> <ul> <li>Trend channels</li> <li>When prices trend between 2 parallel lines, this is referred to a channel<ul> <li>It is created by drawing 2 parallel lines</li> <li>Line 1: Basic/main trendline</li> <li>Line 2: Return/channel line</li> </ul> </li> </ul> <p>Use-case</p> <ul> <li>Represents area of support/resistance depending on direction of underlying trend</li> <li>Helps identify potential trend acceleration/reversal</li> </ul> Bullish Bearish"},{"location":"Finance_Electives/SAPM/05_Technical_Analysis/#envelops","title":"Envelops","text":"<p>2 symmetrical parallel lines to moving average</p> <p>This is based on principle that prices fluctuate around a given trend in cyclical movements</p> <p>Envelops consist of points of maximum and minimum divergence from some moving average</p>"},{"location":"Finance_Electives/SAPM/05_Technical_Analysis/#bollinger-bands","title":"Bollinger Bands","text":"<ul> <li>Middle line: \\(\\text{MA(close, \\(w\\))}\\)</li> <li>Upper band: \\(\\text{MA} + z_{\\alpha/2} * \\sigma (\\text{close}, w)\\)</li> <li>Lower band: \\(\\text{MA} - z_{\\alpha/2} * \\sigma (\\text{close}, w)\\)</li> </ul> <p>Whenever bands narrow, a change in trend occurs: Whenever bands narrow, they have been stable for a while and it is followed by movement which is more volatile and in opposite direction</p>"},{"location":"Finance_Electives/SAPM/05_Technical_Analysis/#patterns","title":"Patterns","text":""},{"location":"Finance_Electives/SAPM/05_Technical_Analysis/#psychological-barriers","title":"Psychological barriers","text":"<p>Support &amp; resistance levels</p> <ul> <li>Bargain hunters \u201csupport\u201d the lower level upwards</li> <li> <p>Profit takers \u201cresist\u201d the upper level downwards</p> </li> <li> <p>Breakout: prices go outside the support/resistance level</p> </li> <li>Pullback: prices return within support/resistance level</li> </ul> <p></p>"},{"location":"Finance_Electives/SAPM/05_Technical_Analysis/#patterns_1","title":"Patterns","text":"Pattern Trend Signal Head &amp; Shoulders Uptrend Bearish(reversal) Inverted head &amp; shoulders Downtrend Bullish(reversal) Symmetric triangle Uptrend Bullish Symmetric triangle Downtrend Bearish Ascending triangle Uptrend Bullish Rectangle Uptrend Bullish Rectangle Downtrend Bearish Flag Uptrend Bullish Flag Downtrend Bearish Pennant Uptrend Bullish Pennant Downtrend Bearish Cup &amp; Handle Bullish Inverted Cup &amp; Handle Bearish"},{"location":"Finance_Electives/SAPM/05_Technical_Analysis/#momentumoscillator","title":"Momentum/Oscillator","text":"<p>Measures the velocity of price move</p> Indicates Formula MACDMoving Averages Convergence Divergence Trend-Deviation \\(\\dfrac{\\text{Faster EMA}}{\\text{Slower EMA}}\\)or\\(\\text{Faster EMA} - \\text{Slower EMA}\\) Signal EMA of MACD RSIRelative Strength Index Buying/selling ratio\\(\\text{RSI} &gt; 0.70 \\implies\\) Overbought\\(\\text{RSI} &lt; 0.30 \\implies\\) Oversold \\(\\dfrac{\\text{RS}}{1+\\text{RS}}\\)\\(\\text{RS} = \\dfrac{\\text{Avg(gains)}_w}{\\text{Avg(losses)}_w}\\)where \\(w=\\) window size"},{"location":"Finance_Electives/SAPM/06_Portfolio/","title":"Portfolio","text":"<p>Pool of securities combined such that</p> <ul> <li>Maximizes expected returns</li> <li>Minimizes unsystematic risk</li> </ul> <p>Concept of hedging</p> <p>Try to diversify on all 4 pillars of GEIC</p>"},{"location":"Finance_Electives/SAPM/06_Portfolio/#aspects","title":"Aspects","text":"<ul> <li>What set of securities to be selected</li> <li>What proportions</li> <li>Selection of optimum portfolio</li> </ul>"},{"location":"Finance_Electives/SAPM/06_Portfolio/#characteristics","title":"Characteristics","text":"<p>Let \\(w_i\\) be the fraction of investment allocation to security \\(i\\)</p> <ul> <li>\\(w_i \\in [0, 1]\\)</li> <li>\\(\\sum_i w_i = 1\\)</li> <li>\\(w_i&lt;0 \\implies\\) Taking loan?</li> </ul> <p>Note: We assume Gaussian distribution of returns for all securities. If violated, then analyze accordingly $$ \\begin{aligned} E[R_p] &amp;= \\sum_i^n w_i R_i \\</p> <p>\\sigma^2_{R_p} &amp;= \\sum_i^n (w_i \\sigma_i)^2 + 2 \\sum_{i=1}^{\\lceil n/2 \\rceil} \\sum_{j&gt;i}^n w_i w_j \\sigma_i \\sigma_j \\rho_{ij} \\ \\beta_p &amp;= \\sum_i^n w_i \\beta_i \\end{aligned} $$</p> <p>where</p> <ul> <li>\\(\\rho_{ij} =\\) correlation between 2 securities \\(i\\) and \\(j\\)</li> <li>\\(\\beta_i =\\) \\(\\beta\\) of security \\(i\\)</li> <li>Given +ve portfolio weights on 2 shares, the lower the correlation between them, the lower the variance of the portfolio</li> </ul>"},{"location":"Finance_Electives/SAPM/06_Portfolio/#minimum-variance-portfolio","title":"Minimum Variance Portfolio","text":"<p>A portfolio of group of shares that minimizes the return variance is the portfolio that has equal variance with every share return $$ w^* = \\arg \\min \\sigma^2_{R_p} \\ \\implies w^* = w   @   \\dfrac{d \\sigma^2_{R_p}}{dw} = 0 $$</p>"},{"location":"Finance_Electives/SAPM/06_Portfolio/#2-securities","title":"2 Securities","text":"\\[ \\begin{aligned} w^* &amp;= (w_1^*, 1-w_1^*) \\\\ w_1^* &amp;= \\dfrac{\\sigma_2^2 - \\sigma_1 \\sigma_2 \\rho_{12}}{\\sigma_1^2 + \\sigma_2^2 - 2 \\sigma_1 \\sigma_2 \\rho_{12}} \\end{aligned} \\]"},{"location":"Finance_Electives/SAPM/06_Portfolio/#types-of-portfolios","title":"Types of Portfolios","text":"Value-Weighted"},{"location":"Finance_Electives/SAPM/06_Portfolio/#benchmark","title":"Benchmark","text":"<ul> <li>60% Equity, 40% Bonds</li> </ul>"},{"location":"Finance_Electives/SAPM/06_Portfolio/#india","title":"India","text":"<p>Nifty50 makes a 12% average return, but actually, entire pool of Indian stock market makes a negative return</p> <p>Retail investors lose money due to single-stock investment</p> <p>Exane, Expose</p>"},{"location":"Finance_Electives/SAPM/07_Portfolio_Theory/","title":"Portfolio Theory","text":""},{"location":"Finance_Electives/SAPM/07_Portfolio_Theory/#markowitz-theory","title":"Markowitz Theory","text":"<p>Naive diversification of portfolio may/may not decrease risk</p> <p>Markowitz: Nature and degree of covariates between securities determine whether portfolio risk can be reduced</p> <p>Diversification pays when the securities have negative correlation</p>"},{"location":"Finance_Electives/SAPM/07_Portfolio_Theory/#contributions","title":"Contributions","text":"<ol> <li>Quantification of risk &amp; return</li> <li>Efficient portfolio</li> </ol>"},{"location":"Finance_Electives/SAPM/07_Portfolio_Theory/#efficient-set","title":"Efficient Set","text":"<p>A set of portfolios is called efficient set, if all the portfolios in it are non-dominated portfolios in terms of mean-variance dominance principle</p>"},{"location":"Finance_Electives/SAPM/07_Portfolio_Theory/#mean-variance-dominance-principle","title":"Mean-Variance Dominance Principle","text":"<p>A portfolio A dominates portfolio B if</p> <ul> <li>For given risk, portfolio A has higher expected return</li> <li>For given expected return, portfolio A has same/lower risk</li> </ul>"},{"location":"Finance_Electives/SAPM/07_Portfolio_Theory/#efficient-frontier","title":"Efficient Frontier","text":""},{"location":"Finance_Electives/SAPM/07_Portfolio_Theory/#mean-variance-analysis","title":"Mean-Variance Analysis","text":"\\[ \\text{Obj}_\\max = E[R_p] - \\lambda \\sigma^2_{R_p} \\]"},{"location":"Finance_Electives/SAPM/07_Portfolio_Theory/#limitations","title":"Limitations","text":"Limitation Solution Variance is not ideal risk measurement since it penalizes both unwanted high losses and desired low losses Semi-deviationVaR, CVaR Sensitive to estimated parameters \\(\\mu\\) and \\(\\sigma\\) Robust optimization"},{"location":"Finance_Electives/SAPM/07_Portfolio_Theory/#diversification-reduction-of-dispersion","title":"Diversification &amp; Reduction of Dispersion","text":"<p>Shows how naive diversification reduces discretion of returns in a stock portfolio</p> \\(\\text{Risk} \\propto \\dfrac{1}{\\text{no of securities}}\\) \\(\\text{Marginal Risk Reduction} = -k\\) Diminishing reduction with no of securities 15-20 securities found to be the most appropriate"},{"location":"Finance_Electives/SAPM/07_Portfolio_Theory/#sharpes-single-factorindex-model","title":"Sharpe\u2019s Single Factor/Index Model","text":"<p>Linear relation between return of a security and the market index, in the absence of risk-free asset $$ R_i = \\alpha + \\beta R_m + \\epsilon $$</p> <ul> <li>\\(\\beta=\\) systematic risk</li> <li>\\(\\alpha=\\) return independent of market</li> </ul>"},{"location":"Finance_Electives/SAPM/07_Portfolio_Theory/#capital-market-line","title":"Capital Market Line","text":"<p>Upon introduction of Risk-Free Asset, Efficient frontier becomes a straight line rising from risk-free rate and tangential to Markowitz Efficient Frontier, called Capital Market Line</p> <p></p> <ul> <li>Lending portfolio: Portfolio where risk-averse investors in low risk assets</li> <li>Borrowing portfolio: Portfolio where risk-tolerant investors borrow at risk-free rate and invest in risky assets</li> </ul> <p>Consists of efficient portfolios constructed by combining risk-free security and market portfolio</p> <p>Represents equilibrium in the capital market. It does not show the relationship between expected rate of return of asset with individual risk</p> <p>All portfolios on CML are perfectly correlated with market portfolio and hence they are completely diversified, and hence possess no unsystematic risk; the portfolio that the investor should pick depends on the risk tolerance</p> <p>The slope of the CML gives the Sharpe Ratio of the market portfolio</p> \\[ E(R_p) = R_f + \\left[ \\dfrac{R_m-R_f}{\\sigma_m} \\right ] \\sigma_p \\]"},{"location":"Finance_Electives/SAPM/07_Portfolio_Theory/#systematic-risk-principle","title":"Systematic Risk Principle","text":"<p>In an equilibrium situation, the market will price only systematic risk</p> <p>Hence, expected return on asset only depends on systematic risk</p>"},{"location":"Finance_Electives/SAPM/07_Portfolio_Theory/#capital-asset-pricing-model","title":"Capital Asset Pricing Model","text":""},{"location":"Finance_Electives/SAPM/07_Portfolio_Theory/#assumptions","title":"Assumptions","text":"<ul> <li>Investments judged only on associated risk and return</li> <li>Investors</li> <li>maximize expected utility determined on associated risk and return</li> <li>rational</li> <li>risk-averse</li> <li>Markowitz efficient</li> <li>have same holding time horizon as others</li> <li>can have unlimited borrowing and lending at risk-free rate</li> <li>Market</li> <li>perfectly-competitive</li> <li>frictionless<ul> <li>no transaction cost</li> <li>no information cost</li> </ul> </li> <li>Capital market is in equilibrium</li> <li>Capital assets are perfectly divisible</li> </ul>"},{"location":"Finance_Electives/SAPM/07_Portfolio_Theory/#sml","title":"SML","text":"<p>Linear relationship between \\(E[R_i]\\) vs \\(\\beta_i\\) $$ E[R_i] = R_f + \\left[ \\dfrac{R_m-R_f}{\\beta_m} \\right ] \\beta_i $$</p> <p>Isn\u2019t always \\(\\beta_m = 1\\)</p> \\(R_i &gt; E[R_i]\\) underpriced \\(R_i &lt; E[R_i]\\) overpriced \\(\\beta_i &gt; 1\\) aggressive \\(\\beta_i &lt; 1\\) defensive <p>Slope is the Treynor Ratio</p> <p></p>"},{"location":"Finance_Electives/SAPM/07_Portfolio_Theory/#arbitrage-pricing-theory","title":"Arbitrage Pricing Theory","text":"<p>Multi-factor pricing model, based on Arbitrage and Law of One Price</p> <p>Linear relationship between expected return of security and \\(\\beta\\)s of \\(k\\) factors</p> <p>Only sources of risk relevant in a security are systematic risk caused by \\(k\\) factors</p> <p>Breaks market risk into \\(k\\) components $$ E[R_i] = R_f + \\sum_{\\text{Factor: }j=1}^k \\Big( E[R_{j}] - R_f \\Big) \\beta_{ji} $$ These \\(k\\) factors could be</p> <ul> <li>Chen, Roll &amp; Ross</li> <li>Growth rate in industrial production</li> <li>Expected &amp; unexpected inflation rate</li> <li>Spread between long-term and short-term interest rates</li> <li>Spread between low-grade and high-grade bonds</li> <li>Fama &amp; French</li> <li>Firm size</li> <li>\\(\\dfrac{\\text{Book value}}{\\text{market value}}\\)</li> <li>Market portfolio</li> <li>Carhart: Fame &amp; French model + Momentum</li> </ul>"},{"location":"Finance_Electives/SAPM/07_Portfolio_Theory/#arbitrage","title":"Arbitrage","text":"<p>Arbitrage opportunity arises when investor can earn risk-free profits without making net investment</p>"},{"location":"Finance_Electives/SAPM/07_Portfolio_Theory/#law-of-one-price","title":"Law of One Price","text":"<p>If 2 assets are equivalent in all economically relevant aspects, they should have the same market price</p> <p>Enforced by arbitrageurs: if they observe any deviation, they engage in arbitrage and this will eliminate the opportunity</p>"},{"location":"Finance_Electives/SAPM/07_Portfolio_Theory/#limitations_1","title":"Limitations","text":"<ul> <li>Does not suggest any optimum portfolio that is to be selected by investor</li> <li>Does not explain how investors decide investment portfolio</li> </ul>"},{"location":"Finance_Electives/SAPM/07_Portfolio_Theory/#von-neumann-morgenstern-utility-theory","title":"Von Neumann-Morgenstern Utility Theory","text":"<ul> <li>Rational portfolio choice must apply preferences based on expected utility</li> <li> <p>Optimal portfolio solves the expected utility max problem</p> </li> <li> <p>Wealth after one period: \\(W = W_0 (1+R_p)\\)</p> </li> <li>Expected utility: \\(E[u(W)] = E[ \\ u \\Big( W_0(1+R_p) \\Big) \\ ]\\)</li> </ul>"},{"location":"Finance_Electives/SAPM/08_Portfolio_Evaluation/","title":"Portfolio Evaluation","text":"<p>Evaluation of portfolio as whole, without examining the individual securities.</p> <p>However, for portfolio revision, you need to examine the individual securities.</p>"},{"location":"Finance_Electives/SAPM/08_Portfolio_Evaluation/#metrics","title":"Metrics","text":"Ratio Sharpe \\(\\dfrac{R_P - R_f}{\\sigma_p}\\) Price premium per unit risk Treynor \\(\\dfrac{R_P-R_f}{\\beta_P}\\) Price premium per unit \\(\\beta\\) Jensen \\(\\alpha\\) \\(R_p - R_\\min\\) Excess return more than required Calmar \\(\\dfrac{R_p}{\\text{Max Drawdown}}\\) Sterling \\(\\dfrac{R_p}{\\text{Max Drawdown} - 10 \\%}\\)"},{"location":"Finance_Electives/SAPM/08_Portfolio_Evaluation/#drawdown","title":"Drawdown","text":"<p>Percentage peak-to-trough decline during a specific time period</p> <p>Measured once a new high is reached, because a minimum cannot be measured yet since the value could decrease further</p>"},{"location":"Finance_Electives/SAPM/08_Portfolio_Evaluation/#sharpe-ratio","title":"Sharpe Ratio","text":""},{"location":"Finance_Electives/SAPM/08_Portfolio_Evaluation/#limitations","title":"Limitations","text":"<p>Selection bias of strategies results in false-positives regarding the success of a strategy</p>"},{"location":"Finance_Electives/SAPM/08_Portfolio_Evaluation/#deflated-sharpe-ratio","title":"Deflated Sharpe Ratio","text":"<p>Probability that SR is statistically-significant, after controlling for inflationary effect of</p> <ul> <li>No of independent trials with the strategy \\(k\\)</li> <li>List all the returns of all strategies</li> <li> <p>Find the independent series</p> </li> <li> <p>Data Dredging \\(V \\left[ \\widehat{\\text{SR}}_k \\right]\\)</p> </li> <li>Non-normality of returns: \\(\\hat y_3, \\hat y_4\\)</li> <li>Length of time series \\(T\\)</li> </ul> <p>Can help identify if the benefits is due to chance</p>"},{"location":"Finance_Electives/SAPM/09_Fund_Management_Evaluation/","title":"Fund Management Evaluation","text":""},{"location":"Finance_Electives/SAPM/09_Fund_Management_Evaluation/#metrics","title":"Metrics","text":"Metric Formula Meaning Net Selectivity Measure\\(R_r\\) \\(R_P - (R_f+R_S+R_U)\\) Excess return obtained solely through portfolio optimization Expense Ratio \\(\\dfrac{\\text{Total Expenses}}{\\text{Net Assets}}\\) Portfolio Turnover Ratio \\(\\dfrac{\\min(\\text{Purchases}, \\text{Sales})}{\\text{Assets}}\\) How quickly securities in fund are bought/sold by fund manager Tracking Error \\(\\sigma(R_P-R_B)\\) How well portfolio tracks the benchmark"},{"location":"Finance_Electives/SAPM/09_Fund_Management_Evaluation/#famas-decomposition-of-total-return","title":"Fama\u2019s Decomposition of Total Return","text":"\\[ R_P = R_f + R_s + R_u + R_r \\] \\(R_s\\) Return from systematic risk \\((R_m-R_f) \\beta_P\\) \\(R_u\\) Return from unsystematic risk \\((R_m-R_f)  \\left(\\dfrac{\\sigma_P}{\\sigma_m} - \\beta_P \\right)\\) \\(R_r\\) Residual return/Net Selectivity Measure"},{"location":"Finance_Electives/SAPM/09_Fund_Management_Evaluation/#expense-ratio","title":"Expense Ratio","text":"Type Typical \\(\\%\\) Active \\([0.50, 0.75]\\) Passive \\([0.02, 0.20]\\) <p>Fund with a smaller amount of assets has high expense ratio due to limited funds for covering costs</p> <p>International funds may have high operational expenses due to staffing in multiple countries</p>"},{"location":"Finance_Electives/SAPM/09_Fund_Management_Evaluation/#portfolio-turnover-ratio","title":"Portfolio Turnover Ratio","text":"<p>Funds with high PTR will tend to have higher fees to reflect turnover costs</p> <p>However, high PTR tends to translate higher overall returns, thus mitigating the impact of additional fees</p>"},{"location":"Finance_Electives/SAPM/09_Fund_Management_Evaluation/#goals-for-fund-manager","title":"Goals for Fund Manager","text":"<ul> <li>Minimize \\(\\beta\\)</li> <li>Maximize \\(\\alpha\\)</li> <li>Minimize \\(\\text{ER}\\)</li> <li>Maximize \\(R_r\\)</li> </ul>"},{"location":"Finance_Electives/SAPM/99_IDK/","title":"Triple Barrier","text":""},{"location":"Finance_Electives/SAPM/99_Portfolio_Optimization/","title":"Portfolio Optimization","text":""},{"location":"Finance_Electives/SAPM/99_Portfolio_Optimization/#key-words","title":"Key Words","text":"Delta Relationship of whole book to underlying stock(1<sup>st</sup> derivative of something) Gamma Change of the portfolio(1<sup>st</sup> derivative of delta) Theta How trading book is carrying/bleeding away money, when nothing changes in market/position Vega/Kappa Book/Portfolio/Positions\u2019s sensitivity to volatility OTC Over The Counter"},{"location":"Finance_Electives/SAPM/99_Portfolio_Optimization/#variables","title":"Variables","text":"Variable Meaning Interest rate sensitivity Equity exposure Commodity exposure Credit Distribution/Linearity of price behavior Regularity of cash flow/prepayment Correlation across sectors &amp; classes"},{"location":"Finance_Electives/SAPM/99_Portfolio_Optimization/#variance-of-portfolio","title":"Variance of Portfolio","text":"<p>If the portfolio has one unit of each security whose prices are tracked in the Covariance matrix, the portfolio variance is the sum of the items in the covariance matrix.</p> <p>If set of positions \\(X=\\{ x_1, x_2, \\dots \\}\\), then the variance of the portfolio is given by \\(\\hat \\sigma_p^2 = X' \\text{Cov}_{XX}  X\\)</p>"},{"location":"Finance_Electives/SAPM/99_Portfolio_Optimization/#index-trackingbenchmark-replication","title":"Index Tracking/Benchmark Replication","text":"<p>Portfolio compression strategy aimed at mimicking the risk/return profile of a financial instrument, by focusing on a reduced basket of representative assets</p> <p>Intuitively similar to L1 regularization $$ \\begin{aligned} \\text{Tracking error TE}(w) &amp;= {\\vert\\vert r_b - Xw \\vert\\vert}_2 \\ \\implies \\min \\text{TE}(w) &amp; + \\lambda {\\vert\\vert w \\vert\\vert}_0 \\end{aligned} $$ where</p> <ul> <li>\\(r_b \\in R^T\\) : returns of benchmark instrument in the past T days</li> <li>\\(X = [r_1, \\dots r_T]^T \\in R^{T \\times N}\\) : returns of \\(N\\) stocks in the past T days</li> </ul>"},{"location":"Finance_Electives/SAPM/99_Portfolio_Optimization/#pairs-trading-portfolio","title":"Pairs Trading Portfolio","text":"<p>Spread \\(z_t = y_{1t} - \\gamma y_{2t}\\) with weights \\(w = \\begin{bmatrix} 1 \\\\ -\\gamma \\end{bmatrix}\\)</p> <p>Use VECM modelling of the universe of stocks</p> <p>From the parameter \\(\\beta\\) contained in the low-ranked matrix \\(\\Pi = \\alpha \\beta^T\\), one can simply use any/all column(s) of \\(\\beta\\)</p> <p>\\(\\beta\\) defines a co-integration subspace and we can then optimize the portfolio within that con integration subspace</p>"},{"location":"Finance_Electives/SAPM/99_Portfolio_Optimization/#conversion-from-yield-to-price","title":"Conversion from Yield to Price","text":"<p>Fixed-income securities (such as bonds) trade as yield (ROI) $$ \\text{Price} = \\text{PV01} \\cdot \\text{Close} \\cdot 100 $$ \u201cPV01\u201d of a portfolio of assets is the sensitivity of the total scheme assets to a one basis point (or 0.01 per cent) change in interest rates</p>"},{"location":"Finance_Electives/SAPM/99_Portfolio_Optimization/#duration-vs-dv01","title":"Duration vs DV01","text":"Duration DV01 Measures Measures the weighted average time to a security's cash flows, where the weighting is the cash flow. Signifies Also shows the % change in price per change in yield Shows the % change in price per 1million of face value Preferred for Equities Fixed-Income Securities <p>Either measure is fine, but be mindful of units</p>"},{"location":"Finance_Electives/SAPM/99_Portfolio_Optimization/#spread-pv01","title":"Spread PV01","text":"<p>For credit-risky securities, we should distinguish b/w interest rate risk &amp; credit risk</p> <p>Credit spread takes default (and recovery) into consideration</p> <p>If recovery = 0, PV01 = CSPV01</p> <p>Different sources of spread</p> <ul> <li>Calculated</li> <li>CDS</li> <li>Asset Swap Spreads</li> </ul> <p></p> <p>Larger the credit spread, higher the probability of credit defaults</p>"},{"location":"Finance_Electives/SAPM/99_Portfolio_Optimization/#game-theory","title":"Game Theory","text":"<p>When designing your portfolio, you need to incorporate external factors and others\u2019 ideas as well (kinda like Game Theory)</p>"},{"location":"Finance_Electives/SAPM/99_Portfolio_Optimization/#kelly-criterion","title":"Kelly Criterion","text":""},{"location":"Finance_Electives/SAPM/99_Portfolio_Optimization/#simulation-for-optimization","title":"Simulation for Optimization","text":"<ul> <li> <p>Simulate the validation prices series</p> </li> <li> <p>Even a simple AR(1) is fine</p> </li> <li> <p>Naive Benchmark</p> </li> <li> <p>Buy if expected log return &gt; \\(k \\sigma_0\\)</p> </li> <li>Sell if expected log return &lt; \\(-k \\sigma_0\\)</li> <li> <p>Flatten, otherwise</p> </li> <li> <p>Find trading parameters that</p> </li> <li> <p>maximizes the average Sharpe Ratio over all simulated price series</p> <ul> <li>\\(\\implies\\) Solving HJB Equation</li> </ul> </li> <li> <p>or</p> <p>maximizes the average Sharpe Ratio over all simulated series</p> <ul> <li>\\(\\implies\\) Solving MLE</li> </ul> </li> </ul> <p></p>"},{"location":"Finance_Electives/SAPM/99_Regularized_Pricing_and_Risk_Models/","title":"Regularized Pricing &amp; Risk Models","text":"<p>https://www.youtube.com/watch?v=aga-Tak3c3M&amp;list=PLUl4u3cNGP63ctJIEC1UnZ0btsphnnoHR</p>"},{"location":"Finance_Electives/SAPM/99_Regularized_Pricing_and_Risk_Models/#bond-duration","title":"Bond Duration","text":"<p>Sensitivity of bond price \\((\\ln P)\\) to bond yield \\(y\\)</p> <ul> <li>Duration gives the \u201cweighted time\u201d</li> <li>Duration of zero coupon bond = maturity</li> <li>Duration of regular coupon bond &lt; maturity</li> <li>As there is only a fixed \\(y\\) for all payment dates, the duration is a sensitivity to \u201cparallel\u201d move</li> </ul> <p>Good measure for price changes for small variation in yield</p> <p>Second derivative required for large changes in yield</p> <p>$$ \\begin{aligned} P &amp;= \\sum \\limits_{i=1}^n e^{-y t_i} C_i \\ P_y &amp;= \\dfrac{\\partial P}{\\partial y}= - \\sum \\limits_{i=1}^n t_i e^{-y t_i} C_i \\ \\implies d &amp;= \\dfrac{P_y}{P} \\ c &amp;= \\dfrac{\\partial^2 P}{\\partial y^2} = \\sum \\limits_{i=1}^n t_i^2 e^{-y t_i} C_i \\ \\end{aligned} $$ where</p> <ul> <li>\\(d=\\) Bond Duration</li> <li>\\(c =\\) Bond convexity: Always positive</li> <li>\\(P=\\) price of bond</li> <li>\\(y=\\) yield of bond</li> <li>\\(C_i = i\\)th cashflow</li> </ul>"},{"location":"Finance_Electives/SAPM/99_Regularized_Pricing_and_Risk_Models/#swaps","title":"Swaps","text":"<p>Valuing fixed and float legs of the swap</p> <p>Swap can be hedged with bond $$ \\begin{aligned} \\text{PV}\\text{fixed} &amp;= \\sum_i C \\delta_i \\Alpha_i = C \\sum_i w_i \\ \\text{PV}\\text{float} &amp;= \\sum_i C r_i \\delta_i \\Alpha_i = \\sum_i r_i w_i \\ \\text{PV}\\text{fixed} &amp;= \\text{PV}\\text{float} \\ \\implies C &amp;= \\dfrac{\\sum_i r_i w_i}{\\sum_i w_i} \\end{aligned} $$ where</p> <ul> <li>\\(c =\\) swap rate (fixed leg coupon)</li> <li>Weighted sum of forward rates (assuming same frequency of payments of fixed &amp; floating legs)</li> <li>\\(\\Alpha_i =\\) discount factor for payment date \\(i\\)</li> <li>\\(\\delta_i =\\) day count fraction</li> <li>\\(r_i =\\) forward rate (floating rate of future payment)</li> </ul>"},{"location":"Finance_Electives/SAPM/99_Regularized_Pricing_and_Risk_Models/#yield-curve","title":"Yield Curve","text":"<ol> <li>Select input instruments</li> <li>Choose interpolation</li> <li>Interpolation space (daily forward rates, zero rates, etc)</li> <li>Spline (piece-wise constant, linear, tension spline, etc)</li> <li>Knot points and model parameters</li> <li>Calibrate</li> <li>Solve for spline parameters such that input instruments are re-priced at par</li> </ol>"},{"location":"Finance_Electives/SAPM/99_Regularized_Pricing_and_Risk_Models/#bond-spread","title":"Bond Spread","text":"\\[ P= \\sum_{i=1}^n e^{-s t_i} \\Alpha_i C_i \\] <p>where</p> <ul> <li>\\(\\Alpha_i =\\) discount factor for payment date \\(i\\) computed from curve</li> <li>\\(s=\\) bond spread</li> <li>\\(t_i =\\) future time of payment in years</li> <li>\\(C_i = i\\)th cashflow</li> </ul> <p>If the model is available for typical movements of the curve embedded in \\(\\Alpha_i\\) we can build more effective risk model for bond, rather than using single \u201cparallel\u201d shift mode (bond duration)</p>"},{"location":"Finance_Electives/SAPM/99_Regularized_Pricing_and_Risk_Models/#hedging","title":"Hedging","text":"\\[ x = \\arg \\min {\\left \\vert \\left \\vert F^T (r + Hx ) \\right \\vert \\right \\vert}^2 \\] <p>where</p> <ul> <li>\\(r =\\) portfolio risk</li> <li>\\(H =\\) hedging portfolio risks</li> <li>\\(x =\\) weights of hedging instruments</li> <li>\\(F =\\) market scenarios (factors)</li> </ul>"},{"location":"Finance_Electives/SAPM/99_Regularized_Pricing_and_Risk_Models/#pca","title":"PCA","text":"<p>Use SVD to decompose market movements data \\(D\\) into principal comments \\(P\\) and corresponding uncorrelated market dynamics \\(U\\) with weights \\(S\\) $$ D = U \\cdot S \\cdot P^T $$ Use few SVD components with largest singular values - low rank approximation of market data $$ P^T (r + Hx) = 0 $$</p>"},{"location":"Finance_Electives/SAPM/99_Regularized_Pricing_and_Risk_Models/#pca-risk-model","title":"PCA Risk Model","text":"<p>\u201cFormally\u201d tuned to historical data</p> <p>Hedge coefficients are unstable, especially if historical window is short</p> <p>Costly to re-hedge when PC factors change</p> <p>Instability is coming from PCs corresponding to small singular values</p> <p>Over-fitting to historical data</p> <p>NO assumptions of shape of yield curve</p>"},{"location":"Finance_Electives/SAPM/99_Regularized_Pricing_and_Risk_Models/#regularized-risk-models","title":"Regularized Risk Models","text":"<p>Assumption: Forward rates move smoothly $$ H^T R = I \\ {\\vert \\vert L \\cdot J \\cdot R \\vert \\vert}^2 \\to \\min \\ R \\sim \\Big(HH^T + \\lambda^2 (L \\cdot J)^T \\cdot L \\cdot J \\Big)^{-1} $$ where</p> <ul> <li>\\(J =\\) Jacobean matrix translating shifts of yield curve inputs to movements of forward rates</li> <li>\\(L=\\) Smoothness regularity matrix</li> <li>\\(\\lambda =\\) regularization parameter</li> </ul>"},{"location":"Finance_Electives/SAPM/99_Regularized_Pricing_and_Risk_Models/#pricing-model","title":"Pricing Model","text":""},{"location":"Finance_Electives/SAPM/99_Regularized_Pricing_and_Risk_Models/#hjm-heath-jarrow-morton-model","title":"HJM Heath-Jarrow-Morton Model","text":"<p>Evolution of forward rates $$ {df}{t, s} = \\mu^\\beta V(t, s) \\rho (t, s) \\cdot dB_t^Q $$ where} dt + f_{t, s</p> <ul> <li>\\(f =\\) forward rate</li> <li>\\(\\mu=\\) drift</li> <li>\\(\\beta=\\) model skew factor</li> <li>\\(\\rho=\\) Correlation/factor structure</li> <li>\\(V(t, s)=\\) parametric volatility surface</li> <li>\\(d B_t^Q =\\) Brownian motion</li> </ul>"},{"location":"Finance_Electives/SAPM/99_Regularized_Pricing_and_Risk_Models/#regularized-volatility-surface","title":"Regularized Volatility Surface","text":""},{"location":"Finance_Electives/SAPM/99_Regularized_Pricing_and_Risk_Models/#challenges","title":"Challenges","text":"<ul> <li>High dimensionality</li> <li>Need to calibrate many elements</li> <li>Large memory requirement to store matrix</li> <li>Relatively small number of calibration instruments</li> <li>Under-determined problem</li> <li>Sensitivity areas of calibration instruments overlap significantly</li> <li>Ill-posed inverse problem</li> <li>Unstable, noisy solution</li> <li>Need regularity conrtaints</li> <li>Has to be smooth to produce realistic prices for similar instruments</li> </ul>"},{"location":"Finance_Electives/SAPM/99_Regularized_Pricing_and_Risk_Models/#idk","title":"IDK","text":"<p>Represent volatility surface as linear combination of \\(n\\) basis functions $$ v = v_0 + \\beta x $$ where</p> <ul> <li>\\(v =\\) vector of volatility grid elements</li> <li>\\(\\beta=\\) matrix corresponding to basis functions</li> <li>\\(x=\\) vector of weights</li> </ul> <p>Make \\(n\\) equivalent to number of calibration instruments \\(M\\)</p> <p>\u201cFormally\u201d unambiguous</p> <p>Make basis functions piecewise constant matching sensitivity of calibration instruments, 0 otherwise</p>"},{"location":"Finance_Electives/SAPM/99_Regularized_Pricing_and_Risk_Models/#sensitivities","title":"Sensitivities","text":"\\[ \\begin{aligned} J_{ij} &amp;= \\dfrac{\\partial q_i}{\\partial x_j} \\\\ q &amp;= J \\cdot x \\\\ &amp;= \\ln \\dfrac{q_{mdl}}{q_0} \\\\ q_\\text{in} &amp;= \\ln \\dfrac{q_\\text{market}}{q_0} \\end{aligned} \\] <p>where</p> <ul> <li>\\(q_{mdl} =\\)  model price</li> <li>\\(q_\\text{market} =\\)  market price</li> <li>\\(q_0 =\\) base price</li> <li>\\(x=\\) vector basis functions coefficients</li> </ul>"},{"location":"Finance_Electives/Strategic_Management/","title":"Strategic Management","text":""},{"location":"Finance_Electives/Strategic_Management/#references","title":"References","text":"<ul> <li> https://www.youtube.com/playlist?list=PLgMDNELGJ1CZGHvxBcvmDQzsNhPHZGSsN</li> </ul>"},{"location":"HSS_Electives/IDS/","title":"Introduction of Development Studies","text":""},{"location":"HSS_Electives/IDS/#recommend-watching","title":"Recommend Watching","text":"<ul> <li> The Railway Men</li> <li> BP Oil Spill Documentary</li> <li> Matrabhumi</li> </ul>"},{"location":"HSS_Electives/IDS/01_Introduction/","title":"Introduction","text":""},{"location":"HSS_Electives/IDS/01_Introduction/#culture","title":"Culture","text":"<ul> <li>What should be adapted</li> <li>What should be preserved</li> </ul>"},{"location":"HSS_Electives/IDS/01_Introduction/#idk","title":"IDK","text":"<p>Swadeshi movement</p>"},{"location":"HSS_Electives/IDS/01_Introduction/#under-development-causes","title":"Under-development Causes","text":"<ul> <li>Lack of leadership</li> <li>Political instability</li> <li>Corruption</li> <li>Law of civic culture</li> <li>Cultural resistance</li> <li>Misues of foreign assistant</li> <li>Over-dependence on agriculture</li> <li>Lowe per capita incme</li> <li>Low literacy, Low skill, Lack of basic services</li> <li>Lack of natural resources</li> </ul>"},{"location":"HSS_Electives/IDS/01_Introduction/#reasons-for-japans-success","title":"Reasons for Japan\u2019s Success","text":"<ul> <li>Adapt to geographical situation despite natural disasters</li> <li>Highly-literate population</li> <li>Civic culture</li> <li>Early development of transport &amp; banking systems</li> <li>Niche technology development</li> <li>Strong cooperation between govt &amp; businesses</li> </ul>"},{"location":"HSS_Electives/IDS/01_Introduction/#indicators-of-development","title":"Indicators of Development","text":"<ul> <li>Economic development: GDP, GNP, PPP (Purchasing Power Parity)</li> <li>Occupational structure</li> <li>Energy consumption</li> <li>Transportation &amp; communication</li> <li>Consumption of manufactured metal per peron</li> <li>Literacy rate</li> <li>% of income spent on consumption of food</li> <li>Nutritional intake: Caloric, Proteins</li> <li>Per capita savings</li> <li>Infrastructure: Roads, Railways</li> <li>GNH</li> </ul> <p>These are not causes of development</p>"},{"location":"HSS_Electives/IDS/01_Introduction/#gnh-gross-national-happiness","title":"GNH - Gross National Happiness","text":"<ul> <li>Sustainable &amp; equitable socio-economic development</li> <li>Environmental conservation</li> <li>Preservation of identity &amp; culture</li> <li>Good governance</li> </ul> <p>Case study of Bhutan</p>"},{"location":"HSS_Electives/IDS/02_Modernization/","title":"Modernization Theory","text":"<p>Shift from traditional to \u201cmodern\u201d similar to developed countries</p> <ul> <li>Formal education</li> <li>Market-based economics</li> <li>Focus on economic growth as primary indicator of development</li> <li>Political systems</li> <li>Democratic</li> <li>Decentralized</li> <li>Secular <ul> <li>Religion should be unto individual choice</li> </ul> </li> </ul> <p>Claims that economic growth, cultural change, and political change go together</p> <p>Emphasize on dualism: </p>"},{"location":"HSS_Electives/IDS/02_Modernization/#walt-rostov-5-stages-to-development","title":"Walt Rostov 5 Stages to Development","text":"<ol> <li>Change from rigid traditional systems, and eliminate any resistance to them</li> <li>Preconditions for take-off</li> <li>Progressive leadership</li> <li>Greater flexibility</li> <li>Openness to new technology</li> <li>Greater diversity of products</li> <li>Take off</li> <li>Industrial growth</li> <li>Urbanization</li> <li>Drive to maturity</li> <li>Use of technology: Increase productivity and efficiency</li> <li>International trade: Bargaining power</li> <li>Emphasis on population<ul> <li>Demographic dividend/disaster</li> <li>Education &amp; Skill development</li> </ul> </li> <li>Final Stage</li> <li>Mass consumption</li> <li>High incomes</li> <li>Majority of workspace in service sector</li> <li>Move towards welfare state</li> </ol>"},{"location":"HSS_Electives/IDS/02_Modernization/#limitations","title":"Limitations","text":"<ul> <li>Euro-centric model and not easily applicable for other countries</li> <li>Assumes that all countries have the same resources and capability</li> <li>Does not account for problems that developing countries face</li> <li>Assumes that all countries can take such policies</li> <li>Focus on service sector causes scarcity in the agricultural and industrial sector, such as cleaning, etc</li> </ul>"},{"location":"HSS_Electives/IDS/02_Modernization/#criticism","title":"Criticism","text":"<ul> <li>Not Value neutral and promotes capitalism and western values</li> <li>May increase inequality within countries</li> <li>Exploits unlimited resources for industrial expansion at the cost of ecological issues</li> <li>Devalues traditional values and social institutions</li> </ul>"},{"location":"HSS_Electives/IDS/02_Modernization/#case-study-asian-tiger-economies","title":"Case Study: Asian Tiger Economies","text":"<p>Implemented modernization without ensuring decentralization</p>"},{"location":"HSS_Electives/IDS/02_Modernization/#idk","title":"IDK","text":"<ul> <li>Capitalism</li> <li>Socialism</li> <li>Communism</li> </ul>"},{"location":"HSS_Electives/IDS/03_IDK/","title":"Case Studies","text":""},{"location":"HSS_Electives/IDS/03_IDK/#_1","title":"Case Studies","text":""},{"location":"HSS_Electives/IDS/03_IDK/#ghana-chocolate","title":"Ghana Chocolate","text":""},{"location":"HSS_Electives/IDS/04_Neo_Liberalism_Development_Theory/","title":"Neo Liberalism Development Theory","text":"<p>Unregulated capitalist system with set of economic policies that limit restrictions on manufacturing and reduces barriers to commerce, reduce tariffs and free trade</p>"},{"location":"HSS_Electives/IDS/04_Neo_Liberalism_Development_Theory/#features","title":"Features","text":"<ul> <li>Rule of market, regardless of any social harm or market failure</li> <li>Deregulation: Limit govt regulation on anything that will reduce safety of jobs and profits</li> <li>Minimum Govt Intervention: \u201cMinimum Government, Max Governance\u201d</li> <li>Reduce subsidy on Goods and Services</li> <li>Encouraged Foreign Collaborations</li> <li> <p>New Industrial, Trade and Tax Policy</p> </li> <li> <p>Individual freedoms and responsibility: Meritocracy</p> </li> <li>Privatization</li> <li>Market Society, on top of Market Economy</li> <li>Import technology</li> </ul>"},{"location":"HSS_Electives/IDS/04_Neo_Liberalism_Development_Theory/#limitations","title":"Limitations","text":"<ul> <li>Misguided free market approach to public services</li> <li>Monopoly</li> <li>Increased financial instability</li> <li>Inequality</li> <li>New economic colonization: Debt Traps</li> </ul>"},{"location":"HSS_Electives/IDS/04_Neo_Liberalism_Development_Theory/#privatization","title":"Privatization","text":""},{"location":"HSS_Electives/IDS/04_Neo_Liberalism_Development_Theory/#advantages","title":"Advantages","text":"<ul> <li>Improved efficiency</li> <li>Sense of responsibliltiy</li> <li>Scientific management</li> <li>Reduction in political interference</li> <li>Encouragement of new innovations</li> </ul>"},{"location":"HSS_Electives/IDS/04_Neo_Liberalism_Development_Theory/#disadvantages","title":"Disadvantages","text":"<ul> <li>Lack of social welfare</li> <li>Class struggle</li> <li>Increased inequality</li> <li>Increased unemployment</li> <li>Exploitation of weaker section</li> </ul> <p>Due to the above, increased crime and/or violence</p>"},{"location":"HSS_Electives/IDS/04_Neo_Liberalism_Development_Theory/#what-is-good-governance","title":"What is good governance?","text":""},{"location":"HSS_Electives/IDS/04_Neo_Liberalism_Development_Theory/#case-studies","title":"Case Studies","text":"Market Failure Bhopal Gas Tragedy Coca-Cola Adoption in India Greece Govt Intervention BP Oil Spill Economic Colonization China Debt Trap on African countries, Pakistan, Sri Lanka"},{"location":"HSS_Electives/IDS/05_HDI/","title":"Human Development Index Model","text":"<ul> <li>Argues for holistic development</li> <li>Capability approach</li> </ul> <p>Freedom </p> <p>Economic growth should only be a means and not an end goal</p> <p>Focus on individual development, rather than market development</p> <ul> <li>Health</li> <li>Education</li> <li>Standard of living</li> </ul> <p>State should be a integral part of the system</p>"},{"location":"HSS_Electives/IDS/05_HDI/#hdi","title":"HDI","text":"<p>Human Development Index</p> Dimension Indicator Min Max Health Life expectancy at birth Education Expected years of schooling 15 Average years of schooling 15 Standard of living Gross National Income per capita"},{"location":"HSS_Electives/IDS/05_HDI/#encourages","title":"Encourages","text":"<ul> <li>Capability</li> <li>Beings: Conscience</li> <li>Productivity</li> <li>Empowerment</li> </ul>"},{"location":"HSS_Electives/IDS/05_HDI/#significance","title":"Significance","text":"<ul> <li>Helps control population</li> <li>Increase efficiency</li> <li>Other resources are better utilized</li> <li>Society becomes healthy and safe</li> <li>Conservation of environment</li> </ul>"},{"location":"HSS_Electives/IDS/05_HDI/#idk","title":"IDk","text":""},{"location":"HSS_Electives/IDS/05_HDI/#welfare-states","title":"Welfare States","text":"<ul> <li>Financial services</li> <li>Social services</li> <li>Non-cash benefits</li> </ul> <p>Modern welfare states: France, Finland, Belgium, Netherlands</p>"},{"location":"HSS_Electives/IDS/05_HDI/#idk_1","title":"IDK","text":"<ul> <li>Military expenditure</li> <li>Vicious cycle of offensive and defensive</li> <li>Govts don\u2019t keep education as a point in their agenda</li> <li>Life expectancy: Mental health is often forgotten </li> <li>Electricity consumption as an indicator of development is not ideal, as governments will be incentivized to increase consumption and hence increase CO2</li> <li>Creating more awareness about smoking</li> </ul>"},{"location":"HSS_Electives/IDS/05_HDI/#case-studies","title":"Case Studies","text":""},{"location":"HSS_Electives/IDS/05_HDI/#bihar-sithamali","title":"Bihar Sithamali","text":"<p>Open Defecation: Gives a chance to interact with each other</p>"},{"location":"HSS_Electives/IDS/05_HDI/#kerala-development-model","title":"Kerala Development Model","text":""},{"location":"HSS_Electives/IDS/05_HDI/#advantages","title":"Advantages","text":""},{"location":"HSS_Electives/IDS/05_HDI/#disadvantages","title":"Disadvantages","text":"<ul> <li>Agricultural stagnation</li> <li>Industrial stagnation</li> <li>Power deficiency</li> <li>Lack of good infrastructure</li> <li>Private sector declining</li> <li>Rising local unemployment in the state</li> <li>Poor investment in climate (2018, 2019, 2020 Floods)</li> <li>Headland workers: Harthal and Bandh</li> </ul>"},{"location":"HSS_Electives/IDS/05_HDI/#idk_2","title":"IDK","text":"<ul> <li>Remove table</li> <li>3<sup>rd</sup> person: This paper highlights/attempts</li> <li>1<sup>st</sup> person may cause bias</li> <li>Future/present-perfect study<ul> <li>Will analyze</li> <li>analyzes</li> </ul> </li> </ul>"},{"location":"HSS_Electives/IDS/05_HDI/#drawbacks","title":"Drawbacks","text":"<p>Measurement</p> <ul> <li>Civil war</li> <li>We are not considering other external factors such as</li> <li>Invasion</li> <li>Natural disasters</li> </ul>"},{"location":"HSS_Electives/IDS/05_HDI/#steps","title":"Steps","text":"<ul> <li>Objective</li> <li>Measurement</li> <li>Outcome</li> </ul>"},{"location":"HSS_Electives/IDS/06_Basic_Needs/","title":"Basic Needs Model","text":"<p>Everyone\u2019s talking about poverty, but no one has taken action</p>"},{"location":"HSS_Electives/IDS/07_Right_To_Development/","title":"Right to Development","text":"<p>Development must be seen as a fundamental human right, regardless of whatever the type of government</p> <p>Rights are those claims and demands of an individual/group of individuals to live a good life which are accepted by the community/society and recognized by the State</p> <p>Human rights are generally defined as these rights, which are inherent in our nature, and without, which we cannot live as human being.</p> <p>Goal of development is enlargement of human choices</p>"},{"location":"HSS_Electives/IDS/07_Right_To_Development/#aspects-of-development","title":"Aspects of Development","text":"<ul> <li>Human security</li> <li>Economic freedom</li> <li>Political security</li> </ul>"},{"location":"HSS_Electives/IDS/07_Right_To_Development/#human-security","title":"Human Security","text":"<p>Protecting people from critical and pervasive threats and situations, building on their strengths and aspirations. It also means creating systems that give people the building blocks of survival, dignity and livelihood</p> <p>Lack of human security is costly for development, mainly through reduction in productivity</p> <p>Imbalanced development that involves horizontal inequalities is a major source of conflict, which leads to a vicious cycle.</p>"},{"location":"HSS_Electives/IDS/07_Right_To_Development/#traditional-vs-human-security","title":"Traditional vs Human Security","text":"Traditional Human Promote demands ascribed to the state protection of individuals Oriented State-centered People-centered Examples State boundariesPeopleInstitutionsValues Below Participants StateCentralized CommunityDecentralized"},{"location":"HSS_Electives/IDS/07_Right_To_Development/#aspects-of-security","title":"Aspects of security","text":"Threats Economic UnemploymentEconomic inequality Food Health Infectious disease Environment Environmental pollution <ul> <li>Health</li> <li>Environmental</li> <li>Person</li> <li>Community</li> <li>Political</li> </ul>"},{"location":"HSS_Electives/IDS/07_Right_To_Development/#unemployment","title":"Unemployment","text":"<p>Data in India is not collected the right way</p> <p>Only those who report themselves as unemployed to state government are calculated as part of the </p>"},{"location":"HSS_Electives/IDS/07_Right_To_Development/#idk","title":"IDK","text":"<ul> <li>Rights-Holders</li> <li>entitled to rights</li> <li>entitled to claim rights</li> <li>entitled to hold duty-bearer accountable</li> <li>responsible to respect rights of others</li> <li>Duty-bearer</li> <li>Obligated to </li> </ul>"},{"location":"HSS_Electives/IDS/07_Right_To_Development/#principles-of-hrba","title":"Principles of HRBA","text":"<ul> <li>Direct links to human rights</li> <li>Common Participation</li> </ul>"},{"location":"HSS_Electives/IDS/07_Right_To_Development/#types-of-models","title":"Types of Models","text":"<ul> <li>Top-down</li> <li>Bottom-up: Better</li> </ul>"},{"location":"HSS_Electives/IDS/07_Right_To_Development/#case-studies","title":"Case Studies","text":""},{"location":"HSS_Electives/IDS/07_Right_To_Development/#amul","title":"Amul","text":""},{"location":"HSS_Electives/International_Relations/","title":"International Relations","text":"Class Instructor Lecture Dr. Devika Sharma"},{"location":"HSS_Electives/International_Relations/#overview","title":"Overview","text":"<p>How countries interact with each other</p> <ol> <li>History</li> <li>Theories of International Relations</li> <li>Issues of International Relations</li> </ol>"},{"location":"HSS_Electives/International_Relations/#theme-for-research-project","title":"Theme for research project","text":"<p>Suggest a proposal to UN about possible conflict, with sufficient reasons</p>"},{"location":"HSS_Electives/International_Relations/#watch","title":"Watch","text":"<ul> <li> how to become a tyrant</li> <li> Turning point</li> <li> The Mauritanian</li> <li> VICE movie on 2<sup>nd</sup> Gulf War</li> <li> Oslo</li> <li> BBC World Service - The Lazarus Heist</li> </ul>"},{"location":"HSS_Electives/International_Relations/01_Intro/","title":"01 Intro","text":""},{"location":"HSS_Electives/International_Relations/01_Intro/#international-relations","title":"International Relations","text":"<p>The study of what governs international peace and conflict</p> <p>International relations is not about morals, it's about survival.</p>"},{"location":"HSS_Electives/International_Relations/01_Intro/#why-learn-ir","title":"Why Learn IR","text":"<p>Why not just learn about internal affairs of a country? To see the global picture, we have to see the relations b/w countries</p>"},{"location":"HSS_Electives/International_Relations/01_Intro/#stages-of-political-relations","title":"Stages of Political Relations","text":"<p>For a country to have relations with another country</p> <ol> <li>recognize the other 'state'</li> <li>treaty</li> <li> <p>trade with them</p> <p>if you trade, then you don't go to war because </p> <ol> <li>you are dependent on each other</li> <li>help each other out</li> </ol> </li> </ol>"},{"location":"HSS_Electives/International_Relations/01_Intro/#unipolar-vs-bipolar-world","title":"Unipolar vs Bipolar world","text":"<p>Which is better? \ud83e\udd14</p> <p>Realists say that bipolar world is the most stable, and unipolar is least stable</p>"},{"location":"HSS_Electives/International_Relations/01_Intro/#afghanistan","title":"Afghanistan","text":"<p>Why is it important</p> <ol> <li>Geopolitical</li> <li>Resources</li> <li>Terrorism</li> <li>Instability of the State</li> <li>Global Power Balance</li> <li>Refugees</li> <li>Democracy/Liberacy/Women's Rights</li> </ol>"},{"location":"HSS_Electives/International_Relations/01_Intro/#us","title":"US","text":"<p>military</p> <p>economics</p>"},{"location":"HSS_Electives/International_Relations/01_Intro/#us-in-afghanistan","title":"US in Afghanistan","text":"<p>Afghanistan was initially dependent on the soviets. But US did not like that. Hence, it supported the Mujahidin guerillas to overthrow the govt. Then the govt was unstable and the values were against the US; then 9/11 happened. Then, the US went to intervene. I feel like US just went there to exploit the opium resources of Afghanistan</p>"},{"location":"HSS_Electives/International_Relations/01_Intro/#failure-in-afghanistan","title":"Failure in Afghanistan","text":"<p>it highlights America's decline</p> <p>Now, everyone's opinion of US will go down because they just spent 10-15yrs in a war (which wasn't even theirs), used up so much resources, failed and came back; and within a few weeks or so, the Taliban takes over.</p>"},{"location":"HSS_Electives/International_Relations/02_History/","title":"02 History","text":""},{"location":"HSS_Electives/International_Relations/02_History/#need-for-history","title":"Need for History","text":"<ol> <li>Give examples for theory</li> <li>Justifications for proposals and ideas</li> <li>To understand the present (Lot of conflicts in middle east are due to World War 1)</li> <li>Notice patterns</li> </ol>"},{"location":"HSS_Electives/International_Relations/02_History/#world-war-1","title":"World War 1","text":"<p>Started in Europe, mainly due high speed of industrialization; everyone believed that the best way to progress was through industrialization.</p>"},{"location":"HSS_Electives/International_Relations/02_History/#germany","title":"Germany","text":"<p>was dissatisfied and get surrounded by its enemies quickly</p>"},{"location":"HSS_Electives/International_Relations/02_History/#reason-for-dissatisfaction","title":"Reason for dissatisfaction","text":"<ol> <li>nationalism and wanted an empire</li> <li>Franz Ferdinand got assassinated</li> </ol> <p>The political objective of the assassination was to free Bosnia of Austria-Hungarian rule and established of a common South Slav (\"Yugoslav\") state. The assassination precipitated the July crisis which lead to Austria-Hungary declaring war on Serbia and the start of the First World War</p>"},{"location":"HSS_Electives/International_Relations/02_History/#parties","title":"Parties","text":"Allies (west) Alliance (germany) US (joined late) Germany france Austro-hungary russia Italy (switched sides from the allies) UK Ottoman empire China Japan <p>Italy switched sides cuz it felt like it didn't get much from the war, even though it did a lot</p>"},{"location":"HSS_Electives/International_Relations/02_History/#why-us-joined-ww1","title":"Why US joined WW1","text":"<p>Mostly financial</p> <ol> <li>To get european allies by supporting</li> <li>Maritime - Blockade of ships by Germany</li> </ol> <p>Germany sank many American merchant ships around the British Isles, using submarines, prompting the American entry into the war.</p>"},{"location":"HSS_Electives/International_Relations/02_History/#far-east","title":"Far East","text":"<p>China v Japan were having an ongoing conflict over territory</p> <p>Both Japan and China were with the Allies against germany and Austria-hungary</p>"},{"location":"HSS_Electives/International_Relations/02_History/#china","title":"China","text":"<p>basically china wanted to help the Allies in exchange for help with land disputes with japan</p>"},{"location":"HSS_Electives/International_Relations/02_History/#japan","title":"Japan","text":"<p>Japan already had a military alliance with Britain, but that did not obligate it to enter the war. It joined the Allies in order to make territorial gains: because they wanted to acquire the lands in Far East under German control</p>"},{"location":"HSS_Electives/International_Relations/02_History/#world-war-2","title":"World War 2","text":""},{"location":"HSS_Electives/International_Relations/02_History/#reasons","title":"Reasons","text":"<ol> <li>Treaty of Versailles was too unfair to Germany    At the end of WW1, the entire reparation costs and blame was put on Germany, even though they could not afford to do so</li> <li>The people felt very embarassed and angry and hence wanted to change    Hitler used this angst as fuel for his propaganda</li> <li>Hitler's occupations and torturing of jews and communists</li> </ol>"},{"location":"HSS_Electives/International_Relations/02_History/#parties_1","title":"Parties","text":"Allies Axis France Germany UK Italy US Japan Soviet China <p>US and Soviet were together?! \ud83d\ude32</p>"},{"location":"HSS_Electives/International_Relations/02_History/#why-did-japan-join-axis","title":"Why did Japan join axis","text":"<p>It wanted to become independent from the western world</p>"},{"location":"HSS_Electives/International_Relations/02_History/#end-of-ww2","title":"End of WW2","text":"<p>When Germany had already retreated, Japan was still trying to fight.</p> <p>Japan bombed pearl harbour; US bombed hiroshima and nagasaki using an atomic bomb.</p>"},{"location":"HSS_Electives/International_Relations/02_History/#why-did-us-actually-end-up-using-the-atomic-bomb","title":"Why did US actually end up using the atomic bomb","text":"<p>US claimed that it just wanted to get everything over with and this marked the end of the war.</p> <p>However, many experts say that US just wanted to try out the bomb to research further about it. This claim is supported by the fact that the first people to reach the site after the bombing were actually researchers like Richard Feynmann</p>"},{"location":"HSS_Electives/International_Relations/02_History/#us-and-soviet","title":"US and Soviet","text":"<p>Another theory about why US used the bomb is to show it's superiority, because the soviets were also developing nuclear weapons at the time. US didn't even tell soviets about the bombing, because they didn't want the soviets to get there first.</p> <p>After WW1 and WW2, naked expansion has been frowned upon. Previously, colonialism and expansion of territories was accepted</p>"},{"location":"HSS_Electives/International_Relations/02_History/#indian-wars","title":"Indian Wars","text":""},{"location":"HSS_Electives/International_Relations/02_History/#naxalitemaoist-insurgency","title":"Naxalite\u2013Maoist Insurgency","text":"<p>insurgency (violent, armed rebellion against authority)</p> <p>fought for improved land rights and more jobs for neglected agricultural laborer and the poor</p>"},{"location":"HSS_Electives/International_Relations/02_History/#siachen-glacier","title":"Siachen Glacier","text":"<p>gives India the \u201ctactical advantage of dominating height.\u201d Sitting at much lower altitudes, Pakistani soldiers are completely \u201cshut off from a view of the Siachen Glacier\u201d and are thus at a \u201csevere tactical disadvantage\u201d all along the AGPL. Demilitarizing the glacier would amount to India surrendering its advantage.</p>"},{"location":"HSS_Electives/International_Relations/02_History/#kargil-war","title":"Kargil War","text":"<p>Prior to 1984 neither India nor Pakistan had any permanent presence in the area. However, in 1984, India violated agreements and acquired the entire Siachen glacier. This provoked Pakistan to fight.</p>"},{"location":"HSS_Electives/International_Relations/02_History/#cold-war","title":"Cold War","text":""},{"location":"HSS_Electives/International_Relations/02_History/#communism","title":"Communism","text":"<p>Communism from Russian Revolution</p> <p>There are 2 class - haves / have-nots</p> <p>fight for</p> <ol> <li>resources</li> <li>power</li> <li>politics</li> </ol>"},{"location":"HSS_Electives/International_Relations/02_History/#reason-against-communism","title":"Reason against communism","text":"<p>have nots believed that the only way is by a violent uprising. the world govts got afraid</p> <p>they believed that there shouldn\u2019t be a \u2018state\u2019</p>"},{"location":"HSS_Electives/International_Relations/02_History/#reason-for-communism","title":"Reason for communism","text":"<p>because the people benefit</p>"},{"location":"HSS_Electives/International_Relations/02_History/#context","title":"Context","text":"<p>idealogical conflict - war of ideas - fight of who's way of life is better capitalism vs communism other wars were mostly about expanding territory/exploiting resources of a particular territory</p> <p>India tried being non-aligned because india was fed up of fighting others' wars; it just wanted to be independent from others; but actually comes closer to Soviet Union, because US was closer to Pakistan</p> <p>relatively non-violent</p> <ul> <li>nothing happened in europe, us, soviet</li> <li>violence happened in other countries through proxy wars</li> </ul>"},{"location":"HSS_Electives/International_Relations/02_History/#europe","title":"Europe","text":"<p>Iron Curtain was an idea/concept, not really a physical one; it represented the divide b/w the 2 ideologies</p> <p>Western Germany was controlled by 3 countries after WW2</p> <ol> <li>US</li> <li>UK</li> <li>France</li> </ol>"},{"location":"HSS_Electives/International_Relations/02_History/#berlin","title":"Berlin","text":"<p>Berlin was an enclave (a portion of territory surrounded by a larger territory whose inhabitants are culturally or ethnically distinct; like Musandam in UAE)</p> <p>itself was divided into east and west Berlin</p> <p>everybody wanted to move to the west Berlin side because 'western' was considered to be better</p>"},{"location":"HSS_Electives/International_Relations/02_History/#korea","title":"Korea","text":"<p>North freed by Soviet, South freed by US</p>"},{"location":"HSS_Electives/International_Relations/02_History/#south-east-asia","title":"South-East Asia","text":"<ul> <li>Vietnam<ul> <li>north - soviet</li> <li>south - US</li> </ul> </li> <li>Cambodia</li> <li>Laos</li> </ul>"},{"location":"HSS_Electives/International_Relations/02_History/#why-did-us-hold-on-to-vietnam-even-though-it-was-losing","title":"Why did US hold on to Vietnam, even though it was losing?","text":"<p>it felt that if it retreated, then soviet would expand itself into the south also; then, they would further expand themselves into Cambodia and Laos as well</p>"},{"location":"HSS_Electives/International_Relations/02_History/#cuban-missile-crisis","title":"Cuban Missile Crisis","text":"<p>Soviet places missile launch point in Cuba to attack US</p> <p>US places missile launch point Turkey to attack Soviet</p> <p>Nuclear deterrence nuclear weapons are intended to deter other states from attacking with their nuclear like 2 equally powerful drug cartels opting for cooperation instead of demolition</p>"},{"location":"HSS_Electives/International_Relations/02_History/#afghanistan","title":"Afghanistan","text":"<p>Soviet established a pro-Soviet govt.</p> <p>US supported the Mujahideen guerilla to fight against the Soviets.</p>"},{"location":"HSS_Electives/International_Relations/02_History/#collapse-of-soviet-union","title":"Collapse of Soviet Union","text":"<p>after the Chernobyl disaster, the govt opened up and everyone over-used the freedom they got and started asking for independence</p> <p>Similar thing happening in north korea if you give freedom, then people will take advantage</p>"},{"location":"HSS_Electives/International_Relations/02_History/#miscellaneous-points","title":"Miscellaneous Points","text":""},{"location":"HSS_Electives/International_Relations/02_History/#militiary-coup","title":"Militiary Coup","text":"<p>Militiary has the final power to protect the country</p>"},{"location":"HSS_Electives/International_Relations/02_History/#insurgence","title":"Insurgence","text":"<p>people who target govt expecting a change; they aren't exactly terrorists cuz they don't cause civil harm</p>"},{"location":"HSS_Electives/International_Relations/02_History/#diplomats","title":"Diplomats","text":"<p>you cannot attack diplomat in embassy; embassy are like enclaves</p> <ul> <li>Havana Syndrome</li> <li>Jamal Khashoggi</li> </ul>"},{"location":"HSS_Electives/International_Relations/02_History/#un-security-council","title":"UN Security council","text":"<p>Countries which volunteer its military services for global issues</p> <p>5 permanent members</p> <p>10 elected members</p>"},{"location":"HSS_Electives/International_Relations/03_Current_World/","title":"03 Current World","text":""},{"location":"HSS_Electives/International_Relations/03_Current_World/#chinas-rise","title":"China's rise","text":""},{"location":"HSS_Electives/International_Relations/03_Current_World/#reason-china-is-getting-better-so-quickly","title":"Reason china is getting better so quickly","text":"<p>Because they aren't wasting their money unnecessarily on military endeavors, like the US</p>"},{"location":"HSS_Electives/International_Relations/03_Current_World/#why-isnt-india-rising-so-quickly","title":"Why isn't India rising so quickly","text":"<p>In india, the govt just focuses on implementing policies so as to gain enough popularity to win the next election. hence the policies are always short-sighted, because the ruling party knows that if it makes good long term policies instead of short term, they will lose the next elections and all the benefits of their policies will actually be enjoyed by their successors.</p> <p>meanwhile, in china, there is only one 'communist' party (which isn't exactly communist), which is always the ruling party; hence, it makes very long term policies</p>"},{"location":"HSS_Electives/International_Relations/03_Current_World/#military","title":"Military","text":"<p>Not even close to US in terms of physical military</p> <p>Trying to attack digitally</p>"},{"location":"HSS_Electives/International_Relations/03_Current_World/#but-why-does-us-still-spend-so-much-on-military","title":"But why does US still spend so much on military","text":"<p>because, that is the only superiority it has over countries. If it loses this advantage, then all the countries that are dependent on it for military purposes might go away</p>"},{"location":"HSS_Electives/International_Relations/03_Current_World/#allies","title":"Allies","text":"<p>Not many allies compared to US</p>"},{"location":"HSS_Electives/International_Relations/03_Current_World/#quad-vs-aukus","title":"Quad vs AUKUS","text":"<p>Both are groups of countries against China</p> Quad AUKUS US US Australia Australia Japan UK India Not much progress, as Japan and China do not want to invite animosity from China; if they join, then they'll get stuck with it Anti-China nuclear submarine alliance; gained more progress than the Quad <p>Terrorism is more a tactic of the weak, rather than of the strong.</p>"},{"location":"HSS_Electives/International_Relations/03_Current_World/#how-regional-conflicts-affect-international-relations","title":"How regional conflicts affect international relations","text":""},{"location":"HSS_Electives/International_Relations/03_Current_World/#kurdish-turkish-conflict","title":"Kurdish-Turkish Conflict","text":"<p>The Kurdish\u2013Turkish conflict is an armed conflict between the Republic of Turkey and various Kurdish insurgent groups who have either demanded separation from Turkey to create an independent Kurdistan, or attempted to secure autonomy and greater political and cultural rights for Kurds inside the Republic of Turkey</p> <p>It is concerning for other countries as they are scared that the kurds in their own country would start asking for a state of their own</p>"},{"location":"HSS_Electives/International_Relations/03_Current_World/#india-sri-lanka-conflict","title":"India-Sri Lanka Conflict","text":"<p>India interfered in the domestic aspects of Sri Lanka, and instigated a conflict, which ended up being an international conflict</p> <p>Fear in pakistan about nuclear weapons cuz of fear that it will fall under the wrong hands like terrorists</p> <ol> <li>Even in india and pak\u2019s case in war, india would always be limited in its strategic decisions. india doesn\u2019t want the wrong people to get hold of the ruins and the nuclear weapons</li> <li>pakistan could just take out the political sphere in order to knock india out, but it can\u2019t because it wants to avoid a new govt that is more against it, and to avoid a headless chicken in charge of the neighboring enemy</li> </ol>"},{"location":"HSS_Electives/International_Relations/03_Current_World/#cooperation","title":"Cooperation","text":"<p>according to realists, cooperation takes place only as long as it benefits yourself; otherwise, the cooperation breaks off</p> <p>liberals say that once cooperation starts, laws and treaties are to be established to ensure it doesn\u2019t break</p>"},{"location":"HSS_Electives/International_Relations/03_Current_World/#iaea","title":"IAEA","text":"<p>international atomic energy agency</p> <p>the treaty said that nuclear for energy is fine, but the thingies shouldn\u2019t be used for military</p>"},{"location":"HSS_Electives/International_Relations/03_Current_World/#biological-weapons-convention-bwc","title":"Biological Weapons Convention (BWC)","text":"<p>it happens because it is not seen as a real military weapon</p>"},{"location":"HSS_Electives/International_Relations/03_Current_World/#indus-water-treaty","title":"Indus Water Treaty","text":"<p>even though they are at war, India and Pakistan realized that they shouldn\u2019t fight for water as it is a basic human requirement</p> <p>Liberals say that they is scope for cooperation even b/w countries at war</p>"},{"location":"HSS_Electives/International_Relations/04_Theories_of_IR/","title":"04 Theories of IR","text":""},{"location":"HSS_Electives/International_Relations/04_Theories_of_IR/#realism","title":"Realism","text":"<p>This theory says that every country just cares about survival; everything a state does is for its security</p> <p>Realists say that rules are made by people in power to maintain their power China says why should we follow rules the west made</p> <p>Feminists say that if there are more women, then the state would be softer; realists say that doesn't matter</p> <p>Some realists believe that a most unstable world is a uni-polar world, and that the most stable is a bipolar world</p> <p>Alliances occur due to interests and convenience, not exactly due to trust; there are no points for loyalty</p>"},{"location":"HSS_Electives/International_Relations/04_Theories_of_IR/#classical-realism","title":"Classical Realism","text":"<p>says that wars occur due to the dark side of human nature - desire for power</p> <p>Hans J. Morgenthau</p> <p>Eg: hitler</p> <p>it also says that we can predict the possibility of war from the nature of individuals in the govt</p>"},{"location":"HSS_Electives/International_Relations/04_Theories_of_IR/#structural-realism","title":"Structural Realism","text":"<p>says that classical realism is not scientifically sound; states act in response to what is thrown at them; human nature doesn\u2019t matter</p> <p>Wars happen between states due to systemic anarchy: lack of a central authority (mostly during civil wars)</p> <p>autocratic countries do their own thing but at the same time, they're more nervous that people will rise up, because</p> <ol> <li>not democractic</li> <li>common people die serving a country that isn't even democratic</li> </ol> <p>We can predict how leaders would behave, as they are responsible for their own state's survival. Therefore, they will act very rational, as they wouldn't be in that position if they weren't rational</p>"},{"location":"HSS_Electives/International_Relations/04_Theories_of_IR/#defensive-structural-realism","title":"Defensive structural realism","text":"<p>Kenneth Waltz</p> <p>just maximize defense</p> <p>just as much power you need to protect yourself from other self-seeking states</p> <p>Eg: Israel's iron dome</p>"},{"location":"HSS_Electives/International_Relations/04_Theories_of_IR/#offensive-structural-realism","title":"Offensive structural realism","text":"<p>John Mearsheimer (longer name so defense and offense)</p> <p>maximize power (resources, military defense and offense), as much as you can</p> <p>Countries can be defensive and/or offensive at different times Eg: China was defensive realist during 1962 for the land dispute with India, but now it is on the offensive in the South China Sea</p>"},{"location":"HSS_Electives/International_Relations/04_Theories_of_IR/#liberalism","title":"Liberalism","text":""},{"location":"HSS_Electives/International_Relations/04_Theories_of_IR/#democratic-peace-theory","title":"Democratic Peace Theory","text":"<p>theory under liberalism</p> <p>says that democratic countries do not fight wars</p> <p>Newly-established democracies are more unstable and are hence more prone to fight wars</p> <p>But it contradicts the working of the US; US has been waging wars</p> <p>The main reason for conflict is due to lack of trust</p> <p>According to liberalists, security dilemma is due to absolute gains as well as relative gains</p>"},{"location":"HSS_Electives/International_Relations/04_Theories_of_IR/#points-against-realism","title":"Points Against Realism","text":"<p>According to liberalists, realism can only explain conflicts but cannot explain peace</p> <p>Moreover, realism fails to explain why many of the treaties bw US and soviet was signed during the cold war</p>"},{"location":"HSS_Electives/International_Relations/04_Theories_of_IR/#neo-liberalism","title":"Neo-liberalism","text":"<p>mix of realism and liberalism</p> <p>States are rational, and in an international anarchical system, but believe that the world is more peace-loving than realists</p>"},{"location":"HSS_Electives/International_Relations/04_Theories_of_IR/#constructivism","title":"Constructivism","text":"<p>there is no reality other than what we give to it; it\u2019s ideas that matter</p> <p>how countries view each other</p> <p>Power resides where people believe it resides</p> <p>Power resides where men believe it resides Varys - Game of Thrones</p> <p>A state\u2019s identity is what threatens us</p> <p>anarchy isn\u2019t exactly objective reality, but it is the meaning people give it</p> <p>they don\u2019t think that anarchy is a factor for international relations</p> <p>value thingy - an example is paradox of value example</p> <p>better theory to explain change in relations</p> <p>they say that nuclear weapons provide a prestigue, identity and status for countries, as it\u2019s a small club</p> <p>they say that countries fight for ideology, not just power; US govt went after the communists within US also</p>"},{"location":"HSS_Electives/International_Relations/04_Theories_of_IR/#us-and-uk","title":"US and UK","text":"<p>US doesn\u2019t think of UK\u2019s military improvement as a threat; so clearly constuctivism is a good theory</p>"},{"location":"HSS_Electives/International_Relations/04_Theories_of_IR/#constructivists-view-of-hiroshima-bombing","title":"Constructivist\u2019s view of Hiroshima Bombing","text":"<p>because japanese were demonised in the minds of the US people through propaganda</p> <p>however, US didn\u2019t use atomic bomb in germany during ww2 cuz they were hitler and the germans are europeans; despite hitler using chemical and biological weapons in concentration camps (like the japanese) and other horrible stuff</p> <p>So constructivists say that this difference in perception is the reason the bombing took place</p>"},{"location":"HSS_Electives/International_Relations/04_Theories_of_IR/#marxism","title":"Marxism","text":"<p>everything occurs only for economic reasons, but disguised by the haves as other stuff (like nationalism, religion, patriotism) etc\u2026</p> <p>says that wars occur because have-nots are fooled by haves</p> <p>game of thrones - the seven - lannisters didn\u2019t care about religion</p> <p>it says that the economy is a driving force/defining feature of the world (just like how structural realism considers systemic anarchy) that determines state behavior</p> <p>projected a better, prosperous and equal world</p> <p>explains why US came to Saudi for oil</p> <p>says that radicalization occurs if people aren\u2019t doing so well economically</p>"},{"location":"HSS_Electives/International_Relations/04_Theories_of_IR/#classes","title":"Classes","text":"<ul> <li>capital class (haves) (bourgeoisie)</li> <li>working class (have-nots) (proletariats)</li> </ul> <p>marxists say that middle class is just fake; they just pretend to care for the working class, but hope to eventually be a part of the capital class</p>"},{"location":"HSS_Electives/International_Relations/04_Theories_of_IR/#history","title":"History","text":"<p>every epoch(era) includes the haves exploiting the have-nots for their own benefit</p> <ol> <li>primitive communism (hunter gatherers)</li> <li>slavery</li> <li>feudalism</li> <li>capitalism</li> <li>socialism</li> <li>communism</li> </ol>"},{"location":"HSS_Electives/International_Relations/04_Theories_of_IR/#marxism-vs-communism","title":"Marxism vs Communism","text":"<p>Communism is just marxism, but they also believed there should not be a state, as they believed that the State just represents the haves.</p> <ul> <li>social classes disappear</li> <li>everyone owns the means of production</li> <li>state belongs to the people and does not reflect the interests of the rich/haves</li> </ul> <p>communists believed in revolution/wars, as the haves wouldn\u2019t just let the have-nots</p> <p>therefore, this threat was the reason why the US made sure that it did not spread</p> <p>communism failed because we all care about personal glory and invidual gain; we don\u2019t really care about if others get money or not eg: universal basic income is highly argued against</p>"},{"location":"HSS_Electives/International_Relations/04_Theories_of_IR/#explanation-for-conflicts","title":"Explanation for Conflicts","text":"<p>marxists say that conflicts are due to the class divide bw the haves and have nots</p> <p>in GBR, there was no revolution because the wealth that came from colonisation by GBR helped get money and better standard of living of the working class</p> <p>marxists say that due to colonisation, the class conflict becomes global</p> <ul> <li>Colonizers become the haves</li> <li>colonized become the have-nots</li> </ul>"},{"location":"HSS_Electives/International_Relations/04_Theories_of_IR/#explanation-for-world-wars","title":"Explanation for World Wars","text":"<p>despite the motive being \u2018fighting for the country\u2019, the soldiers are just fighting for the sake of the haves</p> <p>marxists say that soldiers join the armies just for survival - to make a living</p>"},{"location":"HSS_Electives/International_Relations/04_Theories_of_IR/#why-is-marxism-better","title":"Why is marxism better?","text":"<p>While other theories explain at a macroscopic level, marxism explains in microscopic level (soldiers)</p>"},{"location":"HSS_Electives/International_Relations/04_Theories_of_IR/#why-isnt-it-liked","title":"Why isn\u2019t it liked?","text":"<p>because he says that religion and others stuff matter a lot to the people, but they are just an illusion; this triggered the whole world</p> <p>also, Marx has ignored the fact that there a lot of people who are genuinely religious, nationalist, etc\u2026 which seems pretty ignorant on his part</p>"},{"location":"HSS_Electives/International_Relations/04_Theories_of_IR/#examples-of-war-from-the-marxist-perspective","title":"Examples of war from the Marxist Perspective","text":"<p>western countries trying to exploit the have-nots from the other countries</p> <p>capitalists disallow trade unions in order to maximize profits</p>"},{"location":"HSS_Electives/International_Relations/04_Theories_of_IR/#us-guatemala","title":"US-Guatemala","text":"<p>United Fruit Banana</p> <p>(view slides)</p> <p>the decision to overthrow the Guatemala govt was purely to maintain the profits of the company, rather than just pure anti-communism</p> <p>Guatemala has never recovered from that</p>"},{"location":"HSS_Electives/International_Relations/04_Theories_of_IR/#us-iran","title":"US-Iran","text":"<p>Iran cut off oil supply for UK, and the US overthrew that leader. The new leader was actually a previous Shah who was very corrupt.</p>"},{"location":"HSS_Electives/International_Relations/04_Theories_of_IR/#2nd-gulf-war","title":"2<sup>nd</sup> Gulf War","text":"<p>reasons</p> <ol> <li>Iraq wasn\u2019t supposed to have weapons of mass destruction</li> <li>oil</li> </ol> <p>random definition: despot means autocrat</p> <p>Halliburton was an oil services and eng company it went from 22<sup>nd</sup> -&gt; 7<sup>th</sup> largest military contractor in just a few years</p>"},{"location":"HSS_Electives/International_Relations/04_Theories_of_IR/#another-aspect-of-marxism","title":"Another aspect of Marxism","text":"<p>hegemony - kinda like domination</p> <p>In Marxist philosophy, cultural hegemony is the dominance of a culturally diverse society by the ruling class who manipulate the culture of that society\u2014the beliefs and explanations, perceptions, values, and mores\u2014so that the worldview of the ruling class becomes the accepted cultural norm Wikipedia</p>"},{"location":"HSS_Electives/International_Relations/04_Theories_of_IR/#critical-theory","title":"Critical Theory","text":"<p>states that all theories are just projection of theorists\u2019 values</p> <p>it\u2019s not objective; it\u2019s subjective</p> <p>it says that there is no such thing as a grand theory</p>"},{"location":"HSS_Electives/International_Relations/04_Theories_of_IR/#miscellaneous-points","title":"Miscellaneous Points","text":""},{"location":"HSS_Electives/International_Relations/04_Theories_of_IR/#security-dilemma","title":"Security Dilemma","text":"<p>Let's just say a country has conflicts with another country.</p> <p>If the defensive structural realist country beefs up its security, then the offensive structural realist country will feel threatened; therefore, it will also improve its offense in response</p> <p>According to realists, this is due to focus on relative gains</p>"},{"location":"HSS_Electives/International_Relations/04_Theories_of_IR/#how-did-eu-become-possible","title":"How did EU become possible","text":"<ol> <li>realization</li> <li>wars are too expensive</li> <li>benefits of scale and trade</li> <li>trust brought about through<ol> <li>laws</li> <li>treaties</li> </ol> </li> <li>common enemy against the soviets</li> <li>begins with the ECSC</li> </ol>"},{"location":"HSS_Electives/International_Relations/04_Theories_of_IR/#game-theory","title":"Game theory","text":"<p>(i did not really understand properly)</p> <p>cooperation occurs as it is better than individual effort</p> <p>but then one individual just goes against the others and takes everything</p>"},{"location":"HSS_Electives/International_Relations/04_Theories_of_IR/#ecsc","title":"ECSC","text":"<p>European Coal &amp; Steel Community</p>"},{"location":"HSS_Electives/International_Relations/04_Theories_of_IR/#why-coal-steel","title":"Why coal &amp; steel?","text":"<p>because they were the most important resources at the time, and for which conflicts (like hitler\u2019s occupations) were waged</p>"},{"location":"HSS_Electives/International_Relations/04_Theories_of_IR/#why-would-nuclear-cooperation-make-sense","title":"Why would nuclear cooperation make sense","text":"<ol> <li>countries will not longer fear each other</li> <li>Share information to avoid accidental action</li> <li>keep the other one in check</li> </ol>"},{"location":"HSS_Electives/International_Relations/04_Theories_of_IR/#track-to-track-diplomacy","title":"Track to track diplomacy","text":"<p>cricket matches</p>"},{"location":"HSS_Electives/International_Relations/04_Theories_of_IR/#pok","title":"POK","text":"<p>Pakistan-Occupied Kashmir</p>"},{"location":"HSS_Electives/International_Relations/04_Theories_of_IR/#escalation-order","title":"Escalation Order","text":"<p>the order of responses based on the circumstance</p>"},{"location":"HSS_Electives/International_Relations/05_Issues_of_IR/","title":"05 Issues of IR","text":""},{"location":"HSS_Electives/International_Relations/05_Issues_of_IR/#issues-of-international-relations","title":"Issues of International Relations","text":""},{"location":"HSS_Electives/International_Relations/05_Issues_of_IR/#history-of-nuclear-weapons","title":"History of nuclear weapons","text":"<p>Nuclear weapons are more of a political weapons, rather than an actual military weapon; cuz it\u2019s more about signaling/deterrence than actually using it</p> <p>Nuclear weapons were only used once - when the victim did not have any nuclear weapons to deter them; ie, US on Japan; no MAD (mutually assured deterrence)</p> <p>But the fact that it hasn\u2019t been used so far doesn\u2019t mean that it cannot happen now</p> <p>Western world was caught off guard when India started nuclear testing (according to the media, at least)</p> <p>Indian strategists were actually glad that Pakistan also got nuclear weapons, because Pakistan now became secure; an insecure Pakistan would try to attack India in other ways; now it\u2019s just about deterrence, rather than fighting</p> <p>This could be possible for Iran too, cuz it\u2019s been cornered by everyone; but if it becomes nuclear, then it will become secure and may reduce terrorism</p> <p>Terrorism is the weapon of the weak ; the strong do not do that</p> <p>all this is like gun laws for civilians</p>"},{"location":"HSS_Electives/International_Relations/05_Issues_of_IR/#present","title":"Present","text":"<p>All countries are allowed to have civilian nuclear programs for power generation; but the countries must sign treaties to allow inspection to ensure that nothing is get diverted into the weapon development</p> <p>Iran is producing nuclear weapons to protect itself from Israel. Should we be worried? Yes, because (the below reason)</p> <p>Arguably, we should monitor which countries newly avail nuclear weapons, because we do not know how they would use them</p> <p>It is better if nuclear weapons are under a civilian government, rather than the military</p> <p>The permanent 5 of UN were somehow okay with India and Pakistan going nuclear, but then they are not okay with other countries cuz it changes the power balance</p>"},{"location":"HSS_Electives/International_Relations/05_Issues_of_IR/#treaties","title":"Treaties","text":""},{"location":"HSS_Electives/International_Relations/05_Issues_of_IR/#disarmament","title":"Disarmament","text":"<p>Japan has been educating countries to denuclearisation</p>"},{"location":"HSS_Electives/International_Relations/05_Issues_of_IR/#non-proliferation-treaty","title":"Non-Proliferation Treaty","text":"<p>Stopped horizontal proliferation (no of countries)</p> <p>India criticized the treaty saying that the treaty did nothing about vertical proliferation (no of weapons, trials); India said that then they should ban testing</p>"},{"location":"HSS_Electives/International_Relations/05_Issues_of_IR/#idk-treaties","title":"IDK Treaties","text":"<p>i don\u2019t agree with this, but this theory says that</p> <p>even the defensive missile weapons systems should be controlled so that the countries that have nuclear weapons do not feel insecure</p>"},{"location":"HSS_Electives/International_Relations/05_Issues_of_IR/#elements-of-stable-nuclear-deterrence","title":"Elements of Stable Nuclear Deterrence","text":"<p>According to Scott Sagan</p> <ol> <li>disallow pre-emptive war when one side has a temporary advantage</li> <li>eg: 2<sup>nd</sup> Gulf War</li> <li>develop survivable second-strike forces</li> <li>to deter the enemy if they nullify the first attack</li> <li>avoid accidental nuclear war</li> <li>keep weapons away from terrorists</li> </ol>"},{"location":"HSS_Electives/International_Relations/05_Issues_of_IR/#determinants-of-going-nuclear","title":"Determinants of going nuclear","text":"<ol> <li>to improve national security</li> <li>to improve prestige, national identity</li> <li>to improve political image</li> <li>Western world wants to affect the common people of countries, so that the people riot against the country</li> <li>But if the country makes nuclear weapons, then it appeases the people</li> <li>economic costs</li> </ol>"},{"location":"HSS_Electives/International_Relations/05_Issues_of_IR/#more-vs-none","title":"More vs None","text":"<p>what is better? more or no nuclear weapons?</p>"},{"location":"HSS_Electives/International_Relations/05_Issues_of_IR/#for-more","title":"For More","text":"<p>helps small countries feel secure which will stop them from doing other nonsense</p>"},{"location":"HSS_Electives/International_Relations/05_Issues_of_IR/#nuclear-doctrinespostures","title":"Nuclear Doctrines/Postures","text":"<ol> <li>every country wants a minimum deterrence</li> <li>second strike/survivable weapons capability</li> <li>No First Use    eg: India and China</li> <li>assured and massive retaliation    eg: Cold war</li> <li>Nuclear strategy prefers counterforce</li> <li>Counterforce - attack military threats like missile launch points</li> <li>Countervalue - attack commercial places like malls, tourist destinations, wonders of the world</li> <li>Asymmetric escalation    not responding proportional</li> <li>Launch on Warning (LoW)    keep missiles and nuclear weapons separately to give the enemy some time to reconsider their current decision</li> <li>Nuclear strategies</li> <li>Nuclear ambiguity/opacity       eg: Israel</li> <li>latent nuclear capacity       eg: North Korea</li> <li>Extended nuclear deterrence/ nuclear umbrella       eg: US for the countries dependent on it</li> </ol>"},{"location":"HSS_Electives/International_Relations/05_Issues_of_IR/#recent-developments","title":"Recent Developments","text":"<p>Countries are still keeping on increasing their nuclear weapons</p> <p>we are further away from nuclear-free than ever </p>"},{"location":"HSS_Electives/International_Relations/05_Issues_of_IR/#china","title":"China","text":"<p>has been accelerating its nuclear capability</p> <p>hypersonic missiles can evade the radars of the enemy</p> <p>Nuclear posture</p> <ol> <li>defensive</li> <li>clearly no use on nuclear-less countries</li> </ol>"},{"location":"HSS_Electives/International_Relations/05_Issues_of_IR/#india","title":"India","text":"<p>india has always been no first use</p> <p>despite this being a great policy for a safer world, India has recently become more ambiguous about this, saying that they might consider using it for certain circumstances to signal others not to mess with them</p>"},{"location":"HSS_Electives/International_Relations/05_Issues_of_IR/#terrorism","title":"Terrorism","text":"<p>United Nations has never been able to come up with a working definition. The definition changes with context.</p>"},{"location":"HSS_Electives/International_Relations/05_Issues_of_IR/#terrorist","title":"Terrorist","text":"<p>The govt tends to label anybody against the state as a terrorist. Kinda messed up and unfair defintion, but yeah what to do.</p>"},{"location":"HSS_Electives/International_Relations/05_Issues_of_IR/#klf","title":"KLF","text":"<p>Khalistan Liberation Front</p> <p>Pakistan funded the movement</p>"},{"location":"HSS_Electives/International_Relations/05_Issues_of_IR/#kurdistan-worker-party","title":"Kurdistan Worker\u2019 Party","text":"<p>Kurds have wanted their own state for a long time</p> <ul> <li>Why was this a threat?</li> <li>cuz other groups will also start asking for their states</li> </ul> <p>But the UN did not interfere, cuz they cannot interfere in the internal affairs of a country</p>"},{"location":"HSS_Electives/International_Relations/05_Issues_of_IR/#ira","title":"IRA","text":"<p>Irish Republican Army</p> <p>Northern Ireland wanted to separate </p>"},{"location":"HSS_Electives/International_Relations/05_Issues_of_IR/#eta","title":"ETA","text":"<p>Spain - Catalonya</p> <p>Basque separatists</p>"},{"location":"HSS_Electives/International_Relations/05_Issues_of_IR/#why-do-terrorist-take-up-arms","title":"Why do terrorist take up arms?","text":"<ol> <li>other measures have not succeeded</li> <li>perceived/real victimhood</li> <li>minority persecution</li> <li>unfair govt</li> <li>misfits</li> <li>sociopaths</li> </ol>"},{"location":"HSS_Electives/International_Relations/05_Issues_of_IR/#terrorists","title":"Terrorists","text":"<ul> <li>non-state actors</li> <li>they do not differentiate bw civillians vs the govt</li> <li>for them, the end justifies the means</li> <li>in reality they\u2019re weak</li> </ul>"},{"location":"HSS_Electives/International_Relations/05_Issues_of_IR/#pathology-of-terrorism","title":"Pathology of Terrorism","text":"<p>many terrorist groups in countries is funded by other countries</p> <p>a lot of funding comes from diaspora and from illegal trades like fake clothing, merch, etc.</p> <p>a lot of govts consider NGOs a forefront for terrorism funding</p> <p>2 happenings of terrorism</p> <ol> <li>as a method to meddle in another country\u2019s affairs</li> <li>as an organisation</li> </ol>"},{"location":"HSS_Electives/International_Relations/05_Issues_of_IR/#tibet","title":"Tibet","text":"<p>China has done the following tricks in Tibet</p> <ol> <li>intermingle the population there</li> <li>connect the country with the rest of the country</li> </ol> <p>so the average tibetan doesn\u2019t want an independent country anymore, cuz they\u2019re anyways happy with what they\u2019re getting rn</p>"},{"location":"HSS_Electives/International_Relations/06_Softpower/","title":"06 Softpower","text":""},{"location":"HSS_Electives/International_Relations/06_Softpower/#soft-power","title":"Soft Power","text":"<ul> <li>trying to change the perception of the country, without using any force</li> <li>helps in diplomatic relations</li> <li>healthier than hard power??</li> </ul> <p>eg:</p> <ul> <li>US: hip hop</li> <li>Japan: anime</li> <li>Korea: KPop, KDrama</li> </ul>"},{"location":"HSS_Electives/International_Relations/06_Softpower/#japan","title":"Japan","text":"<ul> <li>japan is trying to stay close allies with the US</li> <li>but doesn't always get along with them, for eg<ul> <li>Paris Climate Agreement</li> <li>Recognition of Jerusalem as the capital of Israel</li> </ul> </li> <li>Japan sees China\u2019s soft power tactics as pure propaganda</li> <li>Japan is unnotoriously closed off to outsiders<ul> <li>they\u2019re afraid of diluting their culture</li> <li>however, now it\u2019s slowly changing cuz they\u2019re afraid of the aging population</li> </ul> </li> </ul>"},{"location":"HSS_Electives/International_Relations/06_Softpower/#us","title":"US","text":"<p>Softpower has gone down, especially after the storming of the White House in Jan 2021.</p>"},{"location":"HSS_Electives/International_Relations/06_Softpower/#india","title":"India","text":""},{"location":"HSS_Electives/International_Relations/07_Poverty/","title":"07 Poverty","text":""},{"location":"HSS_Electives/International_Relations/07_Poverty/#development-and-poverty-alleviation","title":"Development and Poverty Alleviation","text":"<p>Policies of one country for development has a high possibility of affecting the development of another country</p> <p>eg: </p> <ol> <li>dams (China - india)</li> <li>nuclear plant (uae-china)</li> </ol>"},{"location":"HSS_Electives/International_Relations/07_Poverty/#mainstream-view","title":"Mainstream View","text":"<p>maximize \u2018wealth\u2019 using a capitalist system</p> <p>obsessed with GDP</p>"},{"location":"HSS_Electives/International_Relations/07_Poverty/#critical-view","title":"Critical View","text":"<p>more about happiness and sustainability</p> <ol> <li>health</li> <li>education</li> <li>standard of living</li> <li>self-fulfillment</li> </ol>"},{"location":"HSS_Electives/International_Relations/07_Poverty/#india","title":"India","text":"<p>India liberalized its economy cuz it took funds from the IMF, which said that it is compulsory to adopt certain principles</p> <p>One of the concerns in the north east is that they look at natural resources as sacred, rather than just for human consumption</p>"},{"location":"HSS_Electives/International_Relations/07_Poverty/#gini-coefficient","title":"GINI Coefficient","text":"<p>is a measure of statistical dispersion intended to represent the income inequality or the wealth inequality within a nation or a social group.</p>"},{"location":"HSS_Electives/International_Relations/08_Role_of_Economics_in_IR/","title":"08 Role of Economics in IR","text":""},{"location":"HSS_Electives/International_Relations/08_Role_of_Economics_in_IR/#great-depression","title":"Great Depression","text":"<p>Countries did not perform trade, as exporting countries wanted to save resources</p> <p>So many unemployed people were there in Germany, while the Jews were pretty well off; this was the appeal that Hitler had</p> <p>The world realized that economics should never be a reason for wars again. If we are trading together, then there is more room for cooperation and less likeliness for wars (liberal view)</p>"},{"location":"HSS_Electives/International_Relations/08_Role_of_Economics_in_IR/#international-organizations","title":"International Organizations","text":""},{"location":"HSS_Electives/International_Relations/08_Role_of_Economics_in_IR/#imf-world-bank","title":"IMF, World Bank","text":"<p>When a country is defaulting, IMF gives them loans for development</p> <p>eg: india in 1991, greece in 2010s</p>"},{"location":"HSS_Electives/International_Relations/08_Role_of_Economics_in_IR/#wto","title":"WTO","text":"<p>established the liberal international trading system</p> <p>fairer than the UN, because it has no security council</p>"},{"location":"HSS_Electives/International_Relations/08_Role_of_Economics_in_IR/#impact-of-liberal-trading-financial-system","title":"Impact of Liberal Trading &amp; Financial System","text":"<p>IMF Conditionalities require loan-taking countries to implement a liberal economy</p> <p>The short-term effects were actually bad, because the sudden changes cause instability</p> <p>EU took US to the WTO in 2018, because the US imposed tariffs on steel and aluminium imports from the EU</p>"},{"location":"HSS_Electives/International_Relations/08_Role_of_Economics_in_IR/#bilateral-trade","title":"Bilateral Trade","text":""},{"location":"HSS_Electives/International_Relations/08_Role_of_Economics_in_IR/#russia-germany","title":"Russia-Germany","text":"<p>Russia provides raw materials like crude oil and natural gas, Germany gives final goods like machinery</p> <p>But in Dec 2021, Germany has threatened Russia to block the pipelines (and not import) if Russia does anything harmful to Ukraine</p>"},{"location":"HSS_Electives/International_Relations/08_Role_of_Economics_in_IR/#china-taiwan","title":"China-Taiwan","text":""},{"location":"HSS_Electives/International_Relations/08_Role_of_Economics_in_IR/#us-china","title":"US-China","text":""},{"location":"HSS_Electives/International_Relations/08_Role_of_Economics_in_IR/#russia-saudi","title":"Russia-Saudi","text":"<p>Saudi wanted to reduce prices, but Russia did not want to. This shows that </p>"},{"location":"HSS_Electives/International_Relations/08_Role_of_Economics_in_IR/#india-malaysia","title":"India-Malaysia","text":"<p>Malaysia said something against India about Kashmir, and India banned Malaysia</p>"},{"location":"HSS_Electives/International_Relations/08_Role_of_Economics_in_IR/#china-australia","title":"China-Australia","text":"<p>Australia said that Covid-19 is a bioweapon, and China imposed tariffs</p>"},{"location":"HSS_Electives/International_Relations/09_Class_Presentations/","title":"09 Class Presentations","text":""},{"location":"HSS_Electives/International_Relations/09_Class_Presentations/#cyber-security","title":"Cyber Security","text":""},{"location":"HSS_Electives/International_Relations/09_Class_Presentations/#past","title":"Past","text":"<ol> <li>Russia attack on Ukraine</li> <li>Russia attack on German</li> <li>China attack on US</li> <li>Russia caused power outage in Ukraine</li> <li>allegdly Korean WannaCry ransomware</li> <li>NotPetya by Russia</li> </ol>"},{"location":"HSS_Electives/International_Relations/09_Class_Presentations/#present","title":"Present","text":"<p>the investment in cyber crime has been actively increasing</p>"},{"location":"HSS_Electives/International_Relations/09_Class_Presentations/#why-is-it-the-next-big-threat","title":"Why is it the next big threat","text":"<ol> <li>easy to do as</li> <li>it is cost-effective</li> <li>no bloodshed</li> <li>minimal losses</li> <li>limited repercussions</li> <li>lack of solid rules to regulate cyber crimes</li> <li>not under the attention of leaders</li> <li>world becoming more digital</li> <li>countries could lose large amounts</li> <li>viruses</li> <li>hard to detect</li> </ol>"},{"location":"HSS_Electives/International_Relations/09_Class_Presentations/#what-can-countries-do","title":"What can countries do","text":"<ol> <li>cyber warfare training for defence</li> <li>offensive cyber weapons development for deterrance</li> <li>defensive cyber security protocol</li> </ol>"},{"location":"HSS_Electives/International_Relations/09_Class_Presentations/#cryptocurrency","title":"Cryptocurrency","text":"<p>as cryptocurrencies are fast, secure, and de-centralized, terrorist organizations can easily get funding without any hinderance</p>"},{"location":"HSS_Electives/International_Relations/09_Class_Presentations/#cyberattacks","title":"Cyberattacks","text":"<p>china spear phishing</p> <p>Spear phishing is an email or electronic communications scam targeted towards a specific individual, organization or business. Although often intended to steal data for malicious purposes, cybercriminals may also intend to install malware on a targeted user\u2019s computer.</p>"},{"location":"HSS_Electives/International_Relations/09_Class_Presentations/#why-is-it-a-threat-in-international-level","title":"Why is it a threat in international level","text":"<p>as it is possible to disrupt the enemy\u2019s</p> <ol> <li>infrastructure</li> <li>production and GDP</li> </ol>"},{"location":"HSS_Electives/International_Relations/09_Class_Presentations/#data-ai","title":"Data &amp; AI","text":"<ol> <li>increase in unemployment</li> <li>lead to more conflicts</li> <li>more susceptible to radicalization</li> <li>more robots increase the civillians\u2019 fear of wars</li> <li>manipulate our decisions</li> <li>we won\u2019t be choosing our leaders</li> </ol>"},{"location":"HSS_Electives/International_Relations/09_Class_Presentations/#semiconductor","title":"Semiconductor","text":"<ol> <li>chip war is in favor of US</li> <li>US has the best</li> <li>china cannot solve the problem by throwing resources</li> <li>it\u2019s not easy to produce a completely new techology</li> <li>banning of chinese students in US would wipe out the ambition of Chinese tech domination</li> <li>China doesn\u2019t invade taiwan cuz TSMC is a Taiwanese company</li> <li>China is forced to cooperate with the US</li> </ol>"},{"location":"HSS_Electives/International_Relations/09_Class_Presentations/#refugees","title":"Refugees","text":""},{"location":"HSS_Electives/International_Relations/09_Class_Presentations/#why-is-it-important","title":"Why is it important","text":"<ol> <li>people are the foundation of a democracy</li> <li>internal issues</li> <li>recent refugee movements</li> <li>overpopulation</li> <li>exploitation</li> <li>criminal</li> <li> <p>where does the funding go?</p> </li> <li> <p>why refugees move for monetary reasons highlights marxism the failure of the political system </p> </li> <li>how countries deal with refugees highlights realism</li> </ol>"},{"location":"HSS_Electives/International_Relations/09_Class_Presentations/#how-do-they-come-up","title":"How do they come up","text":"<ol> <li>wars</li> <li>social/political issues</li> <li>sexual discrimination</li> <li>lack of access to basic needs</li> <li>environmental conditions (not thaaaat common)</li> <li>climate change</li> </ol>"},{"location":"HSS_Electives/International_Relations/09_Class_Presentations/#solutions","title":"Solutions","text":"<ol> <li>investigation of trafficking and criminal organizations</li> <li>more funding for host nations</li> <li>solve the inner stigma among people</li> <li>better conditions for refugees</li> <li>better organizations</li> </ol>"},{"location":"HSS_Electives/International_Relations/09_Class_Presentations/#resource-conflicts","title":"Resource Conflicts","text":"<p>opposition of ideas for the use of resources</p> <ol> <li>it can happen any time</li> <li>it can happen for a long time</li> <li>exploitation</li> <li>inequality</li> </ol> <p>debt traps lead to resource conflicts</p>"},{"location":"HSS_Electives/International_Relations/09_Class_Presentations/#zombie-apocalypse","title":"Zombie Apocalypse","text":"<p>Sai Krishna</p>"},{"location":"HSS_Electives/International_Relations/09_Class_Presentations/#north-korea","title":"North Korea","text":"<p>Is it more dangerous that North Korea has nuclear weapons compared to democratic countries like India??</p> <p>One more reason why China getting powerful is a threat is because it is threatening that a non-democratic country will be the one of - if not - the most powerful country.</p>"},{"location":"HSS_Electives/International_Relations/09_Class_Presentations/#indo-china","title":"Indo-China","text":"<p>india and china had a war in 1961</p> <p>China has had problems with India, because India has given asylum to the Dalai Lama.</p> <p>India is trying to keep it non-violent, because</p> <ol> <li>it is not capable to do so yet</li> <li>it cannot, as it might lose with Pakistan in Kashmir</li> <li>it cannot risk angering USA</li> </ol> <p>What\u2019s the point of democracy if India is censoring maps, but it\u2019s fine??</p> <p>Why do you say that a democracy better than a dictatorship?</p>"},{"location":"HSS_Electives/International_Relations/09_Class_Presentations/#graphs","title":"Graphs","text":""},{"location":"HSS_Electives/International_Relations/09_Class_Presentations/#iran","title":"Iran","text":"<p>Iran </p>"},{"location":"HSS_Electives/International_Relations/Project/Report_on_%27The_Uncontrolled_Rise_of_China%27/","title":"Report on 'The Uncontrolled Rise of China'","text":"<p>[toc]</p>"},{"location":"HSS_Electives/International_Relations/Project/Report_on_%27The_Uncontrolled_Rise_of_China%27/#introduction","title":"Introduction","text":""},{"location":"HSS_Electives/International_Relations/Project/Report_on_%27The_Uncontrolled_Rise_of_China%27/#objective","title":"Objective","text":"<p>This report takes a look at how China is getting too powerful with no one to keep them in check.</p>"},{"location":"HSS_Electives/International_Relations/Project/Report_on_%27The_Uncontrolled_Rise_of_China%27/#pre-research-thoughts","title":"Pre-Research Thoughts","text":"<p>I believe that China is expanding its power in an uncontrolled manner that is dangerous to the rest of the world. However, despite the severe propaganda against China, I believe that the damage China causes is minimal compared to the atrocities by major superpowers in the last century or so.</p>"},{"location":"HSS_Electives/International_Relations/Project/Report_on_%27The_Uncontrolled_Rise_of_China%27/#scope","title":"Scope","text":"<p>This report will cover</p> <ul> <li>rapid rise in China and the reasons behind it</li> <li>arguments against China</li> <li>arguments defending China</li> </ul>"},{"location":"HSS_Electives/International_Relations/Project/Report_on_%27The_Uncontrolled_Rise_of_China%27/#discussion","title":"Discussion","text":""},{"location":"HSS_Electives/International_Relations/Project/Report_on_%27The_Uncontrolled_Rise_of_China%27/#rapid-rise","title":"Rapid Rise","text":"<p>Prior to a few decades, China was a country that was weighed down by crippling poverty. It was very inefficient and isolated from the rest of the world.</p> <p>China set up various new policies that instigated rapid growth and pulled millions out of poverty. One such policy was the introduction of SEZs (Special Economic Zones) with tax and business incentives to attract foreign investments. China had a large inflow of capital, in exchange for providing cheap labor. This cheap labor factor is what significantly reduced unemployment. The government prioritizes the growth of the community more than the growth of individuals. This has ensured the development of the entire nation, rather than just the richest people.</p> <p>In India, the govt just focuses on implementing policies so as to gain enough popularity to win the next election. Hence the policies are always short-sighted, because the ruling party knows that if it makes good long term policies instead of short term, they will lose the next elections and all the benefits of their policies will actually be enjoyed by their successors. Meanwhile, in China, there is a one-party democratic system; the Chinese Communist Party always stays in power; hence, it makes strategic long term policies, without any fear of losing the next elections.</p> <p>Furthermore, China has spent more of its resources into its development instead of using it on military endeavors, like the US; thereby allowing all the other sectors to flourish and get millions of people out of poverty.</p>"},{"location":"HSS_Electives/International_Relations/Project/Report_on_%27The_Uncontrolled_Rise_of_China%27/#arguments-against-china","title":"Arguments Against China","text":"<p>There are various aspects that make China\u2019s expansion a worrying concern. It is not being hindered in its efforts for rapid expansion of power despite the following:</p>"},{"location":"HSS_Electives/International_Relations/Project/Report_on_%27The_Uncontrolled_Rise_of_China%27/#human-rights-violations","title":"Human rights violations","text":"<p>The basic rights such as labor rights are violated. In particular, minorities (such as Uyghur Muslims) are being persecuted severely. They are forced to convert religions, stripped-off of their culture, and exploited for cheap labor. Furthermore, they are deprived of basic freedom to speech which contradicts the very essence of a democracy - highlighting the totalitarian nature of the government.</p>"},{"location":"HSS_Electives/International_Relations/Project/Report_on_%27The_Uncontrolled_Rise_of_China%27/#hong-kong","title":"Hong Kong","text":"<p>The human part should tell us this is wrong</p> <p>The violation of human rights threatens the very essence of a civilization, and is obviously a concern for the rest of the world. </p>"},{"location":"HSS_Electives/International_Relations/Project/Report_on_%27The_Uncontrolled_Rise_of_China%27/#censorship","title":"Censorship","text":"<p>Only Chinese apps and media are allowed, as a means of censoring all content that is against the Chinese government. Despite there being various apps with different features, once again, there is only an illusion of freedom of choice as everything is under the control of the state.</p> <p>Moreover, various Chinese apps have allegations/history of spying on their users, as a means of collecting data, with popular apps like Tiktok being one of them recently. This data gets sold to data mining companies, which then target the users with specialized advertisements for products.</p> <p>Even when the Covid-19 outbreak emerged, the media was prevented from announcing it to the world, for maintaining China\u2019s image; they ended up risking the lives of millions across the world by doing so.</p>"},{"location":"HSS_Electives/International_Relations/Project/Report_on_%27The_Uncontrolled_Rise_of_China%27/#cyber-crimes","title":"Cyber Crimes","text":"<p>China has been involved in cyber crimes with large-scale implications, with many breaches being backed by the state. When looking at past occurrences, the US, the EU, NATO, the UK and other countries accused China of exploiting vulnerabilities in Microsoft Exchange Server (mail/calendar server)<sup>4</sup>. It affected around 250,000 organizations worldwide and allowed hackers to steal company emails and to breach a compromised Exchange server.</p>"},{"location":"HSS_Electives/International_Relations/Project/Report_on_%27The_Uncontrolled_Rise_of_China%27/#international-waters","title":"International Waters","text":"<p>China builds artificial islands and sets up military bases there in order to claim disputed ocean waters. This obviously harms the biodiversity and sustainability of these natural environments, affecting both the flora and fauna (i.e. both plants and animal biospheres). China does this in order to extract the natural resources of those regions and to simultaneously prevent other countries from doing so. Moreover, they are able to avail more efficient trading routes.</p> <p>This can especially be seen at the South China Sea. There are many \u2018islands\u2019 that didn\u2019t even exist up until a few years ago. Satellite images show numerous ships pumping sand out of the sea. China has heavily militarized the cluster of islands in the region, and build its own naval bases, to keep other countries in check. China insists that its intentions are non-militaristic, but its actions say otherwise. <sup>5</sup></p>"},{"location":"HSS_Electives/International_Relations/Project/Report_on_%27The_Uncontrolled_Rise_of_China%27/#biowarfare","title":"Biowarfare","text":"<p>There is research that suggests that Covid-19 was a bioweapon. It seems very suspicious that the origin of the virus was from Wuhan, the same place where there is a research center for virology(Wuhan Institute of Virology).<sup>3</sup> Moreover, the fact that China hid the news of the outbreak makes it seem even more obvious that this was intentional.</p> <p>One possibility is that China used the virus to cripple the rest of the world, as no country was even prepared for the possibility of a pandemic. Countries\u2019 economies, healthcare, education system, and other sectors were heavily tested; many countries were just not able to cope. For instance, Italy\u2019s health care was not ready and the death toll clearly showed that.</p>"},{"location":"HSS_Electives/International_Relations/Project/Report_on_%27The_Uncontrolled_Rise_of_China%27/#debt-traps","title":"Debt traps","text":"<p>China has distributed large amounts of loans to low and middle income countries. It seems generous, until the clauses and contracts for these loans are analyzed. China intentionally gives out loans to countries that definitely cannot repay them, with the aim of using those countries for its benefit. Clearly, the Belt and Road Initiative (BRI) not only acts as a geopolitical influence, but also as a weapon - once the country gets crippled by these tempting loans, it becomes a puppet for China and its expansion, by the taking over the infrastructure of the borrower.</p> <p>In the case of the Sri Lankan port of Hambantota, China pushed Sri Lanka into borrowing money from Chinese banks to fund the project, despite the lack of large potential commercial gains.<sup>6</sup> However, Sri Lanka could not keep up with the loans, and China demanded the port as collateral, forcing the government to give away control to a Chinese company. In the case of Africa, China has been accused of spying. After \u2018gifting\u2019 the African Union building, China intentionally left a backdoor to the servers, to secretly access data. Moreover, microphones were found all over the building. <sup>2</sup></p> <p>Some of the known regions where China has invested in large amounts and hence under risk of these debt traps</p> <ul> <li>Indian subcontinent<ul> <li>Sri Lanka</li> <li>Pakistan</li> </ul> </li> <li>Indo-Pacific<ul> <li>Thailand</li> <li>Laos</li> <li>Cambodia</li> </ul> </li> <li>All of Africa except Swaziland</li> </ul>"},{"location":"HSS_Electives/International_Relations/Project/Report_on_%27The_Uncontrolled_Rise_of_China%27/#dependence-of-other-countries","title":"Dependence of other countries","text":"<p>A large majority of countries in the world are dependent on China for production, imports and exports of goods. This dependence has led to countries being helpless when they are against policies that they disagree with.</p> <p>Taking an example, Australia is extremely dependent on China for a huge fraction of its economy, with China being its largest trading partner. Around 40% of Australia\u2019s exports is with China.<sup>8</sup> This has caused fears in the country about China\u2019s growing influence, with many saying that this is of utmost concern. Australia has hence formed alliances with other countries such as the Quad and AUKUS, in order to address this.</p>"},{"location":"HSS_Electives/International_Relations/Project/Report_on_%27The_Uncontrolled_Rise_of_China%27/#arguments-defending-china","title":"Arguments Defending China","text":"<p>However, despite the above, China isn\u2019t exactly the monster that it seems when compared to the other superpowers like the US and the Soviet Union from the last century.</p> <p>Regarding cyber crimes, China isn\u2019t the only country which is a culprit. Various other countries have also been involved in scandals and tampering incidents; Russia, for instance, has been accused of tampering in the 2016 US Presidential Elections. It seems unfair to put the blame solely on a single country - China.</p> <p>When it comes to the BRI, there is research<sup>7</sup> that suggest that Chinese lenders are willing to renegotiate the terms of loans<sup>6</sup>; there are various instances where Chinese investment has genuinely improved the standard of living in countries, especially in African ones like Lesotho. While other states use force to exploit resources of countries, China hardly harms the governments of these countries. It\u2019s just being smart, with non-violent strategies to bring more countries under its control. Despite extensive militarization, China limits its use of violence, which is in stark contrast to the US\u2019 expeditions for resources.</p> <p>Despite claims that Covid-19 was a bioweapon, it hasn\u2019t been confirmed yet and there is a possibility that it was genuinely just an accidental leak. Hence, unless it has been proved, China can\u2019t exactly be pronounced guilty.</p> <p>Finally, online resources are heavily backed by Western media; they show only the Westerners\u2019 side of the story. Hence, despite incomplete data and a lack of the full picture, China is portrayed and interpreted as a super villain.</p>"},{"location":"HSS_Electives/International_Relations/Project/Report_on_%27The_Uncontrolled_Rise_of_China%27/#conclusions","title":"Conclusions","text":"<p>The rapid and uncontrolled rise of China is indeed concerning; however, the points defending China highlight that it isn\u2019t the super villain it is portrayed to be; it is no different from any other emerging superpower throughout history. It is evident that further unbiased research must be performed.</p> <p>However, it is also evident that it is too dangerous to just let China continue its current efforts without any hinderance. This is clearly an international issue with worldwide consequences, and not just a regional one.</p> <p>The following measures may be taken to address the issue of China\u2019s rapid rise and growing dominance:</p> <ul> <li>Products made through exploited labor should be boycotted, to hinder the Government\u2019s efforts at exploiting workers. Even if a company does produce goods in China, it should ensure that employees and laborers avail good working conditions.</li> <li>As the digital world is still relatively recent, more ideation is required to address the issue of cyber crimes. New laws must be agreed upon to properly police cyber crimes, as it could have serious repercussions if left unhindered. A suggestion would be for the UN to issue heavy penalties for such crimes.</li> <li>The UN must come in and make stricter laws for governing international waters, to ensure that China doesn\u2019t just bully its neighbors.</li> <li>China\u2019s microbiology research must be monitored to ensure that their bioweapons are kept in check and other countries can be prepared in the case of an \u2018accidental\u2019 leak.</li> <li>Other countries should support countries that are under the BRI, in order to reduce China\u2019s dominance over them.</li> <li>International companies should find alternative countries to produce their goods. It may not be easy, but it is necessary to reduce the current dependence on China.</li> </ul> <ol> <li> <p>How China Became So Powerful \u21a9</p> </li> <li> <p>China's Rush Into Africa, Explained. \u21a9</p> </li> <li> <p>Was COVID Created in a Lab? Here's What We Know \u21a9</p> </li> <li> <p>China accused of cyber-attack on Microsoft Exchange servers \u21a9</p> </li> <li> <p>Why China is building islands in the South China Sea \u21a9</p> </li> <li> <p>There Is No Chinese \u2018Debt Trap\u2019 \u21a9\u21a9</p> </li> <li> <p>Debt Relief with Chinese Characteristics \u21a9</p> </li> <li> <p>Australia monthly trade data \u21a9</p> </li> </ol>"},{"location":"HSS_Electives/International_Relations/Project/presentation/","title":"Presentation","text":""},{"location":"HSS_Electives/International_Relations/Project/presentation/#focus-on","title":"Focus on","text":"<ol> <li>try to stick to only a few \u2018for\u2019 and \u2018against\u2019 arguments</li> <li>add graphs</li> <li>future</li> <li>theories of international relations</li> <li>economics</li> <li>feelings and the \u2018human\u2019 part</li> <li>add qr code for the report</li> </ol>"},{"location":"HSS_Electives/International_Relations/Project/presentation/#credits","title":"Credits","text":"Image Name Source Red Neon Signage Airam Dato-on Pexels World Map - Wikipedia Commons city Zhang Kaiyv Pexels hongKongProtests - Wikipedia Commons Security Pixabay Pexels Chicago Max Bender Pexels Honk Kong Dimitris.s12 Pexels Wall of China Paulo Marcelo Martins Pexels laborExploitation AP Video Guardian coalition - End Uyghur Forced Labour cyber graph - RFA humanRights Sora Shimazaki Pexels ocean Donald Tong Pexels timelapse Google Google Timelapse waters - Wikipedia Commons covid Markus Spiske Pexels wuhan - Wikipedia Commons mask August de Richelieu Pexels concentrationMaps Google BBC Article <p>Thanks to all of them</p>"},{"location":"Masters/GRE/","title":"GRE","text":"<ul> <li>https://www.dilipoakacademy.com</li> <li>https://ets.org/</li> <li>https://www.ets.org/gre/test-takers/general-test/prepare/powerprep.html</li> <li>https://www.gregmat.com</li> </ul>"},{"location":"Masters/GRE/#what-is-gre","title":"What is GRE?","text":"<p>Graduate Re Examination</p> <ul> <li>Math</li> <li>Verbal</li> <li>Writing</li> </ul>"},{"location":"Masters/GRE/#what-does-gre-test","title":"What does GRE test?","text":"<ul> <li>Critical thinking</li> <li>Accumulated knowledge</li> <li>Not IQ test</li> <li>Not inherent measure of intelligence</li> </ul>"},{"location":"Masters/GRE/#how-to-learn","title":"How to \u201clearn\u201d","text":"<ul> <li>Learning best approach</li> <li>Accurate prep material</li> <li>Practice</li> </ul>"},{"location":"Masters/GRE/#format","title":"Format","text":"Measure Section # of Qns Allotted Time(minutes) Analytical Writing 1 1(Analyze an Issue) 30 English 1 12 18 2 15 23 Math 1 12 21 2 15 26 Total 5 01:58 Math Problem-Solving Numeric EntryData InterpretationMCQsFive-Answer MCQ Quantitative Comparison English Text Completion One-BlankTwo-BlankThree-Blank Sentence Equivalence Reading Comprehension Paragraph Argument"},{"location":"Masters/GRE/#scoring","title":"Scoring","text":"Math 130-170 English 130-170 Essay 0-6"},{"location":"Masters/GRE/#adaptive","title":"Adaptive","text":"<ul> <li>Sections are adaptive</li> <li>Difficulty of second section adapts to performance on the first section</li> <li> <p>First section is always medium difficulty</p> </li> <li> <p>Questions are not adaptive</p> </li> </ul>"},{"location":"Masters/GRE/#practice","title":"Practice","text":"<ul> <li>https://ereg.ets.org/ereg/testPrep/viewEbooksSerives</li> </ul>"},{"location":"Masters/GRE/English/01_Vocabulary/","title":"Vocabulary","text":""},{"location":"Masters/GRE/English/01_Vocabulary/#types","title":"Types","text":"<ul> <li>Denotative: meaning</li> <li>Connotative: usage</li> </ul> <p>GRE requires connotative understanding</p>"},{"location":"Masters/GRE/English/01_Vocabulary/#tools","title":"Tools","text":"<ul> <li>wordnik.com</li> <li>nytimes.com</li> <li>dictionary.com</li> </ul>"},{"location":"Masters/GRE/English/01_Vocabulary/#idk","title":"IDK","text":"<ul> <li>Understand how words function</li> <li>Read &amp; study words as you learn</li> </ul>"},{"location":"Masters/GRE/English/01_Vocabulary/#tips","title":"Tips","text":"<ul> <li>Flashcards</li> <li>Don\u2019t use word lists: easy to memorize due to non-randomization</li> <li>Reading</li> <li>Active usage</li> <li>Grouping/association</li> <li>Vocabulary games</li> <li>Questions</li> </ul>"},{"location":"Masters/GRE/English/02_Blanks/","title":"Blanks","text":"<ol> <li>Read entire sentence</li> <li>Find clues/words/phrase in sentence</li> <li>Identify sentence type</li> <li>Look for keywords</li> <li>Come up with own words and match</li> <li>Eliminate options</li> <li>Do not be afraid to pick word you don\u2019t know</li> </ol> <p>Types</p> <ol> <li>Single</li> <li>Double</li> <li>Triple</li> </ol> <p>Types</p> <ul> <li>Single sentences</li> <li>Multiple sentences</li> <li>Dependent blanks</li> <li>Back-to-back blanks</li> <li>Other blank is the clue</li> </ul>"},{"location":"Masters/GRE/English/02_Blanks/#no-shifts","title":"No Shifts","text":"<ul> <li>Likewise</li> <li>Infact</li> <li>indeed</li> <li>so</li> <li>Just as \u2026 so (too)</li> <li>Not only \u2026 but also</li> </ul>"},{"location":"Masters/GRE/English/02_Blanks/#elaboration","title":"Elaboration","text":"<ul> <li>hyphen</li> <li>colon</li> <li>semi-colon</li> </ul>"},{"location":"Masters/GRE/English/02_Blanks/#apposition","title":"Apposition","text":"<p>Two similar/related words (usually adjectives) follow one another</p> <p>Eg: \u201cpointed, even polemical\u201d</p>"},{"location":"Masters/GRE/English/02_Blanks/#cause-effect","title":"Cause-Effect","text":"<ul> <li>because</li> <li>given that</li> <li>as result of</li> <li>since</li> <li>therefore</li> <li>consequently</li> <li>thus</li> </ul>"},{"location":"Masters/GRE/English/02_Blanks/#shifts","title":"Shifts","text":""},{"location":"Masters/GRE/English/02_Blanks/#reversal","title":"Reversal","text":"<ul> <li>Though</li> <li>Although</li> <li>Even though</li> <li>Despite</li> <li>Yet</li> <li>But</li> <li>However</li> <li>Notwithstanding</li> <li>Nonetheless</li> <li>Regardless</li> <li>Not without detractors</li> <li>Typically</li> </ul> <p>Types</p> <ul> <li>Single reversal</li> <li>Double reversal</li> </ul>"},{"location":"Masters/GRE/English/02_Blanks/#time-shifts","title":"Time shifts","text":"<ul> <li>Once</li> <li>At first</li> <li>Then \u2026 now</li> <li>Initially</li> </ul>"},{"location":"Masters/GRE/English/02_Blanks/#perception-shift","title":"Perception Shift","text":"<ul> <li>Since \u2026</li> <li>Only with</li> </ul>"},{"location":"Masters/GRE/English/02_Blanks/#false-shift","title":"False shift","text":"<ul> <li>albeit</li> </ul>"},{"location":"Masters/GRE/English/02_Blanks/#sentence-equivalence","title":"Sentence Equivalence","text":"<p>2 answers </p> <p>Both will be synonyms</p>"},{"location":"Masters/GRE/English/03_Reading_Comprehension/","title":"Reading Comprehension","text":""},{"location":"Masters/GRE/English/03_Reading_Comprehension/#usual-topics","title":"Usual Topics","text":"<ul> <li>Social Sciences</li> <li>Sociology</li> <li>Psychology</li> <li>Business, Law, Government</li> <li>Hard Sciences</li> <li>Literature</li> <li>History</li> </ul>"},{"location":"Masters/GRE/English/03_Reading_Comprehension/#types-of-passages","title":"Types of passages","text":"Type Short Paragraph argument Short Critical reading Medium 20-40 lines Long 40+ lines"},{"location":"Masters/GRE/English/03_Reading_Comprehension/#types-of-questions","title":"Types of questions","text":"<ul> <li>Line summarizing the passage</li> <li>Meaning of certain word, in the context of the passage</li> <li>Replace the word with a blank and try to match it with the options</li> <li>Meaning of certain line, in the context of the passage</li> <li>Inference: Parallel reasoning</li> <li>Multiple correct answers (denoted with alphabet bulleted options)</li> <li>Select the sentence in the passage</li> <li>Which of the following options does the author imply?</li> </ul>"},{"location":"Masters/GRE/English/03_Reading_Comprehension/#tips","title":"Tips","text":"<ul> <li>Read actively</li> <li>Identify key words</li> <li>Read for general meaning</li> <li>Understand structure</li> <li>Eliminate wrong options</li> <li>Incorrect</li> <li>Out of scope</li> <li>Extreme</li> <li>Too many assumptions</li> <li>Rotten fruit: one/two words make the sensible part insensible</li> <li>Too broad/narrow</li> <li>True, but doesn\u2019t answer the question and hence is not relevant<ul> <li>True in the real world</li> <li>Pertains to another part of passage</li> </ul> </li> </ul>"},{"location":"Masters/GRE/English/03_Reading_Comprehension/#paragraph-argument","title":"Paragraph Argument","text":""},{"location":"Masters/GRE/English/03_Reading_Comprehension/#elements-of-argument","title":"Elements of argument","text":"<ul> <li>Premises = Facts</li> <li>Conclusion = Statement tying facts together</li> <li>Gap: Something that the conclusion does not take into account</li> </ul> <p>5 options</p>"},{"location":"Masters/GRE/English/03_Reading_Comprehension/#types-of-questions_1","title":"Types of questions","text":"<ul> <li>Weaken/casts the most doubt</li> <li>Strengthening</li> <li>Paradox argument</li> <li>Bold-faced</li> <li>Percentage vs Numbers: Likelihood</li> </ul>"},{"location":"Masters/GRE/English/03_Reading_Comprehension/#elimination-of-choices","title":"Elimination of choices","text":"<ul> <li>Irrelevant</li> <li>Opposite</li> </ul>"},{"location":"Masters/GRE/English/04_Writing/","title":"Writing","text":""},{"location":"Masters/GRE/English/04_Writing/#issue-task","title":"Issue Task","text":"<ul> <li>Directions</li> <li>Agree/Disagree</li> <li>Support your position</li> <li>Choose a side</li> <li>Concession point: Other side is also valid in some cases</li> </ul>"},{"location":"Masters/GRE/English/04_Writing/#topics","title":"Topics","text":"<ul> <li>Government &amp; Power</li> <li>Education</li> <li>Culture &amp; society</li> </ul>"},{"location":"Masters/GRE/English/04_Writing/#argument","title":"Argument","text":"<ul> <li>Introduction</li> <li>The argument is flawed for numerous reasons</li> <li>Body</li> <li>Focus on 3 logical fallacies</li> <li>Use a paragraph for each fallacy</li> <li>Strengthen argument<ul> <li>\u201cThe argument would have been stronger had it provided information regarding \u2026\u201d</li> </ul> </li> <li>Conclusion</li> </ul>"},{"location":"Masters/GRE/Math/Algebra/","title":"Algebra","text":"<ul> <li>Like terms: Terms with identical variables that can be added/subtracted</li> <li>Identities</li> <li>Factoring</li> <li>Quadratic formula</li> <li>\\(x = \\dfrac{-b \\pm \\sqrt{D}}{2a}, D=b^2 - 4ac\\)</li> <li>\\(D&lt;0 \\implies\\) no solution</li> <li>Simultaneous equations</li> <li>No soln: \\(0x+0y=k, k \\ne 0\\)</li> <li>Unique soln</li> <li>Infinitely-many solns: \\(0x+0y=0\\)</li> <li>Extraneous roots</li> <li>occur for<ul> <li>even exponents</li> <li>absolute value</li> </ul> </li> <li>Functions</li> <li>Defined in the question</li> <li>Custom operators</li> <li>Formula (area, volume, temp conversion)</li> <li>Inequalities</li> <li>Only adding inequalities always holds true<ul> <li>Hence, avoid sub, mul, div</li> </ul> </li> <li>Quadratic<ul> <li>Set expo to zero</li> <li>Find solutions</li> <li>Record on number line</li> <li>Test number on each region</li> <li>Solve inequality</li> </ul> </li> </ul> <p>Questions</p> <ul> <li>Age</li> <li>Distance</li> <li>Speed: Avg speed = total distance/total time</li> <li>Time</li> <li>Work</li> <li>Double matrix</li> <li>3 criteria Venn diagram</li> <li>Sequences</li> <li>Batch</li> <li>Recursive</li> <li>Sum of sequences</li> <li>Number of terms</li> <li>Exclusive: \\(x_2 - x_1\\)</li> <li>Inclusive: \\(x_2 - x_1+1\\)</li> <li>Growth/Decline</li> <li>Mixture</li> <li>Formula algebraic expressions</li> </ul>"},{"location":"Masters/GRE/Math/Arithmetic/","title":"Arithmetic","text":"<ul> <li>Real Numbers</li> <li>Integers</li> <li>Properties of</li> <li>Addition</li> <li>Subtraction</li> <li>Multiplication</li> <li>Division</li> <li>Operations with signed numbers</li> <li>PEDMAS</li> <li>Decimals</li> <li>10<sup>th</sup> Places</li> <li>Addition, Subtraction, Multiplication, Division</li> <li>Rounding up/down</li> <li>Fractions</li> </ul>"},{"location":"Masters/GRE/Math/Coordinate_Geometry/","title":"Coordinate Geometry","text":"<ul> <li>X-axis, Y-axis</li> <li>X coordinate, y coordinate</li> <li>Quadrants: 1, 2, 3, 4 in Anti-clockwise direction</li> <li>Distance between 2 points: Euclidean distance</li> <li>Lines</li> <li>Equation</li> <li>Slope</li> <li>Intercept</li> </ul>"},{"location":"Masters/GRE/Math/Counting/","title":"Counting","text":""},{"location":"Masters/GRE/Math/Counting/#fundamental-counting-principle","title":"Fundamental counting principle","text":"<p>No of ways \\(= n_1 \\times n_2 \\times \\dots\\)</p> <p>How many different \\(x\\) digit numbers can be created using \\(n\\) digits only: \\(y^x\\)</p>"},{"location":"Masters/GRE/Math/Counting/#factorial","title":"Factorial","text":"<p>\\(n\\) unique objects can be arranged in \\(n!\\) ways</p>"},{"location":"Masters/GRE/Math/Counting/#restrictions","title":"Restrictions","text":"<p>No of ways to follow rule = no of ways to ignore rule - no of ways to break rule</p>"},{"location":"Masters/GRE/Math/Counting/#duplicates","title":"Duplicates","text":"<p>When there are duplicate objects $$ \\begin{aligned} &amp;= \\dfrac{n!}{\\prod_i^k n_i!} \\end{aligned} $$ where </p> <ul> <li>\\(k=\\) no of unique groups</li> <li>\\(n_i =\\) no of objects in group \\(k\\)</li> <li>\\(\\sum_i^k n_i = n\\)</li> </ul>"},{"location":"Masters/GRE/Math/Counting/#permutation","title":"Permutation","text":""},{"location":"Masters/GRE/Math/Counting/#combination","title":"Combination","text":"<ul> <li>Order doesn\u2019t matter</li> <li>Outcomes of one stage are same as outcomes of other stages</li> </ul>"},{"location":"Masters/GRE/Math/Data_Interpretation/","title":"Data Interpretation","text":"<ul> <li>Graph</li> <li>Table</li> </ul> <p>Types of questions</p> <ul> <li>MCQ</li> <li>MCQ Multiselect</li> <li>Numeric entry</li> </ul> <p>Usually 1 set of data interpretation questions per quantitative section</p> <ul> <li>Grouped together</li> <li>All refer to same info source</li> </ul> <p>Topics</p> <ul> <li>Real-life math</li> <li>Statistics</li> <li>Percents</li> <li>Ratios</li> <li>Probability</li> <li>Interpretation</li> </ul> <p>Time allotment</p> Min Per question Quantitative comparison 1.25 MCQ 2 Numeric entry 2 Data interpretation 2"},{"location":"Masters/GRE/Math/Data_Interpretation/#strategy","title":"Strategy","text":"<ol> <li>Understand big picture</li> <li>Read any accompanying text</li> <li>Pay attention to units of measurement</li> <li>For graphics with axes<ol> <li>Read axis labels</li> <li>Check if axis starts at zero</li> <li>Check axis increment: linear/log</li> </ol> </li> <li>Identify trends/relationships<ol> <li>Spike/level out/cyclical</li> <li>One factor influences another</li> </ol> </li> <li>Read the question</li> <li>Check the units of question and units in data</li> <li>Check answer choices before performing calculations</li> <li>Indicate correct form of answer (fraction/decimal)</li> <li>Degree of accuracy</li> </ol>"},{"location":"Masters/GRE/Math/Data_Interpretation/#notes","title":"Notes","text":"<ul> <li>All visual graphics are drawn to scale</li> <li>Estimate whenever possible</li> <li>Do not confuse absolutes with rates/percents</li> </ul>"},{"location":"Masters/GRE/Math/Data_Interpretation/#diagrams","title":"Diagrams","text":"<ul> <li>Venn Diagram</li> <li>Scatter Plot: Correlation, Trend line</li> </ul>"},{"location":"Masters/GRE/Math/Geometry/","title":"Geometry","text":"<ul> <li>Line</li> <li>Bisector: divide into 2</li> <li>Perpendicular bisector</li> <li>Transversal: passes through 2 || lines<ul> <li>All angles formed are equal due to VOA and AIA</li> </ul> </li> <li>Angle</li> <li>GRE only focuses on degrees<ul> <li>No need to learn radians</li> </ul> </li> <li>Vertically-opposite angles are equal</li> <li>Alternate interior angles are equal</li> </ul>"},{"location":"Masters/GRE/Math/Geometry/#triangles","title":"Triangles","text":"<ul> <li>Vertices</li> <li>Angles add upto 180</li> <li>Edges</li> <li>Angles-Edges<ul> <li>Shortest edger oppo to smallest angle</li> <li>Longest edge oppo to largest angle</li> </ul> </li> <li>\\(\\vert a-b \\vert &lt; c &lt; (a+b)\\)</li> </ul> <p>Area = \\(\\dfrac{1}{2} bh\\)</p>"},{"location":"Masters/GRE/Math/Geometry/#isosceles","title":"Isosceles","text":"<ul> <li>2 equal angles</li> <li>2 equal edges</li> </ul>"},{"location":"Masters/GRE/Math/Geometry/#equlaterial","title":"Equlaterial","text":"<ul> <li>3 equal angles: 60,60,60</li> <li>3 equal edges</li> </ul> <p>Area = \\(\\dfrac{\\sqrt{3}}{4} a^2\\)</p> <p>Altitudes of isosceles and equilateral triangles always bisect the base</p>"},{"location":"Masters/GRE/Math/Geometry/#right-triangle","title":"Right-Triangle","text":"<ul> <li>Pythagorean theorem: \\(a^2+b^2 = c^2\\)</li> <li> <p>Pythagorean triplet: Set of 3 integers that can be the sides of a right triangle</p> <ul> <li>Common</li> <li>3-4-5</li> <li>5-12-13</li> <li>8-15-17</li> <li>7-24-25</li> <li>Multiple of triples is also a triple</li> <li>2 corresponding sides are required for a triple</li> </ul> </li> <li> <p>45-45-90: \\(x, x, x \\sqrt{2}\\)</p> </li> <li>Hiding in squares</li> <li>30-60-90: \\(x, x\\sqrt{3}, 2x\\)</li> <li>Hiding in equilateral triangle</li> </ul>"},{"location":"Masters/GRE/Math/Geometry/#similar-triangles","title":"Similar Triangles","text":"<p>All three angles are equal</p> <p>Ratio of any pair of corresponding sides is the same</p>"},{"location":"Masters/GRE/Math/Geometry/#quadrilaterals","title":"Quadrilaterals","text":"<p>4 edges</p> <p>Angles add upto 360 deg</p> <ul> <li>Parallelogram</li> <li>Opposite sides are parallel</li> <li>Opposite sides are equal</li> <li>Opposite angles are equal</li> <li>Rhombus</li> <li>Parallelogram with equal edges</li> <li>Diagonals are perpendicular bisectors</li> <li>\\(\\dfrac{1}{2} d_1 d_2\\)</li> <li>Rectangle</li> <li>Parallelogram with all angles 90</li> <li>Diagonals are equal length</li> <li>Square</li> <li>Rectangle with equal edges</li> <li>Diagonals are perpendicular bisectors</li> <li>Diagonals are equal length</li> <li>Trapezoid/Trapezium</li> <li>1 pair of parallel edges</li> <li>\\(\\dfrac{1}{2}h(a+b)\\)</li> </ul>"},{"location":"Masters/GRE/Math/Geometry/#polygons","title":"Polygons","text":"<p>Sum of angles of polygon with \\(n\\) edges = \\(180(n-2)\\)</p> <p>Convex polygon: all interior angles &lt; 180</p> <ul> <li>Pentagon: angles add up to 540</li> <li>Hexagon: angles add up to 720</li> </ul> <p>Regular polygon: equal sides and equal angles</p>"},{"location":"Masters/GRE/Math/Geometry/#circles","title":"Circles","text":"<ul> <li>center</li> <li>radius</li> <li>Chord: line segment passing connecting 2 points of circumference</li> <li>diameter: chord passing through center</li> <li>Arc</li> <li>Minor arc</li> <li>Major arc</li> <li>Sector</li> <li>Circumference: \\(2 \\pi r\\)</li> <li>Area: \\(\\pi r^2\\)</li> </ul> <p>Properties</p> <ul> <li>Inscribed angles on the same side of a chord/arc held are equal</li> <li>Inscribed angles holding  chord/arc of equal lengths are equal</li> <li>Inscribed angles holding diameter is 90deg</li> <li>Central angle is 2 x inscribed angle holding the same chord/arc</li> <li>Radius \\(\\perp\\) tangent at the point of intersection</li> </ul>"},{"location":"Masters/GRE/Math/Geometry/#volume","title":"Volume","text":"<ul> <li>Cube: \\(a^3\\)</li> <li>Cuboid: \\(lbh\\)</li> <li>Cylinder: \\(\\pi r^2 h\\)</li> </ul>"},{"location":"Masters/GRE/Math/Geometry/#surface-area","title":"Surface Area","text":"<ul> <li>Cube: \\(6 a^2\\)</li> <li>Cuboid: \\(2(lb + bh + lh)\\)</li> <li>Cylinder: \\(2 \\pi r^2 + 2 \\pi r h\\)</li> </ul>"},{"location":"Masters/GRE/Math/Geometry/#units-of-measurement","title":"Units of Measurement","text":"<ul> <li>Metric: km, kg, L</li> <li>English/Imperial: miles, pounds, gallons</li> </ul> <p>All conversions will be given in GRE</p> <p>Only units of time are expected to be known</p>"},{"location":"Masters/GRE/Math/Geometry/#strategies","title":"Strategies","text":"<ul> <li>Redraw figures</li> <li>Add all given information</li> <li>Add all information that can be deduced</li> <li>Add/extend lines</li> <li>Assign vars and use algebra</li> <li>2/more triangles &amp; lengths required</li> <li>Look for similar triangles</li> <li>Right triangle</li> <li>Look out for triples</li> <li>Use pythagorean theorem</li> <li>Circles</li> <li>Look out for circle properties</li> <li>Look out for isosceles triangles</li> </ul>"},{"location":"Masters/GRE/Math/Integers/","title":"Integers","text":"<ul> <li>Divisibility: No remainder</li> <li>Divisor = Factor</li> <li>Multiple</li> <li>Rules<ul> <li>2: Number is even</li> <li>3: sum of digits is divisible by 3</li> <li>4: 2 trailing digits divisible by 4</li> <li>5: last digit is 0 or 5</li> <li>6: Number divisible by 2 and 3</li> <li>7:</li> <li>8:</li> <li>9: Sum of digits divisible by 9</li> <li>10: Last digit is 0</li> </ul> </li> <li>Prime numbers</li> <li>+ve integer with only 2 divisors: 1 and itself </li> <li>1 is neither prime nor composite</li> <li>2 is only even prime number</li> <li>Prime factorization</li> <li>Any integer &gt; 1 is either prime or can be expressed be expressed as product of prime numbers</li> <li>If \\(n= p^a q^b r^c \\cdots\\) where \\(p, q, r, \\dots\\) are prime factors of \\(n\\), then total number of positive divisors of \\(n\\) is \\((a+1)(b+1)(c+1) \\cdots\\)</li> <li>Squares of integers</li> <li>called perfect squares</li> <li>Prime factorization will always have even no of each prime</li> <li>Will always have odd numbers of +ve divisors</li> <li>GCD/HCF</li> <li>Names<ul> <li>Greatest Common Divisor</li> <li>Greatest Common Factor</li> <li>Highest Common Factor</li> </ul> </li> <li>Greatest +ve common divisor shared by 2/more numbers</li> <li>LCM</li> <li>Least common multiple</li> <li>Smallest positive integer that is a multiple of both numbers</li> <li>\\(\\text{HCF}(x, y) \\times \\text{LCM}(x, y) = x \\times y\\)</li> <li>Operations with odd/even integers</li> <li>Product of odd numbers is always odd<ul> <li>Add/sub</li> <li>Odd +- odd = even</li> <li>Odd +- even = odd</li> <li>Even +- even = even</li> <li>Mul</li> <li>Odd x odd = odd</li> <li>Odd x even = even</li> <li>even x even = even</li> <li>Div</li> <li>Even/Even can be anything</li> <li>Odd/even = non-integer</li> <li>Even/odd = non-integer or even integer</li> <li>Odd/odd: non-integer or odd integer</li> </ul> </li> <li>Consecutive integers</li> <li>Every \\(n\\)th number is divisible by \\(n\\)</li> <li>\\(n\\) consecutive integers \\(\\implies\\) 1 number must be divisible by \\(n\\)</li> <li>Remainders</li> <li>Remainder \\(\\in\\) [0, Divisor)</li> <li>Dividend = divisor x quotient  +  remainder</li> <li>If \\(n/D = Q \\text{ with } R\\), then possible values of \\(n\\) are \\(R + aD\\), where \\(a \\ge 0\\)</li> </ul>"},{"location":"Masters/GRE/Math/Percents_Ratios/","title":"Percents &amp; Ratios","text":"<ul> <li>Basics</li> <li>What is 40% of 90?</li> <li>15% of what number is 60?</li> <li>120 is what percent of 80?</li> <li>Find \\(x\\%\\) of \\(y\\)</li> <li>Percent change</li> <li>Increase</li> <li>Decrease</li> <li>Interest</li> <li>Simple Interest</li> <li>Compound Interest</li> <li>Ratios</li> <li>Equivalent ratios</li> <li>Portioning</li> <li>Combining ratios</li> </ul>"},{"location":"Masters/GRE/Math/Powers_and_Roots/","title":"Powers &amp; Roots","text":"<ul> <li>Base</li> <li>Exponent</li> <li>\\(x^n = \\prod \\limits_{i=1}^n x\\)</li> <li>Negative number raised to even power: positive</li> <li>Negative number raised to odd power: negative</li> <li>Power laws</li> <li>\\(a^m \\times a^n = a^{m+n}\\)</li> <li>\\(a^m \\times b^m = (a b)^{m}\\)</li> <li>\\(a^0 = 1\\)</li> <li>\\((a^m)^n = a^{mn}\\)</li> <li>\\(x^{-n}=\\dfrac{1}{x^n}\\)</li> <li>\\((a^m b^n)^o = (a^{mo} b^{no})\\)</li> <li>\\(\\sqrt[n]{x} = x^{1/n}\\)</li> <li>\\(x^{m/n} = (x^m)^{1/n} = (x^{1/n})^m\\)</li> <li>\\(x^m=x^n \\iff m=n \\quad (x \\not \\in\u00a0[-1, 0, 1])\\)</li> <li>Roots</li> <li>Odd root of negative number will be negative</li> <li>Odd root of positive number will be positive</li> <li>We cannot find even root of negative number</li> <li>Rationalizing</li> <li>Multiply numerator and denominator by conjugate of denominator</li> </ul>"},{"location":"Masters/GRE/Math/Powers_and_Roots/#units-digit","title":"Units Digit","text":"<ul> <li>Look for repeating pattern</li> <li>Figure out where pattern will be at desired power</li> <li>The units digit of any product will be influenced only by the units digits of the 2 factors</li> </ul> <p>Eg: What is the unit\u2019s digit of \\(57^{123}\\)</p>"},{"location":"Masters/GRE/Math/Probability/","title":"Probability","text":"<ul> <li>Basic formula</li> <li>Complement</li> <li>Useful for \u201cat least one\u201d questions</li> <li>Mutually-exclusive</li> <li>Independent events</li> </ul>"},{"location":"Masters/GRE/Math/Quantitative_Comparison/","title":"Quantitative Comparison","text":""},{"location":"Masters/GRE/Math/Quantitative_Comparison/#options","title":"Options","text":"<ul> <li>A &gt; B</li> <li>B &gt; A</li> <li>A = B</li> <li>Cannot be determined</li> </ul>"},{"location":"Masters/GRE/Math/Quantitative_Comparison/#notes","title":"Notes","text":"<ol> <li>Do not perform more calculations than necessary</li> <li>Do not select D if comparison does not contain unknown values</li> <li>Geometry figures are not necessarily drawn to scale, unless stated otherwise</li> <li>Pay attention to shared information</li> </ol>"},{"location":"Masters/GRE/Math/Quantitative_Comparison/#strategies","title":"Strategies","text":"<ol> <li>Approximation</li> <li>Matching operations: perform same operations on both quantities</li> <li>Add both sides</li> <li>Subtract both sides</li> <li>Multiply both sides by +ve constant</li> <li>Divide both sides by +ve constant<ol> <li>Variable can be positive/negative</li> </ol> </li> <li>Plugging in numbers</li> <li>Looking for equality</li> <li>Number sense</li> </ol>"},{"location":"Masters/GRE/Math/Statistics/","title":"Statistics","text":"<ul> <li>Average</li> <li>Mean<ul> <li>If a set is evenly spaced, the mean and median are equal; However, the converse may/may not hold true</li> </ul> </li> <li>Median</li> <li>Mode</li> <li>Weighted<ul> <li>p_a avg_a + p_b avg_b</li> </ul> </li> <li>Dispersion</li> <li>Range: max-min</li> <li>Variance</li> <li>Standard deviation</li> <li>Rank-based</li> <li>Quartiles</li> <li>Percentiles: Can never be 100<sup>th</sup> percentile</li> <li>Normal distribution</li> <li>Plots</li> <li>Histogram</li> <li>Box plot</li> </ul>"},{"location":"Masters/GRE/Math/Word_Problems/","title":"Word Problems","text":"<ol> <li>Understand question &amp; restrictions</li> <li>Eliminate impossible choices</li> <li>Consider testing answer choices</li> <li>Assign variables</li> <li>Create an equation</li> <li>Solve equation (if necessary)</li> <li>Reread question and confirm required value</li> </ol>"},{"location":"Masters/IELTS/","title":"IELTS Academic","text":""},{"location":"Math_Electives/Numerical_Analysis/","title":"Numerical Analysis","text":""},{"location":"Math_Electives/Numerical_Analysis/#references","title":"References","text":"<ul> <li> MIT 10.34 Numerical Methods Applied to Chemical Engineering</li> <li> StudySession | Numerical Analysis</li> </ul>"},{"location":"Math_Electives/Optimization/","title":"Optimization","text":"<p>Applied mathematics that deals with maximizing/minimizing an objective, with/without constraint(s). This has various applications, such as Revenue Maximization, Cost Minimization, etc</p> <p>Taught by Dr. Maneesha</p>"},{"location":"Math_Electives/Optimization/01_Linear_Programming/","title":"01 Linear Programming","text":""},{"location":"Math_Electives/Optimization/01_Linear_Programming/#linear-programming","title":"Linear Programming","text":"<p>Objective/constraint will be in linear form</p>"},{"location":"Math_Electives/Optimization/01_Linear_Programming/#solution-properties","title":"Solution Properties","text":"Property Meaning How to obtain Feasibility All constraints satisfied Substitute var values in constraints Optimality Max/min value of obj() Substitute var values in obj()"},{"location":"Math_Electives/Optimization/01_Linear_Programming/#graphical-method","title":"Graphical Method","text":"<ol> <li>Define obj()</li> <li>Draw all constraints</li> <li>Draw line for LHS=RHS</li> <li>Draw appropriate arrows show the direction of inequality</li> <li>Find vertices of soln space (feasible region)</li> <li>Find the value of obj() at these vertices</li> <li>Pick the min/max value</li> </ol>"},{"location":"Math_Electives/Optimization/02_Primal_Simplex/","title":"02 Primal Simplex","text":"<p>Vertices can be determined algebraically, when all constraints RHS and variables are non-negative.</p> <p>Always start a problem on the left-side sheet; else you\u2019ll waste time flipping pages</p>"},{"location":"Math_Electives/Optimization/02_Primal_Simplex/#optimality-feasibility","title":"Optimality &amp; Feasibility","text":"Max Prob Min Prob Feasibility All basic vars \\(\\ge 0\\) Same as \\(\\leftarrow\\) Optimality Coeff of all basic vars in obj() \\(\\ge 0\\) Coeff of all basic vars in obj() \\(\\le 0\\)"},{"location":"Math_Electives/Optimization/02_Primal_Simplex/#direct-simplex-method","title":"Direct Simplex Method","text":"<ol> <li>Make sure all RHS constraints are +ve. Else, multiply by -1 to change the sign</li> <li>Convert the constraints inequality into equality</li> </ol> Constraint LHS represents RHS represents Difference Example \\(\\le\\) usage of limited resources for activities limit on the availability of resources Slack (unused) amount of resources \\(4x + 3y \\le 240\\)\\(\\implies 4x + 3y \\textcolor{hotpink}{+ s_k} = 240\\) \\(\\ge\\) usage of limited resources for activities minimum requirement of resource utilization Surplus amount of resources \\(4x + 3y \\le 240\\)\\(\\implies 4x + 3y \\textcolor{hotpink}{- x_k} = 240\\) <ol> <li>Transpose obj()</li> </ol> <p>Example: \\(\\max Z = 70x+50y \\implies \\max Z - 70x-50y=0\\)</p> <ol> <li>Select Entering/Exiting vars</li> </ol> Type Entering var= Non-basic var with most ___ coefficient of obj() Exiting var= Basic var with  ___ ratio Maximization -ve(entering causes fastest increase in value of obj()) least Minimization -ve(entering causes fastest decrease in value of obj()) least <ol> <li>Calculate pivot row</li> <li>Check optimality is reached</li> <li>if obj() coeff \\(\\ge 0 \\forall\\) non-basic vars</li> <li>Verify feasibility</li> <li> <p>end here</p> </li> <li> <p>Else</p> </li> <li> <p>Calculate other rows</p> </li> <li> <p>Repeat steps 4-6</p> </li> </ol> Term Meaning/Formula Ratio \\(\\text{Ratio} = \\frac{\\text{Solution for basic variable}}{\\text{Constraint coeff for entering var}}\\)Denominator &gt; 0If constraint coeff \\(&lt; 0\\), ignore that case, and check other variables\u2019 ratio Pivot Element Intersection of entering and exiting var Pivot Row \\(\\text{Pivot Row} = \\frac{\\text{Leaving Row}}{\\text{Pivot Element}}\\) Other rows \\(\\text{New row} = \\text{Old row} - (\\text{Coeff of var in pivot column} \\times \\text{Pivot row})\\) <p>If \\(x_3, x_4, \\dots\\) come in one equation each, treat them as basic vars. Use row operations to ensure </p> Equation Coeff of \\(x_3, x_4\\) Constraint 1 obj() 0"},{"location":"Math_Electives/Optimization/02_Primal_Simplex/#artificial-starting-solution","title":"Artificial Starting Solution","text":"<ul> <li>Surplus var is not initially included in the list of basic vars</li> <li>If slack and artificial var are tied for leaving var, artificial var leaves</li> </ul> <p>Starting Steps</p> <ol> <li>If any constraints have negative RHS, multiply by -1</li> <li>Convert to equality</li> </ol> Constraint Sign Introduce \\(\\le\\) + Slack var \\(=\\) + Artificial var \\(R_1, R_2, \\dots\\) \\(\\ge\\) - Surplus var (subtraction)+ Artificial var \\(R_1, R_2, \\dots\\)"},{"location":"Math_Electives/Optimization/02_Primal_Simplex/#big-m-method","title":"Big-M Method","text":"Step Maximization Minmization Introduce artificial var to transposed obj()\\(M=\\) very large number, say a millionFor the sign (column on right), think of it as: making \\(R_1, R_2, \\dots\\) very anti-entering \\(+ MR_1 + M R_2 \\ \\dots\\) \\(- MR_1 - M R_2 \\ \\dots\\) Make the table as usual Perform row transformation to eliminate \\(R_1, R_2, \\dots\\) in obj() row(keeping basic rows the same) \\(Z - MR_1 - MR_2\\) \\(Z + MR_1 + MR_2\\) Solve as usual"},{"location":"Math_Electives/Optimization/02_Primal_Simplex/#two-phase-method","title":"Two-Phase Method","text":"Phase obj() Goal 1 \\(\\min R = \\sum R_i\\)(for both max/min problems) Force artificial vars to be 0 2 original LP\u2019s obj() Determine optimal soln"},{"location":"Math_Electives/Optimization/02_Primal_Simplex/#phase-1-steps","title":"Phase 1 Steps","text":"<ul> <li>Do transformation to make \\(R_i\\) as 0 in \\(R\\)\u2019s row; other rows unchanged</li> <li>Solve as usual</li> </ul>"},{"location":"Math_Electives/Optimization/02_Primal_Simplex/#phase-2-steps","title":"Phase 2 Steps","text":"Optimal value of\\(R = \\sum R_i\\) Artificial varsin basis \\(\\implies\\) Optimal Soln = Steps \\(&gt; 0\\) \u274c(infeasible soln) \\(=0\\) \\(0\\) Optimal soln of phase 2 LP - Drop columns in phase 1 table corr to artificial variables- Use original obj() with constraints from optimal phase 1 table (ignore initial constraints). This gives phase 2 obj()- Perform row transformation to eliminate phase 1 basic vars in phase 2 obj() row- Solve as usual \\(= 0\\) \\(&gt; 0\\) Optimal soln of original LP - Drop all\u00a0\u00a0- Non-basic artificial vars from optimal phase 1 table\u00a0\u00a0- Variables from original prob with -ve coeff in row of optimal phase 1 table- Solve as usual"},{"location":"Math_Electives/Optimization/02_Primal_Simplex/#special-cases","title":"Special Cases","text":"Cases Meaning Description Identification in simplex Degeneracy(Degenerate soln) \\(\\ge 1\\) redundant constraints Nothing alarmingMay lead to Cycling (Var enters &amp; exits basis repeatedly w/o reaching optimality)Can be temporary/permanent Solution for one basic var \\(= 0\\)Usually happens when there is a tie for the leaving variable AlternativeOptima obj() \\vert  \\vert binding constraint obj() will assume the same optimal value at more than one corner point, \\(\\implies \\exists\\) alternative soln Coeff of non-basic var \\(=0 \\implies\\) var enters basic vars &amp; obj() will not change UnboundedSoln Unbounded soln space in \\(\\ge 1\\) direction Values of some decision vars can be inc indefinitely, w/o violating constraint(s) - Entries for one/more non-basic var column \\(\\le 0 \\ \\forall\\) constraint rowsand- Entry for same non-basic var in obj() row\\(\\le 0\\) : max\\(\\ge 0\\) : min No feasibleSoln Inconsistent contraints Artificial var(s) exist(s) in basis even after optimality"},{"location":"Math_Electives/Optimization/02_Primal_Simplex/#summary-of-methods","title":"Summary of Methods","text":"Constraints Method to use All \\(=\\) Solve algebraically All \\(\\le\\) Direct Simplex Method All \\(\\ge\\) Big-M Method / 2 Phase Method Mixture of \\(\\le, =, \\ge\\) Big-M Method / 2 Phase Method"},{"location":"Math_Electives/Optimization/03_Duality/","title":"03 Duality","text":"<p>Dual problem is a linear programming prob derived from the primal (original) LP model. It is basically a transposed version of the primal problem. Optimal Soln of Primal Prob = Optimal Soln of Dual Prob</p> <p>Dual Form is preferred when no of constraints increases</p> Form Numerical work \\(\\propto\\) Preferred for Simplex No of constraints Fewer constraints/Many vars Dual No of vars Fewer vars/Many constraints"},{"location":"Math_Electives/Optimization/03_Duality/#conversion-of-primal-to-dual","title":"Conversion of Primal \\(\\to\\) Dual","text":""},{"location":"Math_Electives/Optimization/03_Duality/#initial-steps","title":"Initial Steps","text":"<ul> <li>All constraints RHS &amp; vars are \\(\\ge 0\\)</li> <li>Introduce slack and surplus vars (don\u2019t introduce artificial vars)</li> <li>Dual var is defined \\(\\forall\\) primal constraint</li> <li>Dual constraint is defined \\(\\forall\\) primal var (including slack and surplus vars)</li> </ul> Primal obj() \\(\\implies\\) Dual obj() Inequality of new Constraints max min \\(\\ge\\) min max \\(\\le\\) \\[ \\text{Dual Obj()} = \\sum \\text{RHS constraints} \\] \\[ \\begin{aligned} &amp;\\text{Dual constraint j} \\implies \\\\ &amp; \\small{\\sum \\text{Constraint coeff of } x_i \\text{ &lt;Inequality&gt; } \\text{Non-transposed Obj() coeff of } x_i} \\end{aligned} \\]"},{"location":"Math_Electives/Optimization/03_Duality/#computations","title":"Computations","text":"Formula [Primal constraints column in iteration \\(i\\)](Primal basic vars) [Inverse in iteration \\(i\\)] x [Primal constraints col] Primal obj() coeff of \\(x_j\\)(Primal non-basic vars) LHS of non-transposed dual constraint\\(_j\\) - RHS of non-transposed dual constraint\\(_j\\) [Dual vars in iteration \\(i\\)] [RHS of basic vars in non-transposed dual constraint] x [Inverse in iteration \\(i\\)] Inverse [Inverse] = [Columns in optimal primal table corr to vars not in obj()]"},{"location":"Math_Electives/Optimization/03_Duality/#checking-optimality-feasibility","title":"Checking Optimality &amp; Feasibility","text":"Max Prob Min Prob Feasibility All primal basic vars \\(\\ge 0\\) Same as \\(\\leftarrow\\) Optimality All primal non-basic vars \\(\\ge 0\\) All primal non-basic vars \\(\\le 0\\)"},{"location":"Math_Electives/Optimization/04_Dual_Simplex/","title":"04 Dual Simplex","text":"<p>This is not the same as duality</p> Problem starts at Successive iterations continue to be __ obtained in last iteration Primal Simplex basic feasible solution feasible optimality Dual Simplex infeasible solution, which is better than optimal optimal feasibility"},{"location":"Math_Electives/Optimization/04_Dual_Simplex/#steps","title":"Steps","text":"<p>Obj() must satisfy optimality condition of regular simplex method</p> <ol> <li>Change all equalities to \\(\\le\\)</li> </ol> Constraint Type Change \\(\\le\\) \\(\\ge\\) Multiply both sides by -1 \\(=\\) Convert into inequality of both types\\(x_1 + x_2 = 1 \\implies \\quad x_1 + x_2 \\le 1, \\quad -x_1 - x_2 \\le -1\\) <ol> <li>Add slack vars (there is no surplus/artificial vars for dual simplex)</li> <li>Transpose obj()</li> <li>Select Entering/Exiting vars</li> </ol> Exiting var= Basic var with most ___ value Entering var= Non-basic var with  ___ ratio Max/Min (same for both) -ve least <ol> <li>Calculate pivot row (same as primal)</li> <li>Check feasibility is reached</li> <li>If value of all basic vars \\(\\ge 0\\)</li> <li>Verify optimality (same as primal)</li> <li>end here</li> <li>Else</li> <li>Calculate other rows (same as primal)</li> <li>Repeat steps 4-6</li> </ol>"},{"location":"Math_Electives/Optimization/04_Dual_Simplex/#ratio","title":"Ratio","text":"\\[ \\text{Ratio } = \\left| \\frac{     \\text{Obj() coeffient} }{     \\text{Constraint coeff for exiting var} } \\right| \\qquad (\\text{only magnitude}) \\] <p>Denominator &lt; 0</p> <p>If constraint coeff \\(\\ge 0 \\ \\forall\\) non-basic \\(x_j\\), the problem has no feasible solution</p>"},{"location":"Math_Electives/Optimization/05_Post_Optimal_Analysis/","title":"05 Post Optimal Analysis","text":"<p>Deals with situation of finding new solution in efficient way when parameters are changed</p>"},{"location":"Math_Electives/Optimization/05_Post_Optimal_Analysis/#actions","title":"Actions","text":"Existing SolnFeasible? Existing SolnOptimal? \\(\\implies\\) Action \u2705 \u2705 No action \u2705 \u274c Use primal simplex \u274c \u2705 Use dual simplex \u274c \u274c Use generalized simplex method"},{"location":"Math_Electives/Optimization/05_Post_Optimal_Analysis/#change-in-feasibility","title":"Change in Feasibility","text":"Change Steps RHS of constraint equation changes 1. Check feasibility with inverse method2. Calc new obj(), with values from step 13. Update table4. Use dual simplex5. Calc obj() when feasibility is maintained/obtained Addition of new constraint(s) 1. Check feasibility by substituting existing values into new constraint2. Introduce slack/surplus into equation3. Introduce slack/surplus row &amp; column into table 4. Update introduced basic var row using below formula5. Use dual simplex6. Calc obj() when feasibility is maintained/obtained \\[ \\begin{aligned} &amp; \\text{Updated row of introduced basic var} \\\\ &amp; = \\text{Initial row of introduced basic var} - \\left( \\sum \\text{coeff}_i \\times x_i \\right) \\end{aligned} \\] <p>where</p> <ul> <li>\\(x_i =\\) basic variables in new constraint</li> <li>coeff\\(_i\\) = coeff of \\(x_i\\) in new constraint</li> </ul>"},{"location":"Math_Electives/Optimization/05_Post_Optimal_Analysis/#change-in-optimality","title":"Change in Optimality","text":"<p>Caused due to change in obj()</p> <p>Steps</p> <ol> <li>Find the dual var values, using new obj() coeff</li> <li>Check optimality</li> <li>If optimality is maintained, go to step 6</li> <li>Update obj() row using</li> <li>coeff found when checking optimality</li> <li>original obj() value, using above latest coeff</li> <li>Use primal simplex</li> <li>Calculate latest solution in new obj()</li> </ol>"},{"location":"Math_Electives/Optimization/05_Post_Optimal_Analysis/#new-constraint","title":"New Constraint","text":"Type Meaning Redundant A new constraint that does not affect feasibility of an existing optimum solution. Binding - A new constraint that affects feasibility of an existing optimum solution.- Simplex table after incorporating the constraint"},{"location":"Math_Electives/Optimization/06_Transportation_Model/","title":"Transportation Model","text":"<p>Special case of LP, which deals with shipping of commodities from \\(m\\) sources to \\(n\\) nodes</p> <p>The number of basic vars and independent constraints will be \\((m+n-1)\\)</p> <p>It will always be a minimization problem, since transporation model deals with the shipping cost of commodities</p> <p>To solve, we've to 1. Find initial BFS 2. Find optimal solution</p>"},{"location":"Math_Electives/Optimization/06_Transportation_Model/#matrix","title":"Matrix","text":"<p>\\(m \\times n\\)</p> <p>For every cell \\(v_{ij}\\), make sure that</p> \\[ \\begin{aligned} v_{ij} &amp;= \\text{argmin}(D_i, S_j) \\\\ \\sum v_i &amp;= D_i \\\\ \\sum v_j &amp;= S_j \\end{aligned} \\]"},{"location":"Math_Electives/Optimization/06_Transportation_Model/#balanced-transportation","title":"Balanced Transportation","text":"\\[ \\sum \\text{Supply} = \\sum \\text{Demand} \\]"},{"location":"Math_Electives/Optimization/06_Transportation_Model/#find-initial-bfs","title":"Find initial BFS","text":"<p>(Basic Feasible Soln)</p> <p>Any basic feasible solution will have \\((m+n-1)\\)</p> Basic vars \\(v &gt; 0\\) Non-basic vars \\(v=0\\) <ul> <li>If the problem is balanced, no issues</li> <li>Else add an extra row/column to compensate</li> <li>If no penalties, cell costs = 0</li> <li>If \\(\\exists\\) Penalties, cell costs = penalties</li> </ul> <p>Soln can be find using one of the following methods</p> Method Steps North-West Corner Traverse from top-left to bottom-right Least Cost Vogel\u2019s Approximation(VAM) 1. Calculate row-wise penalties \\(P_i = \\min_2(v_i) - \\min(v_i)\\)2. Calculate row-wise penalties \\(P_j = \\min_2(v_j) - \\min(v_j)\\)3. Pick the row/column with max penalty. Cross out the penalty (and hence entire row/column), if the row/column is completely utilized4. In that row/column, we select the cell with least cost5. Repeat, excluding the recently-allocated cell"},{"location":"Math_Electives/Optimization/06_Transportation_Model/#find-optimal-solution","title":"Find optimal solution","text":"<p>Method of multipliers. 2 conditions need to be satisfied</p> <ul> <li>Supply limits &amp; demand requirements remain satisfied</li> <li>Shipments through all routes must be \\(\\ge 0\\)</li> </ul> <p>Checking optimality</p> <ol> <li> <p>Set \\(u_1 = 0\\)</p> </li> <li> <p>Find \\(u_i, u_j\\) for cells with \\(u_i + v_j = c_{ij}\\)</p> </li> <li>Find BIJ for empty cells: \\(\\text{BIJ} = u_i + v_j-c_{ij}\\)</li> <li>If BIJ \\(\\le 0\\) for empty cells, solution is optimal (this is minimization problem)</li> <li>Else, non-optimal</li> </ol> <p>Optimizing</p> <ol> <li>Find the entering var as the empty cell with the most +ve BIJ</li> <li>Put \\(\\theta\\) there</li> <li>Make a square/rectangle, with corners as non-empty cells or \\(\\theta\\) cell</li> <li>Add</li> <li>\\(-\\theta\\) to corner cells in the same row/col as \\(\\theta\\) cell</li> <li>\\(+\\theta\\) to other corner cells</li> <li>\\(\\theta\\) = Max value that \\(\\theta\\) can assume, which is obtained using the cells in the same row/column of \\(\\theta\\) cell</li> <li>Evaluate all the cells</li> </ol> <p>If 0 appears in non-basic var, and there is no other potential entering var, it implies that optimality is already reached, and future iterations give Alternative Optima.</p>"},{"location":"Math_Electives/Optimization/06_Transportation_Model/#degenerate-bfs","title":"Degenerate BFS","text":"<p>If in a cell we find zero mentioned, it means that all corresponds to basic var that has assumed \\(v=0\\). It implies degenerate basic feasible solution.</p>"},{"location":"Math_Electives/Optimization/06_Transportation_Model/#maximization-problem","title":"Maximization Problem","text":"<p>For eg, if we want to maximize the distribution of foods (not worried about money, for eg during a natural disaster).</p> <p>The values of \\(c_{ij}\\) will be the +ve output. Since transportation problem is always minimization.</p> <ol> <li>\\(c_{ij} = -1 \\times c_{ij}\\)</li> <li>Solve as usual</li> <li>Total output = -1 x total cost</li> </ol>"},{"location":"Math_Electives/Optimization/07_Assignment_Model/","title":"07 Assignment Model","text":"<p>Special case of transportation model, where no of supply nodes always = no of demand nodes</p> <p>Input is a \\(n \\times n\\) matrix, where</p> <ul> <li>\\(n\\) workers are assigned to \\(n\\) jobs</li> <li>cells contain the value of cost associated with assignment</li> </ul>"},{"location":"Math_Electives/Optimization/07_Assignment_Model/#objectives","title":"Objectives","text":"<p>is one of the following</p> <ul> <li>minimize the total time to complete a set of jobs</li> <li>minimize cost of assignments</li> <li>maximize the skill ratings</li> <li>maximize total satisfaction of customers</li> </ul>"},{"location":"Math_Electives/Optimization/07_Assignment_Model/#assumptions","title":"Assumptions","text":"<ul> <li>Each machine/worker is assigned \\(\\le 1\\) job</li> <li>Each job is assigned to exactly 1 machine/worker</li> </ul>"},{"location":"Math_Electives/Optimization/07_Assignment_Model/#steps","title":"Steps","text":"<ol> <li> <p>Operations</p> </li> <li> <p>Row operation \\(R_i = R_i - \\text{min} (R_i)\\)</p> </li> <li> <p>Col operation \\(C_i = C_i - \\text{min} (C_i)\\)</p> <p>or </p> </li> <li> <p>Col operation \\(C_i = C_i - \\text{min} (C_i)\\)</p> </li> <li> <p>Row operation \\(R_i = R_i - \\text{min} (R_i)\\)</p> </li> <li> <p>Assign jobs to workers</p> </li> <li> <p>Only cells with value = 0 can be assigned</p> </li> <li> <p>Assignment of a cell must be unique</p> </li> <li> <p>Cost of completion = Initial values of the assigned cells</p> </li> </ol>"},{"location":"Math_Electives/Optimization/07_Assignment_Model/#special-cases","title":"Special Cases","text":"Case Method No Unique Solution found 1. Draw minimum number of horizontal/vertical lines in the last reduced matrix, passing through all 0s2. Select smallest uncovered element3. Subtract it from every uncovered element4. Add it to every element at the intersection5. If no feasible solution, go to step 16. Else, determine the optimal solution Unbalanced Assignment Add rows/columns as requiredFill empty rows/columns with 0s Maximization Assignment Problem Multiple all cells with -1 (only for first operation)Be careful of -ve signFinal value = -1 x Total Cost Disallowed Assignment If some cell is missing data, fill it in with\u00a0\\(M\\) (a very large number)"},{"location":"Math_Electives/Optimization/08_Project_Management/","title":"08 Project Management","text":"<p>These are network-based methods, designed to assist in planning, scheduling, and control of projects.</p>"},{"location":"Math_Electives/Optimization/08_Project_Management/#objective","title":"Objective","text":"<ol> <li>Determine the minimum possible completion time for the project.</li> <li>Determine a range of start and end time for each activity, so that the project can be completed in minimum time.</li> </ol>"},{"location":"Math_Electives/Optimization/08_Project_Management/#notations","title":"Notations","text":"\\(\\text{ES}_j = \\square_j\\) Earliest occurance time of event \\(j\\) \\(\\text{LC}_j = \\triangle_j\\) Latest completion time of event \\(j\\) \\(D_{ij}\\) Duration of activity between \\(i\\) and \\(j\\)"},{"location":"Math_Electives/Optimization/08_Project_Management/#activities","title":"Activities","text":"Critical Non-Critical Leeway in determining start time \u274c \u2705, within some limit Leeway in determining end time \u274c \u274c Conditions 1. \\(\\square_i = \\triangle_i\\)2. \\(\\square_j = \\triangle_j\\)3. \\(\\square_j - \\square_i = \\triangle_j - \\triangle_i = D_{ij}\\) <p>where \\(D_{ij}\\) =  given distance between 2 nodes</p>"},{"location":"Math_Electives/Optimization/08_Project_Management/#methods","title":"Methods","text":"CPM PERT Full Form Critical Path Method Project Evaluation Review Technique Assumption for activity deterministic durations probabilistic durations Duration of activity Fixed Determined based on- most optimistic time\u00a0\\(a\\)- most likely time\u00a0\\(m\\)- pessimistic time\u00a0\\(b\\) Procedure 2 passes1. Forward pass determines earliest occurance times; take path with max duration if \\(\\exists\\) multiple paths2. Backward pass determines latest completion times; take path with min duration if \\(\\exists\\) multiple paths3. Find critical paths4. Find the float for non-critical activities 1. Calculate distance and variance2. Solve like CPM3. Calculate cumulative E(D_i) and Var4. Calculate required probabilities using \\(z\\)\u00a0distributionIn case of ties, take the max variance path, thereby reflecting more uncertainty Average duration\u00a0\\(\\bar D = \\frac{a+4m+b}{6}\\)Variance\u00a0\\(= \\left(\\frac{b-a}{6}\\right)^2\\)"},{"location":"Math_Electives/Optimization/08_Project_Management/#float","title":"Float","text":"Free Float Total Float \\(\\square_j - \\square_i - D_{ij}\\) \\(\\triangle_j - \\square_i - D_{ij}\\) Case FF = 0 Any delay will cause delay in starting successive activities FF &lt; TF We have leeway in starting the project as FF unitsFor any excess delay (FF &lt; d &lt; T), starting successive activities will be delayed FF\u00a0= TF Activities may be scheduled anywhere between the earliest start time &amp; the latest completion time without delaying the project"},{"location":"Math_Electives/Optimization/09_Game_Theory/","title":"09 Game Theory","text":"<p>Deals with situations where there are conflict of interest between 2 opponents called \u2018players\u2019, who may have finite/infinite strategies or alternatives.</p> <p>These are 2 person zero-sum games, because of the gain of one player is equal to loss to the other</p> <p>Associated with each player is the payoff that one player pays to the other with respect to the each pair of strategies</p> <p>The game is summarized in terms of the payoff to one player.</p> <p>For 2 players with \\(m\\) and \\(n\\) strategies respectively, the payoff matrix will be \\(P_{m \\times n}\\)</p> <p>If \\(A\\) and \\(B\\) use strategy \\(i\\) and \\(j\\) respectively, then payoff to</p> <ul> <li>player A: \\(p_{ij}\\)</li> <li>player B: \\(-p_{ij}\\)</li> </ul>"},{"location":"Math_Electives/Optimization/09_Game_Theory/#examples","title":"Examples","text":"<ul> <li>advertising campaigns for competing products</li> <li>planning war strategies for opposing armies</li> </ul>"},{"location":"Math_Electives/Optimization/09_Game_Theory/#optimal-solution","title":"Optimal Solution","text":"<p>Due to presence of conflict of interest, optimal solution selects the strategies for each player such that any change in strategies will not improve the payoff for either of the players.</p>"},{"location":"Math_Electives/Optimization/09_Game_Theory/#simple-strategies","title":"Simple Strategies","text":"<p>Parties can only pick 1 strategy each.</p> <p>We are trying to find the best variation of the worst-case scenario for both parties</p> <ol> <li>Strategy of \\(A\\) is the strategy for which payoff = max(min) for A</li> <li>Find row-wise min</li> <li>Calculate the max of these</li> <li>Strategy of \\(B\\) is the strategy for which payoff = min(max) for B</li> <li>Find row-wise max</li> <li>Calculate the min of these</li> <li>Saddle point solution = \\((i, j)\\), where \\(i\\) and \\(j\\) are the strategies that </li> <li>if \\(i=j\\), neither \\(i, j\\) would be willing to change their strategy</li> <li>Value of game \\(= [\\text{Sol}_A, \\text{Sol}_B]\\)</li> </ol>"},{"location":"Math_Electives/Optimization/09_Game_Theory/#mixed-strategies","title":"Mixed Strategies","text":"<p>\\(\\not \\exist\\) Saddle point \\(\\implies\\) There is no single strategy for one/more players.</p> <ol> <li> <p>Consider strategies</p> </li> <li> <p>\\(A\\) selects strategies \\(i \\in [1, 2, \\dots]\\) w/ probability \\(x_i\\), such that \\(\\sum x_i = 1\\)</p> </li> <li> <p>\\(B\\) selects strategies \\(i \\in [1, 2, \\dots]\\) w/ probability \\(y_i\\), such that \\(\\sum y_i = 1\\)</p> </li> <li> <p>Draw table of B\u2019s picked strategy and A\u2019s expected payoff</p> </li> </ol> \\[ \\text{Expected Payoff}_A =  \\] B\u2019s Strategy A\u2019s expected payoff <ol> <li> <p>Draw graph</p> </li> <li> <p>idk</p> </li> <li> <p>Maxmin = highest point of lower intersection open area in the graph</p> </li> <li> <p>Minmax = lowest point of highest intersection open area in the graph</p> </li> <li> <p>Equate the expected pay-off of the lines that are involved in maxmin</p> </li> <li> <p>Value of game = value obtained by substituting \\(x_1\\) in the intersecting equations (intersecting equations will give the same value)</p> </li> <li> <p>Find the other person\u2019s </p> </li> </ol> \\[ \\text{Expected Payoff}_B =  \\] A\u2019s Strategy B\u2019s expected payoff"},{"location":"Math_Electives/Optimization/10_Integer_Programming/","title":"10 Integer Programming","text":""},{"location":"Math_Electives/Optimization/10_Integer_Programming/#integer-programming","title":"Integer Programming","text":"<ol> <li>Solve as usual</li> <li>Branch &amp; Bound</li> <li>Split with the variable with larger decimal value</li> </ol> \\[ \\begin{aligned} \\nabla f(x_0) &amp;= 0 \\\\ \\frac{\\partial f}{\\partial x_i} &amp;= 0, \\forall i \\end{aligned} \\]"},{"location":"Math_Electives/Optimization/10_Integer_Programming/#application-sports-scheduling","title":"Application: Sports Scheduling","text":"<ul> <li>Objective: Maximize team preferences</li> <li>Decisions: Which teams should play each other each week</li> <li>Decision variable can be binary or discrete</li> </ul> <p>Steps</p> <ul> <li>Define binary variable \\(x_{ijk}\\)</li> <li>team \\(i\\)</li> <li>team \\(j\\)</li> <li>week \\(K\\)</li> <li>If team \\(I\\) plays team \\(J\\) in week \\(K\\)</li> <li>\\(x_{IJK}=1\\)</li> <li>Else 0</li> <li>Constraints</li> <li>Play other teams in the same division twice<ul> <li>\\(\\sum x_{IJk} = 2, \\forall k\\),\u00a0where \\(I\\) and \\(J\\) are in same divisions</li> </ul> </li> <li>Place teams in other divisions once<ul> <li>\\(\\sum x_{IJk} = 1, \\forall k\\), where \\(I\\) and \\(J\\)\u00a0are in different divisions</li> </ul> </li> <li>Play exactly one team each week<ul> <li>\\(\\sum x_{IjK} = 1, \\forall j\\)</li> </ul> </li> </ul>"},{"location":"Math_Electives/Optimization/11_Goal_Programming/","title":"Goal Programming","text":"<p>This deals with situations with multiple objective functions.</p> <p>Sometimes, some goals will be more important than others</p>"},{"location":"Math_Electives/Optimization/11_Goal_Programming/#deviation-variables","title":"Deviation Variables","text":"<p>Represent the amount by which goal will be violated</p> Deviation ___ RHS of constraint \\(s_i^+\\) above \\(s_i^-\\) below <p>\\(s_i^+\\) and \\(s_i^-\\) are by definition dependent and hence cannot be taken as basic variables simultaneously. This means that in any simplex iteration, \\(\\le 1\\) one the 2 deviation variables can assume +ve values</p>"},{"location":"Math_Electives/Optimization/12_Dynamic_Programming/","title":"Dynamic Programming","text":"<ol> <li>Divide the problem into stages</li> <li>Find the solution stage-by-stage</li> <li>Combine all stages to find overall solution</li> </ol>"},{"location":"Math_Electives/Optimization/13_Non-Linear_Programming/","title":"Non-Linear Programming","text":"<p>Necessary condition</p>"},{"location":"Mech/FEA/","title":"Finite Element Analysis","text":"<p>Finite element analysis (FEA) is a computerized method for predicting how a product reacts to real-world forces, vibration, heat, fluid flow, and other physical effects.</p> <p>FEA helps identify if a product will break, wear out, or work the way it was designed.</p>"},{"location":"Mech/FEA/#steps","title":"Steps","text":"<ul> <li>Create model</li> <li>Create parts</li> <li>Create sets for boundary conditions and loads</li> <li>Merge parts to assembly</li> <li>Materials</li> <li>Create materials and sections</li> <li> <p>Assign materials to sections</p> </li> <li> <p>Create steps</p> </li> <li>Specify output requests</li> <li>Specify boundary conditions</li> <li>Specify loads</li> <li>Generate Mesh</li> <li>Create jobs</li> <li>Run jobs</li> </ul> <p>Job = Simulation</p>"},{"location":"Mech/FEA/#controls","title":"Controls","text":"Apply Calculate Force Control Force Displacement Displacement Control Displacement Force"},{"location":"Mech/FEA/01_Crack/","title":"Cracks","text":"<ul> <li>Advanced topic: Only covered after Bachelor\u2019s and Master\u2019s level courses</li> <li>Rare topic: Most FEA does not focus on cracks, since cracks are due to failure of the initial manufacturing stage and hence crack FEA is an after-thought</li> </ul>"},{"location":"Mech/FEA/01_Crack/#crack-properties","title":"Crack Properties","text":"Characteristic length Fracture energy Proportional to mesh sizeHow much energy required to break one finite element Displacement"},{"location":"Mech/FEA/01_Crack/#reading","title":"Reading","text":"<ul> <li> Research gate crack characteristic length vs fracture energy BUET</li> </ul>"},{"location":"Misc/misc/","title":"Misc","text":""},{"location":"Misc/misc/#digital-business-card","title":"Digital Business Card","text":"<ol> <li>Generate vCard text: https://vcardmaker.com/</li> <li>Generate QR Code from text: https://www.the-qrcode-generator.com/</li> </ol> <p>Example vCard</p> <pre><code>BEGIN:VCARD\n\nVERSION:3.0\n\nFN:Ahmed Thahir\n\nN:Thahir;Ahmed;;;\n\nURL:https://LinkedIn.com/AhmedThahir\n\nEMAIL;TYPE=Work:ahmedthahir2002@gmail.com\n\nTEL;TYPE=WORK,VOICE:+971 55 123 4567\n\nADR;CHARSET=UTF-8;TYPE=WORK:;;;Dubai;;;United Arab Emirates\n\nEND:VCARD\n</code></pre>"},{"location":"Misc/misc/#presentation","title":"Presentation","text":"<p>Audience transformation framework</p> <p></p>"},{"location":"Misc/AI_Healthcare/","title":"AI for Healthcare","text":""},{"location":"Misc/AI_Healthcare/#references","title":"References","text":"<ul> <li> ML for Healthcare | MIT</li> <li> Survival Analysis</li> </ul>"},{"location":"Misc/AI_Healthcare/#current-video","title":"Current Video","text":"<p>https://www.youtube.com/watch?v=PKCMH5KOcxQ&amp;list=PLUl4u3cNGP60B0PQXVQyGNdCyCTDU1Q5j&amp;index=12</p>"},{"location":"Misc/AI_Healthcare/01_Introduction/","title":"Introduction","text":""},{"location":"Misc/AI_Healthcare/01_Introduction/#problems","title":"Problems","text":"<ul> <li>Misdiagnosis</li> <li>Late diagnosis</li> <li>Inappropriate management after diagnosis</li> <li>Medical errors are pervasive</li> </ul>"},{"location":"Misc/AI_Healthcare/01_Introduction/#history","title":"History","text":"Limitations 1970s MYSIN expert system Identifying bacteria 1980s INTERNIST-1/Quick Medical Reference Internal medicine Bayesian network-like 1. Clinicians entered symptoms manually2. Difficult to maintain3. Difficult to generalize (prior probabilities will different across different parts of the world) RX Project 1990s ANN 1. Did not fit well into clinical workflow2. Hard to get enough training data3. Poor generalization to new places"},{"location":"Misc/AI_Healthcare/01_Introduction/#what-has-changed","title":"What has changed?","text":""},{"location":"Misc/AI_Healthcare/01_Introduction/#data-availability","title":"Data availability","text":"<ul> <li>Adoption of Electronics Records</li> <li>Lab tests</li> <li>Imaging</li> <li>Vital signs</li> <li>Genomics</li> <li>Wearable sensors</li> </ul> <p>### Standardization</p> <ul> <li>Reports</li> <li>Data storage</li> <li>APIs</li> </ul> <p>OMOP</p> <p></p>"},{"location":"Misc/AI_Healthcare/01_Introduction/#machine-learning","title":"Machine Learning","text":"<ul> <li>Learning with high-dimensional features</li> <li>Semi-supervised and unsupervised learning</li> <li>Deep learning</li> <li>Democratization of machine learning</li> <li>Open-source software</li> </ul>"},{"location":"Misc/AI_Healthcare/01_Introduction/#overview","title":"Overview","text":"<p>Emergency department</p> <ul> <li>Limited resources</li> <li>Time sensitive</li> <li>Critical decisions</li> </ul>"},{"location":"Misc/AI_Healthcare/01_Introduction/#applications","title":"Applications","text":"<ul> <li>Better triage</li> <li>Faster diagnosis</li> <li>Early detection of adverse events</li> <li>Prevent medical errors</li> <li>Recommend treatment pathway</li> <li>Anticipating clinicians needs</li> <li>Reducing needs for specialist consults</li> <li>Automated documentation &amp; billing</li> <li>Predicting patient\u2019s future disease progression</li> <li>Continuous monitoring</li> <li>Discovery of new disease subtypes</li> <li>Design of new drugs</li> <li>Better targeted clinical trials</li> </ul>"},{"location":"Misc/AI_Healthcare/01_Introduction/#what-makes-ml-in-healthcare-different","title":"What makes ML in healthcare different?","text":"<ul> <li>Life/death decisions, similar to Autonomous Driving</li> <li>Need robust algorithms</li> <li>Checks and balances required for ML deployment</li> <li>Need fair &amp; accountable algorithms</li> <li>Lot of scope for unsupervised learning</li> <li>Causal learning required: just prediction insufficient</li> <li>Very little labelled data: need to use semi-supervised algorithms</li> <li>Small sample size</li> <li>Data quality issues</li> <li>Varying time intervals</li> <li>Missing data</li> <li>Censored labels</li> <li>Data sensitivity</li> <li>Difficulty of de-identifying</li> <li>Difficulty of deploying ML</li> <li>Commercial electronic health record software is difficult to modify</li> <li>Different standards used</li> <li>Careful testing and iteration needed</li> </ul>"},{"location":"Misc/AI_Healthcare/02_Clinical_Care/","title":"Clinical Care","text":""},{"location":"Misc/AI_Healthcare/02_Clinical_Care/#goals-of-health-care","title":"Goals of Health Care","text":"<ul> <li>Cure Morbidity (sickness)</li> <li>Disability</li> <li>Delay Mortality</li> <li>Keep people healthy</li> </ul>"},{"location":"Misc/AI_Healthcare/02_Clinical_Care/#tasks-of-health-care","title":"Tasks of Health Care","text":"<ul> <li>Diagnosis</li> <li>Prognosis</li> <li>Treatment</li> <li>Prevent/Public Health</li> </ul>"},{"location":"Misc/AI_Healthcare/02_Clinical_Care/#medical-cycle","title":"Medical Cycle","text":""},{"location":"Misc/AI_Healthcare/02_Clinical_Care/#enterprise-level-clinical-process-automation","title":"Enterprise-level clinical process automation","text":""},{"location":"Misc/AI_Healthcare/02_Clinical_Care/#how-does-the-health-system-learn","title":"How does the Health System Learn","text":"<p>Randomized Clinical Trials</p>"},{"location":"Misc/AI_Healthcare/02_Clinical_Care/#limitations","title":"Limitations","text":"<ul> <li>Heterogeneity: most cases to which RCT results are applied to do not fit trial criteria</li> <li>Short follow-up</li> <li>Small samples</li> </ul>"},{"location":"Misc/AI_Healthcare/02_Clinical_Care/#paying-for-health-care","title":"Paying for Health Care","text":"<ul> <li>Companies spend lot on healthcare</li> <li>Increased demand</li> <li>Waste: Unnecessary procedures</li> </ul>"},{"location":"Misc/AI_Healthcare/03_Clinical_Data/","title":"Clinical Data","text":""},{"location":"Misc/AI_Healthcare/03_Clinical_Data/#basic-exploration","title":"Basic Exploration","text":"<p>Who are these 300 yr old people? According to law, you are not supposed to specify age of someone older than 90, as they will be easily identified due to small subpopulation size</p> <p>Why are some greater than 300? Their age is 300+time spent in hospital</p>"},{"location":"Misc/AI_Healthcare/03_Clinical_Data/#types-of-data","title":"Types of Data","text":"<ul> <li>Demographics</li> <li>Vital signs</li> <li>Medications</li> <li>Laboratory</li> <li>Pathology</li> <li>Microbiology</li> <li>Notes</li> <li>Discharge summary</li> <li>Attending/resident</li> <li>Nurse</li> <li>Specialist</li> <li>Consultant</li> <li>Referring physician</li> <li>Emergency room</li> <li>Imaging: XRay, CTScan, etc</li> <li>Quantified self</li> <li>Activity</li> <li>Vitals</li> <li>Diet</li> <li>Blood sugar</li> <li>Allergies</li> <li>Mindfulness</li> <li>Mood</li> <li>Sleep</li> <li>Pain</li> <li>Sexual activity</li> <li>Billing data</li> <li>Diagnoses</li> <li>Procedures</li> <li>Diagnose related groups</li> <li>Adminstrative</li> <li>Service</li> <li>Transfers</li> </ul>"},{"location":"Misc/AI_Healthcare/03_Clinical_Data/#issues","title":"Issues","text":"<ul> <li>Missing data</li> <li>Non-stationarity</li> <li>Change in definition of disease over time, resulting in number of people with disease change over time</li> <li> <p>Lab tests performed changes over times</p> </li> <li> <p>Discontinuation medication intake by patient not tracked</p> </li> <li>Standards are often lacking</li> </ul>"},{"location":"Misc/AI_Healthcare/03_Clinical_Data/#coding-systems","title":"Coding Systems","text":"<ul> <li>Medication</li> <li>NDC</li> <li>MedDRA</li> <li>HCPCS</li> <li>GSN</li> <li>CPT</li> <li>Procedure</li> <li>ICD9</li> <li>CPT</li> </ul> <p>CPT: owned by American College of Physicians; codes are copywrited \ud83d\ude33</p>"},{"location":"Misc/AI_Healthcare/03_Clinical_Data/#censors","title":"Censors","text":"<ul> <li>Left-sensored: Missing features</li> <li>Right-sensored: Missing label</li> </ul>"},{"location":"Misc/AI_Healthcare/04_Risk_Stratification/","title":"Risk Stratification","text":"<p>Separating a patient population into high-risk and low-risk of having an outcome</p> <ul> <li>Predicting something in the future</li> <li>Goal is different from diagnosis, with distinct performance metrics</li> <li>Fuzzy classification</li> <li>Diverse data</li> </ul> <p>Risk stratification drives interventions that target high-risk patients</p> <p>Goal is to reduce cost and improve patient outcomes</p>"},{"location":"Misc/AI_Healthcare/04_Risk_Stratification/#applications","title":"Applications","text":"<ul> <li>Pre-mature infant\u2019s risk of severe morbidity</li> <li>Does this patient need to be admitted to coronary-care unit</li> <li>Likelihood of hospital readmission</li> </ul>"},{"location":"Misc/AI_Healthcare/04_Risk_Stratification/#types","title":"Types","text":"Traditional AI Use readily-available data and feed into model Pros Simple - Population-level- Automated: Fits more easily into workflow- Higher accuracy- Quicker to derive Limitations ManualSample-specificNot used as much as required due to high friction Example APGAR Scoring system AI"},{"location":"Misc/AI_Healthcare/04_Risk_Stratification/#apgar-scoring-system","title":"APGAR Scoring system","text":""},{"location":"Misc/AI_Healthcare/04_Risk_Stratification/#framing-for-supervised-ml","title":"Framing for Supervised ML","text":"<p>Why are gaps important? To avoid label leakage</p> <p>Sparsity-encourage models</p> <ul> <li>Easier to interpret</li> <li>Helps deploy model to different clinics where they may not have access to all the data</li> </ul>"},{"location":"Misc/AI_Healthcare/04_Risk_Stratification/#how-to-get-labels","title":"How to get labels","text":"<ul> <li>Manual</li> <li>Label patients\u2019 data by \u201cchart review\u201d<ul> <li>Visualization of individual patient data time series</li> </ul> </li> <li>Automatic</li> <li>Rule-based<ul> <li>Labels may get revised regularly based on standards</li> <li>For eg</li> <li>2020: 200 units of sugar = diabetes</li> <li>2025: 100 units of sugar = diabetes</li> </ul> </li> <li>Machine learning to predict if the patient is \u201ccurrently\u201d diabetic</li> </ul> <p>Based on</p> <ul> <li>medications: may not have record of</li> <li>purchase</li> <li>intake</li> <li>lab data</li> </ul>"},{"location":"Misc/AI_Healthcare/04_Risk_Stratification/#metrics","title":"Metrics","text":"PPVPositive Predictive Value AUC-ROC Calibration"},{"location":"Misc/AI_Healthcare/04_Risk_Stratification/#intervention-tainted-outcomes","title":"Intervention-Tainted Outcomes","text":"<p>Form of Self-Selection Bias</p> <p>Let</p> <ul> <li>Group A: Patients with Pneumonia with history of asthma</li> <li>Group B: Patients with Pneumonia without history of asthma</li> </ul> <p>Observation: Group A dies less often than group B</p>"},{"location":"Misc/AI_Healthcare/04_Risk_Stratification/#discussion","title":"Discussion","text":"<ol> <li>Reason group A dies less is due to more intensive care</li> <li>Long survival time may be due to treatment</li> </ol>"},{"location":"Misc/AI_Healthcare/04_Risk_Stratification/#conclusion","title":"Conclusion","text":"<ol> <li>Does this mean group A has lower risk? No</li> <li>Should we treat group A with less priority? No</li> </ol>"},{"location":"Misc/AI_Healthcare/04_Risk_Stratification/#hacks","title":"Hacks","text":"<ol> <li>Remove such features from the model; not feasible for high-dimensional data</li> <li>Redefine outcome by finding a pre-treatment surrogate (such as lactate levels)</li> <li>Consider treated patients as right-censored by treatment</li> </ol>"},{"location":"Misc/AI_Healthcare/04_Risk_Stratification/#solutions","title":"Solutions","text":"<ol> <li>Interpretable models are very important</li> <li>Causality modelling: Reframe question to \u201cWill admission to ICU lower likelihood of death for patient\u201d</li> </ol>"},{"location":"Misc/AI_Healthcare/04_Risk_Stratification/#deep-learning-for-risk-stratification","title":"Deep Learning for Risk Stratification","text":"<p>Not very big gains</p> <p></p> <p>Baseline is L1-regularized Logistic Regression, with good structural features</p> <p>Sequential data in medicine is very different from language modelling</p> <ul> <li>Many time scales</li> <li>Significant missing data</li> <li>Multi-variate observations</li> <li>Not enough data to learn subtle non-linear interactions</li> </ul>"},{"location":"Misc/AI_Healthcare/05_Survival_Modelling/","title":"Survival Modelling","text":"<p>Variable studies is the time until an event occurs</p> <p>Predict when some event will happen</p> <p>Focus on right-censored data</p> <p></p> <p>Always finite time period: start &amp; end time period</p>"},{"location":"Misc/AI_Healthcare/05_Survival_Modelling/#cases","title":"Cases","text":"Treatment Event knowledge of true survival duration within time period within time period \u2705 within time period within time periodbut not noticed \u274c within time period not within time period \u274c within time period Discontinued study \u274c within time period Unexpected event \u274c"},{"location":"Misc/AI_Healthcare/05_Survival_Modelling/#challenges-to-classification","title":"Challenges to classification","text":"<ul> <li>Less training data</li> <li>Pessimistic estimates due to choice of window</li> </ul>"},{"location":"Misc/AI_Healthcare/05_Survival_Modelling/#challenges-to-regression","title":"Challenges to regression","text":"<ul> <li>Non-gaussian: \\(T\\) is non-negative; may want long tails</li> <li>If we just naively remove censored events, model will be biased predict lower survival, since the people who got removed due to censoring did not get diabetes (ever/yet)</li> </ul>"},{"location":"Misc/AI_Healthcare/05_Survival_Modelling/#notation-formalization","title":"Notation &amp; Formalization","text":"<p>Let</p> <ul> <li>Data</li> <li>\\(X=\\) features</li> <li>\\(t=\\) time</li> <li>\\(b(t)=\\{ 0, 1 \\} =\\) binary indicator denoting whether time is of censoring/event occurrence</li> <li>\\(f(t)=P(t) =\\) probability of death at time \\(t\\)</li> <li>\\(S(t) =\\) probability of individual surviving beyond time \\(t\\)</li> <li>\\(T_d=\\) time of death</li> </ul> \\[ \\begin{aligned} s(t) &amp;= P(T_d&gt;t) &amp;&amp;= 1-P(T_d&lt;t) \\\\ &amp;= \\int_{t}^\\infty f(x) \\cdot dx &amp;&amp;= 1-\\int_{0}^t f(x) \\cdot dx \\end{aligned} \\] <p></p>"},{"location":"Misc/AI_Healthcare/05_Survival_Modelling/#methods","title":"Methods","text":""},{"location":"Misc/AI_Healthcare/05_Survival_Modelling/#kaplan-meier-estimator","title":"Kaplan-Meier Estimator","text":"<p>Graphical representation of survival function</p> <ul> <li>Non-parametric model</li> <li>Good for unconditional density estimation</li> </ul> <p></p> <ul> <li>\\(d(t)=\\) no of events at time \\(t\\)</li> <li>\\(n(t)=\\) no of individuals alive and uncensored at time \\(t\\)</li> </ul> \\[ \\hat s_{k-m}(t) = \\prod_{k: y(k) \\le t} \\left( 1-\\dfrac{d(k)}{n(k)} \\right) \\]"},{"location":"Misc/AI_Healthcare/05_Survival_Modelling/#log-rank-test","title":"Log-Rank Test","text":"<p>Compares the time until and event occurs of 2/more independent samples</p> <p>\u201cIs there a significant difference between the 2 curves\u201d</p> <p>Hypothesis test</p> <ul> <li>\\(H_0:\\) identical distribution curves</li> <li>\\(H_1:\\) different distribution curves</li> </ul> <p>\\(p &lt; \\alpha \\implies\\) reject \\(H_0\\)</p>"},{"location":"Misc/AI_Healthcare/05_Survival_Modelling/#cox-proportional-hazards-model","title":"Cox Proportional Hazards Model","text":"<p>Identify if there are other parameters that affect the curve</p>"},{"location":"Misc/AI_Healthcare/05_Survival_Modelling/#evaluation","title":"Evaluation","text":""},{"location":"Misc/AI_Healthcare/05_Survival_Modelling/#c-statisticconcordance-index","title":"C-Statistic/Concordance-Index","text":"<p>Evaluate model\u2019s ability to predict relative survival times</p> <p>Equivalent to AUC for binary classification without censoring $$ \\hat c = \\dfrac{1}{n} \\sum_{i: b_i=0} \\sum_{j: y_i &lt; y_j} I \\Big[ S(\\hat y_j \\vert X_j) &gt; S(\\hat y_i \\vert X_i) \\Big] $$ </p>"},{"location":"Misc/AI_Healthcare/05_Survival_Modelling/#mean-squared-error-for-uncensored-individuals","title":"Mean-Squared Error for uncensored individuals","text":""},{"location":"Misc/AI_Healthcare/05_Survival_Modelling/#held-out-censored-likelihood","title":"Held-out censored likelihood","text":""},{"location":"Misc/AI_Healthcare/05_Survival_Modelling/#binary-classifier","title":"Binary Classifier","text":"<p>Derive binary classifier from learnt model and check calibration</p>"},{"location":"Misc/AI_Healthcare/06_Physiological_Time_Series/","title":"Physiological Time Series","text":""},{"location":"Misc/AI_Healthcare/06_Physiological_Time_Series/#problems","title":"Problems","text":"<ul> <li>Measurements confounded by</li> <li>Interventions</li> <li>Measurement errors</li> </ul>"},{"location":"Misc/AI_Healthcare/06_Physiological_Time_Series/#idk","title":"IDk","text":"<ul> <li>Once measurement issues identified, we must impute the missing data</li> <li>Can help mitigate alarm fatigue by not alerting the clinicians when unnecessary</li> </ul>"},{"location":"Misc/AI_Healthcare/06_Physiological_Time_Series/#switching-linear-dynamical-systems","title":"Switching linear dynamical systems","text":""},{"location":"Misc/AI_Healthcare/06_Physiological_Time_Series/#traditional-modelling","title":"Traditional Modelling","text":""},{"location":"Misc/AI_Healthcare/07_NLP/","title":"Natural Language Processing","text":""},{"location":"Misc/AI_Healthcare/07_NLP/#goals","title":"Goals","text":"<ul> <li>For any word/phrase, assign meaning from taxonomy/ontology/terminology</li> <li>For any word/phase, determine whether it represents protected health information</li> <li>Determine aspects of each entity: time, location, certainty</li> <li>Having identified 2 meaningful phrases in a sentence, determine the relationship between them</li> <li>In a larger document, identify sentences/fragments relevant to answering a specific medical question</li> <li>Summarization</li> </ul>"},{"location":"Misc/AI_Healthcare/07_NLP/#types","title":"Types","text":"<ul> <li>Every word</li> <li>De-identification</li> <li>Extraction of all<ul> <li>EntitiesTime</li> <li>Certainty</li> <li>Causation/association</li> </ul> </li> <li>Aggregate</li> <li>Identify \u201csmoking\u201d</li> <li>Cohort selection</li> </ul>"},{"location":"Misc/AI_Healthcare/07_NLP/#idk","title":"IDK","text":"<p>Billing codes are enforced based on the tests required to evaluate a patient, not the condition</p>"},{"location":"Misc/AI_Healthcare/08_Implementing/","title":"Deploying","text":""},{"location":"Misc/AI_Healthcare/08_Implementing/#hype-cycle","title":"Hype Cycle","text":"<p>Technology adoption cycle</p> <p></p> <p>Venture Capitalists know that 9/10 investments fail, but expect at least 1/10 makes enough money to compensate for the others</p>"},{"location":"Misc/AI_Healthcare/09_Imaging/","title":"Imaging","text":"<ul> <li>Can\u2019t just automate reading image scans because of</li> <li>high-risk</li> <li>Can\u2019t sue AI company</li> </ul> <p>Hence, AI is used to prioritize which scans are most urgent to look at</p> <p>This way the expert can still look at missed diagnoses eventually even if is low priority</p>"},{"location":"Misc/AI_Healthcare/09_Imaging/#problems-with-ai","title":"Problems with AI","text":"<ul> <li>Explainability</li> <li>Localization</li> <li>Patients and provides share in decisions: all parties need to be convinced of the validity of the conclusion</li> </ul>"},{"location":"Misc/AI_Healthcare/09_Imaging/#solution-semantic-segmentation","title":"Solution: Semantic Segmentation","text":"<p>Identifying anomalies in the scans</p>"},{"location":"Misc/AI_Healthcare/11_Differential_Diagnosis/","title":"Differential Diagnosis","text":"<ul> <li>Diagnosis: Identification of nature and cause of certain phenomenon</li> <li>Differential diagnosis: Distinguishing of particular disease/condition from others that present similar clinical features</li> </ul>"},{"location":"Misc/AI_Healthcare/11_Differential_Diagnosis/#models","title":"Models","text":"<ul> <li>Flowcharts</li> <li>Based on associations b/w diseases and {signs, symptoms}</li> <li>\u201cmanifestations\u201d covers all observables, including lab tests, bedside measurements</li> <li>Single disease vs multiple diseases</li> <li>Probabilistic vs categorical</li> <li>Utility theoretic</li> <li>Rule-based</li> <li>Pattern-matching</li> </ul> <p>Models?</p> <ul> <li>Bayesian network</li> </ul>"},{"location":"Misc/Digital_Transformation_Best_Practices/","title":"Digital Transformation Best Practices","text":""},{"location":"Misc/DoE/","title":"Design of Experiments","text":""},{"location":"Misc/DoE/#references","title":"References","text":"<ul> <li> Modern Data Analysis for Economics</li> <li> From Data to Decisions: Measurement, Uncertainty, Analysis and Modeling | Chris Mack | University of Texas</li> <li> Design of Experiments (DoE) simply explained | DATAtab</li> <li> DoE vs Machine Learning | Paul Allen</li> <li> Designing, Running, and Analyzing Experiments | IxD Online: UCSD &amp; Coursera</li> <li> Design And Analysis Of Experiments | IITK</li> <li> Design of Experiments | The Open Educator</li> <li> Design of Experiments | Adam Kashlak</li> </ul>"},{"location":"Misc/DoE/01_Introduction/","title":"Introduction","text":"<p>Planning experiment to promote good decision making, with minimum cost</p> <p>Counter-intuitive point: the best time to design experiment is after experiment is finished, especially the first time when you don\u2019t know the underlying data-generating response process</p>"},{"location":"Misc/DoE/01_Introduction/#experiment","title":"Experiment","text":"<p>Deliberate variation of one/more process variables while observing effect on one/more response variables</p>"},{"location":"Misc/DoE/01_Introduction/#experimental-design","title":"Experimental Design","text":"<ul> <li>Maximize information gain</li> <li>Minimize resources (time and cost)</li> </ul> <p>DOE is a procedure to plan experiments to efficiently provide valid conclusions</p> <p>Does the experiment have enough statistical power to answer research questions?</p>"},{"location":"Misc/DoE/01_Introduction/#process","title":"Process","text":"<ol> <li>Define objectives of experiment</li> <li>Define inputs to the process</li> </ol>"},{"location":"Misc/DoE/01_Introduction/#types-of-input-vars","title":"Types of Input Vars","text":"Dealing Controlled &amp; observed/Factors Direct control of experimenter Variation + repeats/replication experiments in a systematic way Uncontrolled &amp; observed/Nuisance vars Not controlled by experimenter, but measured - Blocking- Analysis of covariance Uncontrolled &amp; unobserved Unknown to experimenter, but affects output response Randomization: Impact of variable averages out to 0"},{"location":"Misc/DoE/01_Introduction/#uses","title":"Uses","text":"<ul> <li>Exploratory work</li> <li>Comparison: Choosing between alternatives</li> <li>Screening: Selecting key factors that affect a response</li> <li>Response surface modeling: Optimize given process</li> <li>Hitting and control target response with minimum variability</li> <li>Maximize/minimize response/goal</li> <li>Increase process robustness</li> <li>Prediction</li> </ul>"},{"location":"Misc/DoE/01_Introduction/#rsm","title":"RSM","text":"<p>Response Surface Methodology</p> <p>Looks for quadratic/higher order trends</p> <p>Assumes all variables significant</p> <p>Quadratic response always has a stationary point (min/max/saddle point)</p> <p>Can be used to optimize a process $$ y_i = \\beta_0 + \\sum_{i=1}^k \\beta_i x_i + \\sum_{i=1}^k \\beta_{ii} x_i^2 + \\sum_{i} \\sum_{j&gt;i} \\beta_{ij} x_i x_j + u_i $$ One at a time</p> <p></p> <p></p>"},{"location":"Misc/DoE/01_Introduction/#properties","title":"Properties","text":"Orthogonality No collinearity/multi-collinearity of factors Rotatability Variance of response at \\(x\\) depends only on distance of \\(x\\) from design center point, and not direction (ie, \\(x_j\\))- All first-order orthogonal designs are rotatable- Composite face-center design is not rotatable Uniformity Control no of center points to achieve uniform precision, until \\(\\sigma^2_{y, \\text{center}} = \\sigma^2_{y, \\text{extreme}}\\) Efficiency No of required experimental runs"},{"location":"Misc/DoE/01_Introduction/#notes","title":"Notes","text":"<ul> <li>Beware of extrapolation</li> <li>Multiple responses</li> <li>Overlapping response contour plots</li> <li>Combined cost function</li> <li>PCA can be useful</li> </ul>"},{"location":"Misc/DoE/01_Introduction/#pitfalls-of-doe-rsm","title":"Pitfalls of DOE &amp; RSM","text":"<ul> <li>Optimization of process without understanding the process</li> <li>Design assumes a model, so there is no way to test model error as the model always fits</li> <li>A quadric model will always show you an optimum point, but its accuracy depends on the accuracy of the model</li> <li>Exponential data fir with a quadratic model will show an optimum that does not exist</li> </ul>"},{"location":"Misc/DoE/01_Introduction/#doe-for-prediction","title":"DOE for Prediction","text":"<p>Goal: equalize the leverage of every point</p>"},{"location":"Misc/DoE/01_Introduction/#optimal-design","title":"Optimal design","text":"<p>Algorithmic approach to searching the design space and pick values of input vars in experiment to produce desired statistical properties</p> <ul> <li>Smallest SE of model parameters and of predictions, for a given \\(n\\)</li> <li>Smallest \\(n\\), for a given SE of model parameters &amp; predictions</li> </ul>"},{"location":"Misc/DoE/01_Introduction/#limitations","title":"Limitations","text":"<ul> <li>Model must be specified ahead of time</li> <li>Range of each input var must be specified ahead of time</li> <li>With multiple input vars, there can be tradeoffs b/w parameter variances</li> </ul>"},{"location":"Misc/DoE/01_Introduction/#types","title":"Types","text":"Design Use when Advantage Disadvantage \\(\\text{SE}(b_1)\\)For SLR Space-Filling Evenly spaced out \\(x\\) or \\(y\\) check whether the model is correct IntuitiveCan verify model Wasted opportunity cost of SE \\(\\sqrt{3 \\times \\dfrac{n-1}{n+1}} \\times \\dfrac{\\text{RMSE}}{\\sqrt{n} \\left( \\dfrac{x_\\max - x_\\min}{2}  \\right)}\\) Dumbbell - half data points at the lowest \\(x\\) value- half data points at the highest \\(x\\) value structural model already known Lowest SE of parameters Cannot verify structural model \\(1 \\times \\dfrac{\\text{RMSE}}{\\sqrt{n} \\left( \\dfrac{x_\\max - x_\\min}{2}  \\right)}\\) Equal-Thirds - \u2153 highest- \u2153 middle- \u2153 lowest Compromise between space-filling and dumbbell \\(\\sqrt{\\dfrac{3}{2}} \\times \\dfrac{\\text{RMSE}}{\\sqrt{n} \\left( \\dfrac{x_\\max - x_\\min}{2}  \\right)}\\)"},{"location":"Misc/DoE/01_Introduction/#principles","title":"Principles","text":"<ol> <li>Capacity for primary model</li> <li>Capacity for alternate model</li> <li>Minimum variance of estimated model parameters or predict values</li> <li>Except for simple cases, must search for optimal design</li> <li>Sample where the variation is</li> <li>For non-constant variance, \\(n_i \\propto \\sigma_{yi}^2\\)</li> <li>For curves, sample more in steep regions: Think about evenly-space \\(y\\) values rather than evenly-space \\(x\\) values</li> <li>Repeats and replication: To compute a model-independent estimate of process standard deviation</li> <li>Randomization and blocking</li> <li>allows detection of drift</li> <li>reduces influence of effect modifiers</li> </ol>"},{"location":"Misc/DoE/01_Introduction/#what-to-optimize","title":"What to optimize","text":"Optimality Objective Uncorrelated ObsLinear Model Uncorrelated ObsQuadratic Model Correlated Obs (autoregressive)Linear Model Correlated Obs  (autoregressive)Quadratic Model A(Average) Min average variance of estimates of model parameters (trace of covariance matrix) C(Combination) Min variance of predetermined linear combination of model parameters (selected subset of important parameters) D(Determinant) Min determinant of covariance matrixMax determinant of information matrix Dumbbell Equal-Thirds \\(\\approx\\) equally-spaced \\(x\\) \\(\\approx\\) equally-spaced \\(y\\) E(Eigenvalue) Max the minimum eigenvalue of information matrixMin multi-collinearity T Max trace of information matrix G Min the maximum \\(h_{ii}\\)Min maximum prediction variance I(Integrated) Min average prediction variance over design space VVariance Min average prediction variance for \\(m\\) specific points <p>Information matrix \\(= X^T X\\) </p>"},{"location":"Misc/DoE/01_Introduction/#idk","title":"IDK","text":"Repetition Replication Duplication of experiment on some data with same experimental run Repeated experimental runs where entire procedure is repeated independently at a different time Each replicate is subject to same variability, but independently(ie complete block) Include all sources of variation \u274c \u2705 Example Repeat generation of one data point through five \u201creadings\u201d to independently assess variability"},{"location":"Misc/DoE/02_Types_of_Experiments/","title":"Types of Experiments","text":"Type Example Natural/Quasi In non-experimental settings, sometimes implicit randomization occurs, and the treatment occurs \u201cas if\u201d it is random Uni admission cutoff provides a natural experiment on uni education. Students just above/below are likely to be very similar. For these students, uni education is \u201cas if\u201d random. Comparing these students (ones that went to uni/not) produces an estimate of the causal effect of college education. Regression Discontinuity Design Discrete treatment status determined by an underlying continuous variable, which is used for quasi experiments Differences-in-Differences 2 time-series process \\(y_1\\) and \\(y_2\\) have the factors affecting them RCT(Randomized Control Trials)"},{"location":"Misc/DoE/02_Types_of_Experiments/#types-of-rdd","title":"Types of RDD","text":"\\(x\\) Sharp \\(\\begin{cases} 1, &amp; z \\ge z_0\\\\ 0, &amp; \\text{o.w}\\end{cases}\\) Fuzzy \\(\\begin{cases} p(z), &amp; z \\ge z_0\\\\ 0, &amp; \\text{o.w}\\end{cases}\\)"},{"location":"Misc/DoE/03_Nuisance/","title":"Nuisance Removal","text":""},{"location":"Misc/DoE/03_Nuisance/#blocking-vs-randomization","title":"Blocking vs Randomization","text":"<p>Block what you can, randomize what you cannot</p> Blocking Covariate analysis Randomization Group experiments into blocks, each having a fixed value of the variable Put measured uncontrolled inputs into the modelIgnore them when modeling is complete Increase \\(\\text{cov}(T_1, T_2)\\) to decrease \\(\\sigma^2(T_1-T_2)\\) Model impact of variable, and then subtract it out Prevent unknown effect from biasing results Error that is same for \\(T_1\\) and \\(T_2\\) will cancel out Turn systematic errors into random errors which average out to zero Effective for Measured uncontrolled inputs Unmeasured uncontrolled inputs Application Understanding treatment effects Allows for time-series analysis to detect drift Example Randomly assign each person to- one new shoe- one old shoe(randomly assigned to left/right foot) Randomly assign- half of participants w/ new shoes- half of participants w/ old shoes \\[ \\begin{aligned} \\sigma^2(\\text{ATE}) &amp;= \\sigma^2(T_1-T_2) \\\\ &amp;= \\sigma^2_{T_1} + \\sigma^2_{T_2} - 2 \\text{ cov}(T_1, T_2) \\end{aligned} \\] <p>For measured uncontrolled inputs, use blocking or covariate analysis to remove effect of nuisance factors, and reduce known variability</p> <ul> <li>Different measurement tools, prices batches</li> <li>Spatial/temporal variations</li> </ul>"},{"location":"Misc/DoE/03_Nuisance/#random-treatment-assignment","title":"Random Treatment Assignment","text":"<p>Used for removing sources of variation due to nuisance factors</p>"},{"location":"Misc/DoE/03_Nuisance/#blockingrandomized-complete-block-design-rcbd","title":"Blocking/Randomized Complete Block Design (RCBD)","text":"<ul> <li>Complete experiment is performed for each block</li> <li>Each block sees each treatment exactly onec</li> <li>With each block, testing order is randomized</li> <li>Examples of blocks</li> <li>Raw material batches</li> <li>People (operators)</li> <li>Process/measurement tools</li> <li>Time</li> </ul> \\(\\alpha_1\\) \\(\\alpha_2\\) \\(\\alpha_3\\) \\(\\alpha_4\\) \\(T_2\\) \\(T_1\\) \\(T_1\\) \\(T_3\\) \\(T_1\\) \\(T_3\\) \\(T_2\\) \\(T_2\\) \\(T_3\\) \\(T_2\\) \\(T_3\\) \\(T_1\\) \\[ T \\in \\{ T_1, T_2, T_3 \\} \\\\ s \\in \\{ \\alpha_1, \\alpha_2, \\alpha_3, \\alpha_4 \\} \\] <p>Note: The term \u2018blocking\u2019 originated  from agriculture, where a block is typically a set of homogeneous (contiguous) plots of land with similar fertility, moisture, and weather, which are typical nuisance factors in agricultural studies $$ y_{ij} = f(T_i) + \\text{Block}j + u $$</p>"},{"location":"Misc/DoE/03_Nuisance/#balanced-incomplete-block-design","title":"Balanced Incomplete Block Design","text":""},{"location":"Misc/DoE/03_Nuisance/#latin-square-design-lsd","title":"Latin Square Design (LSD)","text":"\\(\\alpha_1\\) \\(\\alpha_2\\) \\(\\alpha_3\\) \\(\\beta_1\\) \\(T_1\\) \\(T_2\\) \\(T_3\\) \\(\\beta_2\\) \\(T_2\\) \\(T_3\\) \\(T_1\\) \\(\\beta_3\\) \\(T_3\\) \\(T_1\\) \\(T_2\\) \\[ \\begin{aligned} T &amp;\\in \\{ T_1, T_2, T_3 \\} \\\\ s_1 &amp;\\in \\{ \\alpha_1, \\alpha_2, \\alpha_3 \\} \\\\ s_2 &amp;\\in \\{ \\beta_1, \\beta_2, \\beta_3 \\} \\end{aligned} \\] <p>A Latin square of order \\(n\\) is an \\(n \\times n\\) array of cells in which \\(n\\) treatments are placed, one per cell, such that each treatment only occurs</p> <ul> <li>once in each row</li> <li>once in each column</li> </ul> <p>Requirements</p> <ul> <li>Number of levels of each blocking var must equal number of levels of primary var</li> <li>No interactions between input vars, especially b/w nuisance vars</li> </ul> <p>Outcome</p> <ul> <li>Not complete block</li> <li>Model is orthogonal, if no interactions</li> </ul> \\[ y_{ijk} = f(T_i) + R_j + C_k + u_{ijk} \\] <p>Example: Test effect of amount of gasoline additive on emissions</p> <p></p>"},{"location":"Misc/DoE/04_Factorial/","title":"Factorial Design","text":""},{"location":"Misc/DoE/04_Factorial/#circular","title":"Circular","text":"<p>Experimental design that sets</p> <ul> <li>which predictor vars to vary</li> <li>Over what range</li> <li>Sampling plan: With what distribution of values</li> </ul> <p>Given nature of model, we can easily decide how to sample</p> Design Limitation No of measurements One at a time Cannot help investigate interactions Full factorial design \\(r \\prod \\limits_{i=1}^F l_i\\)"},{"location":"Misc/DoE/04_Factorial/#full-factorial-design","title":"Full Factorial Design","text":"<ol> <li>Use \\(l_i\\) levels for factor \\(F_i\\)</li> <li>It is common to normalize each factor to \\([-1, 1]\\): coded vars</li> <li>Perform \\(r\\) complete replicates of experiment</li> <li>Replicates are required to estimate error</li> </ol> <p>Adding center point</p> <ul> <li>Center point is often the POR (plan of record) and significant data may already exist about its response</li> <li>For LR</li> <li>Center points do not affect orthogonality of design</li> <li>Center points do not change any model parameter except the intercept</li> <li>Repeated center point can be used to check linear model validity: is non-linear term required?</li> </ul>"},{"location":"Misc/DoE/04_Factorial/#2-level-factorial-design","title":"2-level factorial design","text":"<p>For each factor, run every combination at 2 levels: high and low labelled as \\(-1, +1\\)</p> <p>For \\(F\\) factors there will be \\(r \\times 2^F\\) experimental runs for full factorial design</p> <p>With this, we can detect</p> <ul> <li>linear variations only</li> <li>interactions</li> </ul> <p>We cannot detect</p> <ul> <li>Non-linear variations</li> </ul> <p>This design is completely orthogonal</p>"},{"location":"Misc/DoE/04_Factorial/#fractional-factorial-design","title":"Fractional Factorial Design","text":"<p>Many higher order interactions may be negligible (sparsity-of-effects) principle, and hence redundant</p> <ul> <li>We can reduce number of runs by eliminating higher-order model interactions, especially the ones that are not relevant to us</li> </ul> <p>Choosing subset of full factorial design</p> <ul> <li>Balanced: all combinations have same number of obs</li> <li>Orthogonal design: effects of any factor sum to zero across effects of other factors</li> </ul>"},{"location":"Misc/DoE/04_Factorial/#half-factorial-design","title":"Half-Factorial design","text":"<p>Limitation</p> <ul> <li>Aliasing: some terms may get confounded by 2-factor interactions</li> <li>Not all terms can be distinguished in 8 runs</li> </ul> <p></p> <p>\\((x_1 x_2 = x_3 x_4), (x_1 x_3 = x_2 x_4) \\implies\\) collinearity</p>"},{"location":"Misc/DoE/04_Factorial/#projections","title":"Projections","text":"<p>If one of the factors proves to have no effect on the response, the \\(F\\) factor half-factorial design collapses to a \\(k-1\\) factor full-factorial design</p> <p></p>"},{"location":"Misc/DoE/04_Factorial/#ccd","title":"CCD","text":"<p>Central Composite Design</p> <ol> <li>Take 2-level factorial design</li> <li>Add center point with repeats: middle point b/w all factors</li> <li>Add axial (star) points: center point except w/ one var changed to be at \u00b1 an extreme value. Do this for all vars</li> </ol> <p>\\(n\\) level CCD more efficient than \\((n+1)\\) level factorial design $$ n = r(2^F + 2F + 1) $$</p>"},{"location":"Misc/DoE/04_Factorial/#types","title":"Types","text":"Type Rotatable Circumscribed Every factor data point on radiusequidistant from center: \\(2^{F/4}\\) Face-Centered Every factor data point on the line segments connecting all the initial factors \u274c"},{"location":"Misc/DoE/04_Factorial/#examples","title":"Examples","text":"Level Type 2 Circumscribed 2 Face-Centered 3"},{"location":"Misc/DoE/04_Factorial/#box-behnken-design","title":"Box-Behnken Design","text":"<ol> <li>Put a data point in the center</li> <li>Put a data point at midpoint each edge of process space</li> </ol>"},{"location":"Misc/DoE/04_Factorial/#disadvantages","title":"Disadvantages","text":"<ul> <li>Does not contain embedded factorial design: hence, cannot do pre-survey and add more points</li> <li>No corner (extreme points)</li> </ul>"},{"location":"Misc/DoE/04_Factorial/#repeated-center-points","title":"Repeated Center Points","text":"<ul> <li>Repeated center points are not randomized</li> <li>They are run as the first and last data points</li> <li> <p>Every spread through rest of data collection</p> </li> <li> <p>Help check against process instability</p> </li> <li>All other points should have randomized order</li> </ul> <p>The number of repeated center points can be set to create \u201cuniform precision\u201d, as \\(\\sigma^2_{y \\text{ center}} = \\sigma^2_{y \\text{ corner}}\\)</p>"},{"location":"Misc/DoE/04_Factorial/#sequential-doe","title":"Sequential DOE","text":"<p>Steepest Ascent/Descent</p> <p></p> <ol> <li>Start with factorial design (linear model) about current process (POR: Plan of Record)</li> <li>In scaled coordinates, \\((0, 0, \\dots, 0)\\)\u00a0represents center point</li> <li>Move in direction of steepest ascent/descent</li> <li>Find factor \\(j\\) with max \\(\\vert \\beta_j \\vert\\)</li> <li>Move a distance of the \\(j\\)th factor: \\(\\Delta x_j \\approx 1\\)\u00a0(higher/lower based on judgment)</li> <li>For every other factor, move a distance of \\(\\Delta x_{j'} = \\dfrac{\\Delta x_j \\beta_{j'}}{\\beta_j}\\)</li> <li>Measure response at this new point</li> <li>Keep moving until response goes down</li> </ol>"},{"location":"Misc/DoE/04_Factorial/#idk","title":"IDK","text":"<ol> <li>Start with 2-level full factorial design with repeated center points</li> <li>Extend to central composite design if quadratic model needed</li> </ol> <p>Results in 2 blocks: control number center repeated to ensure uniformity and rotatability</p> No of Factors Factorial Center Repeats Added star center repeats 2 \\(r\\) \\(r\\) 3 \\(1.4 r + 0.5\\) \\(r\\) 4 \\(2r\\) \\(r\\) 6 \\(4 (r-4)\\) \\(r\\)"},{"location":"Misc/DoE/04_Factorial/#mixtures","title":"Mixtures","text":"<p>Factors with constraints</p> <p>Consider</p> <ul> <li>\\(x_j \\in [0, 1] \\quad \\forall j \\in F\\)</li> <li>\\(\\sum_{j}^F x_j = 1\\)</li> </ul>"},{"location":"Misc/DoE/04_Factorial/#simplex-design","title":"Simplex Design","text":"<p>Applicable for Mixtures</p> <p>Each factor taking \\((m+1)\\) evenly space values $$ x_j = { v/m } \\ v \\in [0, m] \\ \\forall j \\in F $$ </p>"},{"location":"Misc/DoE/04_Factorial/#taguchi-methods","title":"Taguchi Methods","text":"<p>Statistical methods for improving manufacturing quality</p> <ol> <li>Optimization involves use of loss function</li> <li>Quality begins with designing a process with inherently high quality</li> <li>Use DOE</li> </ol>"},{"location":"Misc/DoE/04_Factorial/#loss-function","title":"Loss Function","text":"Goal Loss Function Eg More the better Monotonic Production output Less the better Monotonic Pollution emissions Hitting target with min variation Quadratic"},{"location":"Misc/DoE/04_Randomization/","title":"Randomization","text":""},{"location":"Misc/DoE/05_IDK/","title":"Experimental Design","text":"<p>Important goal: equalize leverage of every point during multiple regression $$ h_{ii} = \\dfrac{k}{n}, \\forall i $$</p>"},{"location":"Misc/DoE/05_IDK/#known-functional-form","title":"Known Functional Form","text":"<p>Non-dimensionalization</p>"},{"location":"Misc/DoE/05_IDK/#actions","title":"Actions","text":"<ul> <li>Simplify differential equations</li> <li>Rescale variables to unitless form</li> <li>Get rid of unnecessary parameters</li> <li>Reduce number of experiments needed to test hypothesis</li> </ul>"},{"location":"Misc/DoE/05_IDK/#rules","title":"Rules","text":"<ol> <li>Identify differential equation</li> <li>Identify independent &amp; dependent vars</li> <li>Replace each of them with a quantity scaled relative to characteristic unit of measure to be determined</li> <li>Divided through by the coefficient of the highest order polynomial/derivative term</li> <li>Scale boundary conditions</li> <li>Choose the definition of the characteristic unit for each var so that the coefficients of as many terms as possible become 1</li> <li>Rewrite system of equations in terms of new dimensionless quantities</li> </ol>"},{"location":"Misc/DoE/05_IDK/#example","title":"Example","text":"\\[ \\begin{aligned} c_t = c_0 e^{-kt} &amp;\\implies (c_t/c_0) = e^{-kt} \\\\ 3 \\text{ var} &amp; \\implies 2 \\text{ var} \\end{aligned} \\]"},{"location":"Misc/DoE/05_IDK/#unknown-functional-form","title":"Unknown Functional Form","text":"<p>Buckingham Pi Theorem</p>"},{"location":"Misc/DoE/09_idk/","title":"IDK","text":""},{"location":"Misc/DoE/09_idk/#pre-surveyexploratory-designs","title":"Pre-Survey/Exploratory Designs","text":"<p>Sequential approach: simple screening pre-survey experiment first (such as 2-level factorial design), followed by through investigation</p> <p>Use \\(\\le 25\\%\\) of total available data for collection into screening</p> <p>We want to know: what vars are the most important and over what ranges</p> <p>Do low-cost pre-survey DOE that focuses on high-priority</p> <p>Based on this outcome, perform full experiment</p>"},{"location":"Misc/DoE/09_idk/#experiment-pointers","title":"Experiment Pointers","text":"<ul> <li> <p>Always perform experiment with both trial &amp; control samples</p> </li> <li> <p>Always get the raw data; processing should be done by analyst, not data providers</p> </li> <li> <p>Every data point should have central tendency &amp; uncertainty associated</p> </li> <li>Incorporate all potential uncertainty associated with collecting the data &amp; use Uncertainty Propagation</li> <li>Observation: Use robust summary statistics: Median</li> <li>Spread/Uncertainty of estimate<ul> <li>Standard error, not standard deviation</li> <li>Use non-robust summary statistics</li> </ul> </li> <li>Every data point fed to model should be iid observation</li> </ul> <p></p>"},{"location":"Misc/DoE/09_idk/#data-template","title":"Data Template","text":""},{"location":"Misc/DoE/09_idk/#for-collection","title":"For Collection","text":"Type Category_ID Subcategory_ID Reading_ID Value Control Product A Sample 1 1 x Control Product A Sample 1 2 x Control Product A Sample 1 3 x Control Product A Sample 2 1 x Control Product A Sample 2 2 x Control Product A Sample 2 3 x Control Product B Sample 1 1 x Control Product B Sample 1 2 x Control Product B Sample 1 3 x Control Product B Sample 2 1 x Control Product B Sample 2 2 x Control Product B Sample 2 3 x Trial \u2026 \u2026 \u2026 \u2026"},{"location":"Misc/DoE/09_idk/#for-modelling","title":"For Modelling","text":"<p>We cannot use the collection data directly for modelling as each row is not iid observation. Hence aggregation is required to obtain the central tendency &amp; uncertainty for each iid observation.</p> Type Category_ID Subcategory_ID Central Tendency(Median) Uncertainty(IQR) Control Product A Sample 1 x x Control Product A Sample 2 x x Control Product B Sample 1 x x Control Product B Sample 2 x x Trial \u2026 \u2026 \u2026 \u2026"},{"location":"Misc/Driving/","title":"Driving","text":""},{"location":"Misc/Driving/#theory","title":"Theory","text":""},{"location":"Misc/Driving/#practicals","title":"Practicals","text":"<ul> <li>Vehicle controls</li> <li>Driving in 2-way streets</li> <li>Driving in multi-way streets</li> <li>Special maneuvers</li> <li>Parking</li> <li>60deg parking</li> <li>Garage reverse parking</li> <li>Parallel reverse parking</li> <li>Road</li> <li>Moving off in an incline</li> <li>Emergency brake</li> </ul>"},{"location":"Misc/Driving/01_Introduction/","title":"Introduction","text":""},{"location":"Misc/Driving/01_Introduction/#skills-required","title":"Skills Required","text":"<ul> <li>Car control</li> <li>Visual scanning</li> <li>Identifying and weighing risk</li> <li>Thinking &amp; responding</li> <li>Making decisions</li> </ul>"},{"location":"Misc/Driving/01_Introduction/#road-user","title":"Road User","text":"<p>Anyone using the road</p> <ul> <li>People</li> <li>Pedestrians</li> <li>Children</li> <li>Cyclists/Motorcyclists</li> <li>People of determination</li> <li>Emergency vehicles</li> <li>Official vehicles</li> <li>Police</li> <li>Ambulance</li> <li>Fire Engine</li> <li> <p>Military vehicles</p> </li> <li> <p>Vehicles</p> </li> <li>Cars</li> <li>Trucks</li> <li>Trams</li> </ul>"},{"location":"Misc/Driving/01_Introduction/#attitude-in-driving","title":"Attitude in Driving","text":"<p>Difficult in UAE due to people of different nations, making it hard to predict their move</p> <p>Responsibility to be positive and ensure everyone\u2019s safety</p> <ul> <li>Most traffic violations like beating the red light is usually caused by bad attitude of the driver</li> <li>Flashing headlights to intimidate others increases risk of crash</li> <li>Proper use of indicator provides allows other drivers to react appropriately</li> <li>Driving too fast for the situation is an unacceptable driving behavior</li> </ul>"},{"location":"Misc/Driving/01_Introduction/#always-give-way","title":"Always give way","text":"<ul> <li> <p>Pedestrians</p> </li> <li> <p>Emergency vehicles</p> </li> <li> They will use Highways Fast lane Congested highways Emergency yellow area Internal roads Between vehicles Junction Between vehicles Junction with red light Move to the side without crossing the signal Roundabouts Do not enterIf you are already inside, keep moving and move to the right lane once exited </li> <li> <p>Tram</p> </li> </ul> <p>IDK</p> <ul> <li>Do not follow emergency vehicles</li> <li>Do not use emergency yellow area (road shoulders)</li> <li>People walking along or crossing the roads should be considered as among the many hazards on the road</li> <li>When dealing with pedestrian, drivers must give way, always</li> <li>When approaching a pedestrian crossing where pedestrians are present, drivers must stop and give way to pedestrians</li> <li>You are causing an obstruction to the tram and can attract fines if Stopping or parking near the tram track, and/or   Littering or fixing any hoarding at the tram track.</li> </ul>"},{"location":"Misc/Driving/01_Introduction/#watch-out","title":"Watch out","text":"<ul> <li>Children: Do unexpected things</li> <li>People of determination vehicles</li> <li>School busses: Slow down, stop and wait till the bus starts moving</li> <li>Busses and taxi lanes</li> <li>Motorcyclist and cyclist: When drivers making right turns should watch out for cyclists</li> <li>Animals: unpredictable; Camels</li> <li>Do not horn as they cause them to run</li> </ul> <p>Areas</p> <ul> <li>Residential areas</li> <li>Mosques</li> <li>Playgrounds</li> <li>Shops</li> <li>Shopping malls</li> <li>Parks</li> <li>School areas</li> </ul> <p>Guideline (SSS)</p> <ul> <li>Space: 1.5m</li> <li>Speed: Slow down</li> <li>Sound: Lightly sound horn</li> </ul>"},{"location":"Misc/Driving/01_Introduction/#weird-areas","title":"Weird Areas","text":"<p>Slip lane: free-right separated from main road by a traffic island</p> <p>\u2018Bicycle Shared Starting\u2019 Marking\u2019</p> <p></p> <p>\u2018Bicycle Shared Ending Marking\u2019</p> <p></p>"},{"location":"Misc/Driving/01_Introduction/#insurance","title":"Insurance","text":""},{"location":"Misc/Driving/01_Introduction/#parties","title":"Parties","text":"<ol> <li>Insurer</li> <li>Owner of vehicle and insurance policy</li> <li>Other vehicle involved in car accident</li> </ol>"},{"location":"Misc/Driving/01_Introduction/#policies","title":"Policies","text":"<ul> <li>Comprehensive/Full: All damage cost covered by insurance for both second and third party</li> <li>TPL (Third Party Liability): Will only cover damage cost for third party; second party will be responsible for bearing their own costs</li> </ul> <p>Conditions where insurance does not cover</p> <ul> <li>Driver under influence of alcohol/sedative medicine</li> <li>When driving different vehicle category without license</li> <li>When driving vehicle in a country where insurance does not cover</li> <li>When driver is driving without specs for whom it is mandatory as mentioned in license</li> </ul>"},{"location":"Misc/Driving/01_Introduction/#consequences-of-accidents","title":"Consequences of accidents","text":"<ul> <li>Fatalities</li> <li>Serious injuries</li> <li>Vehicle damage</li> <li>Property damage</li> <li>Vehicle reparation</li> <li>Problems from vehicle insurance</li> <li>Fines</li> </ul>"},{"location":"Misc/Driving/01_Introduction/#vehicle-safety-features","title":"Vehicle Safety Features","text":"<ul> <li>Seat belt</li> <li>Child restraint system add-on</li> <li>Baby seats<ul> <li>Rear-facing</li> <li>It provides more protection to baby head because neck is not fully developed and it is more vulnerable to the forces during brake or collision</li> <li>&lt;1 yr</li> <li>&lt;13 kg</li> <li>&lt;65 cm</li> <li>Integral harness</li> </ul> </li> <li>Child seat<ul> <li>Forward-facing</li> <li>1&gt;4 yrs</li> <li>9-18 kg</li> <li>65-95 cm</li> <li>Internal harness</li> </ul> </li> <li>Booster Seat<ul> <li>4-6 yrs</li> <li>15-25 kg</li> <li>95-125 cm</li> </ul> </li> <li>Booster cushion<ul> <li>6-11 yrs</li> <li>22-36 kg</li> <li>125-140 cm</li> </ul> </li> <li>Airbags: Designed to protect adults; children must be kept away</li> <li>SRS (Supplement Restraint System)</li> <li>Only effective when used with seat belts</li> <li>Distance between face and steering wheel should be &gt;25 cm</li> <li>Head restraint: Prevents whiplash injury</li> <li>Distance between head and restraint should be &lt;10cm</li> <li>Crumble zone</li> </ul>"},{"location":"Misc/Driving/01_Introduction/#driver-assistance-systems","title":"Driver Assistance Systems","text":"<ul> <li>Conventional Cruise Control</li> <li>Adaptive Cruise Control</li> <li>Intelligent Speed Assistance</li> <li>Lane Support Systems: &gt; 50 km/h</li> <li>Lane Departure Warning</li> <li>Lane Keep Assist</li> <li>Lane Centering Assist</li> <li>Blind Spot Monitoring</li> <li>Forward Collision Mitigation</li> <li>Forward COlloysion Warning</li> <li>Autonomous Emergency Braking</li> <li>Parking Assistance</li> <li>Anti-Lock Braking System</li> <li>Electronic Stability Programme</li> </ul>"},{"location":"Misc/Energy/","title":"Urban Energy Systems and Policy","text":"<p>This class is about figuring out together what cities and users can do to reduce their energy use and carbon emissions. Many other classes at MIT focus on policies, technologies, and systems, often at the national or international level, but this course focuses on the scale of cities and users. It is designed for any students interested in learning how to intervene in the energy use of cities using policy, technology, economics, and urban planning.</p> <p>This course examines the choices and constraints regarding sources and uses of energy by households, firms, and governments through a number of frameworks to describe and explain behavior at various levels of aggregation. Examples include a wide range of countries, scope, settings, and analytical approaches.</p> <p>Main goals</p> <ul> <li>Developing basic numerical literacy (GHG pc, over time)</li> <li>Understanding broad issues through specific dives</li> <li>Building a moral case for climate action (US focus)</li> </ul>"},{"location":"Misc/Energy/#dtu","title":"DTU","text":"<ul> <li>Overview of historical developments</li> <li>Participants of market</li> <li>Challenges with renewable energy</li> </ul>"},{"location":"Misc/Energy/#recommend-readings","title":"Recommend Readings","text":"<ul> <li> Sustainable Energy without the Hot Air | David JC MacKay</li> <li>Only focus on physical &amp; technological issues</li> <li>Not on economics/politics</li> </ul>"},{"location":"Misc/Energy/#references","title":"References","text":"<ul> <li> DTU course | 31761 - Renewables in electricity markets</li> <li> Energy and the Environment</li> <li> MIT 11.165 Urban Energy Systems and Policy, Fall 2022</li> <li> The Energy Academy | Modo Energy</li> <li> MIT 15.031J Energy Decisions, Markets, Policies, Spring 2012</li> </ul>"},{"location":"Misc/Energy/01_Introduction/","title":"Introduction","text":""},{"location":"Misc/Energy/01_Introduction/#why-is-energy-special","title":"Why is energy special?","text":"<ul> <li>There should be equilibrium between power generation and consumption</li> <li>Transportation and distribution performed on power network, with specific physical rules</li> <li>Storage is uneconomical</li> <li>Large part of energy demand is of critical nature (hospitals, residencies)</li> <li>Consumers should not differentiate origin, quality and nature of production</li> </ul>"},{"location":"Misc/Energy/01_Introduction/#deregulation","title":"Deregulation","text":"Regulated De-regulated Prices determined by Regularity body \u201cInvisible hand\u201d of market Structure Vertical integration Horizontal restructuring Supplier Fixed Multiple- Competition <p>Chile &gt; UK &gt; Scandinavia &gt; California (crisis, shortage, Enron)</p>"},{"location":"Misc/Energy/01_Introduction/#participants","title":"Participants","text":"Grid Operators TSO (Transmission System Operator) Operates transmissions assetsResponsible for power balance on transmission system DisCo (Distribution Company)DSO (Distribution System Operator) Operates distribution gridOften acts as retailer (not preferable) Producers GenCo (Generating Company)/IPP (Independent Power Producer) Owns production assetsGeneration is offered through energy market Intermediary Retailer Buys energy from wholesale electricity marketSales to end-consumers Consumers Household (small)/Industrial (large) Use energy for various purposesLarge consumers may be allowed to directly participate in whole electricity market Regulator Market designRulesMonitoringCurb misbehavior (collusion, power abuse) Operator Organizes and operates energy marketDefinition of bid products &amp; formsSet up &amp; maintenance of trading platformDaily matching of supply and demand offers"},{"location":"Misc/Energy/01_Introduction/#models","title":"Models","text":"Monopoly Purchasing Agent Wholesale Market Retail Market Consumer-Centric Peer-Peer ModelMicro-Grids Supplier-Centric Consumer-Centric Characteristic Hierarchical Decentralized \u201cProsumers\u201d"},{"location":"Misc/Energy/01_Introduction/#markets","title":"Markets","text":""},{"location":"Misc/Energy/01_Introduction/#types","title":"Types","text":"Capacity For system operator to ensure that sufficient generation capacity is present for reliable system operation in future year and at competitive prices Energy Central place for optimal scheduling and settlement of energy exchanges Ancillary Service Any type of service that supports power system operations, directly bought by system operator- Primary reserves- Secondary reserves- Tertiary receivers (manual)- Black-start capability, short-circuit power, reactive reserves, voltage control"},{"location":"Misc/Energy/01_Introduction/#financial","title":"Financial","text":"Market Meaning Futures Financial contracts with time horizons unto 6 yearsUsed for price hedging and risk management Day-ahead/Spot Central instrument for everyday matching of electricity supply and demand Balancing Close to real-time operator for system operator to ensure power system balance Intra-day Continuous trading platform between day-ahead and balancingAllows to correct original schedules (when plant outages/changes in wind power generation)"},{"location":"Misc/Energy/01_Introduction/#challenges","title":"Challenges","text":"Variable energy demand\u201cDuck Curve\u201d Renewable energy generation is variable and non-dispatchable Renewable energy generation is hard to forecast"},{"location":"Misc/Energy/01_Introduction/#economic-impact-of-renewable-energy","title":"Economic impact of renewable energy","text":"<p>Wind and solar energy induces a downward pressure on market prices</p>"},{"location":"Misc/Energy/01_Introduction/#quest-for-flexibility","title":"Quest for Flexibility","text":"<p>Flexibility is seen as ability to adapt to variable and unforeseen changes in operating conditions</p> <ul> <li>Generation units</li> <li>Power system</li> <li>Demand side</li> <li>Integrated energy systems view (heat and gas energy systems)</li> </ul>"},{"location":"Misc/Energy/01_Introduction/#idk","title":"IDK","text":"Traditional Renewable Producers Fossil fuels WindSolar Fixed Production \u2705 \u274c Fixed Demand \u274c \u274c Cost distribution Demand DemandSupply <p>Probabilistic matching rather than regular price matching</p>"},{"location":"Misc/Energy/02_Spot_Markets/","title":"Spot Markets","text":""},{"location":"Misc/Energy/02_Spot_Markets/#bilateral-contracts","title":"Bilateral Contracts","text":"<pre><code>flowchart LR\nb[Buyer]\nbr[Broker]\ns[Seller]\n\nb &lt;--&gt; br &lt;--&gt; s\nb &lt;--&gt; s</code></pre> <p>Direct exchange of energy between buyer and seller in a decentralized fashion</p> <p>System operator is informed about trades that occur</p>"},{"location":"Misc/Energy/02_Spot_Markets/#types","title":"Types","text":"Customized long-term contracts OTC (Over the counter) Electronic Trading Consistently matches supply and offer bids Flexible \u2705 \u274c Private transactions \u2705 \u274c Transaction costs High(due to Broker) Low \\(\\approx 0\\) Speed Fast(allows for trading \u201cuntil last second\u201d) Trade volume Large Small Duration Long Small"},{"location":"Misc/Energy/02_Spot_Markets/#auctions-in-energy-pool","title":"Auctions in Energy Pool","text":"<ul> <li>All generation bids and consumption offers are placed at same time</li> <li>No one knows about others\u2019 bids and offers</li> <li>Centralized market clearing decides bids and offers that are retained</li> <li>Eventually, the system operator is informed about the trades that occurred</li> </ul>"},{"location":"Misc/Energy/02_Spot_Markets/#merit-order","title":"Merit Order","text":"<ul> <li>Consumption orders are ranked in dec price order</li> <li>Supply bids are ranked in inc price order</li> </ul>"},{"location":"Misc/Energy/02_Spot_Markets/#social-welfare","title":"Social Welfare","text":"<p>Area between consumption and generation</p> <p>Equilibrium point is that which allows to maximize social welfare</p> <ul> <li>Any buyer is to pay almost what they were ready to pay</li> <li>Any seller will get at least what they were ready to sell for</li> </ul> <p></p>"},{"location":"Misc/Energy/02_Spot_Markets/#market-clearing","title":"Market Clearing","text":"<p>Goal</p> <ul> <li>Schedule for all supply and demand offers</li> <li>Price at which market is cleared</li> </ul> <p>Inputs</p> <ul> <li>All offers in the market are formulated in terms of quantity \\(Q\\) and price \\(P\\)</li> <li>Supply side</li> <li>Set of offers</li> <li>Maximum quantity for offer</li> <li>Price for offer</li> <li>Demand side</li> <li>Set of offers</li> <li>Maximum quantity for offer</li> <li>Price for offer</li> </ul> <p>Decision variables</p> <ul> <li>Generation schedule</li> <li>Consumption schedule</li> </ul> <p>Objective: Maximize social welfare</p> <p>Constraints</p> <ul> <li>Non-negativity of supply and demand</li> <li>Balance of generation and consumption</li> <li>Generation and consumption within limits</li> </ul>"},{"location":"Misc/Energy/02_Spot_Markets/#settlement-process","title":"Settlement Process","text":"<ul> <li>Who should pay what?</li> <li>Who should get paid what amount?</li> </ul>"},{"location":"Misc/Energy/02_Spot_Markets/#approaches","title":"Approaches","text":"Pay-as-bid Uniform Every party pays/receives whatever they bid the same equilibrium amount Advantages Overcomes limitations of pay-as-bidYields budget balance: sum of revenues equal to sum of payments Disadvantages Supplier may receive 0 revenue, which won\u2019t cover their fixed costsConsumers incentivized to lower bids; suppliers incentivized to increase bids <p>Both approaches guarantee</p> <ul> <li>individual rationality: consumers will pay at most what they were ready to pay, and producers will receive at least what they were ready to receive</li> <li>Revenue adequacy: Sum of revenues \\(\\ge\\) Sum of payments</li> </ul>"},{"location":"Misc/Energy/02_Spot_Markets/#geographic-prices","title":"Geographic Prices","text":"<p>Prices vary across various locations, as power has to flow through network involved</p> <p>Exchanges capacity limitations</p> <ul> <li>There is a maximum amount of energy that can be exchanged from one location to another</li> <li>When this limit is reached, there is congestion and prices for connected areas will different</li> <li>Exchange capacity limitations are directly related to network constraints and operational practice</li> </ul>"},{"location":"Misc/Energy/02_Spot_Markets/#approaches-to-handle-exchange-capacity-limitations","title":"Approaches to handle exchange capacity limitations","text":"Zonal Nodal System Operator TSO ISO Market Operator Ind. Market Operator ISO Offers Market Products Unit Capabilities Clearing Supply-demand equilibrium UCED Problem Network representation Simplified Detailed Prices Zonal Nodal Used in Europe US <p>Market is not-budget balanced anymore, as the sum of consumer payments &gt; sum of supplier revenues; difference defines congestion rent to be collected by system operator(s)</p>"},{"location":"Misc/Energy/02_Spot_Markets/#approach-1-split","title":"Approach 1: Split","text":"<p>Due to transmission constraints, the market has to be split and be treated as individual sub-markets</p> <ul> <li>Submarkets have their own supply-demand equilibrium</li> <li>Transmission-related offers: Extra (price-independent) consumption/generation offers representing the transmission from one zone to the next to be added</li> </ul> <p></p>"},{"location":"Misc/Energy/02_Spot_Markets/#approach-2-flow-based-coupling","title":"Approach 2: Flow-based coupling","text":"<p>Instead of boldly splitting market, one could instead acknowledge how power flows</p> <p>This allows clearing a single market with geographically-differentiated prices</p> <p></p>"},{"location":"Misc/Energy/02_Spot_Markets/#regulation-support-schemes","title":"Regulation &amp; Support Schemes","text":"<p>Grid parity = scenario when it is profitable to produce energy, ie Levelized Cost of Energy (cost of energy production) &lt; market price</p> <p>New energy generation tech may need support in order to reach grid parity</p> <ul> <li>Regulation is an instrument for policy makers to support their integration in the market</li> <li>Support schemes consist in financial support to make them competitive in the market</li> </ul> <p>These have impact on participant revenues, offering strategies, market outcomes</p> <p>Types</p> FIT FIP CFT Meaning Feed-in-tariff Fixed Feed-in-Premium Contract for difference/ Sliding Premium Implication Guaranteed price Fixed support regardless of market revenue Compensation of difference between guaranteed price and market revenue Blue: Support revenueGreen: Market Revenue Implication for producer Just ensure you get scheduledBid as low as possible <p>Safe policy to guarantee non-negative equilibrium prices: FIP or CfD at 0</p>"},{"location":"Misc/Energy/03_Intra_Day_and_Balancing_Markets/","title":"Intra-Day &amp; Balancing Markets","text":"<p>Convergence towards real-time operations relies on</p> <ul> <li>BRP</li> <li>Adjustment Market: Intra-day market mechanism</li> <li>Balancing Market: (near) realtime market</li> </ul> <p>If a deviation from the original schedule occurs (for producer/consumer)</p> <ul> <li>Re-dispatch of own units: Compensate with other generation/consumption means within their own portfolio</li> <li>Intra-day/adjustment market: Find ways to adjust through agreements with other players between day-ahed market clearing and actual operation</li> <li>Balancing market: Let system operator put system back to balance</li> </ul>"},{"location":"Misc/Energy/03_Intra_Day_and_Balancing_Markets/#day-ahead-market","title":"Day-Ahead Market","text":"<p>Day-Ahead market is a financial market with</p> <ul> <li>Pool based on auction mechanism</li> <li>Only transactions</li> <li>No generation/consumption</li> <li>Market participants and system operator are informed about market clearing outcomes (price &amp; volumes for each market time unit)</li> <li>In European set-up, the market participants will then self-dispatch, ie determine themselves how they will generate/consume depend on volumes &amp; prices</li> <li>Fairly long time before actual operation [12, 36] hours</li> </ul>"},{"location":"Misc/Energy/03_Intra_Day_and_Balancing_Markets/#brp","title":"BRP","text":"<p>Balance Responsible Parties</p> <p>Companies that can and may handle the balance responsibility for production and consumption units and/or trades actual electricity.</p>"},{"location":"Misc/Energy/03_Intra_Day_and_Balancing_Markets/#intra-day-market","title":"Intra-Day Market","text":"<p>Intra-Day Market is based on bilateral contracts, even though handled through central platform</p> <p>Purpose: Need for corrective actions may highly vary  depending upon how new information disclosure occurs between day-ahead market clearing and actual operation</p> <p>Features</p> <ul> <li>Fewer players</li> <li>Lesser liquidity</li> <li>Low trade volume</li> </ul> <p>Organization: leaning towards electronic reading</p> <p>Flexibility summarizes the impact of operational constraints (ie, minimum up and downtime, ramping, minimum operating point, etc)</p> <p>You need not only buy alone or sell alone; you can do both to exploit any market inefficiencies</p> <p>It may be difficult to foresee the actual imbalance that would need to be fixed eventually</p> <p>Decision-making in such adjustment markets can be complex &amp; stressful</p>"},{"location":"Misc/Energy/03_Intra_Day_and_Balancing_Markets/#balancing-market","title":"Balancing Market","text":"<ul> <li>Regulation market: Participants that offer to buy/sell, prior to hour of operations</li> <li>Balancing market: Participants cover the cost of their contribution to place the system off-balance</li> </ul>"},{"location":"Misc/Energy/03_Intra_Day_and_Balancing_Markets/#cases","title":"Cases","text":"<ul> <li>Positive: Supply &gt; Demand: downward regulation required</li> <li>Negative: Supply &lt; Demand: upward regulation required</li> <li>Balanced: Supply \\(\\approx\\) Demand: no regulation required</li> </ul>"},{"location":"Misc/Energy/03_Intra_Day_and_Balancing_Markets/#payment-settlement","title":"Payment Settlement","text":"<ul> <li>One-price: Total payment/revenue of day-ahead market participants for deviations from schedule equals revenue/payment of balancing generators</li> <li>If one\u2019s own deviation leads to off-balancing system, it leads to a loss</li> <li>If one\u2019s own deviation helps in balancing system, it leads to a profit</li> <li>Two-price</li> <li>Those off-balancing system penalized</li> <li>Those supporting system (unintentionally) will not get extra rewards</li> </ul>"},{"location":"Misc/Energy/03_Intra_Day_and_Balancing_Markets/#causes-of-imbalance","title":"Causes of Imbalance","text":"<ul> <li>Electric load is greater/less than forecasted at the time of market-clearing</li> <li>Renewable energy generation is greater/less than forecasted at the time of market-clearing</li> <li>Outages/operational difficulties of production units</li> <li>Outages/operational difficulties of transmission equipments</li> <li>Internal congestion (within market/balancing zone)</li> </ul>"},{"location":"Misc/Energy/04_Ancillary_Services/","title":"Ancillary Services","text":"<p>Services required by transmission/distribution system operator to enable them to maintain the integrity and stability of transmission/distribution system as well as power quality</p>"},{"location":"Misc/Energy/04_Ancillary_Services/#types","title":"Types","text":""},{"location":"Misc/Energy/04_Ancillary_Services/#reserve-types","title":"Reserve Types","text":"Reserve Primary Shared among all system operatorsDaily day-ahead auctionsInflexible demandNeeds upward &amp; downward capacityEnergy not considered (energy-neutral service ) Secondary Relieve primary reserve which has ben activatedRestore any imbalance on interconnectionsCapacity purchases on monthly basisCombines symmetric upward &amp; downward productsBased on bilateral contracts (negotiated; non-public)Negative revenue in downward regulation case consists in buying back energy that is already sold through day-ahead market Tertiary Daily day-ahead auctionsVarying demandNeed upward &amp; downward capacityEnergy paid for at balancing price"},{"location":"Misc/Energy/04_Ancillary_Services/#activation-approaches","title":"Activation Approaches","text":"Reactive Corrective Proactive Preventive"},{"location":"Misc/Energy/04_Ancillary_Services/#quantify-need-for-ancillary-services","title":"Quantify need for ancillary services","text":"<p>Depends on total uncertainty which is based on</p> <ul> <li>Supply-side uncertainty</li> <li>Demand-side uncertainty</li> </ul>"},{"location":"Misc/Energy/05_Impact_of_Renewables/","title":"Impact of Renewables","text":""},{"location":"Misc/Energy/05_Impact_of_Renewables/#electricity-cost-structure","title":"Electricity Cost Structure","text":"<ul> <li>Electricity Generation</li> <li>Electricity Transport/Transmission</li> <li>Taxes</li> </ul>"},{"location":"Misc/Energy/05_Impact_of_Renewables/#idk","title":"IDK","text":"<p>The forecasts of renewable energy generation is what drives the prices, not the actual generation</p> <p>Wind acts as a stochastic driver since having the lowest short-run marginal cost, with quantiles based on forecasts (13-37 hrs ahead)</p> <p>Higher (forecasted) production of renewable energy \\(\\implies\\)\u00a0lower energy price</p>"},{"location":"Misc/Energy/06_Participation_of_Renewables/","title":"Participation of Renewables","text":""},{"location":"Misc/Energy/06_Participation_of_Renewables/#market-participants","title":"Market Participants","text":"<ul> <li>Price taker: Decisions and resulting offers (buying/selling) does not affect market outcomes</li> <li>Price maker: Decisions and resulting offers (buying/selling) affects market outcomes</li> </ul>"},{"location":"Misc/Energy/06_Participation_of_Renewables/#market-participation-strategy","title":"Market Participation Strategy","text":"<ol> <li>Increase your offer</li> </ol> \\(E_i\\) Limitations Trust the forecastDirectly take forecasts and make offers \\(\\hat E_i\\) Susceptible to balancing costs from under-producing Increase your offer \\(\\tau \\hat E_i\\)\\(\\tau &gt; 1\\) Be bold and just max out generation Unrealistic (Requires knowledge of future balancing prices)"},{"location":"Misc/Energy/06_Participation_of_Renewables/#revenue-analysis","title":"Revenue Analysis","text":"<p>Performance Ratio $$ \\begin{aligned} \\gamma &amp;= (R_\\text{DA} + R_B)/R^*_\\text{DA} \\ \\gamma &amp; \\in (0, 1) \\end{aligned} $$ where</p> <ul> <li>\\(R_\\text{DA} =\\) revenue from day-ahead</li> <li>\\(R_\\text{DA} =\\) revenue from balancing market</li> <li>\\(R_\\text{DA} =\\) Optimal revenue</li> </ul>"},{"location":"Misc/Energy/06_Participation_of_Renewables/#news-vendor-problem","title":"News vendor problem","text":"<p>How much should news vendor buy from printing store to maximize expected revenue</p> <p>Bank cashflow problem: how much a bank should keep in reserves to satisfy request for withdrawal</p>"},{"location":"Misc/Energy/06_Participation_of_Renewables/#requirements","title":"Requirements","text":"<ul> <li>One shot opportunity to decide on quantity of interest</li> <li>Uncertain outcome</li> <li>Known marginal revenue, profit, loss</li> <li>Objective: maximize expected revenue</li> </ul>"},{"location":"Misc/Energy/06_Participation_of_Renewables/#solution","title":"Solution","text":"<p>Optimal number \\(n^*\\) such that $$ \\begin{aligned} \\alpha^* &amp;= \\dfrac{\\pi<sup>+}{\\pi</sup>+ + \\pi^-} \\ n^* &amp;= F<sup>{-1}(\\alpha</sup>*) \\end{aligned} $$ where</p> <ul> <li>\\(\\pi^+ = \\lambda^R - \\lambda^P =\\) unit cost of buying less than needed</li> <li>\\(\\pi^- = \\lambda^P - \\lambda^T =\\) unit cost of buying more than needed</li> <li>\\(\\alpha^* =\\) Normal level of original CDF F</li> </ul>"},{"location":"Misc/Energy/06_Participation_of_Renewables/#uncertainty","title":"Uncertainty","text":"\\[ n^* = \\hat F^{-1}(\\hat \\alpha^*) \\\\ \\implies n_t^* = \\hat F_t^{-1}(\\hat \\alpha_t^*) \\]"},{"location":"Misc/Energy/06_Participation_of_Renewables/#notes","title":"Notes","text":"<p>The optimal strategy can change over time</p>"},{"location":"Misc/Energy/07_Analytics/","title":"Renewable Energy Analytics","text":"<p>Forecasting helps make decisions</p>"},{"location":"Misc/Energy/07_Analytics/#what-to-forecast","title":"What to forecast","text":"<p>Different participants have different needs</p> <ul> <li>Electric load</li> <li>Day-ahead prices</li> <li>Potential imbalance sign</li> <li>Regulation prices/penalties</li> <li>Potential congestion on interconnectors</li> <li>Generation from renewable sources</li> </ul> <p>All these are driven by weather and climate</p>"},{"location":"Misc/Energy/07_Analytics/#use-cases","title":"Use cases","text":"<ul> <li>Definition of reserve requirements</li> <li>Unit commitment and economic dispath</li> <li>Coordination of renewables with storage</li> <li>Design of optimal trading strategies</li> <li>Electricity market clearing</li> <li>Optimal maintenance planning (especially for offshore wind farms)</li> </ul> <p>Inputs to these methods are</p> <ul> <li>deterministic forecasts</li> <li>probabilistic forecasts such as quantiles intervals and predictive distributions</li> <li>probabilistic forecasts in the form of trajectory or scenarios</li> <li>Risk indices</li> </ul>"},{"location":"Misc/Energy/07_Analytics/#features-for-forecasting","title":"Features for forecasting","text":"<ul> <li>Recent power generation measurements</li> <li>Weather forecasts for upcoming future</li> <li> <p>Other: Off-sit measurements, radar image, etc</p> </li> <li> <p>Short-term (&lt;6hrs): power generation measurements are more important</p> </li> <li>Medium-term (6-96hrs): weather forecasts are more important</li> <li>Long-term (&gt;96hrs): weather forecasts become less important, as long-term weather forecasts are not reliable</li> </ul> <p></p>"},{"location":"Misc/Energy/07_Analytics/#power-curve","title":"Power Curve","text":"<p>Power curve shapes the distribution of prediction errors</p> Ideal Actual"},{"location":"Misc/Energy/07_Analytics/#uncertainty","title":"Uncertainty","text":""},{"location":"Misc/Energy/07_Analytics/#causes-of-non-stationarity","title":"Causes of Non-Stationarity","text":"<ul> <li>Seasonality</li> <li>Equipment condition</li> <li>Wind Blades cleanliness</li> <li>Solar panel cleanliness</li> </ul>"},{"location":"Misc/Energy/99_Cities/","title":"Cities","text":""},{"location":"Misc/Energy/99_Cities/#why-cities","title":"Why Cities?","text":"<p>Intersection of Tech, Policies, Markets</p> <p>Urban population has the major concentration of populations, and it is continuously increasing</p> <p></p> <p></p>"},{"location":"Misc/Energy/99_Cities/#the-balance-sheet","title":"\u2018The Balance Sheet\u2019","text":"<p>Energy Consumption vs Sustainable Energy Production</p>"},{"location":"Misc/Energy/99_Cities/#key-sources-of-consumption","title":"Key Sources of Consumption","text":"<ul> <li>Transport</li> <li>Cars</li> <li>Planes</li> <li>Freight</li> <li>AC</li> <li>Heating</li> <li>Cooling</li> <li>Lighting</li> <li>Electronics</li> <li>Food</li> <li>Production</li> <li>Transporting</li> <li>Maintaining</li> <li>Manufacturing</li> </ul>"},{"location":"Misc/Energy/99_Cities/#key-sources-of-sustainable-production","title":"Key sources of sustainable production","text":"<ul> <li>Wind</li> <li>Solar</li> <li>photovoltaics</li> <li>thermal</li> <li>Biomass</li> <li>Hydroelectric</li> <li>Wave</li> <li>Tide</li> <li>Geothermal</li> <li>Nuclear (unclear whether nuclear counts as sustainable)</li> </ul>"},{"location":"Misc/Energy/99_Cities/#key-concepts","title":"Key Concepts","text":""},{"location":"Misc/Energy/99_Cities/#energy","title":"Energy","text":"<ul> <li>Quantitative property of doing work</li> <li>Conserved: can neither be created/destroyed</li> <li>Transformation: Light, heat, mass; \\(E=mc^2\\)</li> <li>Forms: Kinetic, Chemical, potential, mechanical (elastic), biological</li> <li>Units: kWh, Joules, calories, terms</li> <li>Fossil fuels: Barrels, short tons, cubic feet</li> </ul>"},{"location":"Misc/Energy/99_Cities/#power","title":"Power","text":"<ul> <li>Quantitative rate of doing work</li> <li>Energy per time</li> <li>Units: Watts, ergs, horsepower, lumen*</li> </ul>"},{"location":"Misc/Energy/99_Cities/#emissions-intensity","title":"Emissions Intensity","text":"<ul> <li>Unit: mtcde: metric-ton carbon-dioxide equivalent</li> <li>There are many forms of greenhouse gases, so we usually use CO2 equivalent</li> <li>by gas, per unit of energy, per activity, per GDP, by region</li> </ul>"},{"location":"Misc/Energy/99_Cities/#energy-consumption","title":"Energy Consumption","text":"<p>Usually shown using Sankey diagrams</p> <p></p> <p>Rejected energy is the by-product of energy generation that is not used (energy lost due to heat loss, air resistance, etc)</p>"},{"location":"Misc/Energy/99_Cities/#deep-decarbonization","title":"Deep Decarbonization","text":"<p>Goal of getting to net-zero emissions by 2050</p> <p></p>"},{"location":"Misc/Energy/99_Cities/#aspects-of-urban-energy-planning","title":"Aspects of Urban Energy Planning","text":"<ul> <li>Technological Implementation</li> <li>Geography</li> <li>Politics</li> <li>Land use &amp; built environment</li> </ul>"},{"location":"Misc/Energy/99_Cities/#key-issues-with-monitoring","title":"Key Issues with Monitoring","text":"<ul> <li>City, urban definitions</li> <li>Types of emissions: upstream (import), downstream (exports &amp; waste), goods &amp; services</li> <li>Measurement of affluence: Wealth vs Income</li> </ul>"},{"location":"Misc/Energy/99_Cities/#just-energy-transition","title":"Just Energy Transition","text":"<p>Environmental equity: reducing risk for all communities; distribution and effects os environmental problems and policies and processes to reduce differences in who bears environmental risks</p> <p>Climate Justice</p> <ul> <li>Historical responsibility</li> <li>Inter-generational equity</li> <li>Disproportionate causes and burdens</li> <li>Grassroots movements</li> <li>Legal actions on climate change</li> </ul> <p>Transformative justice</p> <ul> <li>Practices designed to create change in social systems</li> </ul> <p>Energy justice</p> <ul> <li>Disproportionate access, harms, burdens</li> <li>Unions &amp; worker</li> <li>Fossil-fuel dominated communities<ul> <li>Create a lot of jobs</li> </ul> </li> <li>Frontline/EJ communities </li> </ul> Distributional equitable distribution of burdens &amp; benefits of energy and environmental decisions Procedural Right to fair process for different stakeholders to take part equitably in decision-making process Restorative Repair harm done to individuals, instead of focusing upon punishing the offender Recognition Recognizing that parts of society might suffer as result of energy &amp; environmental decisions, and identifying individuals and groups who might be impacted by such decisions Cosmopolitan Reinforces all of the above, but states that \u201cabove forms of justice must apply universally to al human beings\u201d"},{"location":"Misc/Energy/99_Cities/#just-framework","title":"Just Framework","text":""},{"location":"Misc/Energy/99_Cities/#importance-of-land-uses-and-land-cover","title":"Importance of land uses and land cover","text":"<ul> <li>Total sequestration potential is uncertain</li> <li>Natural/existing ecosystems are more efficient than restored ones</li> <li>Policy mechanisms may be wildly different for different ecosystems</li> <li>Farm preservation vs biodiversity protections</li> <li>Carbon storage may fit into different regimes</li> </ul> <p>Controlling the spatial expansion of cities is crucial</p> <p>https://www.youtube.com/watch?v=EWFGkZ64ng4&amp;list=PLUl4u3cNGP63SEOB1q95TFs0hwyf1d7BG&amp;index=5</p>"},{"location":"Misc/HR/","title":"Human Resources","text":""},{"location":"Misc/HR/#references","title":"References","text":"<ul> <li> https://www.youtube.com/playlist?list=PLPjSqITyvDeXSqZIgYD2XKKLGZtjrhDtl</li> </ul>"},{"location":"Misc/Project_Management/","title":"Project Management","text":"<p>Project Management is the planning, organization, securing, motivation and control of resources to successfully complete a project, while minimizing unproductive efforts</p>"},{"location":"Misc/Project_Management/#idk","title":"IDK","text":"<p>Every project should have the following</p> <ul> <li>Clear goal</li> <li>Project valuation: NPV</li> </ul>"},{"location":"Misc/Project_Management/#project-management-models","title":"Project Management Models","text":"Advantage Disadvantages Waterfall - Concept: Setting the goal- Design: Acquire the requirements, such as programmers- Pre-Production: Verify if the concept and design are feasible- Production- Testing  - Alpha  - Beta- Shipping- Maintenance Most of the actual feedback will occur at beta testing, which is too far ahead in the timeline, and hence it will be too late to respond. Classical Agile - Individuals and interactions &gt; processes and tools- Working software &gt; comprehensive documentation- Customer collaboration &gt; contract negotiation- Responding rather &gt; over following a plan Focus on features that consumers want Assumes interchangeable tasks &amp; team membersDepends on good communication Scrum 1. Transparency- Everyone on the team related to a decision is involved- Shared responsibility2. Inspection: Anyone on the team curious on why something is the way it is, can3. Adaptability"},{"location":"Misc/Project_Management/#scrum","title":"Scrum","text":""},{"location":"Misc/Project_Management/#keywords","title":"Keywords","text":"Sprint Group of tasks to be completed by all teams concurrently Sprint Length Duration of sprintUsually 1 week Task Atomic chunk of work an individual can do independently that you can estimate duration forUsually \\(\\le 8 \\text{hrs}\\)Usually assigned to 1 person"},{"location":"Misc/Project_Management/#artifacts","title":"Artifacts","text":"Feature Requirement in User Story format:  \u201cAs the user/designer/artist, I want <code>something testable</code> so that <code>explain reason</code>\" Feature List List of features Backlog(Product/Project/Sprint) Prioritized list of functionality which a product should contain, or a project/sprint should achieveMaintained by Product Owner Tasklist Exhaustive list of tasks Scrumboard Task list visualized based on status like on Trello Target Goal\u201cHas to be done in 2days\u201d Estimation \u201cIt\u2019ll take about 2 days to do it\u201d Plan \u201cI\u2019ll need 2 days to do it\u201d Reality \u201cIt took 2 days to do it\u201d"},{"location":"Misc/Project_Management/#meetings","title":"Meetings","text":"Meeting Goal Details Timebox Sprint Planning Plan sprint backlog by converting project backlog into tasks 1hr Daily Scrum/Standups Accountability &amp; Communication What did you do since last meetingWhat will you do until next meetingWhat is blocking you. Just note this down. Create a new meeting if someone wants to discuss this block 10min Sprint Review Review product Demonstrate working productReview &amp; evaluate productReview &amp; update product backlog 1hr Retrospective Meeting Review process Things to keep doingThings to stop doingNew things to try 30min"},{"location":"Misc/Project_Management/#time-boxing","title":"Time-boxing","text":"<p>Every thing should be assigned a particular max-duration</p> <p>If time gets over, go ahead with the best solution possible rather than wasting more time trying to obtain the \u201cperfect\u201d solution</p>"},{"location":"Misc/Project_Management/#clear-goal-agendas","title":"Clear goal &amp; agendas","text":"<p>If you don\u2019t know why exactly you are having a meeting, you shouldn\u2019t be having it</p>"},{"location":"Misc/Project_Management/#involved-participants","title":"Involved participants","text":"<p>If someone does not need to be there, don\u2019t force them to attend</p>"},{"location":"Misc/Project_Management/#people","title":"People","text":"Person Product Owner Has complete understanding of the productGuides the team Scrum Master AdministrativeRuns the meetingsFacilitatorMediator Team Member <p>Ideally Product Owner and Scrum master should always be different, as each of their roles is very tiring as is.</p>"},{"location":"Misc/Project_Management/#feature-size","title":"Feature Size","text":"<p>Categorize features as S, M, L</p> <p>IDK</p> <ul> <li>Discuss as an entire team whenever possible</li> <li>Each role should try to lend its perspective</li> <li>Listen &amp; try to make sense of what others are saying</li> <li>Offer alternate solutions &amp; point out problems</li> <li>Divide huge features into atomic features</li> <li>Everyone related to the decision should be there</li> <li>Everyone should agree on the feature size</li> <li>Don\u2019t average out the opinions if someone says S &amp; another person says L, don\u2019t conclude it\u2019s M</li> <li>Should not take too long. If you get stuck on a feature, put it aside &amp; come back to it</li> </ul>"},{"location":"Misc/Project_Management/#estimation","title":"Estimation","text":"<p>Get a clear view of project reality to make informed decisions to control project to hit targets. You need to identify what the team is capable of, not what you can/want to happen</p> <pre><code>---\ntitle: Estimation\n---\n\nflowchart LR\n\nsubgraph pb[Product Backlog]\n    direction LR\n    f1[Feature 1]\n    f2[Feature 2]\nend\n\nsubgraph sb[Sprint Backlog]\n    direction LR\n    t1[Task 1]\n    t2[Task 2]\nend\n\n\npb --&gt; Size --&gt; Effort --&gt; People --&gt; Time --&gt; sb</code></pre> <ul> <li>Don\u2019t estimate in \u201cideal work hours\u201d</li> <li>Include breaks, distractions &amp; meetings in the estimate</li> </ul> <p>Need to know the __ of your estimates</p> <ul> <li>Uncertainty</li> <li>Accuracy</li> <li>Precision</li> </ul> <p>You need to commit to a point estimate; don\u2019t use the average of a range.</p>"},{"location":"Misc/Project_Management/#estimation-ne-planning","title":"Estimation \\(\\ne\\) Planning","text":"Planning Estimation Goal Achieve a target Verify if plan is realistic Commitment occurs when Person accepts a task when estimation is accurate(not necessary person takes up the task) Nature Fixed VariableRe-estimate as you work on the task"},{"location":"Misc/Project_Management/#tracking-estimates","title":"Tracking Estimates","text":"Feature Task Original Estimate Elapsed Remaining Current Estimate <p>Get summary statistics of feature\u2019s resources</p> <p>Use this for better estimating in the future</p> <p>Compare against feature size</p>"},{"location":"Misc/Project_Management/#backlog-to-tasklist","title":"Backlog to Tasklist","text":"<p>By breaking up story into tasks, you can understand all dependencies &amp; potential overloading of individuals</p>"},{"location":"Misc/Project_Management/#scrum-board","title":"Scrum Board","text":"Doing Todo-This Sprint Todo-Long Term Discussion Testing Completed Cancelled"},{"location":"Misc/Project_Management/#references","title":"References","text":"<ul> <li>Creating Video Games | MIT</li> <li>IIT Roorkee July 2018 | Project Management</li> <li>Agile Crash Course: Intro to Agile for Developers</li> </ul>"},{"location":"Misc/Reproducible_Research/00/","title":"00","text":"<p>John Hopkins University</p> <p>Coursera</p>"},{"location":"Misc/Reproducible_Research/01/","title":"01","text":"<p>The fundamental problem of data analysis is that \\(\\not \\exists\\) notation system for communicating results of data analysis.</p>"},{"location":"Misc/Reproducible_Research/01/#replication","title":"Replication","text":"<p>The standard for strengthening scientific evidence is replication of findings and conducting studies with independent investigators, data, analytical methods, laboratories, instruments</p> <p>However, replication is not always possible, as it is expensive, time-consuming, unique</p> <p>Hence, we aim for reproducibility, which is a compromise between replication and doing nothing</p>"},{"location":"Misc/Reproducible_Research/01/#reproducibility","title":"Reproducibility","text":""},{"location":"Misc/Reproducible_Research/01/#requirements","title":"Requirements","text":"<ul> <li>Analytic data</li> <li>Analytic code</li> <li>Documentation of code and data</li> <li>Standard means of distribution</li> </ul>"},{"location":"Misc/Reproducible_Research/01/#what-we-get","title":"What we get","text":"<ul> <li>Transparency</li> <li>Availability of data, software, methods</li> <li>Improved transfer of knowledge</li> </ul> <p>However, it does not necessarily ensure correctness of analysis. Reproducible analysis may still be wrong</p>"},{"location":"Misc/Reproducible_Research/01/#problems","title":"Problems","text":"<p>so some of the problems I have with reproducibility </p> <p>are, so the premise of reproducible research is that, </p> <p>you know, with all the data and all the </p> <p>code available to people, people can check each other. </p> <p>You can kind of validate someone else's analysis, and the whole </p> <p>system would kind of be self correcting in the long run. </p> <p>Alright. </p> <p>So one problem which I don't see here is </p> <p>that the long run sometimes is too long and then </p> <p>in terms of the context of the problems that </p> <p>you're dealing with I think reproducibility addresses what I call </p> <p>most, the kind of downstream aspects of scientific dissemination. </p> <p>Now, I'll be more specific about what I mean by that. </p> <p>Meaning that it kind of only happens post publication [INAUDIBLE]. </p> <p>And another key thing which is important in my area, maybe. </p> <p>I mean, particularly in my area, is that, [COUGH] </p> <p>the ideas of reproducibility kind of assume that everyone </p> <p>plays by the same rules, and everyone wants to </p> <p>achieve the same goals, which is definitely not tru</p>"},{"location":"Misc/Reproducible_Research/01/#challenges","title":"Challenges","text":"<ul> <li>Authors must take effort to publish results on the web (resources such as web servers may not be available)</li> <li>Readers must download the data and results individually and combine everything on their own</li> <li>Readers may not have the same resource as authors</li> </ul>"},{"location":"Misc/Reproducible_Research/01/#replication-vs-reproducibility","title":"Replication vs Reproducibility","text":"Replication Reproducibility Ensures validity of Scientific claim Data analysis"},{"location":"Misc/Reproducible_Research/01/#research-pipeline","title":"Research Pipeline","text":""},{"location":"Misc/Reproducible_Research/01/#literate-statistical-programming","title":"Literate (Statistical) Programming","text":"<p>An article is a stream of text and code</p> <p>Analysis code is divided into text and \u2018code chunks\u2019</p> <p>Code chunks load data and computes results</p> <p>Presentation code formats results</p> <p>Literate program is</p> <ul> <li>weaved to human-readable documents</li> <li>tangled to machine-readable documents</li> </ul> <p>idk</p> <ul> <li>Do not save output and import them as images</li> <li>Save data in non-proprietary formats (csv, sqlite)</li> </ul>"},{"location":"Misc/Reproducible_Research/01/#requires","title":"Requires","text":"<ul> <li>Documentation lang (human readable)   for eg: latex, markdown</li> <li>Programming lang (machine readable)   for eg: python, r</li> </ul> <p><code>knitr</code> incorporates both of these through Rmarkdown </p>"},{"location":"Misc/Reproducible_Research/01/#pros","title":"Pros","text":"<ul> <li>Text and code in one place, logical order</li> <li>Data and results update automatically to external changes</li> </ul>"},{"location":"Misc/Reproducible_Research/01/#cons","title":"Cons","text":"<ul> <li>Hard for author to work, as everything is one place</li> <li>Can slow down processing of documents</li> </ul>"},{"location":"Misc/Reproducible_Research/01/#steps-in-data-analysis","title":"Steps in data analysis","text":"<ol> <li>Define question in the most simple, atomic, concrete manner</li> <li>Define ideal dataset</li> <li>Determine data access</li> <li>Obtain data</li> <li>Data Cleaning</li> <li>EDA</li> <li>Statistical Prediction/Modelling</li> <li>Interpreting results</li> <li>Challenge results</li> <li>Synthesize results</li> <li>Create reproducible code</li> </ol>"},{"location":"Misc/Reproducible_Research/02/","title":"Coding Standards","text":"<ul> <li>Always write code in human-readable text files, this ensures everyone will be access your code</li> <li>Indent code</li> <li>Limit width of code to 80 char</li> <li>Limit length of individual functions to performing one particular task</li> </ul>"},{"location":"Misc/Reproducible_Research/02/#markdown","title":"Markdown","text":""},{"location":"Misc/Reproducible_Research/02/#rmarkdown","title":"RMarkdown","text":""},{"location":"Misc/Reproducible_Research/02/#library-to-pretty-print-tables","title":"Library to pretty print tables","text":"<pre><code>library(xtrable)\nxt &lt;- xtable(summary(fit))\nprint(xt, type=\"html\")\n</code></pre>"},{"location":"Misc/Reproducible_Research/02/#default-settings","title":"Default Settings","text":"<pre><code>```{r setoptions, echo=FALSE}\n# do not display this default settings block\nopts_chunk$set(\n  echo=FALSE, # hide code of all code chunks\n  results=\"hide\", # hide outputs of all code chunks\n)\n```\n</code></pre>"},{"location":"Misc/Reproducible_Research/02/#caching","title":"Caching","text":"<pre><code>```{r setoptions, cache = TRUE}\nmy code blah blah()\n```\n</code></pre>"},{"location":"Misc/Reproducible_Research/02/#knitr","title":"<code>knitr</code>","text":"<pre><code>flowchart LR\nrmd --&gt; md --&gt; html &amp; pdf</code></pre>"},{"location":"Misc/Reproducible_Research/03/","title":"Communicating Results","text":"<p>People are busy, hence yo ushould breakdown results into different levels of detail</p>"},{"location":"Misc/Reproducible_Research/03/#research-paper","title":"Research Paper","text":"<ul> <li>Title</li> <li>Author list</li> <li>Abstract</li> <li>Body</li> <li>Results</li> <li>Supplementary Materials (details)</li> <li>Code/Data (more details)</li> </ul>"},{"location":"Misc/Reproducible_Research/03/#email","title":"Email","text":"<ul> <li>Subject</li> <li>Summarize findings in one sentence</li> <li>Email body</li> <li>Brief description of problem</li> <li>Recall what was proposed</li> <li>Recall what was executed</li> <li>Summary findings in 1-2 paragraphs</li> <li>If you want to suggest future actions, make them concrete</li> <li>If you have questions for them to address, make them yes/no</li> <li>Attachments</li> <li>Literate Programming File</li> <li>Links to supplementary materials</li> <li>Code/Software/Data</li> <li>Github Repo/Project Website</li> </ul>"},{"location":"Misc/Reproducible_Research/03/#rpubs","title":"RPubs","text":"<p>Free site to publish work</p>"},{"location":"Misc/Reproducible_Research/03/#reproducible-research-checklist","title":"Reproducible Research Checklist","text":"<p>\u2705 Start with good science</p> <ul> <li>Garbage in, Garbage out</li> <li>Coherent, focused question</li> <li>working with good collaborators</li> <li>Something that\u2019s interesting to you</li> </ul> <p>\u274c Don\u2019t use GUI softwares for analysis, as it is hard to reproduce</p> <p>\u274c Don\u2019t do things by hand</p> <ul> <li>Using spreadsheets to perform operations</li> <li>Editing tables/figures manually</li> </ul> <p>\u2705 Document unavoidable manual operations (not as easy at is it sounds)</p> <ul> <li>Downloading data from a website</li> <li>Moving data to outside project folder</li> <li>Splitting/reformatting data files</li> </ul> <p>\u2705 Teach a computer to do tasks</p> <ul> <li>Downloading data</li> </ul> <pre><code>download.file(\"somelink/file.csv\", \"local_location/file.csv\")\n</code></pre> <ul> <li>Web scraping</li> </ul> <p>\u2705 Use version control (such as Git)</p> <ul> <li>Helps slow down and perform tasks step by step</li> <li>Add changes in small chunks</li> </ul> <p>\u2705 Keep track of software environment</p> <ul> <li>Write cross-platform code</li> <li>For eg, in python be careful when working with paths. Use <code>os.path.join(folder, file)</code> instead of <code>folder + \"/\" + file</code></li> <li>Computer Architecture</li> <li>CPU, GPU specs</li> <li>Operating System</li> <li>Software toolchain</li> <li>Compilers/Interpreter</li> <li>Programming languages</li> <li>Command shell</li> <li>Database backends</li> <li>Data analysis softwares</li> <li>Supporting software</li> <li>Libraries</li> <li>Packages</li> <li>Dependencies</li> <li>External dependencies</li> <li>Websites</li> <li>Data/Software repositories</li> <li>Remote databases</li> <li>Version Numbers</li> </ul> <p>\u274c Don\u2019t save output independent of code</p> <p>\u2705 Set your seed for random number generator</p>"},{"location":"Misc/Risk_and_Decision_Analysis/","title":"Risk &amp; Decision Analysis","text":""},{"location":"Misc/Risk_and_Decision_Analysis/#references","title":"References","text":"<ul> <li> https://ocw.mit.edu/courses/ids-333-risk-and-decision-analysis-fall-2021/</li> </ul>"},{"location":"Misc/Risk_and_Decision_Analysis/01_Forecasts/","title":"Forecasts","text":"<p>Forecasts are never perfect.</p> <p>Forecasting projects is like steering a car using the rear view mirror.</p>"},{"location":"Misc/Risk_and_Decision_Analysis/01_Forecasts/#flaw-of-averages","title":"Flaw of Averages","text":"<p>Fundamental problem in design and evaluation of projects on the \u2018average\u2019/\u2018most-likely\u2019 future forecasts</p> <p>Due to misunderstanding of probability and systems behavior</p>"},{"location":"Misc/Risk_and_Decision_Analysis/01_Forecasts/#jensens-law","title":"Jensen\u2019s Law","text":"\\[ E[ \\ f(x) \\ ] \\ne f( \\ E[x] \\ ), \\\\ \\text{if } f(x) \\text{ is convex curve} \\] <p>where</p> <ul> <li>\\(E[ \\ f(x) \\ ]\\): Average of possible outcomes of \\(f(x)\\)</li> <li>\\(f( \\ E[x] \\ )\\): outcome calculated using average of \\(x\\)</li> <li>Convex function example: \\(x^2 + c\\)</li> </ul> <p>The equality holds true when \\(f(x)\\) is linear, but in reality, most systems are non-linear</p>"},{"location":"Misc/Risk_and_Decision_Analysis/02_DCF/","title":"Discounted Cashflows","text":"<p>Always make sure that you compare value at the same time point.</p> <p>Learnt in Finance</p>"},{"location":"Misc/Risk_and_Decision_Analysis/03_Proactive_Design/","title":"Proactive Design","text":""},{"location":"Misc/Risk_and_Decision_Analysis/03_Proactive_Design/#stages-of-analysis","title":"Stages of Analysis","text":"Stage NPV Base case Consider base case design, for fixed objective (mission, specifications) Recognize reality of uncertainty - This may lead to different results due to system non-linearities- Capacity constraints systematically limit profit from good opportunities, while we suffer fully from risks- Error of forecasts- Distribution of possible outcomes of reality (volatility and uncertainty) Lowest Incorporate flexibility - Adjust project actual needs, based on how future develops, and intelligently develop system over time- Run a Monte Carlo simulation of all possibilities, and then identify what is the best initial case, and what flexibility is optimal Highest Multi-dimensional valuation Not just expected NPV, but also other summary statistics of the possible NPVs from the simulations Conclusion Don\u2019t accept consequences of  distribution of uncertainties- Avoid/reduce downside effects- Take advantage of opportunities- Reduce initial costs"},{"location":"Misc/Risk_and_Decision_Analysis/03_Proactive_Design/#example-mall-parking","title":"Example: Mall Parking","text":"<p>Requirement: Major garage serving mega-mall</p> <p>Actual demand uncertain, due to</p> <ul> <li>Population growth, demographics</li> <li>Mall success probability</li> <li>Competition</li> </ul> <p>Engineering design assumes a fixed forecast!</p>"},{"location":"Misc/Risk_and_Decision_Analysis/03_Proactive_Design/#optimizing-base-case","title":"Optimizing base case","text":"<p>Find highest value design, by looking at each major design alternative. No  </p> <p></p>"},{"location":"Misc/Risk_and_Decision_Analysis/03_Proactive_Design/#recognizing-uncertainty","title":"Recognizing uncertainty","text":"<p>Costs may be easy to estimate, as contracts will give fixed bids.</p> <p>However, demand is highly uncertain, especially as we proceed to the future</p> <p></p>"},{"location":"Misc/Risk_and_Decision_Analysis/03_Proactive_Design/#introducing-flexibility-into-design","title":"Introducing flexibility into design","text":""},{"location":"Misc/Risk_and_Decision_Analysis/03_Proactive_Design/#multi-dimensional-valuation","title":"Multi-Dimensional Valuation","text":""},{"location":"Misc/Risk_and_Decision_Analysis/03_Proactive_Design/#conclusion","title":"Conclusion","text":"<p>Build the mall in a way that we start out only 4 floors, but make the foundation and etc in a way that we can expand to the initial number of floors as required, even though this is more expensive than just building foundation capable for 4 floors.</p>"},{"location":"Misc/Risk_and_Decision_Analysis/04_Simulation/","title":"Simulation","text":"<p>Replicate outcomes of uncertain process</p> <p>Works with any distribution</p> <ul> <li>Regular/irregular</li> <li>Continuous/not</li> <li>Normal/not</li> </ul>"},{"location":"Misc/Risk_and_Decision_Analysis/04_Simulation/#requirements","title":"Requirements","text":"<p>Distributions for key parameters</p> <p>May be observed, estimated, assumed, or guessed</p>"},{"location":"Misc/Risk_and_Decision_Analysis/04_Simulation/#steps","title":"Steps","text":"<ol> <li> <p>Create a valuation model</p> </li> <li> <p>Identify most significant uncertainties</p> </li> <li> <p>Tornado diagram</p> </li> <li> <p>Choose distributions for uncertainties</p> </li> <li> <p>Model distribution of outcomes</p> </li> <li> <p>Set up decision rules for exercising flexibility, to ensure that it is right decision to exercise flexibility</p> </li> </ol> <p>For eg: \u201cExpand if observed demand &gt; Capacity over 2 years. But don\u2019t expand towards the end of life\u201d</p>"},{"location":"Misc/Risk_and_Decision_Analysis/04_Simulation/#flexibility-types","title":"Flexibility Types","text":"Flexibility \u201cin\u201d system Flexibility \u201con\u201d system Feature design to enable system to evolve easily Nothing to do with design Example Building foundation to support additional floors in future Abandon projectDelay project"},{"location":"Misc/Risk_and_Decision_Analysis/05_Drivers_of_Flexibility/","title":"Drivers of Flexibility","text":""},{"location":"Misc/Risk_and_Decision_Analysis/05_Drivers_of_Flexibility/#factors-affecting-flexible-design-vs-immediate-decision","title":"Factors affecting flexible design vs immediate decision","text":"Factor Comment Against Flexibility Comment For Flexibility Uncertainty Benefits of Scale Cheaper to produce at large quantity Benefits not attained when set-up is under-utilized, which is common when set-up is made for larger capacity than requirement Discount Rate Cancels out benefits of scaleFuture investments are better in terms of NPV Learning Efficiency improves with time &amp; experience Competitive Gaming"},{"location":"Misc/Risk_and_Decision_Analysis/05_Drivers_of_Flexibility/#mannes-analysis","title":"Manne\u2019s Analysis","text":"<p>Analytic solution for simple case of constant linear growth</p> <p>Provides simple demonstration of issues, by giving useful insight into trade-offs</p> <p></p> <ul> <li>Optimal cycle time = most economical time between additions of capacity modules</li> <li>Cycle time defines model size (years * annual growth)</li> </ul>"},{"location":"Misc/Risk_and_Decision_Analysis/05_Drivers_of_Flexibility/#interpretation","title":"Interpretation","text":"<ol> <li>Higher discount rate \\(\\implies\\) small size of modules</li> <li>High discount rates reduce present value of future costs, which counteract benefits of scale</li> <li>Smaller \\(\\alpha\\) \\(\\implies\\) Larger benefits of scale</li> </ol>"},{"location":"Misc/Risk_and_Decision_Analysis/05_Drivers_of_Flexibility/#results","title":"Results","text":"<p>Manne\u2019s present cost versus cycle time</p> <ol> <li>Steep for small cycle times</li> <li>Quite flat at bottom for large cycle times</li> </ol>"},{"location":"Misc/Risk_and_Decision_Analysis/05_Drivers_of_Flexibility/#learning-effects","title":"Learning Effects","text":""},{"location":"Misc/Risk_and_Decision_Analysis/06_Decision_Analysis/","title":"Decision Analysis","text":""},{"location":"Misc/Risk_and_Decision_Analysis/06_Decision_Analysis/#decision-analysis-vs-flexibility-analysis","title":"Decision Analysis vs Flexibility Analysis","text":"<p>Due to this, flexibility analysis is preferred over decision analysis</p> <ul> <li> <p>Decision analysis</p> </li> <li> <p>assumes you can define choices to take at any stage</p> </li> <li> <p>Implies focus on a few distant choices</p> </li> <li> <p>Flexibility analysis</p> </li> <li> <p>encourages to explore</p> <ul> <li>What decisions to take</li> </ul> <p>and</p> <ul> <li>When to take them</li> </ul> </li> <li> <p>as granular as required</p> </li> </ul>"},{"location":"Misc/Risk_and_Decision_Analysis/06_Decision_Analysis/#outcome","title":"Outcome","text":"<ol> <li> <p>Strategies for altering choices as future is revealed, not a unique \u201coptimal choice\u201d</p> </li> <li> <p>\u2018Second best\u2019 choices which offer</p> </li> <li> <p>Insurance against downside</p> </li> <li>Opportunity to exploit upside</li> </ol> <p>\u2018Second best\u2019 strategies are sub-optimal for each outcome, but preferable as they offer flexibility to do well in a range of outcomes. It is never the best, but never the worst</p> <ol> <li>Education of client about distribution, range of possible results (Value at Risk)</li> </ol>"},{"location":"Misc/Risk_and_Decision_Analysis/06_Decision_Analysis/#motivation","title":"Motivation","text":"<p>People acting intuitively deal poorly with complex, uncertain situations. They</p> <ul> <li>Process probability info poorly</li> <li>Over-simplify complexity &amp; alter reality</li> <li>Focus on extremes</li> <li>Focus on end states, not whole process</li> </ul> <p>Decision analysis helps overcome this</p>"},{"location":"Misc/Risk_and_Decision_Analysis/06_Decision_Analysis/#characteristics","title":"Characteristics","text":"<ul> <li>Assumes that every decision in the set of choices is discrete</li> <li>Looks over several time periods</li> <li>Deals with uncertainties</li> <li>Standard method</li> <li>Can include utility assessment (such as levels of consumer satisfaction)</li> </ul>"},{"location":"Misc/Risk_and_Decision_Analysis/06_Decision_Analysis/#decision-tree","title":"Decision Tree","text":"<ul> <li>Structure</li> <li>Decision points: Choices</li> <li>Chance points (possible outcomes) after each decision</li> <li>Data</li> <li>Value of each possible outcome</li> <li>Probability</li> <li>Uncertainties</li> </ul> <p>Goal: Maximize expected value of outcomes</p> <p>For each set of alternatives, calculate expected value. Then choose alternative with maximum EV</p>"},{"location":"Misc/Risk_and_Decision_Analysis/07_Value_of_Information/","title":"Value of Information","text":"<p>Decision to collect information is a decision to insert flexibility into development strategy, as the logic behind \u201ctest\u201d is so that you may change you decision once you have test results.</p> <p>Value of information = Value of this flexibility</p>"},{"location":"Misc/Risk_and_Decision_Analysis/07_Value_of_Information/#information-gathering","title":"Information Gathering","text":"<p>Inserting an test stage before decision problems as possible choice reduces uncertainty before commitment to a system design/roll-out</p> <pre><code>flowchart LR\nD --&gt; dp1[Decision Problem] &amp; Test\nTest --&gt; dp2[Decision Problem]</code></pre>"},{"location":"Misc/Risk_and_Decision_Analysis/07_Value_of_Information/#tests","title":"Tests","text":"<p>Any decision problem has initial uncertainties = \u201cprior probabilities\u201d, such as those concerning</p> <ul> <li>Cost of production</li> <li>Likelihood of sales</li> </ul> <p>Tests help get information on these issues, for eg</p> <ul> <li>Running a test plant</li> <li>Carrying out market analysis</li> </ul> <pre><code>flowchart LR\npp[Prior Probabilities] --&gt;|Test| ni[New Information] --&gt;|Revise| pp --&gt; new[New Expected Values for decision]</code></pre>"},{"location":"Misc/Risk_and_Decision_Analysis/07_Value_of_Information/#consequence","title":"Consequence","text":"<p>EV after test \\(\\ge\\) EV without test</p>"},{"location":"Misc/Risk_and_Decision_Analysis/07_Value_of_Information/#evsi","title":"EVSI","text":"<p>Expected value of Sample information</p> <p>Helps understand if test is significantly worth it? $$ \\begin{aligned} \\text{Expected value of Info} &amp;= EV_\\text{after test} - EV_\\text{without test} \\ &amp;= \\sum p_k \\cdot \\text{EV}(D_k^)   -  \\text{EV}(D^) \\end{aligned} $$</p> <ul> <li>\\(D^*\\) is the current best design</li> <li>\\(D_k^*\\) is the best design of test \\(k\\)</li> </ul> <p>Test results will not prove what will happen. They are samples, and hence false positives/negatives possible.</p> <p>Tests merely just update the prior estimates of probabilities using Bayes\u2019 Therom. Each test result implies a different value of project, each with a different probability. This EVSI is complicated</p> <p>Bayes\u2019 theorem is impractical for systems design, as elements of factor for updating the posterior probability are unavailable</p> <ol> <li>Full analysis is complicated process with many possible outcomes</li> <li>Involves many assumptions about what the probability of outcomes of the tests</li> <li>Analysis maybe incorrect even if math is correct</li> </ol>"},{"location":"Misc/Risk_and_Decision_Analysis/07_Value_of_Information/#evpi","title":"EVPI","text":"<p>Expected Value of Perfect information</p> <p>Even though it is hypothetical &amp; not perfect, it helps simplify analysis</p> <p>Establish upper bound on value of test</p> <p>Concept: Imagine a black box \u201cCassandra\u201d machine which predicts exactly which event test result \\(E_j\\)\u00a0occurs. Therefore, the \u201cbest\u201d possible decisions can be made. EV gain over the \u201cno test\u201d EV must be maximum possible, which is the desired upper limit on value of test</p>"},{"location":"Misc/Risk_and_Decision_Analysis/07_Value_of_Information/#characteristics","title":"Characteristics","text":"<ol> <li>Prior probability (occurrence of uncertain event) must equal probability (associated perfect test result)</li> <li>For \u201cperfect test\u201d, the posterior probabilities are either 0/1; no doubt remains</li> <li>Optimal choice generally obvious, once we \u201cknow\u201d what will happen</li> <li>EVPI can generally be written directly, without Bayes\u2019 Theorem</li> </ol>"},{"location":"Misc/Risk_and_Decision_Analysis/07_Value_of_Information/#is-test-worthwhile","title":"Is Test Worthwhile?","text":"<ul> <li>If value is linear</li> <li>if EVPI &lt; cost of test -&gt; Reject test</li> <li>Pragmatic rule of thumb</li> <li>If cost &gt; 50% EVPI -&gt; Reject test</li> </ul>"},{"location":"Misc/Risk_and_Decision_Analysis/08_Project_Evaluation/","title":"Project Evaluation","text":"<p>It is nearly impossible to derive the \u201cbest\u201d choice. Therefore, we try to find the \u201cpreferred solution\u201d</p>"},{"location":"Misc/Risk_and_Decision_Analysis/08_Project_Evaluation/#what-is-best","title":"What is \u201cbest\u201d?","text":"<p>Extreme (high/low) of all possibilities</p> <p>Either</p> <ol> <li>1 metric of performance</li> </ol> <p>or</p> <ol> <li>Metrics can be put on single scale</li> </ol> <p>However, both 1 and 2 are not realistic</p>"},{"location":"Misc/Risk_and_Decision_Analysis/08_Project_Evaluation/#valuepreferenceutility-function","title":"Value/Preference/Utility Function","text":"<p>\\(V(x)\\) is a means of ranking the relative preference of an individual for a bundle of consequences \\(x\\)</p>"},{"location":"Misc/Risk_and_Decision_Analysis/08_Project_Evaluation/#diminishing-marginal-utility-curve","title":"Diminishing marginal utility curve","text":""},{"location":"Misc/Risk_and_Decision_Analysis/08_Project_Evaluation/#exceptions-to-diminishing-marginal-utility","title":"Exceptions to Diminishing Marginal Utility","text":"<p>Very common in real life</p> <ul> <li>Critical mass: only valuable if you have enough</li> <li>Network/Connectivity: more connections \\(\\implies\\) more valuable</li> <li>Threshold/Competition: only valuable if</li> <li>Minimum reached (absolute graded exams)</li> <li>Matches/beats competition (relative grading exams)</li> </ul>"},{"location":"Misc/Risk_and_Decision_Analysis/08_Project_Evaluation/#conditions-for-a-value-function","title":"Conditions for a Value function","text":""},{"location":"Misc/Risk_and_Decision_Analysis/08_Project_Evaluation/#axioms","title":"Axioms","text":"<ol> <li>Completeness/Complete Pre-order: \\(V(x)\\) is defined \\(\\forall x_i\\)</li> <li>Transitivity: \\(V(a)&gt;V(b) \\ \\land \\ V(b)&gt;V(c) \\implies V(a)&gt;V(c)\\)</li> <li>General true for individuals</li> <li>Not necessarily true for groups; not all group members share the same preferences <ul> <li></li> </ul> </li> <li>Ellsberg Paradox: Under ambiguity, transitivity does not always hold, as people will want to choose the non-ambiguous option usually</li> <li>Allais Paradox: </li> <li>Monotonicity/Archimedean Principle</li> <li>\\(V(x)\\) is monotonically-increasing/decreasing</li> <li>\\(a &gt; b \\implies (V(a) &gt; V(b) \\quad \\forall a, b) \\lor (V(a) &lt; V(b) \\quad \\forall a, b)\\)</li> <li>This assumption does not hold for all utility functions<ul> <li>Inflation rate</li> <li>Audio volume</li> <li>Salt on food</li> <li>Problem can be re-formulated as \u201cSalt available on table\u201d</li> </ul> </li> </ol>"},{"location":"Misc/Risk_and_Decision_Analysis/08_Project_Evaluation/#consequences","title":"Consequences","text":"<ul> <li>Existence of \\(V(x)\\)</li> <li>Only ranking \\(x_1, x_2, \\dots\\) possible. We cannot quantify the distances between \\(V(x_1), V(x_2), \\dots\\)</li> <li>Strategic equivalence: Monotonic transformation of \\(V(x) \\equiv V(x)\\); \\(V(x_1, x_2) = {x_1}^2 x_2 \\equiv 2 \\log \\vert x_1 \\vert + \\log \\vert x_2 \\vert\\)</li> <li>Values not good basis for absolute value</li> <li>Arrow\u2019s Impossibility Theorem/Paradox</li> <li>No \u201cfair\u201d voting system, without a dictator, that satisfies everyone\u2019s preferences</li> <li>Hence, concept of \u201cbest\u201d is not meaningful in design of complex systems</li> <li>Therefore, we try to find the \u201cpreferred solution\u201d</li> </ul>"},{"location":"Misc/Risk_and_Decision_Analysis/08_Project_Evaluation/#outcomes","title":"Outcomes","text":"<p>Nature of Evaluation</p> <ul> <li>Many dimensions &amp; metrics of performance</li> <li>Uncertainty about metrics</li> <li>\u201cBest\u201d is undefined</li> <li>We can screen out dominated solutions</li> </ul> <p>Nature of Choice</p> <ul> <li>Any person must make tradeoffs</li> <li>Group inevitably have to negotiate deal</li> </ul>"},{"location":"Misc/Risk_and_Decision_Analysis/08_Project_Evaluation/#concept-of-dominance","title":"Concept of Dominance","text":"<p>One alternative better than others on all dimensions</p> <p>Dominated alternatives can be discarded</p> <p>Feasible region or \u201cTrade Space\u201d is area under &amp; left of the curve</p> <p></p>"},{"location":"Misc/Risk_and_Decision_Analysis/08_Project_Evaluation/#metrics","title":"Metrics","text":"<ul> <li>Expected Value: Useful, but insufficient, as it cannot describe range of effects</li> <li>Worst-case scenario with some notion of probability of loss: People are \u201crisk-averse\u201d; more sensitive to loss</li> <li>Best case scenario</li> <li>CapEx: Capital Expenditure = Investment</li> <li>Some measure of benefit-cost</li> <li>Value-Modelling</li> <li>VAR</li> <li>VAG</li> </ul>"},{"location":"Misc/Risk_and_Decision_Analysis/08_Project_Evaluation/#robustness","title":"Robustness","text":"<p>Taguchi method</p> <p>Robust design is a product whose performance is minimally-sensitive to factors causing variability</p> <p>Robustness measured by standard deviation of distribution of outcomes</p> <p></p> <p>Preferred when we particular result</p> <ul> <li>Tuning into a signal</li> <li>Fitting parts together</li> </ul> <p>However, this is not necessarily value maximizing. We would prefer to</p> <ul> <li>limit downside</li> <li>maximize upside</li> </ul> <p></p>"},{"location":"Misc/Risk_and_Decision_Analysis/09_Summary/","title":"Summary","text":""},{"location":"Misc/Risk_and_Decision_Analysis/09_Summary/#deterministic-analysis","title":"Deterministic Analysis","text":""},{"location":"Misc/Risk_and_Decision_Analysis/09_Summary/#uncertainty-analysis","title":"Uncertainty Analysis","text":""},{"location":"Misc/Risk_and_Decision_Analysis/09_Summary/#characterization-of-enumeration-space-to-find-best-flexible-designs","title":"Characterization of enumeration space to find best flexible designs","text":""},{"location":"Misc/Risk_and_Decision_Analysis/09_Summary/#comparison-of-fixed-vs-flexible-design","title":"Comparison of fixed vs flexible design","text":""},{"location":"Misc/Risk_and_Decision_Analysis/09_Summary/#learning-rates","title":"Learning Rates","text":""},{"location":"Misc/Risk_and_Decision_Analysis/09_Summary/#multi-criteria-decision-table","title":"Multi-Criteria Decision Table","text":"<p>With \\(\\alpha=0.95, \\text{LR}=10\\%\\)</p> <p></p>"},{"location":"Misc/Risk_and_Decision_Analysis/09_Summary/#value-of-flexibility-for-different-economies-of-scale-learning-rates","title":"Value of flexibility for different economies of scale &amp; learning rates","text":""},{"location":"Misc/Systems_Engineering/","title":"Systems Engineering","text":"<p>Interdisciplinary field of engineering and engineering management that focuses on how to design, integrate, and manage complex systems over their life cycles</p>"},{"location":"Misc/Systems_Engineering/#references","title":"References","text":"<ul> <li> Fundamentals of Systems Engineering | MIT</li> </ul>"},{"location":"Misc/Team_Management/","title":"Team Management","text":""},{"location":"Misc/Team_Management/#references","title":"References","text":"<ul> <li> Onboarding your next engineer</li> <li> Software Engineer Onboarding Like A Pro</li> <li> </li> </ul>"},{"location":"Misc/Team_Management/01_Onboarding/","title":"Onboarding","text":""},{"location":"Misc/Team_Management/01_Onboarding/#idk","title":"IDk","text":"<ul> <li>Day -1</li> <li>Friendly follow up</li> <li>Dev setup</li> <li>Buddy assignment</li> <li>Week 1</li> <li>Day 1<ul> <li>Warm welcome by team</li> <li>2:1 with buddy and manager</li> <li>Business<ul> <li>Culture</li> </ul> </li> <li>Team<ul> <li>Vision</li> <li>Technology</li> <li>Culture</li> </ul> </li> <li>Preferences and competence</li> <li>Breathing room</li> </ul> </li> <li>Day 2-5<ul> <li>Tools and access</li> <li>Dev setup</li> <li>Code standards</li> <li>Training</li> </ul> </li> <li>Week 2</li> <li>Day 1<ul> <li>Assign first task</li> </ul> </li> <li>Day 5<ul> <li>Detailed code review</li> <li>Task planning</li> <li>Demo progress to others</li> </ul> </li> <li>After</li> <li>Clear expectations</li> <li>Designated team hour for doubts</li> <li>Coffee chat with each team member<ul> <li>See what they\u2019re working on</li> <li>Learn about different challenges</li> <li>Build rapport</li> </ul> </li> </ul>"},{"location":"Misc/Team_Management/02_Standards/","title":"Standards","text":"<ul> <li>Development guidelines</li> <li>Coding standards</li> <li>Testing requirements</li> <li>Tool/technologies used</li> </ul>"},{"location":"School/Computer_Science/","title":"Computer Science","text":"<p>I had the best ever school teachers: Malini ma\u2019am &amp; Viji ma\u2019am.</p> <p>I even used her notes when studying for university courses. The PDF to that scanned document can be found below: </p> <ul> <li> <p>Part - 1 -  History of C, Tokens, Strucuture of a Program, Data-Types, Escape Sequences, Characters .vs. Strings, Operators &amp; Expressions, Increment &amp; Decrement Operators, Hierarchy of Math Operators, Short Hand Expressions, Literals, Type-Casting, Flow of Control, If-Elif-Else, Loops - While, Do-While &amp; For, Jump, Break</p> </li> <li> <p>Part - 2 - Arrays, Linear Search, Binary Search, Bubble Sort, Insertion Sort, Selection Sort. </p> </li> </ul>"},{"location":"School/Linux_Fundamentals/","title":"Computer Science","text":"<p>Notes have been compiled from Network Chuck's \"Linux For Hackers\" Series and HackerRank Bash Scripting Section. </p>"},{"location":"School/Linux_Fundamentals/bash/","title":"Bash","text":"<ol> <li> <p>Read - Takes Input, '$' sign for variables.     <pre><code>read name\necho (\"Welcome $name\")\n</code></pre></p> </li> <li> <p>For Loops: Have two braces for Math Ops (())     <pre><code>for ((i = 1; i &lt;= 100; i+=2))\ndo\n    $i\ndone\n</code></pre></p> </li> <li> <p>While Loop Structure - do, (stuff), done</p> </li> <li> <p>We use \" | bc\" for math operations.      <pre><code>echo(\"5+3\") | bc\n</code></pre></p> </li> <li> <p>Conditionals : if, elif, else, fi</p> <pre><code>read input\n\nif [ $input == \"y\" ] || [ $input == \"Y\" ]; then \n    echo \"YES\"\nelse \n    echo \"NO\"\nfi\n</code></pre> </li> <li> <p>grep : Will search and print matching. </p> <p><pre><code>grep -iwE \"the|that|then|those\" \n\n# i - To search case-insensitive. \n# w - To ignore case. \n# E - To compare as an extended regex, to allow use of \"|\"\n\ngrep -v \"that\" \n# v - Will show invert of what you're searching.\n</code></pre> 7. sed : To replace. </p> <pre><code>sed 's/the /this /1'\n#Space to avoid words like \"therefore\"\n\nsed -e 's/thy/your/ig' -e 's/Thy/Your/ig' -e 's/tHy/YouR/ig'\n#To replace mutliple words with one\n# -e is for editing\n\nsed -e 's/thy/{&amp;}/gi'\n#Make it highlight in curly braces. \n\nsed -e 's/([0-9]+) ([0-9]+) ([0-9]+) ([0-9]+)/\\4 \\3 \\2 \\1/' \n#Back-Refrencing. \n#E - for extended regular expression \n</code></pre> </li> <li> <p>awk - For comparision and parsing.     <pre><code>awk '{ if ($4 == \"\") print \"Not all scores are available for \"$1}'\nVariable out of the \"\". \n\nawk '{ if (($2 &gt;= 50) &amp;&amp; ($3 &gt;= 50) &amp;&amp; ($4 &gt;=50)) {print $1\" : Pass\"} else {print $1\" : Fail\"}}'\n\n\nawk '{\ntotal = ($2 + $3 + $4)/3\nif (total &gt;= 50 &amp;&amp; total&lt; 60)\n    print $1,$2,$3,$4, \": C\"\nelse if (total &gt;= 60 &amp;&amp; total &lt; 80)\n    print $1,$2,$3,$4, \": B\"\nelse if (total &gt;= 80)\n    print $1,$2,$3,$4, \": A\" \nelse \n    print $1,$2,$3,$4, \": FAIL\" }'\n\n\nawk '\n    BEGIN {count=0}\n\n    {\n        printf \"%s %d %d %d\", $1, $2, $3, $4\n        count++;\n        if (count%2 == 0)\n            printf \"\\n\"  \n        else\n            printf \";\"\n    }\n'\n</code></pre></p> </li> <li> <p>Arrays: </p> <pre><code>while read country\ndo\n    countries=(\"${countries[@]}\" \"$country\")\ndone\necho \"${countries[@]}\"\n# First one is array, second one is element being added to array. \n\n\nwhile read country\ndo \n    countries+=($country)\ndone\necho \"${countries[@]:3:5}\"\n#Sliced\n\nwhile read country\ndo \n    if [[ \"$country\" != *a* &amp;&amp; \"$country\" != *A* ]]; then\n        countries+=(\"$country\")\n    fi\ndone\necho \"${countries[@]}\"\n# Filterting on the basis of letter. \n\nwhile read line\ndo\n    elements+=(i)\ndone \necho ${#elements[@]}\n# counts - \"#\"\n</code></pre> </li> <li> <p>Cut</p> <pre><code>while read lines\ndo\n    echo \"$lines\" | cut -c3\ndone\n# Cut a specifc character. \n\nwhile read lines\ndo\n    echo \"$lines\" | cut -c 2,7\ndone\n# Cut multiple characters. \n\nwhile read lines\ndo\n    echo \"$lines\" | cut -c 2-7\ndone\n# Cut a range of charavters. \n\nwhile read lines \ndo \n    echo \"$lines\" | cut -d $'\\t' -f 1-3\ndone\n# -d : only takes one character so use 'X'\n\nwhile read lines \ndo \n    echo $lines | cut -c 13-\ndone\n# Print from X character to the end. \n\nwhile read line\ndo\n    echo \"${line}\" | cut -d ' ' -f 4\ndone\n# 4th Word, delimeter is ' '.\n</code></pre> </li> <li> <p>Head and Tail: </p> <pre><code>head -n 20 \n# For first 20 lines of text. \n\nhead -c 20 \n# For first 20 characters of text. \n\nhead -n 22 | tail -n +12\n# First 22 characters &amp;&amp; Prints from 12th to last (22nd Character)\n</code></pre> </li> <li> <p>tr and sort : tr replaces, sort sorts (lol)</p> <pre><code>tail -n 20\n# Last 20 lines.\n\ntr '()' '[]'\n# Replace. \n\ntr -d \"(a-z)\"\n# Delete all lower-case characters. \n\ntr -s ' '\n# Squeezes spaces.\n\nsort -f\n# Sorts in alphabetical order, -f ignore case. \n\nsort -r\n# Sorts in reverse alphabetical order. \n\nsort -n \n# Sorts in numerically ascending order. \n\nsort -n -r\n# Sorts in numerically descending order. \n\nsort -r -n -k 2 -t $'\\t'\n# -r : For reverse order \n# -n : numerical sort \n# -k: column ordering \n# -t : tab separted indicator\n</code></pre> </li> <li> <p>Uniq:      <pre><code>    uniq\n    # Eliminates repetitions. \n\n    uniq -c \n    # Counts the repitions. \n</code></pre></p> </li> <li>Paste:     <pre><code>paste -d'\\t' -s\n# '-s' tells the command to write everything sequentially in one line. \n\npaste -s -d ';'\n# Pastes with a delimeter specified. \n</code></pre></li> </ol>"},{"location":"School/Math/","title":"Math","text":"<p>These are just a few of the major math notes from grade 12.</p> <p>Still haven't added all of the notes, however.</p>"},{"location":"School/Math/01_Trignometric/","title":"01 Trignometric","text":"Rule Product \\(2 \\sin x \\cos y\\) \\(\\sin(x+y) + \\sin(x-y)\\) \\(2 \\cos x \\sin y\\) \\(\\sin(x+y) - \\sin(x-y)\\) \\(2 \\cos x \\cos y\\) \\(\\cos(x+y) + \\cos(x-y)\\) \\(2 \\sin x \\sin y\\) \\(\\cos(x-y) - \\cos(x+y)\\) Sum \\(\\sin C + \\sin D\\) \\(2 \\sin \\left( \\frac{C+D}{2} \\right) \\cos \\left( \\frac{C-D}{2} \\right)\\) \\(\\sin C - \\sin D\\) \\(2 \\cos \\left( \\frac{C+D}{2} \\right) \\sin \\left( \\frac{C-D}{2} \\right)\\) \\(\\cos C + \\cos D\\) \\(2 \\cos \\left( \\frac{C+D}{2} \\right) \\cos \\left( \\frac{C-D}{2} \\right)\\) \\(\\cos C - \\cos D\\) \\(-2 \\sin \\left( \\frac{C+D}{2} \\right) \\sin \\left( \\frac{C-D}{2} \\right)\\) \\(\\tan(x+y)\\) \\(\\frac{\\tan x + \\tan y}{1 - \\tan x \\tan y}\\) \\(\\tan(x-y)\\) \\(\\frac{\\tan x - \\tan y}{1 + \\tan x \\tan y}\\) Double \\(\\sin 2x\\) \\(2 \\sin x \\cos x\\) \\(\\frac{2 \\tan x}{1 + \\tan^2 x}\\) \\(\\cos 2x\\) \\(\\cos^2 x - \\sin^2 x\\)\\(2\\cos^2 x - 1\\)\\(1 - 2 \\sin^2 x\\) \\(\\frac{1-\\tan^2 x}{1 + \\tan^2 x}\\) \\(\\tan 2x\\) \\(\\frac{2 \\tan x}{1 - \\tan^2 x}\\) \\(\\cot 2x\\) \\(\\frac{\\cot^2 x - 1}{2 \\cot x}\\) Triple \\(\\sin 3x\\) \\(3 \\sin x - 4\\sin^3 x\\) \\(\\cos 3x\\) \\(4\\cos^3 x - 3\\cos x\\) \\(\\tan 3x\\) \\(\\frac{3\\tan x - \\tan^3 x}{1 - 3 \\tan^2 x}\\) \\(\\cot 3x\\) \\(\\frac{3 \\cot x - \\cot^3 x}{1 - 3 \\cot^2 x}\\) Half \\(\\tan \\frac{x}{2}\\) \\(\\text{cosec }x - \\cot x\\) \\(\\sqrt{\\frac{1-\\cos x}{1+\\cos x}}\\) \\(\\cot \\frac{x}{2}\\) \\(\\text{cosec }x + \\cot x\\) \\(\\sqrt{\\frac{1+\\cos x}{1-\\cos x}}\\)"},{"location":"School/Math/02_Integration/","title":"02 Integration","text":"<p>For simplicy, I\u2019ve excluded</p> <ul> <li>\\(dx\\) for pre-integration</li> <li>\\(+ c\\) for post-integration</li> </ul> Pre-Integration Post-Integration Basic \\(x^n, n \\ne -1\\) \\(\\frac{x^{n+1}}{n+1}\\) \\(\\frac{1}{x}\\) \\(\\log x\\) \\(e^x\\) \\(e^x\\) \\(a^x\\) \\(\\frac{a^x}{\\log a}\\) Coefficient \\(f(ax+b)\\) \\(\\frac{F(ax + b)}{a}\\) Trignometric \\(\\sin x\\) \\(- \\cos x\\) \\(\\cos x\\) \\(\\sin x\\) \\(\\tan x\\) \\(\\log \\vert \\sec x\\vert\\) \\(-\\log\\vert \\cos x \\vert\\) \\(\\cot x\\) \\(\\log \\vert \\sin x\\vert\\) \\(-\\log\\vert \\text{cosec } x \\vert\\) \\(\\sec x\\) \\(\\log\\vert \\sec x + \\tan x\\vert\\) \\(-\\log\\vert \\sec x - \\tan x \\vert\\) \\(\\text{cosec }x\\) \\(\\log\\vert \\text{cosec } x - \\cot x\\vert\\) \\(-\\log\\vert \\text{cosec } x + \\cot x \\vert\\) \\(\\sec x \\tan x\\) \\(\\sec x\\) \\(\\text{cosec }x \\cot x\\) \\(-\\text{cosec } x\\) \\(\\sec^2 x\\) \\(\\tan x\\) \\(\\text{cosec}^2 x\\) \\(- \\cot x\\) IDK \\(\\frac{1}{\\sqrt{1-x^2}}\\) \\(\\sin^{-1} x\\) \\(-\\cos^{-1} x\\) \\(\\frac{1}{\\sqrt{1+x^2}}\\) \\(\\tan^{-1} x\\) \\(-\\cot^{-1} x\\) \\(\\frac{1}{x \\sqrt{x^2 - 1}}\\) \\(\\sec^{-1} x\\) \\(- \\text{ cosec}^{-1} x\\) Squares \\(\\frac{1}{a^2 + x^2}\\) \\(\\frac{1}{a} \\tan^{-1} \\left( \\frac{x}{a} \\right)\\) \\(\\frac{1}{x^2 - a^2}\\) \\(\\frac{1}{2a} \\log\\left\\vert \\frac{x-a}{x+a}\\right \\vert\\) \\(\\frac{1}{a^2 - x^2}\\) \\(\\frac{1}{2a} \\log\\left\\vert \\frac{a+x}{a-x}\\right \\vert\\) Den Roots \\(\\frac{1}{\\sqrt{a^2 - x^2}}\\) \\(\\sin^{-1} \\left( \\frac{x}{a} \\right)\\) \\(\\frac{1}{\\sqrt{x^2 + a^2}}\\) \\(\\log\\left\\vert  x + \\sqrt{x^2 + a^2} \\right \\vert\\) \\(\\frac{1}{x \\sqrt{x^2 - a^2}}\\) \\(\\frac{1}{a} \\sec^{-1} \\left(\\frac{x}{a}\\right)\\) Num Roots \\(\\sqrt{a^2 - x^2}\\) \\(\\frac{x}{2} \\sqrt{a^2 - x^2} + \\frac{a^2}{2} \\sin^{-1}\\left(\\frac{x}{a}\\right)\\) \\(\\sqrt{a^2 + x^2}\\) \\(\\frac{x}{2} \\sqrt{a^2 + x^2} + \\frac{a^2}{2} \\log \\vert x + \\sqrt{a^2 + x^2}  \\vert\\) \\(\\sqrt{x^2 - a^2}\\) \\(\\frac{x}{2} \\sqrt{x^2 - a^2} - \\frac{a^2}{2} \\log \\vert x + \\sqrt{x^2 - a^2} \\vert\\) IDK \\(e^x \\Big(f(x) + f'(x) \\Big)\\) \\(e^x f(x)\\) \\(x \\Big(f(x) + f'(x) \\Big)\\) \\(x f(x)\\) Parts/ILATE \\(\\int (uv) dx\\) \\(u \\int vdx + \\int \\left(u' {\\small \\int} vdx \\right)\\)"},{"location":"School/Math/02_Integration/#partial-fractions","title":"Partial Fractions","text":"Function Partial Fraction \\(\\frac{px+q}{(x-a)(x-b)}\\) \\(\\frac{A}{(x-a)} + \\frac{B}{(x-b)}\\) \\(\\frac{px^2 + qx + r}{(x-a)(x-b)(x-c)}\\) \\(\\frac{A}{(x-a)} + \\frac{B}{(x-b)} + \\frac{C}{(x-c)}\\) \\(\\frac{px+q}{(x-a)^3}\\) \\(\\frac{A}{(x-a)} + \\frac{B}{(x-a)^2} + \\frac{C}{(x-a)^3}\\) \\(\\frac{px^2 + qx + r}{(x-a)^2 (x-b)}\\) \\(\\frac{A}{(x-a)} + \\frac{B}{(x-a)^2} + \\frac{C}{(x-b)}\\) \\(\\frac{px^2 + qx + r}{(x-a) (x^2 + bx + c)}\\) \\(\\frac{A}{(x-a)} + \\frac{Bx + C}{(x^2 + bx + c)}\\)"},{"location":"School/Math/02_Integration/#properties","title":"Properties","text":"\\(\\left(\\int f(x) \\cdot dx \\right)'\\) \\(f(x)\\) \\(\\int f'(x) dx\\) \\(f(x) + c\\) \\(\\int k \\cdot f(x) dx\\) \\(k \\int f(x) dx\\) \\(\\int \\Big(f(x) \\pm g(x) \\Big) \\ dx\\) \\(\\int f(x) \\ dx \\pm \\int g(x) \\ dx\\)"},{"location":"Tools/","title":"Tools","text":""},{"location":"Tools/misc/","title":"Misc","text":""},{"location":"Tools/misc/#mirror-a-website","title":"Mirror a website","text":"<pre><code>wget -m \"https://otexts.com/fpp3\"\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Model_Specification/","title":"Model Specification","text":""},{"location":"Tools/AI%20%26%20Data/Model_Specification/#base","title":"Base","text":"<pre><code>class Math:\n    def __str__(self):\n        return self.latex()\n    def __repr__(self):\n        return str(self)\n    def equation(self):\n        return \"\"\n    def latex(self):\n        return \"\"\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Model_Specification/#cost-functions","title":"Cost Functions","text":"<pre><code>class LogCosh(Math):\n    def cost(self, pred, true, sample_weight, delta=None):\n        error = pred - true\n\n        loss = np.log(np.cosh(error))\n\n        cost = np.mean(\n            sample_weight * loss\n        )\n        return cost\n\n    def latex(self):\n        return r\"\"\"\n        $$\n        \\Large\n        \\text{Mean Log Cosh}: \\text{mean} \\Big\\{ \\log \\left \\vert \\ \\cosh (u_i) \\ \\right \\vert \\Big \\} \\\\\n        \\text{where } u_i = \\text{ Prediction - True}\n        $$\n        \"\"\"\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Model_Specification/#models","title":"Models","text":"<pre><code>from utils.math import *\n\nfrom inspect import getfullargspec, getsource\nfrom string import Template\n</code></pre> <pre><code>class Model(Math):\n    def __init__(self):\n        name = (\n            self.__class__.__name__\n            .replace(\"_\", \" \")\n            .replace(\"Model\", \"\")\n            .strip()\n        )\n        self.__class__.__name__ = name\n        self.__name__ = name\n\n        args = getfullargspec(self.equation).args\n        self.args = tuple([arg for arg in args if arg not in [\"self\", \"x\"]])\n        self.k = len(self.args)\n\n        self.defaults = list(getfullargspec(self.equation).defaults)\n\n        self.param_initial_guess = [\n            x[0]\n            for x\n            in self.defaults\n        ]\n\n        self.param_initial_guess = (\n            [0 for arg in self.args]\n            if (self.param_initial_guess is None) or (self.k != len(self.param_initial_guess))\n            else self.param_initial_guess\n        )\n\n        self.param_bounds = [\n            x[1]\n            for x\n            in self.defaults\n        ]\n\n        self.param_bounds = (\n            [(None, None) for arg in self.args]\n            if (self.param_bounds is None) or (self.k != len(self.param_bounds))\n            else self.param_bounds\n        )\n\n        if \"constraints\" not in dir(self):\n            self.constraints = []\n\n        self.fitted_coeff = None\n\n    def set_fitted_coeff(self, *fitted_coeff):\n        self.fitted_coeff = fitted_coeff\n    def __str__(self):\n        fillers = (\n            self.args\n            if self.fitted_coeff is None\n            else self.fitted_coeff\n        )\n\n        fillers = dict()\n\n        if self.fitted_coeff is None:\n            a, b = self.args, self.args\n        else:\n            a, b = self.args, self.fitted_coeff\n\n        for key, value in zip(a, b):\n            fillers[key] = value\n\n        equation = Template(self.latex()).substitute(fillers).replace(\"$\", \"$$\")\n        # st.code(string)\n\n        return rf\"\"\"\n        $$\n        \\text{{ {self.__class__.__name__} }}\n        $$\n\n        {equation}\n        \"\"\"\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Model_Specification/#example","title":"Example","text":"<pre><code>class Zero_Order(Model):\n    def equation(\n        self, x,\n        k = [0, (0, None)]\n    ):  # hypothesis\n        ca = x[\"Previous_Reading\"]\n        # ta = x[\"Time_Point_Diff\"]\n        # tb = x[\"Time_Point\"]\n        t = x[\"Time_Point_Diff\"]\n        return np.clip(\n            (\n                ca - k * t\n            ),\n            0,\n            np.inf\n        )\n    def latex(self):\n        return r\"\"\"\n        $$\n        \\begin{aligned}\n        {\\huge c_t} &amp;\n        {\\huge = c_0 - \\textcolor{hotpink}{$k} t} \\\\\n        \\\\\n        c_t &amp;= \\text{Concentration} \\\\\n        c_0 &amp;= \\text{Initial Concentration} \\\\\n        t &amp;= \\text{Time (weeks)}\n        \\end{aligned}\n        $$\n        \"\"\"\n\nclass First_Order(Model):\n    def equation(\n        self, x,\n        k = [0, (0, None)]\n    ):  # hypothesis\n        ca = x[\"Previous_Reading\"]\n        # ta = x[\"Time_Point_Diff\"]\n        # tb = x[\"Time_Point\"]\n        t = x[\"Time_Point_Diff\"]\n        return (\n            ca\n            *\n            np.exp(\n                -k\n                *\n                t # (tb-ta)\n            )\n        )\n    def latex(self):\n        return r\"\"\"\n        $$\n        \\begin{aligned}\n        {\\huge c_t} &amp;\n        {\\huge = c_0 \\cdot\n        \\exp \\{\n            \\overbrace{- \\textcolor{hotpink}{$k}}^{\\small \\mathclap{\\small \\mathclap{\\text{Rate const}}}} t\n        \\} } \\\\\n        \\\\\n        c_t &amp;= \\text{Concentration} \\\\\n        c_0 &amp;= \\text{Initial Concentration} \\\\\n        t &amp;= \\text{Time (weeks)}\n        \\end{aligned}\n        $$\n        \"\"\"\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Facebook_Prophet/","title":"Facebook Prophet","text":""},{"location":"Tools/AI%20%26%20Data/Facebook_Prophet/#limitations","title":"Limitations","text":"<ul> <li>Basically glorified curve-fitting to time variable</li> <li>Tends to overfit</li> <li>Does not extrapolate well</li> </ul>"},{"location":"Tools/AI%20%26%20Data/Facebook_Prophet/#improve-fit","title":"Improve <code>.fit()</code>","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport time\nimport datetime\nfrom prophet import Prophet\nimport matplotlib.pyplot as plt\nfrom scipy.stats import pearsonr\n\nfrom batch_elastic_net import BatchElasticNetRegression\n\n\ndef make_sine_wave(length: int, n_cycles: int):\n    \"\"\"\n    Makes a sine wave given some length and the number of cycles it should go through in that period\n    \"\"\"\n    samples = np.linspace(0, length, length)\n    return np.sin(2 * np.pi * n_cycles * samples)\n\n\ndef generate_dataset(n_items):\n    \"\"\"\n    Generates a time series dataset with weekly frequency for two years. Randomly assigns the yearly, monthly and\n    trend values for each item\n    \"\"\"\n    year_in_weeks = 104\n    yealy_s = make_sine_wave(year_in_weeks, 2)\n    monthly_s = make_sine_wave(year_in_weeks, year_in_weeks / 24)\n    trend = np.arange(year_in_weeks) / year_in_weeks\n    all_ys = []\n    for i in range(n_items):\n        d = (np.stack([yealy_s, monthly_s, trend], axis=1) * np.random.rand(3)).sum(axis=1) + np.random.rand(year_in_weeks)\n        all_ys.append(d + (np.random.rand(len(d))-0.45).cumsum())\n    return pd.DataFrame(zip(*all_ys), index = pd.date_range(datetime.datetime(2020, 1, 1), freq='w', periods=len(d)))\n\n\ndef get_changepoint_idx(length, n_changepoints, changepoint_range=0.8):\n    \"\"\"\n    Finds the indices of slope change-points using Prophet's logic: assign them uniformly of the first changepoint_range\n    percentage of the data\n    \"\"\"\n    hist_size = int(np.floor(length * changepoint_range))\n    return np.linspace(0, hist_size - 1, n_changepoints+1).round().astype(int)[1:]\n\n\ndef make_changepoint_features(n, changes_idx):\n    \"\"\"\n    Creates initial slope and slope change-points features given a length of data and locations of indices.\n    The features are 0s for the first elements until their idx is reached, and then they move linearly upwards.\n    These features can be used to model a time series with an initial slope and the deltas of change-points.\n    \"\"\"\n    linear = np.arange(n).reshape(-1,1)\n    feats = [linear]\n    for i in changes_idx:\n        slope_feat = np.zeros(n)\n        slope_feat[i:] = np.arange(0, n-i)\n        slope_feat = slope_feat.reshape(-1,1)\n        feats.append(slope_feat)\n    feat = np.concatenate(feats, axis=1)\n    return feat\n\n\ndef run_prophet():\n    t = time.time()\n    all_prophets_datasets_forecasts = {}\n    for name, dataset in data_sets.items():\n        all_p_forecast = []\n        for i in range(dataset.shape[1]):\n            ds = dataset.iloc[:, i].reset_index()\n            ds.columns = ['ds', 'y']\n            # if uncertainty samples is not None it will take way more time\n            m = Prophet(n_changepoints=n_changepoints, changepoint_prior_scale=change_prior, growth='linear',\n                        uncertainty_samples=None,\n                        yearly_seasonality=True, weekly_seasonality=False, seasonality_prior_scale=seasonality_prior)\n            m.fit(ds)\n            forecast = m.predict(ds)\n            all_p_forecast.append(forecast.yhat)\n        all_prophets_datasets_forecasts[name] = pd.DataFrame(zip(*all_p_forecast), index=ds.ds)\n\n    return all_prophets_datasets_forecasts, time.time() - t\n\n\ndef run_batch_linear():\n    big_num = 20.  # used as std of prior when it should be uninformative\n    p = Prophet()\n    t = time.time()\n    all_BatchLinear_datasets_forecasts = {}\n    for name, dataset in data_sets.items():\n        dates = pd.Series(dataset.index)\n        dataset_length = len(dataset)\n        idx = get_changepoint_idx(dataset_length, n_changepoints)\n\n        seasonal_feat = p.make_seasonality_features(dates, 365.25, 10, 'yearly_sine')\n        changepoint_feat = make_changepoint_features(dataset_length, idx) / dataset_length\n        feat = np.concatenate([changepoint_feat, seasonal_feat], axis=1)\n\n        n_changepoint_feat = changepoint_feat.shape[1] - 1\n        # laplace prior only on changepoints (seasonals get big_num, to avoid l1 regularization on it)\n        l1_priors = np.array([big_num] + [change_prior] * n_changepoint_feat + [big_num] * seasonal_feat.shape[1])\n        # normal prior on initial slope and on seasonals, and a big_num on changepoints to avoid l2 regularization\n        l2_priors = np.array([5] + [big_num] * n_changepoint_feat + [seasonality_prior] * seasonal_feat.shape[\n            1])  # normal prior only on seasonal\n\n        # this is how Prophet scales the data before fitting - divide by max of each item\n        scale = dataset.max()\n        scaled_y = dataset / scale\n\n        blr = BatchElasticNetRegression()\n        blr.fit(feat, scaled_y, l1_reg_params=l1_priors, l2_reg_params=l2_priors, as_bayesian_prior=True, verbose=True,\n                iterations=1500)\n\n        # get the predictions for the train\n        all_BatchLinear_datasets_forecasts[name] = pd.DataFrame(blr.predict(feat) * scale.values, index=dates)\n\n    return all_BatchLinear_datasets_forecasts, time.time() - t\n\n\nif __name__ == '__main__':\n    data_files_names = ['d1', 'd2', 'M5_sample']\n    data_sets = {name: pd.read_csv(f'data_files/{name}.csv', index_col=0, parse_dates=True) for name in data_files_names}\n    data_sets['randomly_generated'] = generate_dataset(500)\n\n    # can play with these params for both predictors\n    change_prior = 0.5\n    # the seasonality_prior is an uninformative prior (hardly any regularization), which is the default for Prophet and usually does not require changing\n    seasonality_prior = 10\n    n_changepoints = 15\n\n    all_prophets_datasets_forecasts, prophet_time = run_prophet()\n    all_BatchLinear_datasets_forecasts, batch_time = run_batch_linear()\n\n    print(f'total number of items: {sum([x.shape[1] for x in data_sets.values()])}')\n    print(f'Prophet time: {round(prophet_time, 2)}; batch time: {round(batch_time, 2)}')\n\n    # plot examples from datasets (copy to notebook and repeat for different items and datasets)\n    name = 'd1'\n    batch_preds = all_BatchLinear_datasets_forecasts[name]\n    prophet_preds = all_prophets_datasets_forecasts[name]\n    orig_data = data_sets[name]\n\n    i = np.random.randint(0, orig_data.shape[1])\n    orig_data.iloc[:, i].plot(label='target')\n    batch_pred = batch_preds.iloc[:, i]\n    prophet_pred = prophet_preds.iloc[:, i]\n    prophet_pred.plot(label='prophet')\n    batch_pred.plot(label='my_batch')\n    plt.title(f'Pearson {round(pearsonr(batch_pred, prophet_pred)[0], 3)}')\n    plt.legend()\n    plt.show()\n\n    # mean pearson\n    all_corrs = {}\n    for name in data_sets.keys():\n        batch_preds = all_BatchLinear_datasets_forecasts[name]\n        prophet_preds = all_prophets_datasets_forecasts[name]\n        corrs = []\n        for i in range(prophet_preds.shape[1]):\n            corrs.append(pearsonr(batch_preds.iloc[:, i], prophet_preds.iloc[:, i])[0])\n        all_corrs[name] = np.mean(corrs)\n    print(all_corrs)\n</code></pre> <p>IDK</p> <pre><code>import numpy as np\nimport torch\nimport torch.optim as optim\nfrom typing import Optional, Union\n\nBIG_STD = 20.  # used as std of prior when it should be uninformative (when we do not wish to regularize at all)\n\n\ndef to_tensor(x):\n    return torch.from_numpy(np.array(x)).float()\n\n\nclass BatchElasticNetRegression(object):\n    \"\"\"\n    Elastic net for the case where we have multiple targets (y), all to be fitted with the same features (X).\n    Learning all items in parallel, in a single \"network\" is more efficient then iteratively fitting a regression for\n    each target.\n    Allows to set different l1 and l2 regularization params for each of the features.\n    Can also be used to estimate the MAP of a Bayesian regression with Laplace or Normal priors instead of L1 and L2.\n    \"\"\"\n    def __init__(self):\n        self.coefs = None\n        self.intercepts = None\n\n    def fit(self,\n            X, y,\n            l1_reg_params: Optional[Union[np.array, float]] = None,\n            l2_reg_params: Optional[Union[np.array, float]] = None,\n            as_bayesian_prior=False, iterations=500, verbose=True, lr_rate=0.1):\n        \"\"\"\n        Fits multiple regressions. Both X and y are 2d matrices, where X is the common features for all the targets,\n        and y contains all the concatenated targets.\n        If as_bayesian_prior==False then the l1 and l2 reg params are regularization params\n        If as_bayesian_prior==True then l1 is treated as the std of the laplace prior and l2 as the std for the normal\n        prior.\n        The reg params / std of priors can either be a single value for all features, or set a different regularization\n        or prior for each feature separately. e.g. if we have 3 features, l1_reg_params can be [0.5, 1.2, 0] to set\n        regularization for each.\n\n        TODO:\n        Add normalization before fitting\n        Requires more work on the optimizer to be faster\n        \"\"\"\n        n_items = y.shape[1]\n        k_features = X.shape[1]\n        n_samples = X.shape[0]\n\n        # TODO: if l1_reg_params is None just don't calculate this part of the loss, instead of multiplying by 0\n        if l1_reg_params is None:\n            l1_reg_params = BIG_STD if as_bayesian_prior else 0.\n        if type(l1_reg_params) == float:\n            l1_reg_params = [l1_reg_params] * k_features\n        if l2_reg_params is None:\n            l2_reg_params = BIG_STD if as_bayesian_prior else 0.\n        if type(l2_reg_params) == float:\n            l2_reg_params = [l2_reg_params] * k_features\n\n        assert len(l1_reg_params) == len(l2_reg_params) == k_features, 'Regularization values must match X.shape[1]'\n        if as_bayesian_prior:\n            assert 0 not in l1_reg_params and 0 not in l2_reg_params, 'Cannot have 0 prior'\n\n        # convert to tensors and set initial params\n        t_features = to_tensor(X)\n        t_target = to_tensor(y)\n        learned_coefs = torch.rand(k_features, n_items, requires_grad=True)\n        learned_intercepts = torch.rand(n_items, requires_grad=True)\n        # TODO: or auto-estimate initial sigma based on data std?\n        est_sigma = torch.ones(n_items)\n        if as_bayesian_prior:\n            # If the params are priors then they must become a matrix, not a simple vector - because the conversion\n            # depends on the sigma of errors for each target y. The actual regularization params will be different\n            # for each item based on its sigma.\n            t_l1_reg_params = to_tensor(np.stack([l1_reg_params] * n_items, axis=1))\n            l1_alpha = self.calc_l1_alpha_from_prior(est_sigma, t_l1_reg_params, n_samples)\n            t_l2_reg_params = to_tensor(np.stack([l2_reg_params] * n_items, axis=1))\n            l2_alpha = self.calc_l2_alpha_from_prior(est_sigma, t_l2_reg_params, n_samples)\n        else:\n            l1_alpha = to_tensor(l1_reg_params)\n            l2_alpha = to_tensor(l2_reg_params)\n\n        # TODO: add scheduler for learning rate\n        optimizer = optim.Adam([learned_coefs, learned_intercepts], lr_rate)\n\n        for i in range(iterations):\n            optimizer.zero_grad()\n            res = torch.matmul(t_features, learned_coefs) + learned_intercepts\n            diff_loss = (1 / (2 * n_samples)) * ((res - t_target) ** 2).sum(axis=0)\n\n            if as_bayesian_prior:\n                reg_loss = (l1_alpha * learned_coefs.abs()).sum(axis=0) + (l2_alpha * learned_coefs ** 2).sum(axis=0)\n            else:\n                reg_loss = torch.matmul(l1_alpha, learned_coefs.abs()) + torch.matmul(l2_alpha, learned_coefs ** 2)\n\n            loss = (diff_loss + reg_loss).sum()\n\n            loss.backward()\n            optimizer.step()\n            if as_bayesian_prior and i % 50 == 0:\n                # if the params are the priors - we must convert them to the equivalent l1/l2 loss params.\n                # This conversion depends on the final sigma of errors of the forecast, which is unknown until we\n                # have a forecast using those same params... We iteratively improve our estimate of sigma and\n                # re-compute the corresponding regularization params based on those sigmas.\n                # The sigma is per target in y, therefore the l1/l2 params are per item.\n                est_sigma = (res - t_target).std(axis=0).detach()\n                l1_alpha = self.calc_l1_alpha_from_prior(est_sigma, t_l1_reg_params, n_samples)\n                l2_alpha = self.calc_l2_alpha_from_prior(est_sigma, t_l2_reg_params, n_samples)\n\n            if i % 50 == 0 and verbose:\n                print(loss)\n            # TODO: early stopping if converges\n\n        self.coefs = learned_coefs.detach().numpy()\n        self.intercepts = learned_intercepts.detach().numpy()\n\n    def predict(self, X):\n        return X @ self.coefs + self.intercepts\n\n    @staticmethod\n    def calc_l1_alpha_from_prior(est_sigma, b_prior, n_samples):\n        \"\"\"\n        Converts from the std of a Laplace prior to the equivalent L1 regularization param.\n        The conversion formula is divided by 2*n_samples since we divided the diff_loss by 2*n_samples as well,\n        to match sklearn's implementation of Lasso.\n        \"\"\"\n        return est_sigma ** 2 / (b_prior * n_samples)\n\n    @staticmethod\n    def calc_l2_alpha_from_prior(est_sigma, b_prior, n_samples):\n        return est_sigma ** 2 / (b_prior ** 2 * 2 * n_samples)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Facebook_Prophet/#improve-predict","title":"Improve <code>.predict()</code>","text":"<p>01/25/23 edit: FB engineers integrated the solution in this post into the package in version release 1.1.2. Due to some implementation differences it is still slightly slower than the solution in the end of this post, but not significantly.</p> <pre><code>import numpy as np\nimport pandas as pd\n\nfrom fbprophet import Prophet\n\n\ndef _make_historical_mat_time(deltas, changepoints_t, t_time, n_row=1):\n    \"\"\"\n    Creates a matrix of slope-deltas where these changes occured in training data according to the trained prophet obj\n    \"\"\"\n    diff = np.diff(t_time).mean()\n    prev_time = np.arange(0, 1 + diff, diff)\n    idxs = []\n    for changepoint in changepoints_t:\n        idxs.append(np.where(prev_time &gt; changepoint)[0][0])\n    prev_deltas = np.zeros(len(prev_time))\n    prev_deltas[idxs] = deltas\n    prev_deltas = np.repeat(prev_deltas.reshape(1, -1), n_row, axis=0)\n    return prev_deltas, prev_time\n\n\ndef prophet_logistic_uncertainty(\n    mat: np.ndarray,\n    deltas: np.ndarray,\n    prophet_obj: Prophet,\n    cap_scaled: np.ndarray,\n    t_time: np.ndarray,\n):\n    \"\"\"\n    Vectorizes prophet's logistic growth uncertainty by creating a matrix of future possible trends.\n    \"\"\"\n\n    def ffill(arr):\n        mask = arr == 0\n        idx = np.where(~mask, np.arange(mask.shape[1]), 0)\n        np.maximum.accumulate(idx, axis=1, out=idx)\n        return arr[np.arange(idx.shape[0])[:, None], idx]\n\n    k = prophet_obj.params[\"k\"][0]\n    m = prophet_obj.params[\"m\"][0]\n    n_length = len(t_time)\n    #  for logistic growth we need to evaluate the trend all the way from the start of the train item\n    historical_mat, historical_time = _make_historical_mat_time(deltas, prophet_obj.changepoints_t, t_time, len(mat))\n    mat = np.concatenate([historical_mat, mat], axis=1)\n    full_t_time = np.concatenate([historical_time, t_time])\n\n    #  apply logistic growth logic on the slope changes\n    k_cum = np.concatenate((np.ones((mat.shape[0], 1)) * k, np.where(mat, np.cumsum(mat, axis=1) + k, 0)), axis=1)\n    k_cum_b = ffill(k_cum)\n    gammas = np.zeros_like(mat)\n    for i in range(mat.shape[1]):\n        x = full_t_time[i] - m - np.sum(gammas[:, :i], axis=1)\n        ks = 1 - k_cum_b[:, i] / k_cum_b[:, i + 1]\n        gammas[:, i] = x * ks\n    # the data before the -n_length is the historical values, which are not needed, so cut the last n_length\n    k_t = (mat.cumsum(axis=1) + k)[:, -n_length:]\n    m_t = (gammas.cumsum(axis=1) + m)[:, -n_length:]\n    sample_trends = cap_scaled / (1 + np.exp(-k_t * (t_time - m_t)))\n    # remove the mean because we only need width of the uncertainty centered around 0\n    # we will add the width to the main forecast - yhat (which is the mean) - later\n    sample_trends = sample_trends - sample_trends.mean(axis=0)\n    return sample_trends\n\n\ndef _make_trend_shift_matrix(mean_delta: float, likelihood: float, future_length: float, k: int = 10000) -&gt; np.ndarray:\n    \"\"\"\n    Creates a matrix of random trend shifts based on historical likelihood and size of shifts.\n    Can be used for either linear or logistic trend shifts.\n    Each row represents a different sample of a possible future, and each column is a time step into the future.\n    \"\"\"\n    # create a bool matrix of where these trend shifts should go\n    bool_slope_change = np.random.uniform(size=(k, future_length)) &lt; likelihood\n    shift_values = np.random.laplace(0, mean_delta, size=bool_slope_change.shape)\n    mat = shift_values * bool_slope_change\n    n_mat = np.hstack([np.zeros((len(mat), 1)), mat])[:, :-1]\n    mat = (n_mat + mat) / 2\n    return mat\n\n\ndef add_prophet_uncertainty(\n    prophet_obj: Prophet,\n    forecast_df: pd.DataFrame,\n    using_train_df: bool = False,\n):\n    \"\"\"\n    Adds yhat_upper and yhat_lower to the forecast_df used by fbprophet, based on the params of a trained prophet_obj\n    and the interval_width.\n    Use using_train_df=True if the forecast_df is not for a future time but for the training data.\n    \"\"\"\n    assert prophet_obj.history is not None, \"Model has not been fit\"\n    assert \"yhat\" in forecast_df.columns, \"Must have the mean yhat forecast to build uncertainty on\"\n    interval_width = prophet_obj.interval_width\n\n    if using_train_df:  # there is no trend-based uncertainty if we're only looking on the past where trend is known\n        sample_trends = np.zeros(10000, len(forecast_df))\n    else:  # create samples of possible future trends\n        future_time_series = ((forecast_df[\"ds\"] - prophet_obj.start) / prophet_obj.t_scale).values\n        single_diff = np.diff(future_time_series).mean()\n        change_likelihood = len(prophet_obj.changepoints_t) * single_diff\n        deltas = prophet_obj.params[\"delta\"][0]\n        n_length = len(forecast_df)\n        mean_delta = np.mean(np.abs(deltas)) + 1e-8\n        if prophet_obj.growth == \"linear\":\n            mat = _make_trend_shift_matrix(mean_delta, change_likelihood, n_length, k=10000)\n            sample_trends = mat.cumsum(axis=1).cumsum(axis=1)  # from slope changes to actual values\n            sample_trends = sample_trends * single_diff  # scaled by the actual meaning of the slope\n        elif prophet_obj.growth == \"logistic\":\n            mat = _make_trend_shift_matrix(mean_delta, change_likelihood, n_length, k=1000)\n            cap_scaled = (forecast_df[\"cap\"] / prophet_obj.y_scale).values\n            sample_trends = prophet_logistic_uncertainty(mat, deltas, prophet_obj, cap_scaled, future_time_series)\n        else:\n            raise NotImplementedError\n\n    # add gaussian noise based on historical levels\n    sigma = prophet_obj.params[\"sigma_obs\"][0]\n    historical_variance = np.random.normal(scale=sigma, size=sample_trends.shape)\n    full_samples = sample_trends + historical_variance\n    # get quantiles and scale back (prophet scales the data before fitting, so sigma and deltas are scaled)\n    width_split = (1 - interval_width) / 2\n    quantiles = np.array([width_split, 1 - width_split]) * 100  # get quantiles from width\n    quantiles = np.percentile(full_samples, quantiles, axis=0)\n    # Prophet scales all the data before fitting and predicting, y_scale re-scales it to original values\n    quantiles = quantiles * prophet_obj.y_scale\n\n    forecast_df[\"yhat_lower\"] = forecast_df.yhat + quantiles[0]\n    forecast_df[\"yhat_upper\"] = forecast_df.yhat + quantiles[1]\n</code></pre> <pre><code>p = Prophet(uncertainty_samples=None) # tell Prophet not to create the interval by itself\np = p.fit(training_df)\n\n# set to your number of periods and freq\nforecast_df = p.make_future_dataframe(periods=10, freq='W', include_history=False)\ntraining_df = p.predict(training_df)\nforecast_df = p.predict(forecast_df)\nadd_prophet_uncertainty(p, training_df, using_train_df=True)\nadd_prophet_uncertainty(p, forecast_df)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/IBIS/","title":"IBIS","text":"<p>Ibis is the portable Python dataframe library:</p> <ul> <li>Fast local dataframes (via DuckDB by default)</li> <li>Lazy dataframe expressions</li> <li>Interactive mode for iterative data exploration</li> <li>Compose Python dataframe and SQL code</li> <li>Use the same dataframe API for 20+ backends</li> <li>Iterate locally and deploy remotely by changing a single line of code</li> </ul>"},{"location":"Tools/AI%20%26%20Data/IBIS/#backends","title":"Backends","text":"<p>Ibis supports 20+ backends:</p> <ul> <li>Apache Arrow DataFusion</li> <li>Apache Druid</li> <li>Apache Flink</li> <li>Apache Impala</li> <li>Apache PySpark</li> <li>BigQuery</li> <li>ClickHouse</li> <li>Dask</li> <li>DuckDB</li> <li>Exasol</li> <li>MySQL</li> <li>Oracle</li> <li>pandas</li> <li>Polars</li> <li>PostgreSQL</li> <li>RisingWave</li> <li>SQL Server</li> <li>SQLite</li> <li>Snowflake</li> <li>Trino</li> </ul>"},{"location":"Tools/AI%20%26%20Data/Lux/","title":"Lux","text":"<p>Python library that facilitate fast and easy data exploration by automating the visualization and data analysis process. By simply printing out a dataframe in a Jupyter notebook, Lux recommends a set of visualizations highlighting interesting trends and patterns in the dataset. Visualizations are displayed via an interactive widget that enables users to quickly browse through large collections of visualizations and make sense of their data.</p> <p>Demo </p>"},{"location":"Tools/AI%20%26%20Data/Numpy/","title":"Numpy","text":""},{"location":"Tools/AI%20%26%20Data/Numpy/#references","title":"References","text":"<ul> <li> https://www.youtube.com/watch?v=GKsCWivmlHg</li> </ul>"},{"location":"Tools/AI%20%26%20Data/Numpy/01/","title":"01","text":""},{"location":"Tools/AI%20%26%20Data/Numpy/01/#import","title":"Import","text":"<pre><code>import numpy as np\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Numpy/01/#basics","title":"Basics","text":"<pre><code>np.array([1, 2, 3, 4, 5])\nnp.arange(1, 100, 10) ## start, step, step\nnp.linspace(1, 100, 10) ## start, step, no of values\n\n## idk\nnp.zeros(10)\nnp.ones(10)\n\n## random\nnp.random.random(10)\nnp.random.randn(10)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Numpy/01/#array-operations","title":"Array Operations","text":"<pre><code>## Element-wise\na+3\n1/a\n\n## Boolean\na &gt; 4\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Numpy/01/#indexing","title":"Indexing","text":"<pre><code>a[2]\n\na[2:]\na[-10:]\n\na[:10]\na[:-10]\n\na[::2] ## even rows\na[1::2] ## odd rows\n\n\n## Masking\na[a &gt; 4]\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Numpy/01/#npvectorize","title":"np.vectorize","text":"<p>Kinda like a for loop</p> <pre><code>names = [\"Thahir\", \"Azhar\"]\nfirst_letter = np.vectorize(lambda x: x[0])(names) \n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Numpy/01/#stats","title":"Stats","text":"<pre><code>np.mean(a)\nnp.median(a)\nnp.std(a)\nnp.quantile(a, 0.90)\nnp.percenile(a, 90)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Numpy/01/#calculus","title":"Calculus","text":"<pre><code>## analytic calculus (for symbolic, use sympy)\ndydx = np.gradient(y, x )\ny_int = np.cumsum(y) * (x[1]-x[0])\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Numpy/01/#multi-dimensional","title":"Multi-Dimensional","text":"<pre><code>a = np.array([\n  [1, 2, 3],\n  [4, 5, 6]\n])\na = np.random.randn(3, 3)\n\na.ravel() ## returns a 1d array\n\na[0] ## first row\na[:,0] ## first column\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Numpy/01/#mesh-grid","title":"Mesh Grid","text":"<pre><code>xv, yv = np.meshgrid(x, y)\nzv = xv**2 + yv**2\nplt.contourf(xv, yv, zv, levels=100)\nplt.colorbar()\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Numpy/01/#linear-algebra","title":"Linear Algebra","text":""},{"location":"Tools/AI%20%26%20Data/Numpy/01/#matrix","title":"Matrix","text":"<pre><code>  a.T\n  a*b ## element-wise operator\n  a@b ## matrix multiplication\n  a.dot(b)\n  a.cross(b)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Numpy/01/#solve-systems-of-equations","title":"Solve systems of equations","text":"<pre><code>  a = np.array([\n    [3, 2, 1],\n    [5, -5, 4],\n    [6, 0, 1]\n  ])\n  b = np.array([\n    4,\n    3,\n    0\n  ])\n\n  x = np.linalg.solve(a, b) ## ax = b\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Numpy/01/#eigenvalues","title":"Eigenvalues","text":"<pre><code>  temp = np.linalg.eig(A)\n  eigen_values = temp[0]\n  eigen_vector = temp[1][:, 0]\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Numpy/01/#find-replace","title":"Find-Replace","text":"<p><code>if</code></p> <pre><code>  prediction['Rating'] = np.where(\n    prediction['Rating'].to_numpy() &gt; 100,\n    100,\n    prediction['Rating'].to_numpy()\n  )\n</code></pre> <p><code>if-else</code></p> <pre><code>  prediction['Rating'] = np.where(\n    prediction['Rating'].to_numpy() &gt; 100,\n    100,\n    0\n  )\n</code></pre> <p><code>if-elseif-else</code></p> <pre><code>  conditions = [\n    prediction['Rating'].to_numpy() &gt; 100,\n    prediction['Rating'].to_numpy() &gt; 50,\n    prediction['Rating'].to_numpy() &gt; 20\n  ]\n\n  values = [\n    100,\n    50,\n    20  \n  ]\n\n  default = 0\n\n  prediction['Rating'] = np.select(\n    conditions,\n    values,\n    default = default\n  )\n</code></pre> <p>nested</p> <pre><code>  conditions = [\n    (prediction['Rating'].to_numpy() &gt; 100 &amp; prediction['Rating'].to_numpy() % 2 == 0),\n    (prediction['Rating'].to_numpy() &gt; 100 &amp; prediction['Rating'].to_numpy() % 3 == 0),\n    (prediction['Rating'].to_numpy() &gt; 100 &amp; prediction['Rating'].to_numpy() % 4 == 0),\n\n    prediction['Rating'].to_numpy() &gt; 50,\n    prediction['Rating'].to_numpy() &gt; 20\n  ]\n\n  values = [\n    102,\n    103,\n    104,\n\n    50,\n    20  \n  ]\n\n  default = 0\n\n  prediction['Rating'] = np.select(\n    conditions,\n    values,\n    default = default\n  )\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Numpy/01/#rounding","title":"Rounding","text":""},{"location":"Tools/AI%20%26%20Data/Numpy/01/#round-to-integer","title":"Round to Integer","text":"<pre><code>  np.around(prediction)\n\n  ## instead of\n  ## prediction = ( round(element) for element in prediction )\n</code></pre> <p>Round to \\(n\\) places</p> <pre><code>  np.around(prediction, n)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Numpy/01/#read-data","title":"Read data","text":"<pre><code>data = np.loadtxt(\n  \"./data.csv\",\n  dtype = \"object\",\n  delimiter = \",\",\n  unpack = True,\n  skiprows = 1 \n)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Numpy/01/#save","title":"Save","text":"<pre><code>np.savetxt(\n    filename + \".csv\",\n  data,\n  delimiter = \",\",\n  fmt = \"%d\",\n  header = \"Col1, Col2\"\n)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/OpenCV/","title":"OpenCV","text":"<pre><code>import cv2 as cv\nimprot numpy as np\nimport matplotlib.pyplot as plt\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/OpenCV/#read-image","title":"Read Image","text":"<pre><code>img = cv.imread(\n  \"/fruits.jpg\",\n  cv.IMREAD_GRAYSCALE # COLOR, GRAYSCALE, UNCHANGED\n) # numpy array\n</code></pre> <pre><code># x, y, channel\nimg[:, :, 0] # particular channel only\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/OpenCV/#show","title":"Show","text":"<pre><code>plt.axis(\"off\")\nplt.imshow(img)\nplt.show()\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/OpenCV/#filters","title":"Filters","text":"<pre><code>img = cv.cvtColor(\n  img,\n  cv.COLOR_BGR2RGB\n)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/OpenCV/#export","title":"Export","text":"<pre><code>cv.imwrite(\n    \"output.png\",\n  img\n)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Pandas/01/","title":"01","text":"<p>Try using numpy for operations whenever possible</p> <p>Use <code>to_numpy()</code> instead of <code>.values</code></p>"},{"location":"Tools/AI%20%26%20Data/Pandas/01/#importing","title":"Importing","text":"<pre><code>import pandas as pd\npd.set_option('max_columns', 200)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Pandas/01/#creating-dataframe","title":"Creating Dataframe","text":""},{"location":"Tools/AI%20%26%20Data/Pandas/01/#dataset","title":"Dataset","text":"<pre><code>  df = pd.read_csv(\"dataset.csv\")\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Pandas/01/#manual","title":"Manual","text":"<pre><code>  perf = pd.DataFrame(\n      columns = ['Season', 'Appearances', 'Goals', 'Assists'],\n      data = [\n          [\"2021/2022\", 39, 24, 3],\n          [\"2020/2021\", 44, 36, 4],\n          [\"2019/2020\", 46, 37, 7]\n      ]\n  )\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Pandas/01/#filtering","title":"Filtering","text":""},{"location":"Tools/AI%20%26%20Data/Pandas/01/#query-and-eval","title":"<code>query</code> and <code>eval</code>","text":"<pre><code>  ## query - better than boolean masking\n  df = df.query(\"\"\"\n  @date_start &lt;= Date &lt;= @date_end and \\\n  Type in @event_type\n  \"\"\")\n\n  mask = df.eval(\"something\") ## gives the boolean mask corresponding to this\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Pandas/01/#boolean-masking","title":"Boolean Masking","text":"<pre><code>  df = df[\n    (df[\"Date\"] &gt;= date_start) &amp;\n    (df[\"Date\"] &lt;= date_end) &amp;\n    (df[\"Type\"].isin(event_type))\n  ]\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Pandas/01/#editing-values","title":"Editing Values","text":"<pre><code>(\n  prediction\n  .assign(\n    Rating = prediction.Rating.to_numpy() * 100,\n    Value = prediction.Value.to_numpy() * 50,\n  )\n)\n\nprediction[['Rating', 'Value']] = prediction[['Rating', 'Value']].to_numpy() * 100\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Pandas/01/#readwrite","title":"Read/Write","text":""},{"location":"Tools/AI%20%26%20Data/Pandas/01/#single-file","title":"Single File","text":"<pre><code>  df = pd.read_csv(\n    file,\n    engine=\"pyarrow\", backend_dtypes=\"pyarrow\"\n  )\n\n  dfs = pd.read_excel('GDSC.xlsx', sheet_name=\"Something\") ## gives all\n  dfs = pd.read_excel('GDSC.xlsx', sheet_name=None) ## gives all\n  for table, df in dfs.items():\n      print(table, df)\n</code></pre> <pre><code>  file_name = file[:-4] + \".csv\"\n  df.to_csv(\n    os.getcwd() + \"\\\\\" + rel + file_name,\n    index = False\n  )\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Pandas/01/#multiple-files","title":"Multiple Files","text":"<pre><code>  raw_formula_student = pd.DataFrame()\n  for file in files:\n      if( \".csv\" == file[-4:] ):\n          raw_formula_student = pd.concat(\n              [raw_formula_student, read_file(file)]\n          )\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Pandas/01/#read-google-sheets","title":"Read Google Sheets","text":"<p>Best method</p> <pre><code>    import gspread\n    gc = gspread.service_account(\"key.json\")\n\n    from gspread_dataframe import get_as_dataframe as get_gsheet, set_with_dataframe as set_gsheet\n\n    from functools import lru_cache\n\n    def gsheet_to_csv(spreadsheet_id, sheet_id=None, sheet_name=None):\n      ## make sure the spreadsheet is publicly viewable\n      if sheet_id is not None:\n        link = f\"https://docs.google.com/spreadsheets/d/{spreadsheet_id}/gviz/tq?tqx=out:csv&amp;gid={sheet_id}\"\n      elif sheet_name is not None:\n        link = f\"https://docs.google.com/spreadsheets/d/{spreadsheet_id}/gviz/tq?tqx=out:csv&amp;sheet={sheet_name}\"\n      else:\n        return None\n\n        df = pd.read_csv(\n          link,\n            engine = \"pyarrow\",\n          backend_dtypes = \"pyarrow\"\n        )\n        return df\n\n    def gsheet_by_api(spreadsheet_id, sheet_id=None, sheet_name=None):\n          gsheet = gc.open_by_key(spreadsheet_id)\n      if sheet_id is not None:\n        sheet = gsheet.get_worksheet_by_id(sheet_id)\n      elif sheet_name is not None:\n        sheet = getattr(gsheet, sheet_name)\n      else:\n        return None\n\n      df = get_gsheet(sheet, evaluate_formulas=True)\n        return df\n\n    @lru_cache(maxsize = 128) ## or st.cache_data(ttl=ttl_long)\n    def read_gsheet(spreadsheet_id, sheet_id=None, sheet_name=None, parse_dates=None, csv=True):\n        if csv is True:\n          df = gsheet_to_csv(spreadsheet_id, sheet_id, sheet_name)\n        else:\n          df = gsheet_by_api(spreadsheet_id, sheet_id, sheet_name)\n\n      df = df.dropna(how= \"all\", axis=\"index\")\n      df = df.dropna(how= \"all\", axis=\"columns\")\n\n      for col_name in df.columns:\n        if \"date\" in col_name.lower() or \"time\".lower() in col_name:\n          df[col_name] = pd.to_datetime(df[col_name])\n\n      return df\n</code></pre> <p>Old method (Not working)</p> <pre><code>    gsheetkey = \"1kax9m1FKah7cWPwylxhdJSyqF5eVALjgRbxyPuPg7g0\"\n    sheet_name = 'Social_Media_Analysis'\n    url= f'[\u0644\u0645 \u064a\u062a\u0645 \u0627\u0644\u0639\u062b\u0648\u0631 \u0639\u0644\u0649 \u0627\u0644\u0635\u0641\u062d\u0629](https://docs.google.com/spreadsheet/ccc?key={gsheetkey}&amp;output=xlsx')\n\n    sheet = pd.read_excel(url, sheet_name = sheet_name)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Pandas/01/#useful-functions","title":"Useful Functions","text":"<pre><code>df.head()\ndf.tail()\n\ndf.describe()\ndf.dropna()\n\ndf.select_dtypes(int)\n\ndf.memory_usage(deep=True).sum()/1_000_000 ## in MBs \n</code></pre> <pre><code>df = df.set_index(\n    \"Season\",\n    \"Player Name\"\n)\n</code></pre> <pre><code>df.unique()\ndf.value_counts()\n100 * df['col'].value_counts() / df['col'].shape[0]\n</code></pre> <pre><code>df['Age'].avg()\n</code></pre> Function Function Application <code>pipe()</code> Table-wise <code>assign()</code> <code>apply()</code> Row/column-wise (not good to use) <code>applymap()</code> Element-wise (not good to use) <pre><code>df.iloc[:, 2:11] = (\n  df.iloc[:, 2:11]\n  .apply(lambda x: x.str.replace(',', '.'))\n  .to_numpy()\n  .astype(float)\n)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Pandas/01/#view-null-values","title":"View null values","text":"<pre><code>  formula_student[\n      formula_student.isna().any(axis=1)\n  ]\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Pandas/01/#correlation","title":"Correlation","text":""},{"location":"Tools/AI%20%26%20Data/Pandas/01/#correlation-matrix","title":"Correlation Matrix","text":"<pre><code>  (\n    formula_student\n    [['Cost', 'Design', 'Overall Scores']]\n    .corr()\n  )\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Pandas/01/#correlation-ranking","title":"Correlation Ranking","text":"<pre><code>  (\n      formula_student\n      .iloc[:, 1:11]\n      .corr(\"Overall Scores\")\n      .sort_values(\"Correlation\", ascending=False)\n      .iloc[1:, :] ## remove the obvious overall scores = 1.00\n  )\n</code></pre> <pre><code>  ## this is unnecessarily complicated way i used before\n  (\n      formula_student\n      .iloc[:, 1:11]\n      .corr()\n      .rename(columns={\"Overall Scores\":\"Correlation\"})\n      [[\"Correlation\"]]\n      .sort_values(\"Correlation\", ascending=False)\n      .iloc[1:, :] ## remove the obvious overall scores = 1.00\n  )\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Pandas/01/#deleting","title":"Deleting","text":"<p>Drop first \\(n\\) records</p> <pre><code>  df = df.iloc[n: , : ]\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Pandas/01/#sorting","title":"Sorting","text":"<pre><code>(\n  perf\n  .sort_values(\"Season\")\n  .reset_index(drop=True)\n)\n</code></pre> <pre><code>(\n  perf\n  .sort_values([\n    \"Season\",\n    \"Rating\",\n    \"Value\"\n  ], ascending=[\n    True,\n    True,\n    False\n  ])\n  .reset_index(drop=True)\n)\n</code></pre> <p>Different approach</p> <pre><code>  (\n    formula_student[[\"Cost\", \"Overall Placing\"]]\n    .sort_values(\"Cost\", ascending = False)\n    .head(10)\n    .sort_values(\"Overall Placing\")\n  )\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Pandas/01/#groupingaggregation","title":"Grouping/Aggregation","text":"<pre><code>meanCols = {\n    \"Value\": \"MeanValue\",\n    \"Overall\": \"MeanOverall\",\n    \"CPIValue\": \"MeanCPIValue\"\n}\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Pandas/01/#single-aggregation-function","title":"Single Aggregation Function","text":"<pre><code>df.values.mean()\n# don't do df.mean().mean()\n\nnp.nanmean(df.values)\nnp.nanstd(df.values)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Pandas/01/#all-columns","title":"All Columns","text":"<pre><code>    (\n      merged\n      .groupby(\n        [\"Year\"],\n        as_index=False ## if Year should not become index of dataframe\n      )\n      .mean()\n      .rename(columns={\n        \"Value\": \"MeanValue\",\n        \"Overall\": \"MeanOverall\",\n        \"CPIValue\": \"MeanCPIValue\"\n      })\n    )\n    ## or mean = merged.groupby([\"Year\"]).mean().reset_index()\n</code></pre> <p>Renaming is for obvious reasons</p>"},{"location":"Tools/AI%20%26%20Data/Pandas/01/#particular-columns","title":"Particular Columns","text":"<pre><code>    mean = merged[[\"Year\",\"Value\"]].groupby(\n      [\"Year\"],\n      as_index=False\n    ).mean().rename(columns=meanCols)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Pandas/01/#multiple-aggregation-function","title":"Multiple Aggregation Function","text":"<pre><code>  summary_df = (\n      df[[\"latitude\", \"longitude\", \"emission\"]]\n      .groupby([\"latitude\", \"longitude\"], as_index=False)\n      .agg({\n          \"emission\": [\"median\",\"std\", \"mean\", \"min\", \"max\"]\n      })\n  )\n  summary_df.columns = summary_df.columns.map('_'.join)\n\n  df = df.merge(summary_df, how=\"inner\")\n</code></pre> <pre><code>  (\n    merged\n    .groupby(\n      [\"Year\"],\n      as_index=False\n    )\n    .agg(\n      ['mean', 'sum']\n    )\n  )\n</code></pre> <p>Round-off all results</p> <pre><code>    formula_student[[\"Car\", \"Overall Scores\"]]\n        .groupby(\n            [\"Competition\"]\n        )\n        .agg({\n            'Car' : [\"count\"],\n            'Overall Scores' : [\"mean\", \"min\", \"max\"]\n        })\n        .round(1)\n</code></pre> <p>Round-off specific results</p> <pre><code>    def mean_func(x):\n        return round(x.mean(), 1)\n\n    formula_student[[\"Car\", \"Overall Scores\"]]\n        .groupby(\n            [\"Competition\"]\n        )\n        .agg({\n            'Car' : [\"count\"],\n            'Overall Scores' : [mean_func, \"min\", \"max\"]\n        })\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Pandas/01/#idk","title":"IDK","text":"<p>The count of each value until it changes to another value</p> <pre><code>  index  value\n      0     10\n      1     10\n      2     23\n      3     23\n      4      9\n      5      9\n      6      9\n      7     10\n      8     10\n      9     10\n     10     10\n     11     12\n\n  ---\n\n  index count\n     10     2\n     23     2\n      9     3\n     10     4\n     12     1\n</code></pre> <pre><code>  col = 'col_name'\n\n  df = (\n    df\n    .groupby(\n      df[col]\n      .ne( ## not equal to previous value; ie change occured\n        df[col]\n        .shift()\n      )\n      .cumsum()\n    )\n    [col]\n    .value_counts()\n    .reset_index(level=0, drop=True)\n  )\n</code></pre> <p>Explanation</p> <pre><code>      ## This is the intermediate dataframe produced\n      ## We then group by cumsum\n\n      index  value  shifted  not_equal  cumsum\n          0     10      NaN       True       1\n          1     10     10.0      False       1\n          2     23     10.0       True       2\n          3     23     23.0      False       2\n          4      9     23.0       True       3\n          5      9      9.0      False       3\n          6      9      9.0      False       3\n          7     10      9.0       True       4\n          8     10     10.0      False       4\n          9     10     10.0      False       4\n         10     10     10.0      False       4\n         11     12     10.0       True       5\n</code></pre> <pre><code>  col = 'col_name'\n\n  changes = (\n    df[col]\n    .diff()\n    .ne(0)\n    .cumsum()\n  )\n\n  df = (\n    df\n    .groupby([changes,col])\n    .size()\n    .reset_index(level=0, drop=True)\n  )\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Pandas/01/#joinmerge","title":"Join/Merge","text":"<pre><code>merged = pd.merge(\n    ratings[ratings[\"Value\"] &gt;= 10000],\n    cpi\n).sort_values(\"Value\").reset_index(drop=True)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Pandas/01/#plotting","title":"Plotting","text":"<pre><code>merged.plot(\n  x=\"Season\",\n  ylabel=\"Feature\",\n  title=\"Features over Time\"\n)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Pandas/01/#plotting-backend","title":"Plotting Backend","text":"<pre><code>  pd.options.plotting.backend = 'plotly'\n  df.plot(backend='plotly')\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Pandas/01/#type-casting","title":"Type Casting","text":"<pre><code>df['Fee'] = df['Fee'].to_numpy().astype(float)\n## df['Fee'] = df['Fee'].astype(float)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Pandas/01/#lagged-value","title":"Lagged Value","text":"<p>alias::shift lag <pre><code>nba[\"wpc_lag\"] = (\n  nba\n  .groupby(\"Team\")\n  [\"wpc\"]\n  .shift(1)\n)\n</code></pre></p>"},{"location":"Tools/AI%20%26%20Data/Pandas/01/#performance-optimization","title":"Performance Optimization","text":"<p>https://pythonspeed.com/datascience</p>"},{"location":"Tools/AI%20%26%20Data/Pandas/01/#selectively-load-columns","title":"Selectively-load columns","text":"<pre><code>  df = pd.read_csv(\n    \"dataset.csv\",\n    usecols = [\n      \"First Name\",\n      \"Last Name\"\n    ]\n  )\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Pandas/01/#selectively-load-rows","title":"Selectively-load rows","text":"<pre><code>  df = pd.read_csv(\n    \"dataset.csv\",\n    nrows = 10\n  )\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Pandas/01/#reducing-variables-in-functions","title":"Reducing variables in functions","text":"<pre><code>  def process_data():\n      return modify2(modify1(load_1GB_of_data()))\n</code></pre> <p>instead of <pre><code>  def process_data():\n      data = load_1GB_of_data() ## \u2190 `data` var lives too long\n      return modify2(modify1(data))\n</code></pre></p>"},{"location":"Tools/AI%20%26%20Data/Pandas/01/#use-correct-dtypes","title":"Use correct <code>dtypes</code>","text":"<pre><code>  df = pd.read_csv(\n    \"dataset.csv\",\n    dtype = {\n      \"Age\": \"uint8\",\n      \"Year\": \"uint16\",\n      \"Time\": \"datetime\"\n      \"Salary\": \"ufloat32\",\n      \"Gender\": \"category\",\n      \"Name\": \"string[pyarrow]\"\n    }\n  )\n</code></pre> <p>Dynamically cleaning up after reading based on datatype; but i would recommend above</p> <pre><code>    def get_optimal_numeric_type(c_min: float, c_max: float, col_type: str) -&gt; str:\n        \"\"\"\n        Determines the optimal numeric data type for a given range of values.\n\n        Parameters\n        ----------\n        c_min : float\n            The minimum value of the data.\n        c_max : float\n            The maximum value of the data.\n        col_type : str\n            The current data type of the column ('int' or 'float').\n\n        Returns\n        -------\n        optimal_type : str\n            The optimal data type for the given range of values.\n        \"\"\"\n        type_info = np.iinfo if col_type == 'int' else np.finfo\n        for dtype in [np.int8, np.int16, np.int32, np.int64, np.float16, np.float32, np.float64]:\n            if col_type in str(dtype):\n                if c_min &gt; type_info(dtype).min and c_max &lt; type_info(dtype).max:\n                    return dtype\n        return None\n\n    \"\"\" Based on the data type and the range of values, the function determines the smallest possible data type that can accommodate the data without losing information. For example, if the data type is an integer and the range of values fits within the bounds of an int8 data type, the function converts the column data type to int8: \"\"\"\n\n    def reduce_memory_usage(df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Reduces memory usage of a pandas DataFrame by converting its columns to the most memory-efficient data types\n        without losing information.\n\n        Parameters\n        ----------\n        df : pd.DataFrame\n            The input pandas DataFrame that needs memory optimization.\n\n        Returns\n        -------\n        df : pd.DataFrame\n            The optimized pandas DataFrame with reduced memory usage.\n        \"\"\"\n\n        ## Iterate through each column in the DataFrame\n        df_copy = df.copy()\n        for col in df_copy.columns:\n            col_type = df_copy[col].dtype\n\n            ## Check if the data type is not an object (i.e., numeric type)\n            if col_type != object:\n                c_min, c_max = df_copy[col].min(), df_copy[col].max()\n                col_type_str = 'int' if 'int' in str(col_type) else 'float'\n                optimal_type = get_optimal_numeric_type(c_min, c_max, col_type_str)\n                if optimal_type:\n                    df_copy[col] = df_copy[col].astype(optimal_type)\n            ## If the data type is an object, convert the column to a 'category' data type\n            else:\n                df_copy[col] = df_copy[col].astype('category')\n\n        ## Return the optimized DataFrame with reduced memory usage\n        return df_copy\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Pandas/01/#use-pyarrow-engine","title":"Use <code>[[pyarrow]]</code> engine","text":"<pre><code>  df = pd.read_csv(\n    \"dataset.csv\",\n    engine = \"pyarrow\"\n  )\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Pandas/01/#use-modin","title":"Use Modin","text":""},{"location":"Tools/AI%20%26%20Data/Pandas/01/#clipping","title":"Clipping","text":"<pre><code>## using pandas\ndf = df.clip(\n  lower = df['Column'].quantile(.25, interpolation=\"midpoint\"),           \n  upper = df['Column'].quantile(.75, interpolation=\"midpoint\")\n)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Pandas/01/#chaining","title":"Chaining","text":"<pre><code>(\n  df\n  .pipe(function)\n  .assign(\n    ensnetns = nesntens\n  )\n  .astype({\"highway\": np.int8})\n)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Pandas/01/#sql","title":"SQL","text":"<pre><code>df = pd.read_sql(query, engine)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Pandas/01/#exporting-to-html","title":"Exporting to HTML","text":"<pre><code>html = \"\"\"\n&lt;head&gt;\n  &lt;link rel=\"stylesheet\" href=\"../../../backend/plugins/bootstrap/bootstrap.min.css\"&gt;\n  &lt;script src=\"../../../backend/plugins/bootstrap/bootstrap.min.js\"&gt;&lt;/script&gt;\n&lt;/head&gt;\n&lt;body&gt;\n\"\"\"\n&lt;body&gt;\nhtml += blah_df.head.to_html(classes='table table-stripped')\nhtml += \"&lt;/body&gt;\"\n\nexporter = open(\"blah_df.html\", 'w')\nexporter.write(html)\nexporter.close()\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Pandas/01/#get","title":"Get","text":""},{"location":"Tools/AI%20%26%20Data/Pandas/01/#get-last-letter-of-every-row","title":"Get last letter of every row","text":"<pre><code>  spf_unrate[\"PERIOD\"].str.get(-1)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Pandas/01/#first-name","title":"First Name","text":"<pre><code>  df[\"Name\"] = df[\"Name\"].str.split().str.get(0)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Pandas/01/#yearmonthdate","title":"Year/Month/Date","text":"<pre><code>  fred[\"YEAR\"] = pd.DatetimeIndex(fred['DATE']).year\n  fred = fred.drop(\"DATE\", axis=1)\n  fred\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Pandas/01/#time-series","title":"Time Series","text":"<pre><code>df.shift(10)\ndf.diff(10)\n\n## window functions\ndf.rolling(10).mean()\ndf.rolling(10, center=True).mean()\n\ndf.expanding(10).mean()\ndf.expanding(10, center=True).mean()\n\n## exponential weighted window\ndf.ewm(10).mean()\ndf.ewm(10, center=True).mean()\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Pandas/01/#missing-values","title":"Missing Values","text":"<pre><code>def check(data):\n  print(data.isnull().values.any())\n\nsheet.apply(check)          ## check if col has missing value\nsheet.apply(check, axis=1)  ## check if row has missing value\nsheet.pipe(check)           ## check if dataframe has missing value\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Pandas/01/#my-cleaning","title":"My Cleaning","text":"<pre><code>def clean_df(data):\n  df = data.copy()\n\n  for column in df.columns:\n    if \"date\" in column:\n      df[column] = pd.to_datetime(sheet[column])\n\n  df = df.apply(lambda col: col.str.strip() if col.dtype == \"object\" else col)\n\n  return df\n\nsheet.pipe(clean_df)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Pandas/02/","title":"02","text":""},{"location":"Tools/AI%20%26%20Data/Pandas/02/#monotonicity","title":"Monotonicity","text":""},{"location":"Tools/AI%20%26%20Data/Pandas/02/#decreasing","title":"Decreasing","text":"<pre><code>df[\"Reading\"] = (\n  df\n  .sort_values(\"Time_Point\", ascending=True) # this is the time column, so always ascending\n  .groupby(\"Temperature\")\n  [\"Reading\"]\n  .cummin()\n)\n# or \ndf_train[\"Reading_Time_Point_Cummin\"] = (\n  df_train\n  .sort_values(\"Time_Point\", ascending=True) # this is the time column, so always ascending\n  .groupby(\"Temperature\")\n  [\"Reading\"]\n  .cummin()\n)\ndf_train = df_train.query(\"Reading &lt;= Reading_Time_Point_Cummin\")\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Pandas/02/#increasing","title":"Increasing","text":"<pre><code>df[\"Reading\"] = (\n  df\n  .sort_values(\"Time_Point\", ascending=True) # this is the time column, so always ascending\n  .groupby(\"Temperature\")\n  [\"Reading\"]\n  .cummax()\n)\n\ndf_train[\"Reading_Time_Point_Cummax\"] = (\n  df_train\n  .sort_values(\"Time_Point\", ascending=True) # this is the time column, so always ascending\n  .groupby(\"Temperature\")\n  [\"Reading\"]\n  .cummax()\n)\ndf_train = df_train.query(\"Reading &gt;= Reading_Time_Point_Cummin\")\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Pandas/02/#sampling","title":"Sampling","text":"<pre><code># if n=frac, then percentage of dataset\ndf.sample(\n  0.10\n)\n\n# if n=int, then count of dataset\ndf.sample(\n  1_000\n)\n\n# sampling with prob weights\ndf.sample(\n  1_000,\n  weights = \"Weight_Column\"\n)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Pandas/02/#multi-index","title":"Multi-Index","text":"<pre><code>df.columns = list(map('_'.join, df.columns.values))\ndf = df.reset_index()\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Pandas/02/#objectname","title":"ObjectName","text":"<pre><code>class Class:\n  pass\n\nmodel = Class()\nmodel_name = type(model).__name__\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/PyTorch/","title":"PyTorch","text":""},{"location":"Tools/AI%20%26%20Data/PyTorch/#references","title":"References","text":"<ul> <li> Learn PyTorch for deep learning in a day | Daniel Bourke</li> <li> Introduction to PyTorch | Siri STEM World</li> <li> Machine Learning and PyTorch Deep Dive | Siri STEM World</li> </ul>"},{"location":"Tools/AI%20%26%20Data/PyTorch/01_Introduction/","title":"Introduction","text":""},{"location":"Tools/AI%20%26%20Data/PyTorch/01_Introduction/#init","title":"Init","text":"<pre><code>import torch\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader\n\nimport torchvision\nfrom torchvision import transforms\nfrom torchvision.datasets import ImageFolder\nimport timm\n\nimport matplotlib.pyplot as plt ## For data viz\nimport pandas as pd\nimport numpy as np\n\nimport sys\nfrom tqdm.notebook import tqdm\n\nprint('System Version:', sys.version)\nprint('PyTorch version', torch.__version__)\nprint('Torchvision version', torchvision.__version__)\nprint('Numpy version', np.__version__)\nprint('Pandas version', pd.__version__)\n\ndevice = (\n  \"mps\" if getattr(torch, \"has_mps\", False)\n  else\n  \"cuda\" if torch.cuda().is_available()\n  else\n  \"cpu\"\n)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/PyTorch/01_Introduction/#tensors","title":"Tensors","text":"<pre><code>torch.mean(image_data, axis=0) ## column-wise mean\n\nluminance_approx = torch.mean(image_array, axis=-1) ## color_channel-wise mean\n\nvalues, indices = torch.max(data, axis=-1)\n</code></pre> <ul> <li><code>int8</code> is an integer type, it can be used for any operation which needs integers</li> <li><code>qint8</code> is a quantized tensor type which represents a compressed floating point tensor, it has an underlying int8 data layer, a scale, a zero_point and a qscheme</li> </ul>"},{"location":"Tools/AI%20%26%20Data/PyTorch/01_Introduction/#creating-tensors","title":"Creating Tensors","text":"<pre><code># \u274c\ntensor.tensor([2, 2]).cuda()\ntensor.rand(2, 2).cuda()\n\n# \u2705\ntensor.tensor([2, 2], device=device)\ntensor.rand(2, 2, device=device)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/PyTorch/01_Introduction/#conversion-to-tensor","title":"Conversion To Tensor","text":"<pre><code># \u274c creates a copy\ntensor = torch.tensor(array, device=device)\n\n# \u2705 avoids copying\ntensor = torch.as_tensor(array, device=device)\ntensor = torch.from_numpy(array, device=device)\n# however, changing array will also affect tensor\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/PyTorch/01_Introduction/#conversion-from-tensor","title":"Conversion From Tensor","text":"<pre><code># \u274c\ntensor.cpu()\ntensor.item()\ntensor.numpy()\n\n# \u2705\ntensor.detach()\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/PyTorch/01_Introduction/#api","title":"API","text":""},{"location":"Tools/AI%20%26%20Data/PyTorch/01_Introduction/#model-mode","title":"Model Mode","text":"<pre><code>model.train()\nmodel.eval()\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/PyTorch/01_Introduction/#sequential","title":"Sequential","text":"<pre><code>nn.Sequential(\n    nn.LazyLinear(100),\n  nn.ReLU()\n)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/PyTorch/01_Introduction/#lazy-layers","title":"Lazy Layers","text":"<p>automatically detect the input size</p> <p>Only specify output size</p> <pre><code>nn.Sequential(\n  nn.LazyLinear(1000),\n  nn.LazyLinear(10),\n  nn.LazyLinear(100)\n)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/PyTorch/01_Introduction/#save-model","title":"Save Model","text":"<pre><code>torch.save(model, \"model.pkl\")\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/PyTorch/01_Introduction/#view-parameters","title":"View Parameters","text":"<pre><code>for param in model.parameters():\n  print(name)\n\nfor name, param in model.named_parameters():\n    if param.requires_grad:\n        print(name, param.data)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/PyTorch/01_Introduction/#custom-loss-function","title":"Custom Loss Function","text":"<pre><code>class loss(nn.module):\n  def forward(self, pred, y):\n    error = pred-y\n    return torch.mean(\n      torch.abs(error)\n    )\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/PyTorch/01_Introduction/#idk","title":"IDK","text":""},{"location":"Tools/AI%20%26%20Data/PyTorch/01_Introduction/#forward-pass","title":"Forward pass","text":"<pre><code>model.train()\nmodel.eval()\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/PyTorch/01_Introduction/#backward-pass","title":"Backward pass","text":"<pre><code>with torch.set_grad_enabled(True): # turn on history tracking\n  # training\n\nwith torch.set_grad_enabled(False): # turn off history tracking\n  # testing\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/PyTorch/01_Introduction/#time-series","title":"Time-Series","text":"<pre><code>class TimeseriesDataset(torch.utils.data.Dataset):   \n    def __init__(self, X, y, seq_len=1):\n        self.X = X\n        self.y = y\n        self.seq_len = seq_len\n\n    def __len__(self):\n        return self.X.__len__() - (self.seq_len-1)\n\n    def __getitem__(self, index):\n        return (self.X[index:index+self.seq_len], self.y[index+self.seq_len-1])\n</code></pre> <pre><code>train_dataset = TimeseriesDataset(X_lstm, y_lstm, seq_len=4)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 3, shuffle = False)\n\nfor i, d in enumerate(train_loader):\n    print(i, d[0].shape, d[1].shape)\n\n&gt;&gt;&gt;\n# shape: tuple((batch_size, seq_len, n_features), (batch_size))\n0 torch.Size([3, 4, 2]) torch.Size([3])\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/PyTorch/02_Basic_Model/","title":"Basic Model","text":""},{"location":"Tools/AI%20%26%20Data/PyTorch/02_Basic_Model/#steps","title":"Steps","text":"<ul> <li>Define model using <code>nn.module</code></li> <li>Cost function</li> <li>Optimizer</li> <li>Epochs</li> <li>Train data</li> <li>Predict</li> </ul>"},{"location":"Tools/AI%20%26%20Data/PyTorch/02_Basic_Model/#device","title":"Device","text":"<pre><code>if torch.cuda.is_available():\n    device = torch.device(\"cuda:0\")\nelse:\n    device = torch.device(\"cpu\")\n\nprint(f\"Using {device}\")\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/PyTorch/02_Basic_Model/#data","title":"Data","text":""},{"location":"Tools/AI%20%26%20Data/PyTorch/02_Basic_Model/#dataset","title":"Dataset","text":"<pre><code>class CTDataset(Dataset):\n    def __init__(self, filepath, device):\n        self.x, self.y = torch.load(filepath, map_location=device)\n        self.x = self.x / 255.0\n        self.y = nn.functional.one_hot(self.y, num_classes=10).to(float)\n\n    def __len__(self):\n        return self.x.shape[0]\n\n    def __getitem__(self, ix):\n        return self.x[ix], self.y[ix]\n</code></pre> <pre><code># https://www.di.ens.fr/~lelarge/MNIST.tar.gz\ntrain_ds = CTDataset(\"./MNIST/training.pt\", device)\n# test_ds = CTDataset('./MNIST/test.pt', device)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/PyTorch/02_Basic_Model/#dataloader","title":"Dataloader","text":"<pre><code>train, dev, valid = random_split(train_ds, [0.6, 0.2, 0.2])\n</code></pre> <pre><code>train_size = min(8, len(train)) # Check if model overfits on small data, to ensure DNN actually is effective\ndev_size = min(8, len(dev))\n\nmin_training_batches = 4\ntrain_batch_size = min(32, max(1, train_size // min_training_batches))\n\nevaluation_batch_size = min(1_024, dev_size)\n</code></pre> <pre><code>train_random_sampler = RandomSampler(train, num_samples=train_size)\ndev_random_sampler = RandomSampler(dev, num_samples=dev_size)\n\ntrain_dl = DataLoader(\n    train, sampler=train_random_sampler, batch_size=train_batch_size, drop_last=True\n)\n\ndev_dl = DataLoader(\n    dev, sampler=dev_random_sampler, batch_size=evaluation_batch_size, drop_last=True\n)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/PyTorch/02_Basic_Model/#model","title":"Model","text":"<pre><code># architecture\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/PyTorch/02_Basic_Model/#train","title":"Train","text":"<pre><code>def get_max_len(arrays):\n    return max(\n        [\n            len(array)\n            for array\n            in arrays\n        ]\n    )\n\ndef pad(array, max_len):\n    return list(np.pad(\n        array,\n        pad_width = (0, max_len-len(array)),\n        constant_values = np.nan\n    ))\n\ndef get_all_nodes(model):\n    network_nodes = []\n\n    layers = model.named_children()\n    for i, layer in enumerate(layers):\n        layer_nodes_formatted = []\n\n        sub_layer = layer[-1]\n        for sub_layer_node in sub_layer:\n            layer_nodes_formatted.append(sub_layer_node)\n\n        network_nodes.append(layer_nodes_formatted)\n\n    return network_nodes\n\ndef get_summary_agg(df, agg=[\"mean\"], precision=2):\n    df = (\n        df\n        .groupby([\"Epoch\", \"Train_Time\", \"Subset\"])\n        .agg({\n            \"Loss\": agg,\n            \"Accuracy\": [\"mean\"]\n        })\n        .round(precision)\n    )\n    df.columns = list(map('_'.join, df.columns.values))\n    df = (\n        df\n        .reset_index()\n        .pivot(\n            index=[\"Epoch\", \"Train_Time\"],\n            columns=\"Subset\",\n            # values = \"Accuracy\"\n        )\n    )\n    df.columns = list(map('_'.join, df.columns.values))\n\n    # should not be part of data collection\n    # df[\"Generalization_Gap\"] = df[\"Loss_mean_Dev\"] - df[\"Loss_mean_Train\"]\n\n    df = df.reset_index()\n\n    return df\n</code></pre> <pre><code># @torch.compile(mode=\"reduce-overhead\")\ndef train_batch(model, optimizer, loss, x, y, train_dl_len, batch_idx, device, accum_iter=1, k_frac=None):\n    x = x.half()\n    y = y.half()\n    # x = x\n    # y = y\n\n    model.train()\n    # with torch.set_grad_enabled(True): # turn on history tracking\n    # forward pass\n    proba = model(x)\n    loss_array = loss(proba, y)\n\n    loss_scalar = loss_array.mean()\n\n    # backward pass\n    optimizer.zero_grad(set_to_none=True) # clear accumulated gradients from backpropagation\n    loss_scalar.backward()\n\n    # weights update\n    # if accum_iter != 1 -&gt; gradient accumulation\n    batch_num = batch_idx + 1\n\n    if (\n        (batch_num % accum_iter == 0)\n        or\n        (batch_num == len(train_dl_len))\n    ):\n        optimizer.step()\n\n\n\n# @torch.compile(mode=\"reduce-overhead\")\ndef train_epoch(dl, model, optimizer, loss, train_dl_len, device, eval=False, k_frac=None):\n\n    # epoch_accuracies = []\n    epoch_losses = []\n    epoch_accuracies = []\n\n    for batch_idx, (x, y) in enumerate(dl):\n        train_batch(model, optimizer, loss, x, y, train_dl_len, batch_idx, device, accum_iter=1, k_frac=k_frac)\n\n        # epoch_accuracies += eval_batch(model, x, y)\n        if eval:\n            temp = eval_batch(model, x, y, loss, device)\n            epoch_losses += temp[0]\n            epoch_accuracies += temp[1]\n\n    return epoch_losses, epoch_accuracies\n\n# @torch.compile(mode=\"reduce-overhead\")\ndef eval_batch(model, x, y, loss, device):\n    x = x.half()\n    y = y.half()\n\n    # x = x\n    # y = y\n\n    model.eval()\n    with torch.inference_mode(): # turn off history tracking\n        # forward pass\n        proba = model(x)\n\n        loss_value = loss(proba, y)\n        epoch_loss_array = loss_value.detach() # loss_value.item() # batch loss\n\n        true = y.argmax(axis=1)\n        pred = proba.argmax(axis=1)\n        epoch_accuracy_array = (pred == true) # torch.sum()\n\n        return epoch_loss_array, epoch_accuracy_array\n\n# @torch.compile(mode=\"reduce-overhead\")\ndef eval_epoch(dl, model, loss, device):\n    epoch_accuracies = []\n    epoch_losses = []\n    for batch_idx, (x, y) in enumerate(dl):\n        temp = eval_batch(model, x, y, loss, device)\n        epoch_losses += temp[0]\n        epoch_accuracies += temp[1]\n\n    return epoch_losses, epoch_accuracies\n\n\ndef train_model(train_dl, dev_dl, model, loss, optimizer, n_epochs, device, train_eval_every=10, dev_eval_every=10, agg=None, k_frac=None, log=False):\n    print(rf\"\"\"\n    \\n\n    Training with {train_dl, dev_dl, model, loss, optimizer, n_epochs, device, train_eval_every, dev_eval_every, agg, k_frac, log}\n    \"\"\")\n\n    model = model.to(device).half()\n\n    model.train()\n\n    summary_list = []\n\n    train_dl_len = len(train_dl)\n\n    print_epoch_every = dev_eval_every\n\n    train_time = 0\n    for epoch in range(1, n_epochs + 1):\n        print_epoch = False\n        eval_train = False\n        eval_dev = False\n\n        if epoch == 1 or epoch == n_epochs:\n            eval_train = True\n            eval_dev = True\n            if log:\n                print_epoch = True\n        if epoch % train_eval_every == 0:\n            eval_train = True\n        if epoch % dev_eval_every == 0:\n            eval_dev = True\n        if epoch % print_epoch_every == 0:\n            print_epoch = True\n\n        if print_epoch:\n            print(f\"Epoch {epoch}/{n_epochs} started\", end=\"\")\n\n        start_time = time.time()\n        epoch_train_losses, epoch_train_accuracies = train_epoch(train_dl, model, optimizer, loss, train_dl_len, device, eval=eval_train, k_frac=k_frac)\n        end_time = time.time()\n        duration = end_time-start_time\n        train_time += duration\n\n        if eval_dev:\n            epoch_dev_losses, epoch_dev_accuracies = eval_epoch(dev_dl, model, loss, device)\n        else:\n            epoch_dev_losses, epoch_dev_accuracies = [], []\n\n        for e, a in zip(epoch_train_losses, epoch_train_accuracies):\n            summary_list.append(\n                [epoch, train_time,  \"Train\", float(e), float(a)]\n            )\n        for e, a in zip(epoch_dev_losses, epoch_dev_accuracies):\n            summary_list.append(\n                [epoch, train_time, \"Dev\", float(e), float(a)]\n            )\n\n        if print_epoch:\n            print(f\", completed\")\n\n    model.eval()\n\n    summary = (\n         pd.DataFrame(\n            columns = [\"Epoch\", \"Train_Time\", \"Subset\", \"Loss\", \"Accuracy\"],\n            data = summary_list\n        )\n    )\n\n    if agg is not None:\n        summary = summary.pipe(get_summary_agg, agg)\n\n    return summary\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/PyTorch/02_Basic_Model/#idea","title":"Idea","text":"<p>I was watching https://youtu.be/VMj-3S1tku0 and got an idea. I\u2019ve put the same here: https://github.com/karpathy/micrograd/issues/78</p>"},{"location":"Tools/AI%20%26%20Data/PyTorch/02_Basic_Model/#context","title":"Context","text":"<p>This is in reference to the step of clearing accumulated gradients at: https://github.com/karpathy/micrograd/blob/c911406e5ace8742e5841a7e0df113ecb5d54685/demo.ipynb#L265</p>"},{"location":"Tools/AI%20%26%20Data/PyTorch/02_Basic_Model/#problem","title":"Problem","text":"<p>People tend to forget to clear the gradients wrt the loss function backward pass.</p>"},{"location":"Tools/AI%20%26%20Data/PyTorch/02_Basic_Model/#idea_1","title":"Idea","text":"<p>Create a way to bind the loss function to the network once, and then automatically clear accumulated gradients automatically when performing the backward pass.</p>"},{"location":"Tools/AI%20%26%20Data/PyTorch/02_Basic_Model/#advantage","title":"Advantage","text":"<p>We can perform backward pass whenever, wherever, and as many times as we want without worrying about accumulated gradient.</p>"},{"location":"Tools/AI%20%26%20Data/PyTorch/02_Basic_Model/#pseudocode","title":"Pseudocode","text":"<pre><code>class Loss(Value):\n  def __init__(self, bound_network):\n    self.bound_network = bound_network\n\n  def __call__(self, batch_size=None):\n    # loss function definition\n    self.data = data_loss + reg_loss\n\n  def backward():\n    # clear gradients of bound network\n    bound_network.zero_grad()\n    super().backward()    \n\ntotal_loss = Loss(\n  bound_network = model\n)\n\nfor k in range(100):\n  # ...\n\n  # model.zero_grad() # since total_loss is bound to network, it should automatically perform model.zero_grad() before doing the backward\n  total_loss.backward()\n\n  # ...\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/PyTorch/02_Basic_Model/#questions","title":"Questions","text":"<ol> <li>Is my understanding of the problem correct?</li> <li>Is this change value-adding?</li> <li>Is the above pseudocode logically correct?</li> <li>If the answer to all the above are yes, I could work on a PR with your guidance.</li> </ol>"},{"location":"Tools/AI%20%26%20Data/PyTorch/02_Basic_Model/#loss-curve","title":"Loss Curve","text":"<pre><code>def plot_summary(df, x, y):\n    df = df.copy()\n    c = \"Optimizer\"\n\n    if \"Accuracy\" in y and \"Generalization\" not in y:\n        sub_title = f\"Higher is better\"\n        percentage = True\n    else:\n        sub_title = f\"Lower is better\"\n        percentage = False\n\n    if percentage:\n        df[y] *= 100\n\n    if \"Accuracy\" in y and \"Generalization\" not in y:\n        range_y = [\n            0,\n            100\n        ]\n    else:\n        range_y = [\n            0,\n            df[\n                df[y] &gt; 0\n            ][y].quantile(0.90)*1.1\n        ]\n\n    # if \"loss\" in y.lower():\n    #   range_y = [0, df[y].quantile(0.90)*1.1]\n    # else:\n    #   range_y = None\n    # if y == \"Generalization_Gap\":\n    #   sub_title = f\"Lower is better\"\n    #   range_y = None\n    # else:\n    #   range_y = [0, 100 if percentage else 1]\n    #   sub_title = f\"Higher is better\"\n\n    title = f'{y.replace(\"_\", \" \")}'\n\n    title += f\"&lt;br&gt;&lt;sup&gt;{sub_title}&lt;/sup&gt;\"\n\n    facet_row = \"Train_Batch_Size\"\n\n    fig = px.line(\n        data_frame=df,\n        x=x,\n        y=y,\n        facet_col=\"Learning_Rate\",\n        facet_row=\"Train_Batch_Size\",\n        facet_row_spacing = 0.1,\n        color = c,\n        title = title,\n        range_x = [df[x].values.min(), df[x].values.max()],\n        range_y = range_y, # df[y].values.min() * 0.95\n        markers=True,\n    )\n\n    n_rows = df[facet_row].unique().shape[0]\n    fig.update_layout(height=300*n_rows)\n    fig.update_traces(\n        patch={\n            \"marker\": {\"size\": 5},\n            \"line\": {\n                \"width\": 1,\n                # \"dash\": \"dot\"\n            },\n        }\n    )\n    fig.update_traces(connectgaps=True) # required for connecting dev accuracies\n    st.plotly_chart(fig, use_container_width=True)\n\n    return fig\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/PyTorch/02_Basic_Model/#multiple-models","title":"Multiple Models","text":"<pre><code>import inspect\n\ndef train_models(loss, model, n_epochs, optimizer_names, learning_rates, train_batch_sizes, device, agg=[\"mean\"], train_eval_every=10, dev_eval_every=10, log=False, output_path = \"summary.csv\"):\n    # summaries = pd.DataFrame()\n    # i=0\n    train_size = min(2_048, len(train)) # Check if model overfits on small data, to ensure DNN actually is effective\n    dev_size = min(2_048, len(dev))\n    train_random_sampler = RandomSampler(train, num_samples=train_size)\n    dev_random_sampler = RandomSampler(dev, num_samples=dev_size)\n\n    evaluation_batch_size = 2_048\n\n    if evaluation_batch_size &gt; dev_size:\n        raise Exception(\"Evaluation batch size &gt; dev size\")\n\n    for train_batch_size in train_batch_sizes:\n        if evaluation_batch_size &gt; train_size:\n            raise Exception(\"Evaluation batch size &gt; dev size\")\n\n        train_dl = DataLoader(\n            train, sampler=train_random_sampler, batch_size=train_batch_size, drop_last=True,\n            # num_workers = 1 # 0\n        )\n\n        dev_dl = DataLoader(\n            dev, sampler=dev_random_sampler, batch_size=evaluation_batch_size, drop_last=True,\n            # num_workers = 1 # 0\n        )\n\n        for learning_rate in learning_rates:\n            if learning_rate &gt; 0.0100:\n                raise Exception(\"Very high learning rate\")\n            for optimizer_name in optimizer_names:\n                model_copy = copy.deepcopy(model)\n                optimizer = getattr(optim_class, optimizer_name)\n                optimizer_kwargs = dict(\n                    params = model_copy.parameters(),\n                    lr=learning_rate\n                )\n                if \"eps\" in list(inspect.getfullargspec(optimizer.__init__)[0]):\n                    optimizer_kwargs.update(eps=1e-4)\n                optimizer = optimizer(**optimizer_kwargs)\n\n                for state in optimizer.state.values():\n                    for k, v in state.items():\n                        if isinstance(v, torch.Tensor):\n                            state[k] = torch.as_tensor(v, device=device).half()\n\n                summary = train_model(\n                    train_dl,\n                    dev_dl,\n                    model_copy,\n                    loss,\n                    optimizer,\n                    n_epochs,\n                    device = device,\n                    train_eval_every=train_eval_every,\n                    dev_eval_every=dev_eval_every,\n                    log=log,\n                    agg = agg\n                )\n                summary[\"Model\"] = str(get_all_nodes(model_copy))\n                summary[\"Optimizer\"] = optimizer_name\n                summary[\"Learning_Rate\"] = learning_rate\n                summary[\"Train_Batch_Size\"] = train_batch_size\n\n                # disabled due too high space complexity\n                # summaries = pd.concat([\n                #   summaries,\n                #   summary\n                # ])\n                summary.to_csv(\n                    output_path,\n                    index = False,\n                    mode = \"a\",\n                    header = not os.path.exists(output_path)\n                )\n                gc.collect(0)\n\n                # i += 1\n                # if i==1:\n                #   break\n\n    return None\n</code></pre> <pre><code>model = NeuralNet(\n    init_data = train,\n    hidden_layers = [\n        nn.Flatten(),\n        nn.LazyLinear(10),\n        nn.ReLU(),\n        # nn.LazyLinear(10),\n        # nn.ReLU()\n        # nn.Sigmoid() # not required\n    ]\n)\n</code></pre> <pre><code>def percentile(p):\n    def percentile_(x):\n        return np.percentile(x, p)\n    percentile_.__name__ = f'Percentile_{p}'#.format(n*100)\n    return percentile_\n</code></pre> <pre><code>optimizer_names = [\n    # 'ASGD',\n    # 'Adadelta',\n    # 'Adagrad',\n    'Adam',\n    # 'AdamW',\n    # 'Adamax',\n    # # 'LBFGS',\n    # 'NAdam',\n    # 'RAdam',\n    # 'RMSprop',\n    # 'Rprop',\n    'SGD',\n    # 'SparseAdam'\n]\n</code></pre> <pre><code>gc.collect()\ngc.set_threshold(0)\nsummaries = train_models(\n  loss = nn.CrossEntropyLoss(reduction=\"none\"),\n    model = model,\n    n_epochs = 20, # 3\n    optimizer_names = optimizer_names,\n    learning_rates = [\n      1e-4, 1e-3, 1e-2\n    ],\n    train_batch_sizes = [\n        16, 32, 64\n    ],\n    device = device,\n    agg = [\n        \"mean\",\n        # \"std\",\n        # \"median\",\n        percentile(2.5),\n        percentile(97.5)\n    ],\n    train_eval_every=3,\n    dev_eval_every=3,\n    log = True\n)\ngc.collect()\ngc.set_threshold(g0, g1, g2)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/PyTorch/03_Architectures/","title":"Architectures","text":""},{"location":"Tools/AI%20%26%20Data/PyTorch/03_Architectures/#basic","title":"Basic","text":"<pre><code>class NeuralNet(nn.Module):\n    def __init__(self, init_data, hidden_layers):\n        super().__init__()\n\n        for x, y in DataLoader(init_data):\n            self.input_size = x.shape[-1]\n            self.output_size = y.shape[-1]\n            break\n\n        output_layer = nn.LazyLinear(self.output_size) # output layer\n\n        layers = (\n            # [input_layer] +\n            hidden_layers +\n            [output_layer]\n        )\n\n        self.network = nn.Sequential(\n            *layers\n        )\n\n        # init lazy layers\n        self.forward(x)\n\n    def reshape(self, x):\n        # batch_size, no_of_channels, width, height\n        return x.view(x.shape[0], 1, x.shape[1], x.shape[2])\n\n    def forward(self, x):\n        return self.network(self.reshape(x)).squeeze()\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/PyTorch/04_Advanced/","title":"Advanced","text":""},{"location":"Tools/AI%20%26%20Data/PyTorch/04_Advanced/#uncertainty","title":"Uncertainty","text":"<pre><code>import numpy as np\nimport torch, torchvision\nfrom torch.autograd import Variable, grad\nimport torch.distributions as td\nimport math\nfrom torch.optim import Adam\nimport scipy.stats\n\n\nx_data = torch.randn(100)+0.0 ## observed data (here sampled under H0)\n\nN = x_data.shape[0] ## number of observations\n\nmu_null = torch.zeros(1)\nsigma_null_hat = Variable(torch.ones(1), requires_grad=True)\n\ndef log_lik(mu, sigma):\n  return td.Normal(loc=mu, scale=sigma).log_prob(x_data).sum()\n\n## Find theta_null_hat by some gradient descent algorithm (in this case an closed-form expression would be trivial to obtain (see below)):\nopt = Adam([sigma_null_hat], lr=0.01)\nfor epoch in range(2000):\n    opt.zero_grad() ## reset gradient accumulator or optimizer\n    loss = - log_lik(mu_null, sigma_null_hat) ## compute log likelihood with current value of sigma_null_hat  (= Forward pass)\n    loss.backward() ## compute gradients (= Backward pass)\n    opt.step()      ## update sigma_null_hat\n\nprint(f'parameter fitted under null: sigma: {sigma_null_hat}, expected: {torch.sqrt((x_data**2).mean())}')\n#&gt; parameter fitted under null: sigma: tensor([0.9260], requires_grad=True), expected: 0.9259940385818481\n\ntheta_null_hat = (mu_null, sigma_null_hat)\n\nU = torch.tensor(torch.autograd.functional.jacobian(log_lik, theta_null_hat)) ## Jacobian (= vector of partial derivatives of log likelihood w.r.t. the parameters (of the full/alternative model)) = score\nI = -torch.tensor(torch.autograd.functional.hessian(log_lik, theta_null_hat)) / N ## estimate of the Fisher information matrix\nS = torch.t(U) @ torch.inverse(I) @ U / N ## test statistic, often named \"LM\" (as in Lagrange multiplier), would be zero at the maximum likelihood estimate\n\npval_score_test = 1 - scipy.stats.chi2(df = 1).cdf(S) ## S asymptocially follows a chi^2 distribution with degrees of freedom equal to the number of parameters fixed under H0\nprint(f'p-value Chi^2-based score test: {pval_score_test}')\n#&gt; p-value Chi^2-based score test: 0.9203232752568568\n\n## comparison with Student's t-test:\npval_t_test = scipy.stats.ttest_1samp(x_data, popmean = 0).pvalue\nprint(f'p-value Student\\'s t-test: {pval_t_test}')\n#&gt; p-value Student's t-test: 0.9209265268946605\n</code></pre> <pre><code>## another example\n\nenv_loss = loss_fn(env_outputs, env_targets)\ntotal_loss += env_loss\nenv_grads = torch.autograd.grad(env_loss, params, retain_graph=True, create_graph=True)\n\nprint(env_grads[0])\nhess_params = torch.zeros_like(env_grads[0])\nfor i in range(env_grads[0].size(0)):\n    for j in range(env_grads[0].size(1)):\n        hess_params[i, j] = torch.autograd.grad(env_grads[0][i][j], params, retain_graph=True)[0][i, j] ##  &lt;--- error here\nprint(hess_params)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/PyTorch/04_Advanced/#early-stopping","title":"Early-Stopping","text":"<p>Class</p> <pre><code>  import copy\n\n\n  class EarlyStopping:\n      def __init__(self, patience=5, min_delta=0, restore_best_weights=True):\n          self.patience = patience\n          self.min_delta = min_delta\n          self.restore_best_weights = restore_best_weights\n          self.best_model = None\n          self.best_loss = None\n          self.counter = 0\n          self.status = \"\"\n\n      def __call__(self, model, val_loss):\n          if self.best_loss is None:\n              self.best_loss = val_loss\n              self.best_model = copy.deepcopy(model.state_dict())\n          elif self.best_loss - val_loss &gt;= self.min_delta:\n              self.best_model = copy.deepcopy(model.state_dict())\n              self.best_loss = val_loss\n              self.counter = 0\n              self.status = f\"Improvement found, counter reset to {self.counter}\"\n          else:\n              self.counter += 1\n              self.status = f\"No improvement in the last {self.counter} epochs\"\n              if self.counter &gt;= self.patience:\n                  self.status = f\"Early stopping triggered after {self.counter} epochs.\"\n                  if self.restore_best_weights:\n                      model.load_state_dict(self.best_model)\n                  return True\n          return False\n</code></pre> <p>Classification</p> <pre><code>  import time\n\n  import numpy as np\n  import pandas as pd\n  import torch\n  import tqdm\n  from sklearn.metrics import accuracy_score\n  from sklearn.model_selection import train_test_split\n  from sklearn.preprocessing import LabelEncoder, StandardScaler\n  from torch import nn\n  from torch.autograd import Variable\n  from torch.utils.data import DataLoader, TensorDataset\n\n  ## Set random seed for reproducibility\n  np.random.seed(42)\n  torch.manual_seed(42)\n\n  def load_data():\n      df = pd.read_csv(\n          \"https://data.heatonresearch.com/data/t81-558/iris.csv\", na_values=[\"NA\", \"?\"]\n      )\n\n      le = LabelEncoder()\n\n      x = df[[\"sepal_l\", \"sepal_w\", \"petal_l\", \"petal_w\"]].values\n      y = le.fit_transform(df[\"species\"])\n      species = le.classes_\n\n      ## Split into validation and training sets\n      x_train, x_test, y_train, y_test = train_test_split(\n          x, y, test_size=0.25, random_state=42\n      )\n\n      scaler = StandardScaler()\n      x_train = scaler.fit_transform(x_train)\n      x_test = scaler.transform(x_test)\n\n      ## Numpy to Torch Tensor\n      x_train = torch.tensor(x_train, device=device, dtype=torch.float32)\n      y_train = torch.tensor(y_train, device=device, dtype=torch.long)\n\n      x_test = torch.tensor(x_test, device=device, dtype=torch.float32)\n      y_test = torch.tensor(y_test, device=device, dtype=torch.long)\n\n      return x_train, x_test, y_train, y_test, species\n\n\n  x_train, x_test, y_train, y_test, species = load_data()\n\n  ## Create datasets\n  BATCH_SIZE = 16\n\n  dataset_train = TensorDataset(x_train, y_train)\n  dataloader_train = DataLoader(\n      dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n\n  dataset_test = TensorDataset(x_test, y_test)\n  dataloader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=True)\n\n  ## Create model using nn.Sequential\n  model = nn.Sequential(\n      nn.Linear(x_train.shape[1], 50),\n      nn.ReLU(),\n      nn.Linear(50, 25),\n      nn.ReLU(),\n      nn.Linear(25, len(species)),\n      nn.LogSoftmax(dim=1),\n  )\n\n  model = torch.compile(model,backend=\"aot_eager\").to(device)\n\n  loss_fn = nn.CrossEntropyLoss()  ## cross entropy loss\n\n  optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n  es = EarlyStopping()\n\n  epoch = 0\n  done = False\n  while epoch &lt; 1000 and not done:\n      epoch += 1\n      steps = list(enumerate(dataloader_train))\n      pbar = tqdm.tqdm(steps)\n      model.train()\n      for i, (x_batch, y_batch) in pbar:\n          y_batch_pred = model(x_batch.to(device))\n          loss = loss_fn(y_batch_pred, y_batch.to(device))\n          optimizer.zero_grad()\n          loss.backward()\n          optimizer.step()\n\n          loss, current = loss.item(), (i + 1) * len(x_batch)\n          if i == len(steps) - 1:\n              model.eval()\n              pred = model(x_test)\n              vloss = loss_fn(pred, y_test)\n              if es(model, vloss):\n                  done = True\n              pbar.set_description(\n                  f\"Epoch: {epoch}, tloss: {loss}, vloss: {vloss:&gt;7f}, {es.status}\"\n              )\n          else:\n              pbar.set_description(f\"Epoch: {epoch}, tloss {loss:}\")\n</code></pre> <p>Regression</p> <pre><code>  import time\n\n  import numpy as np\n  import pandas as pd\n  import torch.nn as nn\n  import torch.nn.functional as F\n  import tqdm\n  from sklearn import preprocessing\n  from sklearn.metrics import accuracy_score\n  from sklearn.model_selection import train_test_split\n  from torch.autograd import Variable\n  from torch.utils.data import DataLoader, TensorDataset\n\n  ## Read the MPG dataset.\n  df = pd.read_csv(\n      \"https://data.heatonresearch.com/data/t81-558/auto-mpg.csv\", na_values=[\"NA\", \"?\"]\n  )\n\n  cars = df[\"name\"]\n\n  ## Handle missing value\n  df[\"horsepower\"] = df[\"horsepower\"].fillna(df[\"horsepower\"].median())\n\n  ## Pandas to Numpy\n  x = df[\n      [\n          \"cylinders\",\n          \"displacement\",\n          \"horsepower\",\n          \"weight\",\n          \"acceleration\",\n          \"year\",\n          \"origin\",\n      ]\n  ].values\n  y = df[\"mpg\"].values  ## regression\n\n  ## Split into validation and training sets\n  x_train, x_test, y_train, y_test = train_test_split(\n      x, y, test_size=0.25, random_state=42\n  )\n\n  ## Numpy to Torch Tensor\n  x_train = torch.tensor(x_train, device=device, dtype=torch.float32)\n  y_train = torch.tensor(y_train, device=device, dtype=torch.float32)\n\n  x_test = torch.tensor(x_test, device=device, dtype=torch.float32)\n  y_test = torch.tensor(y_test, device=device, dtype=torch.float32)\n\n\n  ## Create datasets\n  BATCH_SIZE = 16\n\n  dataset_train = TensorDataset(x_train, y_train)\n  dataloader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n\n  dataset_test = TensorDataset(x_test, y_test)\n  dataloader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=True)\n\n\n  ## Create model\n\n  model = nn.Sequential(\n      nn.Linear(x_train.shape[1], 50), \n      nn.ReLU(), \n      nn.Linear(50, 25), \n      nn.ReLU(), \n      nn.Linear(25, 1)\n  )\n\n  model = torch.compile(model, backend=\"aot_eager\").to(device)\n\n  ## Define the loss function for regression\n  loss_fn = nn.MSELoss()\n\n  ## Define the optimizer\n  optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\n  es = EarlyStopping()\n\n  epoch = 0\n  done = False\n  while epoch &lt; 1000 and not done:\n      epoch += 1\n      steps = list(enumerate(dataloader_train))\n      pbar = tqdm.tqdm(steps)\n      model.train()\n      for i, (x_batch, y_batch) in pbar:\n          y_batch_pred = model(x_batch).flatten()  #\n          loss = loss_fn(y_batch_pred, y_batch)\n          optimizer.zero_grad()\n          loss.backward()\n          optimizer.step()\n\n          loss, current = loss.item(), (i + 1) * len(x_batch)\n          if i == len(steps) - 1:\n              model.eval()\n              pred = model(x_test).flatten()\n              vloss = loss_fn(pred, y_test)\n              if es(model, vloss):\n                  done = True\n              pbar.set_description(\n                  f\"Epoch: {epoch}, tloss: {loss}, vloss: {vloss:&gt;7f}, EStop:[{es.status}]\"\n              )\n          else:\n              pbar.set_description(f\"Epoch: {epoch}, tloss {loss:}\")\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/PyTorch/04_Advanced/#dropout","title":"Dropout","text":"<pre><code>## p = p_drop; NOT p_keep like Tensorflow\nmodel = nn.Sequential(\n  nn.Dropout(p=0.2),\n  ## ...,\n  nn.Dropout(p=0.2),\n  ## ...,\n)\n\n## make sure to specify model.train() and model.eval(), as dropout processing is only required for \"building\" the network. no need to processing for evaluation\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/PyTorch/04_Advanced/#quantization","title":"Quantization","text":"<pre><code>## direct precision reduction\nmodel = model.half() ## changes everything from float32 to float16\n\n## dynamic precision reduction\nmodel = torch.quantization.quantize_dynamic(\n  model,\n  {torch.nn.Linear},\n  dtype=torch.qint8\n)\n\n## static precision reduction\n## very complicated ngl\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/PyTorch/04_Advanced/#constrained-optimization","title":"Constrained Optimization","text":"<p>Warning: this clamping is not communicated to the optimizer, and in particular destroys the gradients. So the optimizer falsely believes that it has moved the parameter in a certain direction, when in fact it is clamped to the same value as before.</p> <pre><code>opt = optim.SGD(model.parameters(), lr=0.1)\nfor i in range(1000):\n    out = model(inputs)\n    loss = loss_fn(out, labels)\n    print(i, loss.item())\n    opt.zero_grad()\n    loss.backward()\n    opt.step()\n\n    # enforce the constraint that the weights fall in the range (-1, 1)\n    with torch.no_grad():\n        for param in model.parameters():\n            param.clamp_(-1, 1)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/PyTorch/04_Advanced/#ensembling","title":"Ensembling","text":""},{"location":"Tools/AI%20%26%20Data/PyTorch/04_Advanced/#approach-1","title":"Approach 1","text":"<pre><code>class MyEnsemble(nn.Module):\n    def __init__(self, modelA, modelB, nb_classes=10):\n        super(MyEnsemble, self).__init__()\n        self.modelA = modelA\n        self.modelB = modelB\n        # Remove last linear layer\n        self.modelA.fc = nn.Identity()\n        self.modelB.fc = nn.Identity()\n\n        # Create new classifier\n        self.classifier = nn.Linear(2048+512, nb_classes)\n\n    def forward(self, x):\n        x1 = self.modelA(x.clone())  # clone to make sure x is not changed by inplace methods\n        x1 = x1.view(x1.size(0), -1)\n        x2 = self.modelB(x)\n        x2 = x2.view(x2.size(0), -1)\n        x = torch.cat((x1, x2), dim=1)\n\n        x = self.classifier(F.relu(x))\n        return x\n\n# Train your separate models\n# ...\n# We use pretrained torchvision models here\nmodelA = models.resnet50(pretrained=True)\nmodelB = models.resnet18(pretrained=True)\n\n# Freeze these models\nfor param in modelA.parameters():\n    param.requires_grad_(False)\n\nfor param in modelB.parameters():\n    param.requires_grad_(False)\n\n# Create ensemble model\nmodel = MyEnsemble(modelA, modelB)\nx = torch.randn(1, 3, 224, 224)\noutput = model(x)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/PyTorch/04_Advanced/#approach-2","title":"Approach 2","text":"<p>When you create an ensemble in PyTorch, it's better to use the <code>nn.ModuleList()</code> class from PyTorch. The <code>nn.ModuleList()</code> has the same functions as a normal Python list like <code>append()</code>. When you create an Ensemble Model like this, you can directly call the <code>backward</code> operations and the gradient descent will occur through the model.</p> <p>Below is an ensemble Neural Network (<code>EnsembleNet</code>) that uses the <code>NeuralNet</code> as individual NN instances for the ensemble.</p> <p>To use bagging, simply create an X_input_list where the different elements of the list are Tensors that have been sampled with replacement from your training data. (Your X_input_list and the num_ensemble must be of the same size)</p> <p>You can modify the <code>EnsembleNet</code> initialization code to take a list of different neural networks as well.</p> <pre><code>class NeuralNet(nn.Module):\n  def __init__(self):\n    super(NeuralNet, self).__init__()\n    self.fc1 = nn.Linear(in_dim, out_dim)\n    self.fc2 = nn.Linear(out_dim, 1)\n\n\n  def forward(self, X):\n    \"\"\" X must be of shape [-1, in_dim]\"\"\"\n    X = self.fc1(X)\n    return torch.sigmoid(self.fc2(X))\n\n\nclass EnsembleNet(nn.Module):\n  def __init__(self, net = NeuralNet, num_ensemble=5, seed_val=SEED):\n      super(EnsembleNet, self).__init__()\n      self.ensembles = nn.ModuleList()\n\n      for i in range(num_ensemble):\n          torch.manual_seed(seed_val*i+1)\n          if torch.cuda.is_available(): # To randomize init of NNs for Ensembles\n              torch.cuda.manual_seed(seed_val*i+1)\n          self.ensembles.append(net)\n\n      self.final = nn.Linear(num_ensemble, 1)\n\n  def forward(self, X_in_list):\n      pred = torch.cat([net(X_in_list[i]) for i,net in enumerate(self.ensembles)])\n      pred = pred.reshape(-1, len(self.ensembles))\n      return torch.sigmoid(self.final(pred))\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/PyTorch/04_Advanced/#sklearn-integration","title":"Sklearn Integration","text":"<pre><code>from skorch import *\n\nmodel = NeuralNetRegressor(\n  Network,\n  max_epochs=100,\n  lr=0.001,\n  verbose=1\n)\n\nmodel = NeuralNetClassifier(\n  Network,\n  max_epochs=10,\n  lr=0.1,\n  # Shuffle training data on each epoch\n  iterator_train__shuffle=True,\n)\n</code></pre> <pre><code>model.fit(X, y)\npred = model.predict(X)\npred_proba = model.predict_proba(X)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/PyTorch/04_Advanced/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<pre><code>from sklearn.model_selection import GridSearchCV\n\nparams = {\n    'lr': [0.001,0.005, 0.01, 0.05, 0.1, 0.2, 0.3],\n    'max_epochs': list(range(500,5500, 500))\n}\n\ngs = GridSearchCV(model, params, refit=False, scoring='r2', verbose=1, cv=10)\n\ngs.fit(X_trf, y_trf)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/PyTorch/05_Performance_Optimization/","title":"Performance Optimization","text":""},{"location":"Tools/AI%20%26%20Data/PyTorch/05_Performance_Optimization/#idk","title":"IDK","text":""},{"location":"Tools/AI%20%26%20Data/PyTorch/05_Performance_Optimization/#gpu","title":"GPU","text":"<pre><code>if torch.cuda.is_available():\n    device = torch.device(\"cuda:0\")\nelse:\n    device = torch.device(\"cpu\")\n\nprint(f\"Using {device}\")\n</code></pre> <pre><code># tensors\ninput = input.to(device).half()\noutput = output.to(device).half()\n\n# model\nmodel = NeuralNet().to(device)\nmodel.half()\n\n# Optimizer\noptimizer = SGD(model.parameters(), lr=learning_rate)\nfor state in optimizer.state.values():\n  for k, v in state.items():\n    if isinstance(v, torch.Tensor):\n      state[k] = v.to(device).half()\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/PyTorch/05_Performance_Optimization/#idk_1","title":"IDK","text":"<pre><code>torch.set_num_threads(int) # number of threads used for intraop parallelism\ntorch.set_num_interop_threads(int) # interop parallelism (e.g. in the JIT interpreter) on the CPU\n</code></pre> <p>If you have 4 cores and need to do, say, 8 matrix multiplications (with separate data) you could use 4 cores to do each matrix multiplication (intra-op-parallelism). Or you could use a single core for each op and run 4 of them in parallel (inter-op-parallelism). In training, you also might want to have some cores for the dataloader, for inference, the JIT can parallelize things (I think). The configuration is documented here, but without much explanation: https://pytorch.org/docs/stable/torch.html#parallelism 1.4k</p>"},{"location":"Tools/AI%20%26%20Data/PyTorch/05_Performance_Optimization/#idk_2","title":"IDK","text":"<pre><code>import torch.distributed as dist\ndist.init_process_group(backend=\"gloo\")\n</code></pre> <pre><code>local_rank = int(os.environ[\"LOCAL_RANK\"])\nmodel = torch.nn.parallel.DistributedDataParallel(\n    model,\n    device_ids=[local_rank],\n    output_device=local_rank,\n)\n</code></pre> <pre><code>train_sampler = DistributedSampler(train_data)\ntrain_loader = DataLoader(\n    ...\n    train_data,\n    shuffle=False, # train_sampler will shuffle for you.\n    sampler=train_sampler,\n)\nfor e in range(1, epochs + 1):\n    train_sampler.set_epoch(e) # This makes sure that every epoch the data is distributed to processes differently.\n    train(train_loader)\n</code></pre> <pre><code># to see which process throws what error\nfrom torch.distributed.elastic.multiprocessing.errors import record\n\n@record\ndef main():\n    # do train\n    pass\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/PyTorch/05_Performance_Optimization/#quantization","title":"Quantization","text":"<pre><code>class VerySimpleNet(nn.Module):\n    def __init__(self, hidden_size_1=100, hidden_size_2=100):\n        super(VerySimpleNet,self).__init__()\n        self.quant = torch.quantization.QuantStub()\n        self.linear1 = nn.Linear(28*28, hidden_size_1) \n        self.linear2 = nn.Linear(hidden_size_1, hidden_size_2) \n        self.linear3 = nn.Linear(hidden_size_2, 10)\n        self.relu = nn.ReLU()\n        self.dequant = torch.quantization.DeQuantStub()\n\n    def forward(self, img):\n        x = img.view(-1, 28*28)\n        x = self.quant(x)\n        x = self.relu(self.linear1(x))\n        x = self.relu(self.linear2(x))\n        x = self.linear3(x)\n        x = self.dequant(x)\n        return x\n</code></pre> <pre><code>net = VerySimpleNet().to(device)\nnet.qconfig = torch.ao.quantization.default_qconfig\nnet.train()\n\n# Insert observers\nnet_quantized = torch.ao.quantization.prepare_qat(net)\n</code></pre> <pre><code>def print_size_of_model(model):\n    torch.save(model.state_dict(), \"temp_delme.p\")\n    print('Size (KB):', os.path.getsize(\"temp_delme.p\")/1e3)\n    os.remove('temp_delme.p')\n\ntrain_model(model, train_dl, net_quantized, epochs=1)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/PyTorch/05_Performance_Optimization/#quantize-the-model-using-the-statistics-collected","title":"Quantize the model using the statistics collected","text":"<pre><code>net_quantized.eval()\nnet_quantized = torch.ao.quantization.convert(net_quantized)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/PyTorch/05_Performance_Optimization/#idk_3","title":"IDK","text":"<p>Instead of feeding PyTorch sparse tensor directly into the dataloader, I wrote a custom Dataset class which only accept scipy coo_matrix or equivalent. Then, I wrote a custom collate function for the dataloader which to transform scipy coo_matrix to pytorch sparse tensor during data loading.</p> <p>~ https://discuss.pytorch.org/t/dataloader-loads-data-very-slow-on-sparse-tensor/117391</p> <pre><code>from typing import Union\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom scipy.sparse import (random, \n                          coo_matrix,\n                          csr_matrix, \n                          vstack)\nfrom tqdm import tqdm\n</code></pre> <pre><code>class SparseDataset2():\n    \"\"\"\n    Custom Dataset class for scipy sparse matrix\n    \"\"\"\n    def __init__(self, data:Union[np.ndarray, coo_matrix, csr_matrix], \n                 targets:Union[np.ndarray, coo_matrix, csr_matrix], \n                 transform:bool = None):\n\n        # Transform data coo_matrix to csr_matrix for indexing\n        if type(data) == coo_matrix:\n            self.data = data.tocsr()\n        else:\n            self.data = data\n\n        # Transform targets coo_matrix to csr_matrix for indexing\n        if type(targets) == coo_matrix:\n            self.targets = targets.tocsr()\n        else:\n            self.targets = targets\n\n        self.transform = transform # Can be removed\n\n    def __getitem__(self, index):\n        return self.data[index], self.targets[index]\n\n    def __len__(self):\n        return self.data.shape[0]\n\ndef sparse_coo_to_tensor(coo:coo_matrix):\n    \"\"\"\n    Transform scipy coo matrix to pytorch sparse tensor\n    \"\"\"\n    values = coo.data\n    indices = np.vstack((coo.row, coo.col))\n    shape = coo.shape\n\n    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    i = torch.LongTensor(indices).to(DEVICE)\n    v = torch.FloatTensor(values).to(DEVICE)\n    s = torch.Size(shape)\n\n    return torch.sparse.FloatTensor(i, v, s).to(DEVICE)\n\ndef sparse_batch_collate2(batch): \n    \"\"\"\n    Collate function which to transform scipy coo matrix to pytorch sparse tensor\n    \"\"\"\n    # batch[0] since it is returned as a one element list\n    data_batch, targets_batch = batch[0]\n\n    if type(data_batch[0]) == csr_matrix:\n        data_batch = data_batch.tocoo() # removed vstack\n        data_batch = sparse_coo_to_tensor2(data_batch)\n    else:\n        data_batch = torch.DoubleTensor(data_batch)\n\n    if type(targets_batch[0]) == csr_matrix:\n        targets_batch = targets_batch.tocoo() # removed vstack\n        targets_batch = sparse_coo_to_tensor2(targets_batch)\n    else:\n        targets_batch = torch.DoubleTensor(targets_batch)\n    return data_batch, targets_batch\n</code></pre> <pre><code>X = random(800000, 300, density=0.25)\ny = np.arange(800000)\nds = SparseDataset(X, y)\nsampler = torch.utils.data.sampler.BatchSampler(\n    torch.utils.data.sampler.RandomSampler(ds,\n                      generator=torch.Generator(device='cuda')),\n    batch_size=1024,\n    drop_last=False)\ndl = DataLoader(ds, \n                      batch_size = 1, \n                      collate_fn = sparse_batch_collate2,\n                      generator=torch.Generator(device='cuda'),\n          sampler = sampler)\n\nfor x, y in tqdm(iter(dl)):\n  pass\n\n# 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 782/782 [00:11&lt;00:00, 71.03it/s]\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Scipy/","title":"Scipy","text":"<p>Check out: https://github.com/OpenSourceEconomics/estimagic</p>"},{"location":"Tools/AI%20%26%20Data/Scipy/01/","title":"01","text":""},{"location":"Tools/AI%20%26%20Data/Scipy/01/#optimization","title":"Optimization","text":"<pre><code>def func(x, m, c):\nreturn m*x + c\n\ndef jacobian(x, m, c):\nreturn ## first derivative\n\ndef hessian(x, m, c):\nreturn ## second derivative\n\nconstraints = (\n## equality constraint\n## fun = 0\ndict(\n  type = 'eq', ## = 0\nfun = lambda x: x.sum() - 1.0,\njac = lambda x: np.ones_like(x) ## optional, but recommended\n),\n## inequality constraint\n## fun &gt;= 0\ndict(\n  type = 'ineq', ## = 0\nfun = lambda x: x.sum() + 1.0,\njac = lambda x: np.ones_like(x) ## optional, but recommended\n),\n)\n\nbounds = (\n(0, None),\n(0, None)\n)\n\nres = minimize(\nfunc=func,\nx0 = [0, 0], ## initial_guess\nmethod = 'SLSQP',\njac = jacobian,\n## hes = hessian,\nbounds = bounds,\nconstraints = constraints\n)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Scipy/01/#interpolation","title":"Interpolation","text":"<pre><code>from scipy.interpolate import interp1d\n\nf = interp1d(\nx,\ny,\nkind=\"linear\" ## \"cubic\"\n)\ninterpolated_values = f(x_dash)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Scipy/01/#curve-fitting","title":"Curve Fitting","text":"<pre><code>from scipy.optimize import curve_fit\n\ndef func(x, a, b):\nreturn (a * x**2) + b\n\npopt, pcov = curve_fit(\nfunc,\nx_data,\ny_data,\np0 = (1, 1)\n) \n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Scipy/01/#pcov","title":"PCov","text":"<pre><code>pcov = (\n  np.sqrt(self.optimization.fun) * np.sqrt(np.diag(\n    self\n    .optimization\n    .hess_inv\n    .todense()\n  ))\n)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Scipy/01/#custom-curve-fit-with-regularization","title":"Custom Curve Fit with Regularization","text":""},{"location":"Tools/AI%20%26%20Data/Scipy/01/#custom-solvers","title":"Custom Solvers","text":""},{"location":"Tools/AI%20%26%20Data/Scipy/01/#solver","title":"Solver","text":"<pre><code>  import numpy as np\n  from scipy.optimize import OptimizeResult\n</code></pre> <p>The below implementations always return <code>success=True</code></p> <p>Shouldn't we check it is actually successful?</p>"},{"location":"Tools/AI%20%26%20Data/Scipy/01/#bgd","title":"BGD","text":"<pre><code>    def bgd(\n        fun,\n        x0,\n        jac,\n        args=(),\n        learning_rate=0.001,\n        mass=0.9,\n        startiter=0,\n        maxiter=1000,\n        callback=None,\n        **kwargs\n    ):\n        \"\"\"``scipy.optimize.minimize`` compatible implementation of batch\n        gradient descent with momentum.\n\n        Adapted from ``autograd/misc/optimizers.py``.\n        \"\"\"\n        x = x0\n        velocity = np.zeros_like(x)\n\n        for i in range(startiter, startiter + maxiter):\n            g = jac(x)\n\n            if callback and callback(x):\n                break\n\n            velocity = mass * velocity - (1.0 - mass) * g\n            x = x + learning_rate * velocity\n\n        i += 1\n        return OptimizeResult(x=x, fun=fun(x), jac=g, nit=i, nfev=i, success=True)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Scipy/01/#rmsprop","title":"RMSProp","text":"<pre><code>    def rmsprop(\n        fun,\n        x0,\n        jac,\n        args=(),\n        learning_rate=0.1,\n        gamma=0.9,\n        eps=1e-8,\n        startiter=0,\n        maxiter=1000,\n        callback=None,\n        **kwargs\n    ):\n        \"\"\"``scipy.optimize.minimize`` compatible implementation of root mean\n        squared prop: See Adagrad paper for details.\n\n        Adapted from ``autograd/misc/optimizers.py``.\n        \"\"\"\n        x = x0\n        avg_sq_grad = np.ones_like(x)\n\n        for i in range(startiter, startiter + maxiter):\n            g = jac(x)\n\n            if callback and callback(x):\n                break\n\n            avg_sq_grad = avg_sq_grad * gamma + g**2 * (1 - gamma)\n            x = x - learning_rate * g / (np.sqrt(avg_sq_grad) + eps)\n\n        i += 1\n        return OptimizeResult(x=x, fun=fun(x), jac=g, nit=i, nfev=i, success=True)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Scipy/01/#adam","title":"Adam","text":"<pre><code>    def adam(\n        fun,\n        x0,\n        jac,\n        args=(),\n        learning_rate=0.001,\n        beta1=0.9,\n        beta2=0.999,\n        eps=1e-8,\n        startiter=0,\n        maxiter=1000,\n        callback=None,\n        **kwargs\n    ):\n        \"\"\"``scipy.optimize.minimize`` compatible implementation of ADAM -\n        [http://arxiv.org/pdf/1412.6980.pdf].\n\n        Adapted from ``autograd/misc/optimizers.py``.\n        \"\"\"\n        x = x0\n        m = np.zeros_like(x)\n        v = np.zeros_like(x)\n\n        for i in range(startiter, startiter + maxiter):\n            g = jac(x)\n\n            if callback and callback(x):\n                break\n\n            m = (1 - beta1) * g + beta1 * m  ## first  moment estimate.\n            v = (1 - beta2) * (g**2) + beta2 * v  ## second moment estimate.\n            mhat = m / (1 - beta1**(i + 1))  ## bias correction.\n            vhat = v / (1 - beta2**(i + 1))\n            x = x - learning_rate * mhat / (np.sqrt(vhat) + eps)\n\n        i += 1\n        return OptimizeResult(x=x, fun=fun(x), jac=g, nit=i, nfev=i, success=True)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Scipy/01/#usage","title":"Usage","text":"<pre><code>  from scipy.optimize import minimize\n\n  res = minimize(..., method = func) ## func = sgd, rmsprop, or adam\n  print(res.x)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Scipy/01/#calculus","title":"Calculus","text":"<p>Differentiation</p> <p>Analytical</p> <pre><code>  from scipy.misc import derivative as d\n\n  def f(x):\n    return x**2\n\n  d(f, x, dx=1e-6)\n</code></pre> <pre><code>  def pd(func, var=0, point=[]):\n      ## partial derivative\n      args = point[:]\n      def wraps(x):\n          args[var] = x\n          return func(*args)\n      return derivative(wraps, point[var], dx = 1e-6)\n\n  pd(foo, 0, [3,1]) ## 6.0000000008386678\n  pd(foo, 1, [3,1]) ## 2.9999999995311555\n</code></pre> <p>Integration</p> <pre><code>from scipy.integrate import quad\n\ndef func(x):\n  return x**2\n\nintegral, integral_error = quad(func, 0, 1)\n</code></pre> <pre><code>from scipy.integrate import dbquad\n\ndef func(x):\n  return x**2 + y**2\n\nintegral, integral_error = dblquad(func, 0, 1, 0, 1)\n</code></pre> <pre><code>from scipy.integrate import nquad\n</code></pre> <p>Differential Equations</p> <pre><code>from scipy.integrate import odeint\n\ndef dvdt(v, t):\n  return 3 * v**2 - 5\n\nv0 = 0\n\nt = np.linspace(0, 1, 100)\nsol = odeint(dvdt, v0, t)\n</code></pre> <pre><code>## coupled\n\ndef dSdx(S, x):\n  y1, y2 = S\n  return [\n    y1 + y2**2 + 3*x,\n    3*y1 + y2**3 - np.cos(x)\n  ]\n\ny1_0 = 0\ny2_0 = 0\nS_0 = (y1_0, y2_0)\n\nx = np.linspace(0, 1, 100)\nsol = odeint(dSdx, S_0, x)\n\ny1 = sol.T[0]\ny2 = sol.T[1]\n</code></pre> <pre><code>## second-order DE must be transformed into 2 coupled first order DE\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Scipy/01/#fourier-transforms","title":"Fourier Transforms","text":""},{"location":"Tools/AI%20%26%20Data/Scipy/01/#numeric","title":"Numeric","text":""},{"location":"Tools/AI%20%26%20Data/Scipy/01/#continuous-time-frequency","title":"Continuous Time &amp; Frequency","text":"<pre><code>from scipy.integrate import quad\n\ndef x(t, k):\n  return np.exp(-k * t**2) * np.sin(k*t) * t**4\n\ndef get_x_ft(x, f, k):\n  x_FT_integrand_real = lambda t: np.real(x(t, k) * np.exp( -2*np.pi*1j*f*t) )\n  x_FT_integrand_complex = lambda t: np.imag(x(t, k) * np.exp( -2*np.pi*1j*f*t) )\n\n  x_FT_real = quad(x_FT_integrand_real, -np.inf, np.inf)[0]\n  x_FT_comp = quad(x_FT_integrand_comp, -np.inf, np.inf)[0]\n\n  return x_FT_real + 1j*x_FT_comp\n</code></pre> <pre><code>f = np.linspace(-4, 4, 100)\nx_FT = np.vectorize(get_x_FT)(x, f, k=2)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Scipy/01/#continuous-time-discrete-frequency","title":"Continuous Time &amp; Discrete Frequency","text":"<pre><code>from scipy.integrate import quad\n\ndef x(t, k):\n  return np.exp(-k * t**2) * np.sin(k*t) /t\n\ndef get_x_ft(x, f, k, T):\n  x_FT_integrand_real = lambda t: np.real(x(t, k) * np.exp( -2*np.pi*1j*(n/T)*t) )\n  x_FT_integrand_complex = lambda t: np.imag(x(t, k) * np.exp( -2*np.pi*1j*(n/T)*t) )\n\n  x_FT_real = quad(x_FT_integrand_real, 0, T)[0]\n  x_FT_comp = quad(x_FT_integrand_comp, 0, T)[0]\n\n  return x_FT_real + 1j*x_FT_comp\n</code></pre> <pre><code>ns = np.arange(0, 20, 1)\nx_FT = np.vectorize(get_x_FT)(x, ns, k=2, T=4)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Scipy/01/#fft","title":"FFT","text":"<p>Discrete Time &amp; Frequency</p> <pre><code># 1D\nfrom scipy.fft import fft, fftfreq\n\ny = fft(x)\nf = fftfreq(len(x), np.diff(t)[0])\n</code></pre> <pre><code># 2D\nfrom scipy.fft import fft2, fftfreq2\n\nimg_FT =_fft2(img)\nfy = fftfreq(img.shape[0], d=10) # suppose the spacing between pixels is 10mm\nfx = fftfreq(img.shape[1], d=10)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Scipy/01/#linear-algebra","title":"Linear Algebra","text":"<p>Not very applicable to me</p>"},{"location":"Tools/AI%20%26%20Data/Scipy/01/#stats","title":"Stats","text":"<pre><code>dist.pmf() ## probability mass function\ndist.cdf() ## cumulative dist function\ndist.ppf() ## inverse of cdf\ndist.rvs() ## generate random variable sample\n</code></pre> <p>beta</p> <pre><code>from scipy.stats import beta\na, b = 2.5, 3.1\n\nmean, var, skew, kurt = beta.stats(a, b, moments=\"mvsk\")\n</code></pre> <p>normal</p> <pre><code>norm.ppf(0.025, mu, sigma)\nnorm.ppf(0.975, mu, sigma)\n</code></pre> <p>skewnorm</p> <pre><code>from scipy.stats import skewnorm\n\n## skewness_factor\na = 5 ## +ve skew\na = -5 ## -ve skew\n\nmean, var, skew, kurt = skewnorm.stats(a, moments='mvsk')\ngenerated_values = skewnorm.rvs(a, size=1000)\n</code></pre> <p>custom</p> <pre><code>import scipy.stats as st\n\nclass my_dist(st.rv_continuous):\n  def _pdf(self, x, a1, a2, b1, b2):\n    return np.sin(x)\n\n  ## def _cdf(self, )\n\n  ## def _rvs(self, )\n\nmy_rv = my_dist(a = 0, b=np.inf)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Scipy/01/#jac","title":"Jac","text":"<p>Need not be the final </p>"},{"location":"Tools/AI%20%26%20Data/Scipy/02_Regression/","title":"Regression","text":"<pre><code>import scipy.optimize as so\nimport sympy as sp\n</code></pre> <pre><code>def linearModelLossRSS(b, X, y):\n    # Make predictions\n    predY = linearModelPredict(b, X)\n    # Compute residuals\n    res = y - predY\n    # Compute the residual sum of squares\n    residual_sum_of_squares = sum(res**2)\n    # Compute the gradient of the loss\n    gradient = -2 * np.dot(res, X)\n    return (residual_sum_of_squares, gradient)\n</code></pre> <pre><code>def linearModelFit(X, y, lossfcn):\n    nrows, ncols = X.shape\n    betas = np.zeros((ncols, 1))\n    # Optimize the loss\n    RES = so.minimize(\n      lossfcn,\n      betas,\n      args=(X, y),\n      jac=True,\n      # hess = 2 # isn't it just 2\n    )\n    # Obtain estimates from the optimizer\n    estimated_betas = RES.x\n    # Compute goodness of fit\n    res = y - np.mean(y)\n    TSS = sum(res**2)\n    RSS, deriv = linearModelLossRSS(estimated_betas, X, y)  # L2 loss and RSS are the same thing\n    R2 = 1 - RSS / TSS\n    return (estimated_betas, R2)\n</code></pre> <pre><code>X = np.array([[1, 0], [1, -1], [1, 2]])\ny = np.array([0, 0.4, 2]) \n\nbeta, R2 = linearModelFit(X, y, linearModelLossRSS)\n\nprint(\"Betas are\", beta)\nprint(\"R2:\\n\", R2)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Scipy/02_Regression/#obtaining-jac-and-hessian","title":"Obtaining Jac and Hessian","text":"<pre><code>## Basic Linear Regression\nx = sp.Symbol('x', constant=True, real=True)\ny = sp.Symbol('y', constant=True, real=True)\nm = sp.Symbol('m', real=True)\nc = sp.Symbol('c', real=True)\n\n\nyhat = m * x + c\nl = (yhat - y)**2\n\n\nvars = [m, b]\n\n\n## First-Order Reaction\n\n\nc0 = sp.Symbol('c_0', constant=True, real=True)\nt = sp.Symbol('t', constant=True, real=True)\nct = sp.Symbol('c_t', constant=True, real=True)\n\n\nk = sp.Symbol('k', real=True)\n\n\nchat = c0 * sp.exp(-k*t)\nl = (chat - ct)**2\n\n\nvars = [k]\n</code></pre> <pre><code>## Printing\n\nprint(\"Function\")\ndisplay(yhat)\n\n\nprint(\"\\nLoss\")\ndisplay(l)\n\n\nprint(\"\\nJac\")\nfor var in vars:\n display(l.diff(var).factor())\n\n\nprint(\"\\nHess\")\nfor var in vars:\n for var_inner in vars:\n   display(l.diff(var).diff(var).factor())\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Scipy/02_Regression/#idk","title":"IDK","text":"<pre><code>from scipy.optimize import minimize\nimport numpy as np\n\n# x = np.array([139, ...])\n# y = np.array([151, ...])\n\n# Define the Model\ndef f(x, a, b): return a * x + b\n\n# The objective Function to minimize (least-squares regression)\ndef obj(x, y, a, b): return np.sum((y - f(x, a, b))**2)\n\n# define the bounds -infty &lt; a &lt; infty,  b &lt;= 0\nbounds = [(None, None), (None, 0)]\n\nres = minimize(lambda coeffs: obj(x, y, *coeffs), x0=np.zeros(2), bounds=bounds)\n# res.x contains your coefficients\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Scipy/03_Jax/","title":"Jax","text":"<pre><code># from scipy.optimize import minimize\n\nfrom jax.scipy.optimize import minimize\nimport jax.numpy as np\n</code></pre> <p>Gradients of fun are calculated automatically using JAX\u2019s autodiff support when required.</p>"},{"location":"Tools/AI%20%26%20Data/Scipy/03_Jax/#speed-up-existing-implementation","title":"Speed Up Existing Implementation","text":"<pre><code>import jax.numpy as np\nfrom jax import jit, value_and_grad\n\ndef costFunction(weights):\n   # reshapes flattened weights into 2d matrix\n   weights = np.reshape(weights, newshape=(100, 75))\n\n   # weighted row-wise sum\n   weighted = np.sum(x * weights, axis=1)\n\n   # squared residuals\n   residualsSquared = (y - weighted) ** 2\n\n   return np.sum(residualsSquared)\n\n# create the derivatives\nobj_and_grad = jit(value_and_grad(costFunction))\n\nminimize(obj_and_grad, x0=startingWeights.flatten(), jac=True)\n</code></pre> <p>Or take the diff using <code>jax.diff</code></p> <ul> <li>Provide the jac &amp; hessian to the minimize function</li> </ul>"},{"location":"Tools/AI%20%26%20Data/Scipy/03_Jax/#use-jaxscipy","title":"Use <code>jax.scipy</code>","text":"<pre><code>import \no.minimize(fun, x0, args=(), *, method, tol=None, options=None)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Sklearn/","title":"Sklearn","text":""},{"location":"Tools/AI%20%26%20Data/Sklearn/#basics","title":"Basics","text":"<pre><code>model = model()\n\nif \"n_jobs\" in dir(model):\n    kwargs[\"n_jobs\"]= -1\nif \"probability\" in dir(model):\n    kwargs[\"probability\"]= True\n\nmodel.set_params(**kwargs)\n\nmodel.fit(X_train, y_train)\nmodel.predict(X)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Sklearn/#pca","title":"PCA","text":"<pre><code>from sklearn.decomposition import PCA\n\ndf = pd.DataFrame(data=np.random.normal(0, 1, (20, 10)))\n\npca = PCA(n_components=5)\npca.fit(df)\n\npca.components_\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Sklearn/#stratified-sampling","title":"Stratified Sampling","text":"<pre><code>X_train, X_test, y_train, y_test = train_test_split(X, y, \n  test_size = 0.5,\n  random_state = 0,\n  stratify = y\n)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Sklearn/#save-load-model","title":"Save &amp; Load Model","text":""},{"location":"Tools/AI%20%26%20Data/Sklearn/#pickle","title":"Pickle","text":"<pre><code>import pickle\nfile_name = \"model.pkl\"\n\n## save\nwith open(file_name, \"wb\") as f:\n    pickle.dump(model, f)\n\n#load\nwith open(file_name, \"rb\") as f:\n  model = pickle.load(f)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Sklearn/#json","title":"Json","text":"<pre><code>## save\nmodel.save_model(\"model.json\")\n\n## load\nmodel_new = xgb.XGBRegressor()\nmodel_new.load_model(\"model.json\")\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Sklearn/#pipelines","title":"Pipelines","text":""},{"location":"Tools/AI%20%26%20Data/Sklearn/#what","title":"What?","text":"<p>Systematic organization of required operations</p>"},{"location":"Tools/AI%20%26%20Data/Sklearn/#parts","title":"Parts","text":""},{"location":"Tools/AI%20%26%20Data/Sklearn/#transformer","title":"Transformer","text":"<p>filter and/or modify data <code>fit()</code> and <code>transform()</code></p>"},{"location":"Tools/AI%20%26%20Data/Sklearn/#estimator","title":"Estimator","text":"<p>Learn from data <code>fit()</code> and <code>predict()</code></p>"},{"location":"Tools/AI%20%26%20Data/Sklearn/#implementation","title":"Implementation","text":""},{"location":"Tools/AI%20%26%20Data/Sklearn/#libraries","title":"Libraries","text":"<pre><code>from sklearn.pipeline import make_pipeline\n##  just `Pipeline` involves naming each step\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Sklearn/#pipeline-used-here","title":"Pipeline Used Here","text":"<p>Data Preprocessing by using Standard Scaler Reduce Dimension using PCA Apply Classifier</p>"},{"location":"Tools/AI%20%26%20Data/Sklearn/#initializing-pipelines","title":"Initializing Pipelines","text":"<p><pre><code>pipeline_lr = make_pipeline(\n  StandardScaler(),\n  LogisticRegression()\n)\n\n## more controlled way\npipeline_dt = Pipeline([\n  ('scaler',StandardScaler()),\n  ('classifier',DecisionTreeClassifier())\n])\n</code></pre> <pre><code>pipelines = [pipeline_lr, pipeline_dt, pipeline_randomforest]\n\n## Dictionary of pipelines and classifier types for ease of reference\npipe_dict = {\n  0: 'Logistic Regression',\n  1: 'Decision Tree'\n}\n\nbest_accuracy = 0.0\nbest_classifier = 0\nbest_pipeline=\"\"\n</code></pre></p>"},{"location":"Tools/AI%20%26%20Data/Sklearn/#pipeline-parameters","title":"Pipeline Parameters","text":"<pre><code>pipe.get_params()\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Sklearn/#trainingfitting","title":"Training/Fitting","text":"<pre><code>## Fit the pipelines\nfor pipe in pipelines:\n  pipe.fit(X_train, y_train)\n    ## pipe.fit(X_train, y_train, classifier__sample_weight=1)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Sklearn/#results","title":"Results","text":"<p><pre><code>for i,model in enumerate(pipelines):\n    print(\n      pipe_dict[i], \"Test Accuracy:\", model.score(X_test,y_test)\n    )\n</code></pre> <pre><code>for i,model in enumerate(pipelines):\n    if model.score(X_test,y_test)&gt;best_accuracy:\n        best_accuracy=model.score(X_test,y_test)\n        best_classifier=i\n        best_pipeline=model\n\nprint('Classifier with best accuracy:{}'.format(pipe_dict[best_classifier]))\n</code></pre></p>"},{"location":"Tools/AI%20%26%20Data/Sklearn/#change-loss_cost-function","title":"Change Loss_Cost Function","text":"<pre><code>def custom_loss(y_true, y_pred):\n    fn_penalty = 5 ## penalty for false negatives\n    fp_penalty = 1 ## penalty for false positives\n\n    ## calculate true positives, false positives, and false negatives\n    tp = ((y_true == 1) &amp; (y_pred == 1)).sum()\n    fp = ((y_true == 0) &amp; (y_pred == 1)).sum()\n    fn = ((y_true == 1) &amp; (y_pred == 0)).sum()\n\n    ## calculate loss\n    loss = fp_penalty * fp + fn_penalty * fn\n\n    return loss\n\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(loss=custom_loss)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Sklearn/#custom-ensembling","title":"Custom Ensembling","text":"<p>Voting Stacking</p>"},{"location":"Tools/AI%20%26%20Data/Sklearn/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<p><pre><code>## create the Pipeline\npipe = Pipeline(\n  [('preprocessor', ct), ('classifier', clf1)],\n  memory = \"cache_name\" # cache results, especially useful for grid-search\n)\n\n## create the parameter dictionary for clf1\nparams = [\n  dict(\n    preprocessor__vectorizer__ngram_range = [(1, 1), (1, 2)],\n    classifier__penalty = ['l1', 'l2'],\n    classifier = [my_random_forest]\n  ),\n  dict(\n    preprocessor__vectorizer__ngram_range = [(1, 1), (1, 2)],\n    classifier__n_estimators = [100, 200],\n    classifier = [my_decision_tree]\n  )\n]\n</code></pre> <pre><code>grid = GridSearchCV(\n  pipe,\n  param_grid = params,\n  cv = 5,\n  refit = False, # True forces to refit best model for the entire dataset at the end; pointless if you only want cv results\n  n_jobs = -1,\n  memory = \"grid_search\" # caching\n)\n\ngrid.fit(X, y)\n\nprint(grid.best_params_)\n</code></pre></p>"},{"location":"Tools/AI%20%26%20Data/Sklearn/#linear-regression-statistical-inference","title":"Linear Regression statistical inference","text":"<p>Parameter standard errors <pre><code>N = len(X)\np = len(X.columns) + 1  ## plus one because LinearRegression adds an intercept term\n\nX_with_intercept = np.empty(shape=(N, p), dtype=np.float)\nX_with_intercept[:, 0] = 1\nX_with_intercept[:, 1:p] = X.values\n\nbeta_hat = np.linalg.inv(X_with_intercept.T @ X_with_intercept) @ X_with_intercept.T @ y.values\nprint(beta_hat)\n\ny_hat = model.predict(X)\nresiduals = y.values - y_hat\nresidual_sum_of_squares = residuals.T @ residuals\nsigma_squared_hat = residual_sum_of_squares[0, 0] / (N - p)\nvar_beta_hat = np.linalg.inv(X_with_intercept.T @ X_with_intercept) * sigma_squared_hat\nfor p_ in range(p):\n    standard_error = var_beta_hat[p_, p_] ** 0.5\n    print(f\"SE(beta_hat[{p_}]): {standard_error}\")\n</code></pre></p>"},{"location":"Tools/AI%20%26%20Data/Sklearn/#parameter-confidence-intervals","title":"Parameter confidence intervals","text":"<pre><code>import numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom sklearn.linear_model import LinearRegression\n\n\ndef get_conf_int(X, y, model, alpha=0.05):\n\n    \"\"\"\n    ## alpha = 0.05 for 95% confidence interval; 0.01 for 99%-CI\n\n    Returns (1-alpha) 2-sided confidence intervals\n    for sklearn.LinearRegression coefficients\n    as a pandas DataFrame\n    \"\"\"\n\n    coefs = np.r_[[lr.intercept_], lr.coef_]\n    X_aux = X.copy()\n    X_aux.insert(0, 'const', 1)\n    dof = -np.diff(X_aux.shape)[0]\n    mse = np.sum((y - model.predict(X)) ** 2) / dof\n    var_params = np.diag(np.linalg.inv(X_aux.T.dot(X_aux)))\n    t_val = stats.t.isf(alpha/2, dof)\n    gap = t_val * np.sqrt(mse * var_params)\n\n    return pd.DataFrame({\n        'lower': coefs - gap, 'upper': coefs + gap\n    }, index=X_aux.columns)\n\n\nmodel = LinearRegression().fit(X_train, Y_train)\nget_conf_int(X_train, y_train, model, alpha = 0.05)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Sklearn/#mean-response-confidence-intervals","title":"Mean response confidence intervals","text":"<pre><code>import numpy as np\nimport pandas as pd\nfrom scipy import stats\n\nX = np.array([\n  [0, 10],\n  [5, 5],\n  [10, 2]\n])\n\nX_centered = X - X.mean()\n\nx_pred = np.array(\n  [[5, 10]]\n)\nx_pred_centered = x_pred-X.mean()\n\nn = X.shape[0]\nk = X.shape[1]\n\nidk = (\n    #(1/n) +\n    np.diag(\n      x_pred_centered.T\n      .dot(\n          np.linalg.inv(\n              X_centered.T\n              .dot(X_centered)\n          )\n      )\n      .dot(x_pred_centered)\n    )\n)\n\nse_confidence = (\n    X.std()\n    *\n    np.sqrt(\n      idk\n  )\n)\nse_prediction = (\n    X.std()\n    *\n    np.sqrt(\n      1 + idk\n  )\n)\n\nalpha = 0.05\ndof = n - k\nt_val = stats.t.isf(alpha/2, dof)\n\ngap_confidence = t_val * se_confidence\ngap_prediction = t_val * se_prediction\n\nprint(gap_confidence)\n#print(gap_prediction)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Sklearn/#custom-scorer","title":"Custom Scorer","text":"<pre><code>from sklearn.metrics import make_scorer\nfrom sklearn.model_selection import cross_val_score\n\ndef mean_error(y, y_pred):\n    return np.mean(y_pred - y)\ndef std_error(y, y_pred):\n    return np.std(y_pred - y)\n\nmean_error_scorer = make_scorer(mean_error, greater_is_better=False)\nstd_error_scorer = make_scorer(mean_error, greater_is_better=False)\n\nmodel = LinearRegression()\ncross_val_score(model, X, y, scoring=mean_error_scorer)\ncross_val_score(model, X, y, scoring=std_error_scorer)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Sklearn/#scaling","title":"Scaling","text":"<pre><code>## demonstrate data normalization with sklearn\nfrom sklearn.preprocessing import MinMaxScaler\n## load data\ndata = ...\n## create scaler\nscaler = MinMaxScaler()\n## fit and transform in one step\nnormalized = scaler.fit_transform(data)\n## inverse transform\ninverse = scaler.inverse_transform(normalized)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Sklearn/#time-series-split","title":"Time-Series Split","text":"<pre><code>|X||V|O|O|O|\n|O|X||V|O|O|\n|O|O|X||V|O|\n|O|O|O|X||V|\n</code></pre> <p>X / V are the training / validation sets. \"||\" indicates a gap (parameter n_gap: int&gt;0) truncated at the beginning of the validation set, in order to prevent leakage effects.</p> <pre><code>class StratifiedWalkForward(object):\n\n    def __init__(self,n_splits,n_gap):\n        self.n_splits = n_splits\n        self.n_gap = n_gap\n        self._cv = StratifiedKFold(n_splits=self.n_splits+1,shuffle=False)\n        return\n\n    def split(self,X,y,groups=None):\n        splits = self._cv.split(X,y)\n        _ixs = []\n        for ix in splits: \n            _ixs.append(ix[1])\n        for i in range(1,len(_ixs)): \n            yield tuple((_ixs[i-1],_ixs[i][_ixs[i]&gt;_ixs[i-1][-1]+self.n_gap]))\n\n    def get_n_splits(self,X,y,groups=None):\n        return self.n_splits\n</code></pre> <p>Note that the datasets may not be perfectly stratified afterwards, cause of the truncation with n_gap.</p>"},{"location":"Tools/AI%20%26%20Data/Sklearn/#regression-with-custom-loss-function","title":"Regression with Custom Loss Function","text":"<p>using scipy </p>"},{"location":"Tools/AI%20%26%20Data/Sklearn/#decision-boundary","title":"Decision Boundary","text":"<pre><code>x_min, x_max = X[:, 0].min(), X[:,0].max()\ny_min, y_max = X[:, 1].min(), X[:, 1].max()\nresolution = 100\n\nx = np.linspace(x_min - 0.1, x_max + 0.1, resolution)\ny = np.linspace(y_min - 0.1, y_max + 0.1, resolution)\n</code></pre> <pre><code>xx, yy = np.meshgrid(x, y)\n</code></pre> <pre><code>x_in = np.c_[xx.ravel(), yy.ravel()]\ny_pred = model.predict(x_in).reshape(xx.shape)\n</code></pre> <pre><code>plt.contourf(xx, yy, y_pred, cmap=plt.cm.RdYlBu, alpha=0.7 )\nplt.scatter(X[:,0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Sklearn/#svm","title":"SVM","text":"<ul> <li>LinearSVC: Primal</li> <li>SVC: Dual</li> </ul>"},{"location":"Tools/AI%20%26%20Data/Sklearn/#multi-level-model","title":"Multi-Level Model","text":"<pre><code>from sklearn.base import BaseEstimator\nfrom sklearn.datasets import make_regression\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.pipeline import FeatureUnion, Pipeline\nfrom sklearn.preprocessing import StandardScaler, QuantileTransformer\n\nX, y = make_regression(n_samples=100, n_features=10, noise=10.0, random_state=42)\n\nclass Regressor(BaseEstimator):\n    def __init__(self, n_features):\n        self.n_features = n_features\n        self.linear_reg = LinearRegression()\n        self.boosting_reg = GradientBoostingRegressor(\n            n_estimators=1,\n            init=\"zero\",\n            random_state=42\n        )\n\n    def fit(self, X, y):\n        self.linear_reg.fit(X=X[:, :self.n_features], y=y)\n        y_pred = self.linear_reg.predict(X=X[:, :self.n_features])\n        residual = y - y_pred\n        self.boosting_reg.fit(X=X[:, self.n_features:], y=residual)\n        return self\n\n    def predict(self, X):\n        y_pred_linear_reg = self.linear_reg.predict(X=X[:, :self.n_features])\n        residual = self.boosting_reg.predict(X=X[:, self.n_features:])\n        y_pred = y_pred_linear_reg + residual\n        return y_pred\n\n\nunion = FeatureUnion(\n    transformer_list=[(\"ss\", StandardScaler()),\n                      (\"qt\", QuantileTransformer(n_quantiles=2))]\n)\n\nX_union = union.fit_transform(X=X)\n\nn_features = 10\npipe = Pipeline(\n    steps=[\n      (\"reg\", Regressor(n_features=n_features))\n    ]\n).fit(X=X_union, y=y)\ny_pred = pipe.predict(X=X_union)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Sklearn/01_Performance_Optimization/","title":"01 Performance Optimization","text":""},{"location":"Tools/AI%20%26%20Data/Sklearn/01_Performance_Optimization/#performance-optimization","title":"Performance Optimization","text":""},{"location":"Tools/AI%20%26%20Data/Sklearn/01_Performance_Optimization/#n_jobs-1","title":"<code>n_jobs=-1</code>","text":"<p>Multi-threading</p> <pre><code>## !pip install scikit-learn-intelex\nfrom sklearnex import patch_sklearn\npatch_sklearn()\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Sklearn/01_Performance_Optimization/#config","title":"Config","text":"<pre><code>with sklearn.config_context(\n  assume_finite = True\n):\n  # reduce validation overhead: will not throw a ValueError if X contains NaN or infinity.\n\n  pass # do learning/prediction here with reduced validation\n</code></pre> <pre><code>with sklearn.config_context(\n    working_memory = 128 # MB\n):\n\n  pass # do chunked work here\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Sklearn/01_Performance_Optimization/#model-compression","title":"Model Compression","text":"<p>Linear models</p> <pre><code>model = SGDRegressor(penalty='elasticnet', l1_ratio=0.25)\nmodel.fit(X_train, y_train)\nmodel.sparsify()\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Sklearn/01_Performance_Optimization/#warm-start","title":"Warm Start","text":"<p>Useful for re-using previous training as initial values</p> <p>Useful for hyper-parameter tuning</p> <pre><code>max_estimators = 100\n\nrf = RandomForestClassifier(\n  warm_start=True\n)\n\nn_estimators = 1\nwhile n_estimators &lt;= max_estimators:\n  rf.n_estimators = n_estimators\n    rf.fit(X_train, y_train)\n\n  n_estimators *= 2\n</code></pre> <p>The advantage here is that the estimators would already be fit with the previous parameter setting, and with each subsequent call to fit, the model will be starting from the previous parameters, and we're just analyzing if adding new estimators would benefit the model.</p>"},{"location":"Tools/AI%20%26%20Data/Sklearn/01_Performance_Optimization/#mini-batchonline-learning","title":"Mini-Batch/Online Learning","text":"<pre><code>model = LinearRegression()\n\nmodel.partial_fit(data_1)\nmodel.partial_fit(data_2)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Sklearn/02_Custom/","title":"Custom","text":""},{"location":"Tools/AI%20%26%20Data/Sklearn/02_Custom/#regression-with-custom-loss-function","title":"Regression with Custom Loss Function","text":"<pre><code>import utils\n\nfrom sklearn.base import BaseEstimator\n#from sklearn.utils.estimator_checks import check_estimator\nfrom sklearn.utils.validation import check_X_y, check_array, check_is_fitted\nfrom sklearn.metrics import (\n    mean_absolute_percentage_error as mape,\n    # mean_squared_error as mse,\n    root_mean_squared_error as rmse,\n    mean_absolute_error as mae,\n    r2_score as r2\n)\n\nimport numpy as np\nimport pandas as pd\nfrom scipy import optimize as o, stats\n\nfrom inspect import getfullargspec, getsource\n# import copy\n</code></pre> <pre><code>class CustomRegression(BaseEstimator):\n    \"\"\"\n    All variables inside the Class should end with underscore\n    \"\"\"\n    def __str__(self):\n        return str(self.model)\n    def __repr__(self):\n        return str(self)\n\n    def rmse(self, pred, true, sample_weight):\n        error = pred - true\n\n        loss = error**2\n\n        cost = np.sqrt(\n            # median is robust to outliers than mean\n            np.mean(\n                sample_weight * loss\n            )\n        )\n\n        return cost\n\n    def loss(self, pred, true):\n        return self.error(pred, true, self.sample_weight)\n\n    def l1(self, params):\n        return np.sum(np.abs(params-self.model.param_initial_guess))\n    def l2(self, params):\n        return np.sum((params-self.model.param_initial_guess) ** 2)\n    def l3(self, params, alpha=0.5):\n        return alpha * self.l1(params) + (1-alpha)*self.l2(params)\n\n    def reg(self, params, penalty_type=\"l3\", lambda_reg_weight = 1.0):\n        \"\"\"\n        lambda_reg_weight = Coefficient of regularization penalty\n        \"\"\"\n\n        if penalty_type == \"l1\":\n            penalty = self.l1(params)\n        elif penalty_type == \"l2\":\n            penalty = self.l2(params)\n        elif penalty_type == \"l3\":\n            penalty = self.l3(params)\n        else:\n            raise Exception\n\n        return lambda_reg_weight * penalty/self.sample_size\n\n    def cost(self, params, X, y):\n        pred = self.model.equation(X, *params)\n        return self.loss(pred, true=y) #+ self.reg(params) # regularization requires standardised parameters\n\n    def check_enough_samples(self):\n        return self.enough_samples\n\n    def check_optimization_success(self):\n        return self.optimization.success\n\n    def fit(self, X, y, model, method=\"Nelder-Mead\", error = None, sample_weight=None, alpha=0.05):\n        check_X_y(X, y) #Using self.X,self.y = check_X_y(self.X,self.y) removes column names\n\n        self.X = X\n        self.y = y\n\n        self.n_features_in_ = self.X.shape[1]\n\n        if sample_weight is None or len(sample_weight) &lt;= 1: # sometimes we can give scalar sample weight same for all\n            self.sample_size = self.X.shape[0]\n        else:\n            self.sample_size = sample_weight[sample_weight &gt; 0].shape[0]\n\n        self.sample_weight = (\n            sample_weight\n            if sample_weight is not None\n            else np.full(self.sample_size, 1) # set Sample_Weight as 1 by default\n        )\n\n        self.error = (\n            error\n            if error is not None\n            else self.rmse\n        )\n\n        self.model = model # copy.deepcopy(model)\n\n        params = getfullargspec(self.model.equation).args\n\n        params = [param for param in params if param not in ['self', \"x\"]]\n\n        self.optimization = o.minimize(\n            self.cost,\n            x0 = self.model.param_initial_guess,\n            args = (self.X, self.y),\n            method = method, # \"L-BFGS-B\", \"Nelder-Mead\", \"SLSQP\",\n            constraints = self.model.constraints,\n            bounds = self.model.param_bounds,\n            # [\n            #     (-1, None) for param in params # variables must be positive\n            # ]\n            options = dict(\n                maxiter = 1_000,\n                maxfev = 1_000,\n                # xatol=1e-4,\n                # fatol=1e-4,\n                # adaptive=False,\n            )\n        )\n\n        self.dof = self.sample_size - self.model.k - 1 # n-k-1\n\n        if self.dof &lt;= 0:\n            self.enough_samples = False\n            # self.popt = [0 for param in params]\n            # st.warning(\"Not enough samples\"\n            return self\n        else:\n            self.enough_samples = True\n\n        # success = self.optimization.success\n        # if success is False:\n        #     st.warning(\"Did not converge!\")\n\n        self.popt = (\n            self.optimization.x\n        )\n\n        self.rmse = rmse(\n            self.output(self.X),\n            self.y,\n            sample_weight = self.sample_weight\n        )\n\n        cl = 1 - (alpha/2)\n\n        if \"hess_inv\" in self.optimization:\n            self.covx = (\n                self.optimization\n                .hess_inv\n                .todense()\n            )\n\n            self.pcov = list(\n                np.diag(\n                    self.rmse *\n                    np.sqrt(self.covx)\n                )\n            )\n\n            self.popt_with_uncertainty = [\n                f\"\"\"{{ (\n                    {utils.round_f(popt, 4)}\n                    \u00b1\n                    {utils.round_f(stats.t.ppf(cl, self.dof) * pcov.round(2), 2)}\n                )}}\"\"\" for popt, pcov in zip(self.popt, self.pcov)\n            ]\n        else:\n            self.popt_with_uncertainty = [\n                f\"\"\"{{ {utils.round_f(popt, 4)} }}\"\"\" for popt in self.popt\n            ]\n\n        self.model.set_fitted_coeff(*self.popt_with_uncertainty)\n\n        return self\n\n    def output(self, X):\n        return (\n            self.model\n            .equation(X, *self.popt)\n        )\n\n    def get_se_x_cent(self, X_cent):\n        return self.rmse * np.sqrt(\n            (1/self.sample_size) + (X_cent.T).dot(self.covx).dot(X_cent)\n        )\n    def get_pred_se(self, X):\n        if False: # self.covx is not None: # this seems to be abnormal. check this\n            X_cent = X - self.X.mean()\n            se = X_cent.apply(self.get_se_x_cent, axis = 1)\n        else:\n            se = self.rmse\n        return se\n\n    def predict(self, X, alpha=0.05):\n        check_is_fitted(self) # Check to verify if .fit() has been called\n        check_array(X) #X = check_array(X) # removes column names\n\n        pred = (\n            self.output(X)\n            .astype(np.float32)\n        )\n\n        se = self.get_pred_se(X)\n\n        cl = 1 - (alpha/2)\n\n        ci =  stats.t.ppf(cl, self.dof) * se\n\n        return pd.concat([pred, pred+ci, pred-ci], axis=1)\n\nclass CustomRegressionGrouped(CustomRegression):\n    def __str__(self):\n        x = \"\"\n        for key, value in self.get_params().items():\n            x += f\"{str(key)}: {str([utils.round_f(v, 4) for v in list(value)])} \\n\\n\"\n\n        return str(x)\n    def __init__(self, group):\n        super().__init__()\n        self.group = group\n        self.optimization = dict()\n        self.model = dict()\n        self.enough_samples = dict()\n        self.sample_size = 0\n\n    def get_params(self, how=\"dict\"):\n        params = dict()\n        for key, value in self.model.items():\n            params[key] = value.popt\n\n        if how == \"df\":\n            params = pd.DataFrame(params).T\n        return params\n\n    def model_group(self, X, y, model, solver):\n        return(\n            CustomRegression()\n            .fit(\n                X = X,\n                y = y,\n                model = model, # copy.deepcopy(model)\n                method = solver\n            )\n        )\n\n    def check_enough_samples(self):\n        enough_samples = True\n        for e in self.enough_samples.values():\n            if not e:\n                enough_samples = False\n\n        return enough_samples\n\n    def check_optimization_success(self):\n        success = True\n        for o in self.optimization.values():\n            if not o.success:\n                success = False\n\n        return success\n\n    def apply_model_multiple_group(self, X, y, group, model, solver):\n        for g in X[self.group].unique():\n            mask = X.eval(f\"{self.group} == {g}\")\n\n            m = self.model_group(\n                X[mask],\n                y[mask],\n                model,\n                solver\n            )\n\n            self.model[g] = m\n            self.enough_samples[g] = m.enough_samples\n            self.optimization[g] = m.optimization\n            self.sample_size += m.sample_size\n\n    def fit(self, X, y, model, method=\"Nelder-Mead\", error = None, sample_weight=None, alpha=0.05):\n        self.apply_model_multiple_group(X, y, self.group, model, method)\n    def predict(self, X, alpha=0.05):\n        Xs = []\n        preds = pd.DataFrame()\n\n        for g in X[self.group].unique():\n            if g not in self.model.keys():\n                return\n            else:\n                Xg = X.query(f\"{self.group} == {g}\")\n\n                pred = self.model[g].predict(\n                    X = Xg\n                )\n                Xs.append(Xg)\n\n                preds = pd.concat([preds, pred])\n\n        return preds.sort_index()\n</code></pre> <pre><code>curve_fit = CustomRegression()\nprint(curve_fit) ## prints latex\n\ncurve_fit.fit(\n  X_train,\n  y_train,\n  model = First_Order(),\n  error = regression.LogCosh().cost,\n  method = \"Nelder-Mead\"\n)\n\nprint(curve_fit) ## prints latex with coefficent values\npred = curve_fit.predict(X_test)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Sklearn/02_Custom/#holt-winters","title":"Holt-Winters","text":"<pre><code>class HoltWinters(BaseEstimator):\n    \"\"\"Scikit-learn like interface for Holt-Winters method.\"\"\"\n\n    def __init__(self, season_len=24, alpha=0.5, beta=0.5, gamma=0.5):\n        self.beta = beta\n        self.alpha = alpha\n        self.gamma = gamma\n        self.season_len = season_len\n\n    def fit(self, series):\n        # note that unlike scikit-learn's fit method, it doesn't learn\n        # the optimal model paramters, alpha, beta, gamma instead it takes\n        # whatever the value the user specified the produces the predicted time\n        # series, this of course can be changed.\n        beta = self.beta\n        alpha = self.alpha\n        gamma = self.gamma\n        season_len = self.season_len\n        seasonals = self._initial_seasonal(series)\n\n        # initial values\n        predictions = []\n        smooth = series[0]\n        trend = self._initial_trend(series)\n        predictions.append(smooth)\n\n        for i in range(1, len(series)):\n            value = series[i]\n            previous_smooth = smooth\n            seasonal = seasonals[i % season_len]\n            smooth = alpha * (value - seasonal) + (1 - alpha) * (previous_smooth + trend)\n            trend = beta * (smooth - previous_smooth) + (1 - beta) * trend\n            seasonals[i % season_len] = gamma * (value - smooth) + (1 - gamma) * seasonal\n            predictions.append(smooth + trend + seasonals[i % season_len])\n\n        self.trend_ = trend\n        self.smooth_ = smooth\n        self.seasonals_ = seasonals\n        self.predictions_ = predictions\n        return self\n\n    def _initial_trend(self, series):\n        season_len = self.season_len\n        total = 0.0\n        for i in range(season_len):\n            total += (series[i + season_len] - series[i]) / season_len\n\n        trend = total / season_len\n        return trend\n\n    def _initial_seasonal(self, series):\n        season_len = self.season_len\n        n_seasons = len(series) // season_len\n\n        season_averages = np.zeros(n_seasons)\n        for j in range(n_seasons):\n            start_index = season_len * j\n            end_index = start_index + season_len\n            season_average = np.sum(series[start_index:end_index]) / season_len\n            season_averages[j] = season_average\n\n        seasonals = np.zeros(season_len)\n        seasons = np.arange(n_seasons)\n        index = seasons * season_len\n        for i in range(season_len):\n            seasonal = np.sum(series[index + i] - season_averages) / n_seasons\n            seasonals[i] = seasonal\n\n        return seasonals\n\n    def predict(self, n_preds=10):\n        \"\"\"\n        Parameters\n        ----------\n        n_preds: int, default 10\n            Predictions horizon. e.g. If the original input time series to the .fit\n            method has a length of 50, then specifying n_preds = 10, will generate\n            predictions for the next 10 steps. Resulting in a prediction length of 60.\n        \"\"\"\n        predictions = self.predictions_\n        original_series_len = len(predictions)\n        for i in range(original_series_len, original_series_len + n_preds):\n            m = i - original_series_len + 1\n            prediction = self.smooth_ + m * self.trend_ + self.seasonals_[i % self.season_len]\n            predictions.append(prediction)\n\n        return predictions\n</code></pre> <pre><code>def timeseries_cv_score(params, series, loss_function, season_len=24, n_splits=3):\n    \"\"\"\n    Iterating over folds, train model on each fold's training set,\n    forecast and calculate error on each fold's test set.\n    \"\"\"\n    errors = []    \n    alpha, beta, gamma = params\n    time_series_split = TimeSeriesSplit(n_splits=n_splits) \n\n    for train, test in time_series_split.split(series):\n        model = HoltWinters(season_len, alpha, beta, gamma)\n        model.fit(series[train])\n\n        # evaluate the prediction on the test set only\n        predictions = model.predict(n_preds=len(test))\n        test_predictions = predictions[-len(test):]\n        test_actual = series[test]\n        error = loss_function(test_actual, test_predictions)\n        errors.append(error)\n\n    return np.mean(errors)\n</code></pre> <pre><code>x = [0, 0, 0]\ntest_size = 20\ndata = series.values[:-test_size]\nopt = minimize(\n  timeseries_cv_score,\n  x0=x, \n  args=(data, mean_squared_log_error),\n  method='TNC',\n  bounds=((0, 1), (0, 1), (0, 1))\n)\n</code></pre> <pre><code>alpha_final, beta_final, gamma_final = opt.x\n\nmodel = HoltWinters(season_len, alpha_final, beta_final, gamma_final)\nmodel.fit(data)\npredictions = model.predict(n_preds=50)\n\nprint('original series length: ', len(series))\nprint('prediction length: ', len(predictions))\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Sympy/","title":"Sympy","text":""},{"location":"Tools/AI%20%26%20Data/Sympy/#import","title":"Import","text":"<pre><code>import sympy as smp\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Sympy/#basics","title":"Basics","text":"<p>Declaring symbols</p> <pre><code>x = smp.symbols(\"x\")\nx = smp.symbols(\"x\", real=True, positive=True)\n\nx, y, z = smp.symbols(\"x y z\")\n</code></pre> <p>Declaring functions</p> <pre><code>f = smp.symbols(\"f\", cls=smp.Function)\n</code></pre> <p>Numbers</p> <pre><code>x = smp.Rational(5, 1)\nfrac = smp.Rational(1, 2)\n</code></pre> <p>Useful Functions</p> <pre><code>y = smp.sin(x)\nz = x**2 + y**2\n</code></pre> <pre><code>z.factor()\nz.expand()\nz.simplify()\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Sympy/#solve","title":"Solve","text":"<pre><code>smp.solve(z, x) ## find value of x that makes z(x) = 0 \nsmp.solve(z, y)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Sympy/#convert-to-numerical","title":"Convert to Numerical","text":"<p>Lambdify</p> <pre><code>expr = smp.sin(x) + smp.sin(y)\nexpr_f = smp.lambdify([x, y], expr)\n</code></pre> <p>Substitute</p> <pre><code>expr = smp.sin(x) + smp.sin(y)\nexpr.subs([\n    (x, 10)\n])\nexpr.subs([\n  (x, 10),\n  (y, 5)\n])\nexpr.subs([\n  (x, 10),\n  (y, smp.sin(x))\n])\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Sympy/#calculus","title":"Calculus","text":""},{"location":"Tools/AI%20%26%20Data/Sympy/#differentiation","title":"Differentiation","text":"<pre><code>dfdx = smp.diff(f) ## f = function symbol, which is a function of  \ndfdx_sub = dfdx.sub([\n    (g, smp.sin(x))\n])\n\ndfdx_sub_value = dfdx_sub.doit()\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Sympy/#integration","title":"Integration","text":"<p>Indefinite</p> <pre><code>## does not give +c\nsmp.integrate(\n  expr,\n  x\n)\n</code></pre> <p>Definite</p> <pre><code>smp.integrate(\n  expr,\n  (x, 0, smp.log(4))\n)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Sympy/#vectors","title":"Vectors","text":"<pre><code>u1, u2, u3 = smp.symbols(\"u1 u2 u3\")\nu = smp.Matrix([u1, u2, u3])\n\nv1, v2, v3 = smp.symbols(\"v1 v2 v3\")\nv = smp.Matrix([v1, v2, v3])\n</code></pre> <pre><code>2*u + v\nu.norm()\n\nu.dot(v)\nu.cross(v)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Sympy/#fourier-transform-analytic","title":"Fourier Transform (Analytic)","text":""},{"location":"Tools/AI%20%26%20Data/Sympy/#continuous-time-frequency","title":"Continuous Time &amp; Frequency","text":"<pre><code># symbols need to be defined with correct characteristics\n\nt, f = smp.symbols(\"t, f\", real=True)\nk = smp.symbols(\"k\", real=True, positive=True)\nx = smp.exp(-k * t**2) * k * t\nx\n</code></pre> <pre><code>from sympy.integrals.transforms import fourier_transform as ft\nx_FT = ft(x, t, f)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Sympy/#continuous-time-discrete-frequency","title":"Continuous Time &amp; Discrete Frequency","text":"<pre><code>t = smp.symbols(\"t\", real=True)\nk, n, T = smp.symbols(\"k, n, T\", real=True, positive=True)\nfn = n/T\nx = smp.exp(-k * t)\n</code></pre> <pre><code>x_FT = smp.integrate(\n  1/T * x * smp.exp(-2*smp.pi*smp.I*fn*t),\n  (t, 0, T)\n).simplify()\n</code></pre> <pre><code>get_FT = smp.lambdify([k, T, n], x_FT)\nns = np.arange(0, 20, 1)\nxFT = get_FT(k=1, T=4, n=ns)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Tensorflow/","title":"Tensorflow","text":""},{"location":"Tools/AI%20%26%20Data/Tensorflow/#references","title":"References","text":"<ul> <li> Learn TensorFlow and Deep Learning (beginner friendly code) | Daniel Bourke</li> </ul>"},{"location":"Tools/AI%20%26%20Data/Tensorflow/01/","title":"01","text":""},{"location":"Tools/AI%20%26%20Data/Tensorflow/01/#basics","title":"Basics","text":"<pre><code>from tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import Dense\nfrom sklearn.metrics import accuracy_score\n</code></pre> <pre><code>model = Sequential()\nmodel.add(Dense(units=32, activation='relu', input_dim=len(X_train.columns)))\nmodel.add(Dense(units=64, activation='relu'))\nmodel.add(Dense(units=1, activation='sigmoid'))\n</code></pre> <pre><code>model.compile(loss='binary_crossentropy', optimizer='sgd', metrics='accuracy')\n</code></pre> <pre><code>model.fit(X_train, y_train, epochs=200, batch_size=32)\n</code></pre> <pre><code>pred = model.predict(X_test)\npred = np.where(\n  pred &gt; 0.5,\n  1,\n  0\n)\n</code></pre> <pre><code>model.save('tfmodel')\ndel model\nmodel = load_model('tfmodel')\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/Tensorflow/01/#quantization","title":"Quantization","text":"<pre><code>tf_lite_converter = tf.lite.TFLiteConverter.from_keras_model(model)\ntf_lite_converter.optimizations = [tf.lite.Optimize.DEFAULT]\ntf_lite_converter.target_spec.supported_types = [tf.float16]\ntflite_model = tf_lite_converter.convert()\n\ntf_lite_converter  = tf.lite.TFLiteConverter.from_keras_model(model)\ntf_lite_converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\ntflite_model = tf_lite_converter.convert()\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/httpx/","title":"httpx","text":"<pre><code># requirements\nhttpx[http2]\naiolimiter\n</code></pre> <pre><code>async def get_data(client, limiter, series):\n  async with limiter:\n    response = await client.get(\n        URL_BASE + ENDPOINT + series,\n        params = params\n    )\n\n  return response.json()\n\n\nasync with httpx.AsyncClient(http2=True) as client:\n  limiter = AsyncLimiter(\n    10,     # asynchronous requests\n    1       # delay in s\n  )\n\n  tasks = []\n\n  for series in series_list:\n      tasks.append(asyncio.create_task(\n          get_data(client, limiter, series)\n      ))\n\n  data = await asyncio.gather(*tasks)\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/pywav/","title":"PyWavelets","text":"<pre><code>pip install pywavelets\n</code></pre> <pre><code>import pywt\nimport numpy as np\n</code></pre> <pre><code># wavelet decomposition\ncoeffs = pywt.wavedec(y, 'sym5', mode='symmetric')\n\ny_rec = pywt.waverec(coeffs, 'sym5', mode='symmetric')[1:]\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/pywav/#smoothing","title":"Smoothing","text":"<pre><code>def smooth_with_wavelets(y):\n    \"\"\"\n    FUNCTION TO SMOOTH SIGNAL VIA WAVELET DECOMPOSITION\n\n    INPUTS:\n    - y = array-like signal to smooth\n\n    OUTPUTS:\n    - y_rec = smoothed version of input signal\n\n    DEPENDENCIES:\n    - PyWavelets 1.3.0\n    - numpy 1.21.5\n\n    CODE AUTHORED BY: SHAWHIN TALEBI\n    \"\"\"\n\n\n    # wavelet decomposition\n    coeffs = pywt.wavedec(y, 'sym5', mode='symmetric')\n\n    # zero out last 5 detail coefficents\n    for i in range(5):\n        coeffs[i+5] = np.zeros(coeffs[i+5].shape)\n\n    # wavelet recomposition\n    y_rec = pywt.waverec(coeffs, 'sym5', mode='symmetric')[1:]\n\n    return y_rec\n</code></pre> <pre><code>y_smoothed = smooth_with_wavelets(y)\n</code></pre> <pre><code># plot result\nplt.figure(figsize=(24,8))\nplt.rcParams.update({'font.size': 16})\nplt.plot(x, y, x, y_smoothed, linewidth=2)\nplt.legend(['original', 'smoothed'])\nplt.savefig('smoothed_signal_plot.png', facecolor='white')\nplt.show()\n</code></pre>"},{"location":"Tools/AI%20%26%20Data/pywav/#references","title":"References","text":"<ul> <li> https://www.youtube.com/watch?v=gd6oUg608FI</li> <li> https://medium.com/@shouke.wei/how-to-plot-filter-bank-of-a-disctete-wavelet-in-python-42fbb6eb418d</li> <li> https://www.kaggle.com/code/asauve/a-gentle-introduction-to-wavelet-for-data-analysis</li> </ul>"},{"location":"Tools/API/FastAPI/","title":"FastAPI","text":"<p>Python package to create REST APIs </p> <ul> <li>Easy to learn &amp; use</li> <li>Fast development</li> <li>Async -&gt; High performance</li> <li>Automatic Documentation</li> </ul>"},{"location":"Tools/API/FastAPI/#installation","title":"Installation","text":"<pre><code>pip install fastapi uvicorn\n</code></pre>"},{"location":"Tools/API/FastAPI/#execution","title":"Execution","text":"<pre><code>uvicorn main:app --reload\n</code></pre>"},{"location":"Tools/API/FastAPI/#docs","title":"Docs","text":"<p>Go to</p> <ul> <li><code>http://127.0.0.1/8000/docs</code></li> </ul> <p>or</p> <ul> <li><code>http://127.0.0.1/8000/redoc</code></li> </ul> <p>or</p> <ul> <li><code>http://127.0.0.1/8000/openapi.json</code></li> </ul>"},{"location":"Tools/API/FastAPI/#imports","title":"Imports","text":"<pre><code>from fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\n</code></pre>"},{"location":"Tools/API/FastAPI/#basic-app","title":"Basic App","text":"<pre><code>app = FastAPI()\n\n\nclass Item(BaseModel):\n    text: str = None\n    is_done: bool = False\n\n\nitems = []\n\n\n@app.get(\"/\")\ndef root():\n    return {\"Hello\": \"World\"}\n\n\n@app.post(\"/items\")\ndef create_item(item: Item):\n    items.append(item)\n    return items\n\n\n@app.get(\"/items\", response_model=list[Item])\ndef list_items(limit: int = 10):\n    return items[0:limit]\n\n\n@app.get(\"/items/{item_id}\", response_model=Item)\ndef get_item(item_id: int) -&gt; Item:\n    if item_id &lt; len(items):\n        return items[item_id]\n    else:\n        raise HTTPException(status_code=404, detail=f\"Item {item_id} not found\")\n</code></pre>"},{"location":"Tools/API/FastAPI/#return-files","title":"Return Files","text":"<pre><code>import os\n\nfrom fastapi import FastAPI \nfrom fastapi.responses import FileResponse\n\napp = FastAPI()\n\npath = \"/home/anthony/fastapifileexample\"\n\n@app.get(\"/\")\ndef index():\n    return {\"Hello\": \"World\"}\n\n@app.get(\"/cat\", responses={200: {\"description\": \"A picture of a cat.\", \"content\" : {\"image/jpeg\" : {\"example\" : \"No example available. Just imagine a picture of a cat.\"}}}})\ndef cat():\n    file_path = os.path.join(path, \"files/cat.jpg\")\n    if os.path.exists(file_path):\n        return FileResponse(file_path, media_type=\"image/jpeg\", filename=\"mycat.jpg\")\n    return {\"error\" : \"File not found!\"}\n</code></pre>"},{"location":"Tools/API/Flask/","title":"Flask","text":"<pre><code>from flask import (\n    Flask,\n    request,\n    send_file\n)\n\napp = Flask(__name__)\n\n@app.route('/badge-going')\ndef return_badge_going():\n    # badge_type = request.args.get('badge_type')\n    file_name = \"badge_going.jpg\"\n    try:\n        return send_file(\n            f\"./{file_name}\",\n            # attachment_filename = file_name\n        )\n    except Exception as e:\n        return str(e)\n</code></pre>"},{"location":"Tools/API/Robyn/","title":"RobynAPI","text":"<p>Similar to FastAPI but written in Rust</p>"},{"location":"Tools/API/Robyn/#workers-flag","title":"Workers Flag","text":"<pre><code>python app.py --workers=5\n</code></pre> <p><code>--workers</code> flag allows Robyn to serve a higher throughput. Default 1 is set for prototyping/development</p>"},{"location":"Tools/App_Dev/Streamlit/01_Introduction/","title":"Streamlit","text":""},{"location":"Tools/App_Dev/Streamlit/01_Introduction/#structure","title":"Structure","text":"<pre><code>.\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 app\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 components.py\n\u2502   \u251c\u2500\u2500 p0.py\n\u2502   \u251c\u2500\u2500 p1.py\n\u2502   \u251c\u2500\u2500 p2.py\n\u2502   \u2514\u2500\u2500 p3.py\n\u251c\u2500\u2500 assets\n\u2502   \u2514\u2500\u2500 doe.svg\n\u251c\u2500\u2500 data\n\u2502   \u251c\u2500\u2500 foo.xlsx\n\u2502   \u2514\u2500\u2500 bar.xlsx\n\u251c\u2500\u2500 main.py\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 styles.css\n\u2514\u2500\u2500 utils\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 custom_regression.py\n    \u251c\u2500\u2500 data.py\n    \u251c\u2500\u2500 math.py\n    \u251c\u2500\u2500 model.py\n    \u251c\u2500\u2500 models.py\n    \u251c\u2500\u2500 plots.py\n    \u2514\u2500\u2500 regression.py\n</code></pre>"},{"location":"Tools/App_Dev/Streamlit/01_Introduction/#files","title":"Files","text":""},{"location":"Tools/App_Dev/Streamlit/01_Introduction/#mainpy","title":"<code>main.py</code>","text":"<pre><code>import app\n\nimport utils\nfrom utils import regression, models, custom_regression, plots\n# Not the best way to import, but it's convenient. Please make this nicer\n\nimport gc\nimport glob\n\nimport streamlit as st\n\ndef main():\n    st.set_page_config(\n        layout=\"wide\",\n        page_title=app.TITLE,\n    )\n\n    st.markdown(f\"\"\"\n    &lt;style&gt;\n    {utils.get_styles()}\n    &lt;/style&gt;\n    \"\"\", unsafe_allow_html=True)\n\n    gc.set_threshold(0) # disable garbage collection\n    app.main()\n    gc.collect()\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"Tools/App_Dev/Streamlit/01_Introduction/#stylescss","title":"<code>styles.css</code>","text":"<pre><code>.block-container {\n    padding-top: 1rem;\n    padding-bottom: 1rem;\n    padding-left: 1.5rem;\n    padding-right: 1.5rem;\n}\n[data-testid=\"stHeader\"]\n{\n    display: none !important\n}\n[data-testid=\"stSidebarHeader\"] {\n    position: absolute;\n    top: 0;\n    right: 0;\n    z-index: 10;\n}\n</code></pre>"},{"location":"Tools/App_Dev/Streamlit/01_Introduction/#utils__init__py","title":"<code>utils/__init__.py</code>","text":"<pre><code>@st.cache_data(ttl=60*60)\ndef get_styles():\n    with open(\"styles.css\", \"r\") as f:\n        styles = f.read()\n    return styles\n\n\ndef get_page_title(menu_selected_external, menu_selected_internal):\n    header = (\n        menu_selected_external +\n        (\": \" + menu_selected_internal if menu_selected_internal is not None else \"\")\n    )\n    st.subheader(header)\n\n\ndef internal_navigation(menu_selected_external, menu_options_external):\n    # with st.sidebar:\n    #     st.divider()\n\n    menu_options_internal_mappings = {\n        0: [\n\n        ],\n        1: [\n            \"Foo\",\n            \"Bar\"\n        ],\n    }\n    menu_options_internal = (\n        menu_options_internal_mappings\n        .get(menu_options_external.index(menu_selected_external), [])\n    )\n    # menu_selected_internal = st.radio(\n    #     label=\"Sub Menu\",\n    #     label_visibility = \"visible\",\n    #     options=menu_options_internal\n    # )\n\n    menu_selected_internal = option_menu( # st.radio\n        menu_title = None, # \"Menu\",\n        options = menu_options_internal,\n        orientation = \"horizontal\",\n        styles = {\n            \"container\": {\"margin\": \"0 !important\", \"padding\": \"0 !important\"}, # , \"background\": \"none\"\n            \"nav\": {\"font-size\": \"0.75em\"},\n            \"icon\": {\"display\": \"none\"},\n            \"nav-link\": {\"margin\":\"0\", \"padding\": \"0.5ex 1.5ex\"},\n        },\n    )\n\n    return menu_selected_internal, menu_options_internal\n</code></pre>"},{"location":"Tools/App_Dev/Streamlit/01_Introduction/#app__init__py","title":"<code>app/__init__.py</code>","text":"<pre><code>import utils\nfrom utils import data, regression, custom_regression, models, plots\n\nfrom app import (\n    components,\n    p0,\n    p1,\n    p2,\n    p3\n)\n\nimport streamlit as st\nfrom streamlit_option_menu import option_menu\n\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nfrom sklearn.metrics import (\n    mean_absolute_percentage_error as mape,\n    # mean_squared_error as mse,\n    root_mean_squared_error as rmse,\n    mean_absolute_error as mae,\n    r2_score as r2\n)\n\nimport numpy as np\nimport pandas as pd\nimport polars as pl\n\nTITLE = \"Chemical Kinetics Modelling\"\n\ndef main():\n    data = utils.data # why???\n\n    menu_options_external = [\n        \"Home\",\n          \"Foo\",\n        \"Bar\",\n    ]\n\n    with st.sidebar:\n        st.title(TITLE)\n        # menu_selected_external = st.radio(\n        #     \"Menu\",\n        #     menu_options_external,\n        #     label_visibility = \"collapsed\"\n        # )\n    menu_selected_external = option_menu( # st.radio\n        menu_title = None, # \"Menu\",\n        options = menu_options_external,\n        orientation = \"horizontal\",\n        styles = {\n            \"container\": {\"margin\": \"0 !important\", \"padding\": \"0 !important\",},  # \"background\": \"none\"\n            \"nav\": {\"font-size\": \"0.75em\"},\n            \"icon\": {\"display\": \"none\"},\n            \"nav-link\": {\"margin\":\"0\", \"padding\": \"0.5ex 1.5ex\"},\n        }\n        # label_visibility = \"collapsed\"\n    )\n\n    if menu_selected_external == menu_options_external[0]:\n        p0.main()\n\n    menu_selected_internal, menu_options_internal = utils.internal_navigation(menu_selected_external, menu_options_external)\n\n    utils.get_page_title(menu_selected_external, menu_selected_internal)\n\n    if menu_selected_external == menu_options_external[1]:\n        p1.main(menu_selected_internal, menu_options_internal)\n\n    if menu_selected_external == menu_options_external[2]:\n        p2.main(menu_selected_internal, menu_options_internal, df)\n</code></pre>"},{"location":"Tools/App_Dev/Streamlit/01_Introduction/#p1py","title":"<code>p1.py</code>","text":"<pre><code>from utils import data\nimport streamlit as st\n\ndef main(menu_selected_internal, menu_options_internal):\n    if menu_selected_internal == menu_options_internal[0]:\n        st.dataframe(\n            data.get_details().collect(),\n            use_container_width=True,\n            hide_index=True\n        )\n    elif menu_selected_internal == menu_options_internal[1]:\n        st.dataframe(\n            data.get_readings().collect(),\n            use_container_width=True,\n            hide_index=True\n        )\n    st.stop()\n</code></pre>"},{"location":"Tools/App_Dev/Streamlit/01_Introduction/#appcomponentspy","title":"<code>app/components.py</code>","text":"<pre><code>import streamlit as st\nimport utils\n\ndef input_filters(df, menu_selected_external, menu_options_external, menu_selected_internal, menu_options_internal):\n    with st.sidebar:\n        # st.divider()\n        st.subheader(\"Data Input Filters\")\n\n        filter_cols = [\"Study_Identifier\", \"Temperature\"] # , \"Sample_Identifier\"\n        filters_selected = {}\n\n        n_cols = 3\n        cols = st.columns([2, 1])\n        current_col = 0\n        for col in filter_cols:\n            comparison_page == (menu_selected_external==menu_options_external[3] and menu_selected_internal==menu_options_external[2])\n\n            with cols[current_col]:\n                if (\n                    (col == filter_cols[0] and not comparison_page)\n                    or\n                    (col == filter_cols[1] and comparison_page)\n                ):\n                    single_only = True\n                else:\n                    single_only = False\n\n                filters_selected.update({\n                    col: generate_filter(df, col, single_only)\n                })\n                current_col = (current_col+1)%n_cols\n\n    keys = list(filters_selected.keys())\n    values = list(filters_selected.values())\n\n    return keys, values\n\ndef generate_filter(df, col, single_only=False):\n    \"\"\"\n    returns list for modularity and ease\n    \"\"\"\n\n    options = generate_options(df, col)\n\n    if single_only:\n        selected = [st.selectbox(\n            label = col.split(\"_\")[0],\n            options = options\n        )]\n    else:\n        selected = st.multiselect(\n            label = col.split(\"_\")[0],\n            options = options\n        )\n    if len(selected) == 0:\n        selected = options\n\n    return selected\n</code></pre>"},{"location":"Tools/Apps/logseq/","title":"Plugins","text":"<ol> <li>Bear Theme</li> <li>Focus Mode</li> <li>Tabs</li> </ol>"},{"location":"Tools/Apps/logseq/#export","title":"Export","text":"<p>You can export the entire logseq database as a standalone website, with every feature except editing. \ud83d\ude32</p> <ol> <li>Make sure you go to settings and set All pages public when publishing as True.</li> <li>Then export the graph</li> </ol>"},{"location":"Tools/Apps/typora/","title":"Mermaidjs update","text":"<p><code>/Applications/Typora.app/Contents/Resources/TypeMark/lib/diagram/diagram.min.js</code></p> <ol> <li>Make a copy of the existing one</li> <li>Replace current library with latest version of mermaidjs</li> </ol>"},{"location":"Tools/Apps/typora/#shortcuts","title":"shortcuts","text":"Shorcut Function Ctrl p (so that everything is on left hand) quick open Ctrl d toggle side bar ctrl t insert table ctrl enter add row"},{"location":"Tools/Data_Visualization/Matplotlib/01/","title":"01","text":""},{"location":"Tools/Data_Visualization/Matplotlib/01/#initialization","title":"Initialization","text":"<pre><code>from matplotlib import pyplot as plt\n%matplotlib inline\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats('svg')\nplt.figure(figsize=(16, 9), dpi=1920/16)\n</code></pre> <pre><code>plt.figure(\n  figsize=(6, 6),\n  dpi=80\n)\n</code></pre>"},{"location":"Tools/Data_Visualization/Matplotlib/01/#example","title":"Example","text":"<pre><code>df = df.sort_values(x).reset_index(drop=True)\nplt.plot(\n  df[x],\n  df[y],\n  'o'\n)\nplt.xlabel(x), plt.ylabel(y)\n\n#add linear regression line to scatterplot\nm, b = np.polyfit(df[x], df[y], 1)\nplt.plot(df[x], m*df[x] + b)\n\nplt.show()\n</code></pre>"},{"location":"Tools/Data_Visualization/Matplotlib/01/#regression-line","title":"Regression Line","text":"<pre><code>m, b = np.polyfit(x, y, 1)\nplt.plot(x, m*x + b)\n</code></pre>"},{"location":"Tools/Data_Visualization/Matplotlib/01/#handrawn-style","title":"Handrawn Style","text":"<pre><code>with plt.xkcd():\n    plt.show()\n</code></pre>"},{"location":"Tools/Data_Visualization/Matplotlib/01/#animation","title":"Animation","text":"<pre><code>import numpy as np\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib_inline\nmatplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n\nfrom matplotlib.animation import FuncAnimation, writers, PillowWriter, FFMpegWriter\n\nimport seaborn as sns\nsns.set_theme() ## affects all matplotlib and seaborn plots\nplt.style.use('ggplot')\n</code></pre> <pre><code>## hyperparameters\nx_values = [1, 2, 3]\ny_values = [1, 2, 3]\nduration = 3 ## seconds\npause_duration = 2 ## seconds\nres_w, res_h = 1920, 1080 ## video_resolution\naspect_w, aspect_h = (16, 9) ## aspect_ratio\n</code></pre> <pre><code>## plot\nactual_no_of_frames = len(y_values)\nfig = plt.figure(\n    figsize = (aspect_w, aspect_h), ## inches\n    dpi = res_w/16 ## \n)\nplt.tight_layout()\nax = plt.gca()  ## Get current axes\nax.set_title(\"Title\")\nax.set_xlim(xmin=0, xmax=(len(y_values)-1)*1.05)\nax.set_ylim(ymin=0, ymax=max(y_values)*1.05)\nax.set_xticks(x_values, [\"\"] + list(x_ticks))\nax.grid(False, axis=\"x\")\nline, = ax.plot(0, 0, linewidth=3)\n\ndef animation_frame(i):\n  ## avoid changing axes, titles, etc in updates: low fps issue\n  ## ax.set_title(y_values[i])\n  if i not in [0, 1] and i &lt;= actual_no_of_frames:\n    ax.text(x=i-1, y=y_values[i-1], s=y_values[i-1], backgroundcolor=\"white\", size=10, blit=True)\n  line.set_xdata(x_values[:i])\n  line.set_ydata(y_values[:i])\n\n  return line, \n\ninterval = (duration  * 1000)/actual_no_of_frames #ms\nfps = 1000 / interval\n\nno_of_blank_frames = int(fps * pause_duration)\ntotal_no_of_frames = actual_no_of_frames + no_of_blank_frames\n\nfor _ in range(no_of_blank_frames):\n  x_values.append(x_values[-1])\n  y_values.append(y_values[-1])\n\n\n## print(x_values)\n\nanimation = FuncAnimation(\n  fig,\n  func=animation_frame,\n  frames=total_no_of_frames,\n  interval=interval,\n  blit=True ## comment if it causes issue\n)\n</code></pre> <pre><code>\n</code></pre> <pre><code>## setting up writer object\nWriter = writers['ffmpeg'] ## PillowWriter\nwriter = Writer(\n  fps=fps,\n  bitrate=5000,\n  metadata = dict(\n    artist = 'Ahmed Thahir'\n  )\n)\n</code></pre> <pre><code>animation.save('Line Graph Animation.gif', writer)\n## animation.save('Line Graph Animation.mp4', writer)\n</code></pre>"},{"location":"Tools/Data_Visualization/Networks/Networkx/","title":"PyVis","text":""},{"location":"Tools/Data_Visualization/Networks/Networkx/#references","title":"References","text":"<ul> <li> Python for Social Networks | Python Tutorials for Digital Humanities</li> </ul>"},{"location":"Tools/Data_Visualization/Networks/PyVis/","title":"PyVis","text":""},{"location":"Tools/Data_Visualization/Networks/PyVis/#references","title":"References","text":"<ul> <li> How to Use PyVis Library in Python Tutorials | Python Tutorials for Digital Humanities</li> </ul>"},{"location":"Tools/Data_Visualization/Plotly/01/","title":"01","text":""},{"location":"Tools/Data_Visualization/Plotly/01/#importing","title":"Importing","text":"<pre><code>import plotly.express as px\nimport plotly.graph_objs as go\n\nimport plotly.io as pio\npio.templates.default = \"ggplot2\"\npio.renderers.default = \"notebook\"\n## injects plotly.js into the notebook for offline plotly\n## but only works for the first save, for some reason\n</code></pre>"},{"location":"Tools/Data_Visualization/Plotly/01/#plots","title":"Plots","text":"<pre><code>px.scatter()\npx.line()\npx.imshow(df, text_auto=True) ## heatmap with value\n</code></pre>"},{"location":"Tools/Data_Visualization/Plotly/01/#maps","title":"Maps","text":"<pre><code>  fig = px.scatter_geo(\n      df,\n      lat = \"Lat\",\n      lon = \"Lon\",\n      color = \"Name\"\n  ).update_traces(\n      marker = dict(size = 10),\n  ).update_layout(\n      margin = dict(t=0, l=0, r=0, b=0)\n  ).update_geos(\n      projection_type=\"orthographic\",\n      center=dict(lat=17, lon=65.5),\n      projection_rotation=dict(lon=65, lat=10),\n      showcountries=True,\n      countrycolor=\"rgba(0,0,0, 0.1)\"\n  )\n  fig.show()\n</code></pre>"},{"location":"Tools/Data_Visualization/Plotly/01/#sankey-chart","title":"Sankey Chart","text":"<pre><code>  import plotly.graph_objects as go\n\n  fig = go.Figure(data=[go.Sankey(\n    node = dict(\n      pad = 15,\n      thickness = 20,\n      line = dict(color = \"black\", width = 0.5),\n      label = my_sankey['label'],\n      color = \"blue\"\n    ),\n    link = dict(\n      source = my_sankey['source'],\n      target = my_sankey['target'],\n      value = my_sankey['value']\n  ))])\n\n  fig.write_html('test.html')\n</code></pre>"},{"location":"Tools/Data_Visualization/Plotly/01/#useful-configuration","title":"Useful Configuration","text":"<p>Only 2 series as colored</p> <p>Useful when there are a lot of colors</p> <pre><code>  df[\"order\"] = (\n    df[\"Country Name\"]\n    .map({\"Japan\": 1, \"Turkey\": 2})\n    .fillna(3)\n  )\n  ## sort by this order\n  df.sort_values(by=[\"order\",\"years\"], ascending=False, inplace=True)\n\n  ## The line which comes the latest, is drawn on the top.\n</code></pre> <pre><code>  fig.update_traces({\"line\":{\"color\":\"lightgrey\"}})\n\n  fig.update_traces(patch={\"line\":{\"color\":\"blue\", \"width\":5}}, \n                    selector={\"legendgroup\":\"Turkey\"})\n  fig.update_traces(patch={\"line\":{\"color\":\"red\", \"width\":5}}, \n                    selector={\"legendgroup\":\"Japan\"})\n</code></pre>"},{"location":"Tools/Data_Visualization/Plotly/01/#adding-text-at-the-end-of-line-instead-of-legends","title":"Adding text at the end of line, instead of legends","text":"<pre><code>  for i, d in enumerate(fig.data):\n    text = '  ' + d.name + '  ' ## str(d.y[-1])\n\n    ## last non-empty\n    last_index = pd.Series(d.y).last_valid_index()\n    ## last_index = len([x for x in d.y if x != \"nan\"])-1 ## for string/object col\n    fig.add_scatter(\n      x = [d.x[last_index]],\n      y = [d.y[last_index]], ## last non-empty\n      hoverinfo = 'skip',\n      hovertemplate = 'skip',\n      mode = 'text',\n      text = text,\n      textfont = dict(color = d.line.color),\n      textposition = 'top left',\n      showlegend = False\n    )\n\n    ## first non-empty\n    first_index = pd.Series(d.y).first_valid_index()\n    ## idk ## for string/object col\n    fig.add_scatter(\n      x = [d.x[first]],\n      y = [d.y[first]], ## last non-empty\n      hoverinfo = 'skip',\n      hovertemplate = 'skip',\n      mode = 'text',\n      text = text,\n      textfont = dict(color = d.line.color),\n      textposition = 'top right',\n      showlegend = False\n    )\n</code></pre>"},{"location":"Tools/Data_Visualization/Plotly/01/#update_layout","title":"<code>update_layout()</code>","text":"<pre><code>  fig = go.Figure().update_layout(\n    ## Title and Subtitle\n    title = dict(\n      text =  \"&lt;span style='font-size:1.5em'&gt;\" +\n          \"Title\"\n        + \"&lt;/span&gt;&lt;br /&gt;&lt;sup&gt;\" +\n        \"Subtitle\"\n        + \"&lt;/sup&gt;\",\n      x = 0, xref = \"paper\",\n      #y = 0.95, yref = \"paper\",\n    ),\n\n    uirevision=\"foo\", overwrite=True, ## ensures clicks are retained on refresh\n    plot_bgcolor = \"rgba(0, 0%, 0%, 0)\",\n    paper_bgcolor = \"rgba(0, 0%, 0%, 0)\",\n    margin=dict(t=0, r=0, b=0, l=0),\n\n    ## axes titles\n    xaxis_title = \"x_title\",\n    yaxis_title = \"y_title\",\n\n    hovermode = \"x unified\",\n\n    ## legend\n    showlegend = False,\n    legend = dict(\n    groupclick=\"toggleitem\",\n      orientation = 'h',\n\n      ## positioning\n      x = 0,\n      xanchor = \"left\",\n\n      y = 1,\n      yanchor = \"bottom\",\n\n    font = dict(\n            size = 10\n      ),\n      itemsizing = 'constant',\n\n      ## click behavior\n      #itemclick = 'toggleothers',\n      #itemdoubleclick = 'toggle'\n    )\n  )\n</code></pre>"},{"location":"Tools/Data_Visualization/Plotly/01/#custom-menu","title":"custom menu","text":"<p>The updatemenu method determines which plotly.js function will be used to modify the chart. There are 4 possible methods:</p> <p><code>\"restyle\"</code>: modify data or Data attributes</p> <p><code>\"relayout\"</code>: modify layout attributes</p> <p><code>\"update\"</code>: modify data and layout attributes; combination of <code>\"restyle\"</code> and <code>\"relayout\"</code></p> <p><code>\"animate\"</code>: start or pause an animation)</p> <p>Examples</p> <p>restyle</p> <p></p> <p></p> <pre><code>      fig = go.Figure()\n\n      ## Add surface trace\n      fig.add_trace(go.Surface(z=df.values.tolist(), colorscale=\"Viridis\"))\n\n      ## Update plot sizing\n      fig.update_layout(\n          width=800,\n          height=900,\n          autosize=False,\n          margin=dict(t=0, b=0, l=0, r=0),\n          template=\"plotly_white\",\n      )\n\n      ## Update 3D scene options\n      fig.update_scenes(\n          aspectratio=dict(x=1, y=1, z=0.7),\n          aspectmode=\"manual\"\n      )\n\n      ## Add dropdown\n      fig.update_layout(\n          updatemenus=[\n              dict(\n                  type = \"buttons\",\n                  direction = \"left\",\n                  buttons=list([\n                      dict(\n                          args=[\"type\", \"surface\"],\n                          label=\"3D Surface\",\n                          method=\"restyle\"\n                      ),\n                      dict(\n                          args=[\"type\", \"heatmap\"],\n                          label=\"Heatmap\",\n                          method=\"restyle\"\n                      )\n                  ]),\n                  pad={\"r\": 10, \"t\": 10},\n                  showactive=True,\n                  x=0.11,\n                  xanchor=\"left\",\n                  y=1.1,\n                  yanchor=\"top\"\n              ),\n          ]\n      )\n</code></pre> <p>relayout</p> <p></p> <pre><code>      import plotly.graph_objects as go\n\n      ## Generate dataset\n      import numpy as np\n      np.random.seed(1)\n\n      x0 = np.random.normal(2, 0.4, 400)\n      y0 = np.random.normal(2, 0.4, 400)\n      x1 = np.random.normal(3, 0.6, 600)\n      y1 = np.random.normal(6, 0.4, 400)\n      x2 = np.random.normal(4, 0.2, 200)\n      y2 = np.random.normal(4, 0.4, 200)\n\n      ## Create figure\n      fig = go.Figure()\n\n      ## Add traces\n      fig.add_trace(\n          go.Scatter(\n              x=x0,\n              y=y0,\n              mode=\"markers\",\n              marker=dict(color=\"DarkOrange\")\n          )\n      )\n\n      fig.add_trace(\n          go.Scatter(\n              x=x1,\n              y=y1,\n              mode=\"markers\",\n              marker=dict(color=\"Crimson\")\n          )\n      )\n\n      fig.add_trace(\n          go.Scatter(\n              x=x2,\n              y=y2,\n              mode=\"markers\",\n              marker=dict(color=\"RebeccaPurple\")\n          )\n      )\n\n      ## Add buttons that add shapes\n      cluster0 = [dict(type=\"circle\",\n                                  xref=\"x\", yref=\"y\",\n                                  x0=min(x0), y0=min(y0),\n                                  x1=max(x0), y1=max(y0),\n                                  line=dict(color=\"DarkOrange\"))]\n      cluster1 = [dict(type=\"circle\",\n                                  xref=\"x\", yref=\"y\",\n                                  x0=min(x1), y0=min(y1),\n                                  x1=max(x1), y1=max(y1),\n                                  line=dict(color=\"Crimson\"))]\n      cluster2 = [dict(type=\"circle\",\n                                  xref=\"x\", yref=\"y\",\n                                  x0=min(x2), y0=min(y2),\n                                  x1=max(x2), y1=max(y2),\n                                  line=dict(color=\"RebeccaPurple\"))]\n\n      fig.update_layout(\n          updatemenus=[\n              dict(\n                  type=\"buttons\",\n                  buttons=[\n                      dict(label=\"None\",\n                           method=\"relayout\",\n                           args=[\"shapes\", []]),\n                      dict(label=\"Cluster 0\",\n                           method=\"relayout\",\n                           args=[\"shapes\", cluster0]),\n                      dict(label=\"Cluster 1\",\n                           method=\"relayout\",\n                           args=[\"shapes\", cluster1]),\n                      dict(label=\"Cluster 2\",\n                           method=\"relayout\",\n                           args=[\"shapes\", cluster2]),\n                      dict(label=\"All\",\n                           method=\"relayout\",\n                           args=[\"shapes\", cluster0 + cluster1 + cluster2])\n                  ],\n              )\n          ]\n      )\n\n      ## Update remaining layout properties\n      fig.update_layout(\n          title_text=\"Highlight Clusters\",\n          showlegend=False,\n      )\n\n      fig.show()\n</code></pre>"},{"location":"Tools/Data_Visualization/Plotly/01/#update_xaxes","title":"<code>update_xaxes()</code>","text":"<pre><code>  fig.update_xaxes(autorange = 'reversed')\n  fig.update_xaxes(range = [100, -30])\n\n  ## only the first month has the year\n  fig.update_xaxes(\n    type = \"date\",\n    dtick=\"M1\",\n    tickformat=\"%b\\n%Y\"\n  )\n</code></pre>"},{"location":"Tools/Data_Visualization/Plotly/01/#config","title":"<code>config</code>","text":"<pre><code>  export_width = 1000\n  export_height = export_width/2\n\n  config = dict(\n      doubleClickDelay = 400, ## (ms) affects the single click delay; default = 300ms\n      ## displayModeBar = True,\n      displaylogo = False,\n      modeBarButtonsToRemove = [\"zoom\", \"select2d\", \"lasso2d\", \"pan\", \"zoomIn\", \"zoomOut\", \"autoScale\", \"resetScale\"],\n\n      ## scrollZoom = True,\n      showTips = False,\n\n      toImageButtonOptions = dict(\n        format = 'pdf', ## pdf, svg, png, jpeg, webp\n        filename = title,\n        width = export_width,\n        height = export_height,\n        scale = 1 ## Multiply title/legend/axis/canvas sizes by this factor\n      )\n  )\n</code></pre> <pre><code>  fig.show(config = config)\n  dcc.Graph(config = config)\n</code></pre>"},{"location":"Tools/Data_Visualization/Plotly/01/#grey-out-areas-not-selected-by-marquee-select","title":"Grey out areas not selected by marquee select","text":"<pre><code>  scatter = px.scatter_matrix(df)\n  scatter.data[0].update(selected=dict(marker=dict(color='red')),\n                         unselected=dict(marker=dict(color='blue',\n                                                     opacity=0.001)))\n</code></pre>"},{"location":"Tools/Data_Visualization/Plotly/01/#animated","title":"Animated","text":""},{"location":"Tools/Data_Visualization/Plotly/01/#autoplay","title":"Autoplay","text":"<pre><code>  config = {\n    auto_play = True\n  }\n  fig.show(config = config)\n</code></pre>"},{"location":"Tools/Data_Visualization/Plotly/01/#save-as-gif","title":"Save as GIF","text":"<p>Using moviepy</p> <pre><code>    import numpy as np\n    from scipy.spatial import Delaunay\n    import plotly.graph_objects as go\n    import  moviepy.editor as mpy\n    import io \n    from PIL import Image\n\n    def plotly_fig2array(fig):\n        #convert Plotly fig to  an array\n        fig_bytes = fig.to_image(format=\"png\")\n        buf = io.BytesIO(fig_bytes)\n        img = Image.open(buf)\n        return np.asarray(img)\n\n    n = 20 ## number of radii\n    h = 2/(n-1)\n    r = np.linspace(h, 2,  n)\n    theta = np.linspace(0, 2*np.pi, 60)\n    r, theta = np.meshgrid(r,theta)\n    r = r.flatten()\n    theta = theta.flatten()\n\n    x = r*np.cos(theta)\n    y = r*np.sin(theta)\n\n    ## Triangulate the circular  planar region\n    tri = Delaunay(np.vstack([x,y]).T)\n    faces = np.asarray(tri.simplices)\n    I, J, K = faces.T\n\n    f = lambda h: np.sinc(x**2+y**2)+np.sin(x+h)   \n\n    fig = go.Figure(go.Mesh3d(x=x,\n                         y=y,\n                         z=f(0),\n                         intensity=f(0),\n                         i=I,\n                         j=J,\n                         k=K,\n                         colorscale='matter_r', \n                         showscale=False))\n\n    fig.update_layout(title_text='My hat is flying with MoviePy',\n                      title_x=0.5,\n                      width=500, height=500, \n                      scene_xaxis_visible=False, \n                      scene_yaxis_visible=False, \n                      scene_zaxis_visible=False)\n\n    ## No Plotly frames are defined here!! Instead we define moviepy frames by\n    ## converting each Plotly figure to  an array, from which MoviePy creates a clip\n    ## The concatenated clips are saved as a gif file:\n    def make_frame(t):\n        z = f(2*np.pi*t/2)\n        fig.update_traces(z=z, intensity=z)  #These are the updates that usually are performed within Plotly go.Frame definition\n        return plotly_fig2array(fig)\n\n    animation = mpy.VideoClip(make_frame, duration=2) ## or VideoFileClip\n    animation.write_gif(\"image/my_hat.gif\", fps=20)\n    animation.write_videofile(\"gfg_intro.webm\")\n</code></pre> <p>Manual</p> <pre><code>    import plotly.express as px\n    import pandas as pd\n    import numpy as np\n    import io\n    import PIL\n\n    ## sample data\n    df = pd.DataFrame(\n        {\n            \"step\": [1, 2, 3],\n            \"x\": [10, 20, 30],\n            \"y\": [100, 200, 300],\n        }\n    )\n\n    ## smaple plotly animated figure\n    fig = px.bar(df, x=\"x\", y=\"y\", animation_frame=\"step\")\n\n    ## generate images for each step in animation\n    frames = []\n    for s, fr in enumerate(fig.frames):\n        ## set main traces to appropriate traces within plotly frame\n        fig.update(data=fr.data)\n        ## move slider to correct place\n        fig.layout.sliders[0].update(active=s)\n        ## generate image of current state\n        frames.append(PIL.Image.open(io.BytesIO(fig.to_image(format=\"png\"))))\n\n    ## create animated GIF\n    frames[0].save(\n      \"test.gif\",\n      save_all=True,\n      append_images=frames[1:],\n      optimize=True,\n      duration=500,\n      loop=0,\n    )\n</code></pre>"},{"location":"Tools/Data_Visualization/Plotly/01/#scatter-plot-hans-rosling-style","title":"Scatter Plot (Hans Rosling Style)","text":"<pre><code>  fig = px.scatter(data_frame = df, \n             x = 'consumption_co2',\n             y = 'consumption_co2_per_capita', \n             size = 'population', \n             hover_name = 'country', \n             color = 'country',\n             animation_frame = 'year',\n             animation_group = 'country',\n\n             size_max=60,\n\n             log_x=True,\n\n             range_x = [0, 1.05 * np.log10(df[\"consumption_co2\"].max())],\n             range_y = [0, 1.05 * df[\"consumption_co2\"].max()]\n            )\n</code></pre>"},{"location":"Tools/Data_Visualization/Plotly/01/#choropleth","title":"Choropleth","text":"<pre><code>  px.choropleth(gapminder,               \n                locations=\"iso_alpha\",               \n                color=\"lifeExp\",\n                hover_name=\"country\",  \n                animation_frame=\"year\",    \n                color_continuous_scale='Plasma',  \n                height=600             \n  )\n</code></pre>"},{"location":"Tools/Data_Visualization/Plotly/01/#line-chart","title":"Line Chart","text":"<pre><code>  data = pd.read_csv(r\"https://raw.githubusercontent.com/datageekrj/ForHostingFiles/master/income_per_person_gdppercapita_ppp_inflation_adjusted.csv\")\n\n  numOfRows = data.shape[0] ## No of Countries\n  numOfCols = data.shape[1] ## No of years + one column for a country\n  numOfFrames = numOfCols - 1\n  xaxis_range = [0,numOfFrames + 2]\n\n  ## While, testing the code, test with low numbers:\n  ## Initial State of the data\n  ## First we are just seeing it for afghanistan\n\n  x_init = np.array([1])\n\n  initial_data = []\n  for cont_ind in [75,35,184,83,140]:\n      y_axis = np.array(data.iloc[cont_ind,0])\n      initial_data.append(go.Scatter(x =x_init, y = y_axis,mode = \"lines\",name = data.country[cont_ind]))\n  initial_max = 600\n\n  ## Frames\n  frames = []\n  for f in range(1,numOfFrames+1):\n      x_axis = np.arange(1,f+1)\n      curr_data = []\n      title_names = []\n      start = \"For \" + str(1800 + f + 1)\n      for cont_ind in [75,35,184,83,140]:\n          curr_country = data.country[cont_ind]\n          y_axis = np.array(data.iloc[cont_ind,1:f+1])\n          curr_data.append(go.Scatter(x = x_axis, y = y_axis,mode = \"lines\", name = curr_country))\n          title_names.append(curr_country + \": \" + str(y_axis[f-1]/1000) + \"K Dollar. \")\n      title = start + \" \" + \" \".join(title_names)\n      curr_frame = go.Frame(data = curr_data, layout = {\"title\":title})\n      frames.append(curr_frame)\n\n  fig = go.Figure(\n      data = initial_data,\n      layout = {\n          \"title\":\"Line Chart Race\",\n          \"xaxis\":{\"range\":xaxis_range, \"visible\":False, \"showline\":False},\n          \"yaxis\":{\"type\":\"log\", \"visible\":False, \"showline\":False},\n          \"updatemenus\":[{\"type\":\"buttons\",\"buttons\":[{\"method\":\"animate\",\"label\":\"play\", \"args\":[None]}]}]\n          },\n      frames = frames\n      )\n  fig.show()\n</code></pre>"},{"location":"Tools/Data_Visualization/Plotly/01/#performance","title":"Performance","text":""},{"location":"Tools/Data_Visualization/Plotly/01/#webgl-renderer","title":"Webgl renderer","text":"<pre><code>  px.scatter(df, x=\"x\", y=\"y\", render_mode='webgl')\n  px.scatter_polar(df, x=\"x\", y=\"y\", render_mode='webgl')\n\n  px.line(df, x=\"x\", y=\"y\", render_mode='webgl')\n  px.line_polar(df, x=\"x\", y=\"y\", render_mode='webgl')\n\n  go.Scattergl()\n  go.Scatterpolargl()\n  go.Heatmapgl()\n</code></pre>"},{"location":"Tools/Data_Visualization/Plotly/01/#specify-axis-range","title":"Specify axis range","text":"<p>Letting plotly autorange means it needs to do relayouts often and requires it to calculate the range each time.</p> <pre><code>  fig = px.scatter(df, x=\"x\", y=\"y\", range_x=[2, 3], range_y=[10, 20])\n</code></pre>"},{"location":"Tools/Data_Visualization/Plotly/01/#use-heatmap-instead-of-pairplot","title":"Use heatmap instead of pairplot","text":"<pre><code>  import numpy as np\n\n  N = 1000\n  M = 500\n  xx = np.arange(N, dtype=np.float64)\n  yy = np.arange(M, dtype=np.float64)\n  x, y = np.meshgrid(xx, yy)\n  b = N/20.0\n  c = M/2.0\n  r = np.sqrt(((x-c)/b)**2 + ((y-c)/b)**2)\n  a = np.sin(r)\n\n  ## Limits\n  xmin = xx[0]\n  xmax = xx[-1]\n  ymin = yy[0]\n  ymax = yy[-1]\n  amin = np.amin(a)\n  amax = np.amax(a)\n\n  from PIL import Image\n  from matplotlib import cm\n  from matplotlib.colors import Normalize\n\n  ## Some normalization from matplotlib\n  cNorm = Normalize(vmin=amin, vmax=amax)\n  scalarMap  = cm.ScalarMappable(norm=cNorm, cmap='viridis' )\n  seg_colors = scalarMap.to_rgba(a) \n  img = Image.fromarray(np.uint8(seg_colors*255))\n\n  ## Now the plotly code\n  import plotly.graph_objects as go\n\n  ## Create figure\n  fig = go.Figure()\n\n  ## Constants\n  img_width = 900\n  img_height = 600\n\n  ## Add invisible scatter trace.\n  ## This trace is added to help the autoresize logic work.\n  ## We also add a color to the scatter points so we can have a colorbar next to our image\n  fig.add_trace(\n      go.Scatter(\n          x=[xmin, xmax],\n          y=[ymin, ymax],\n          mode=\"markers\",\n          marker={\"color\":[np.amin(a), np.amax(a)],\n                  \"colorscale\":'Viridis',\n                  \"showscale\":True,\n                  \"colorbar\":{\"title\":\"Counts\",\n                              \"titleside\": \"right\"},\n                  \"opacity\": 0\n                 }\n      )\n  )\n\n  ## Add image\n  fig.update_layout(\n      images=[go.layout.Image(\n          x=xmin,\n          sizex=xmax-xmin,\n          y=ymax,\n          sizey=ymax-ymin,\n          xref=\"x\",\n          yref=\"y\",\n          opacity=1.0,\n          layer=\"below\",\n          sizing=\"stretch\",\n          source=img)]\n  )\n\n  ## Configure other layout\n  fig.update_layout(\n          xaxis=dict(showgrid=False, zeroline=False, range=[xmin, xmax]),\n          yaxis=dict(showgrid=False, zeroline=False, range=[ymin, ymax]),\n      width=img_width,\n      height=img_height,\n  )\n\n  fig.show()\n</code></pre> <p>Use summary statistics whenever possible</p>"},{"location":"Tools/Data_Visualization/Plotly/01/#plotly-resampler","title":"Plotly Resampler","text":""},{"location":"Tools/Data_Visualization/Plotly/01/#datashader","title":"Datashader","text":""},{"location":"Tools/Data_Visualization/Plotly/01/#use-plotlyreact-instead-of-plotlynewplot-only-for-plotlyjs","title":"use Plotly.react instead of Plotly.newPlot (only for plotly.js)","text":""},{"location":"Tools/Data_Visualization/Plotly/01/#tutorials","title":"Tutorials","text":"<p>https://www.youtube.com/watch?v=hSPmj7mK6ng</p> <p>https://www.youtube.com/watch?v=pGMvvq7R1IM</p> <p>https://www.youtube.com/watch?v=8d7rArayuzc</p> <p>https://www.youtube.com/watch?v=_b2KXL0wHQg</p>"},{"location":"Tools/Data_Visualization/Plotly/01/#exporting","title":"Exporting","text":""},{"location":"Tools/Data_Visualization/Plotly/01/#static","title":"Static","text":"<pre><code>  fig.write_image(f\"{title}.pdf\")\n  fig.write_image(f\"{title}.svg\")\n  fig.write_image(f\"{title}.png\")\n  fig.write_image(f\"{title}.jpg\")\n  fig.write_image(f\"{title}.webp\")\n</code></pre>"},{"location":"Tools/Data_Visualization/Plotly/01/#interactive","title":"Interactive","text":""},{"location":"Tools/Data_Visualization/Plotly/01/#embed-within","title":"Embed within","text":"<pre><code>    fig.write_html(\"graph_name.html\",\n                   full_html=False,\n                   include_plotlyjs = True,\n                   config = dict(\n                     doubleClickDelay = 400, ## (ms) affects the single click delay; default = 300ms\n                     displayModeBar = False,\n                     showTips = False\n                   )\n                  )\n</code></pre>"},{"location":"Tools/Data_Visualization/Plotly/01/#export-a-js-file-to-the-same-directory","title":"Export a js file to the same directory","text":"<pre><code>    fig.write_html(\"graph_name.html\",\n                   full_html=False,\n                   include_plotlyjs = \"directory_path\",\n                   config = dict(\n                     doubleClickDelay = 400, ## (ms) affects the single click delay; default = 300ms\n                     displayModeBar = False,\n                     showTips = False\n                   )\n                  )\n</code></pre>"},{"location":"Tools/Data_Visualization/Plotly/01/#use-external-source-useful-for-revealjs","title":"Use external source (useful for revealjs)","text":"<pre><code>    fig.write_html(\n      \"graph_name.html\",  ## no need to add _plotly to the file name, because the iframe will request\n      full_html=False,\n      include_plotlyjs = \"../../../backend/plugins/plotly/plotly-basic.min.js\", ## 3 backtracks because it is an iframe in the assets folder, so it has to an extra backtrack\n      include_mathjax = \"\",\n      config = config\n    )\n</code></pre>"},{"location":"Tools/Data_Visualization/Plotly/01/#interactive-hover","title":"Interactive Hover","text":"<pre><code>def hover_fn(trace, points, state):\n    ind = points.point_inds[0]\n    details.value = cars_df.iloc[ind].to_frame().to_html()\n\nscatter.on_hover(hover_fn)\n</code></pre>"},{"location":"Tools/Data_Visualization/Plotly/01/#multiple-traces-from-the-same","title":"Multiple Traces from the same","text":"<pre><code>for i in range(4): ## 0-3\n    subset = spf_unrate[\n        spf_unrate[\"PREDICTION_PERFORMED_LAG\"] == i\n    ]\n    fig.add_trace(go.Scattergl(\n        x = subset.index.astype(str),\n        y = subset[\"UNRATE\"],\n        name = f\"SPF (Forecast) with Lag = {i}\"\n    ))\n</code></pre>"},{"location":"Tools/Data_Visualization/Plotly/01/#maps_1","title":"Maps","text":"<p>Default</p> <pre><code>  fig = px.scatter_geo(\n    df,\n    lat=\"Lat\",\n    lon=\"Lon\",\n    hover_name=\"Name\",\n    projection=\"orthographic\",\n    ## center = (0, 0)\n  )\n\n  fig.update_geos(\n    ## (meters) : higher number --&gt; lower detail --&gt; smoother\n    resolution=110,\n    bgcolor='hsla(0, 0%, 0%, 0)',\n\n    showcountries=True, countrycolor=\"Grey\",\n    showcoastlines=True, coastlinecolor=\"Grey\",\n\n    showland=True, landcolor=\"LightYellow\",\n    showocean=True, oceancolor=\"LightBlue\",\n    ## showlakes=True, lakecolor=\"LightBlue\",\n    ## showrivers=True, rivercolor=\"LightBlue\",\n  )\n\n  fig.update_layout(\n    margin=dict(r=0, t=0, l=0, b=0),\n    uirevision=\"foo\",\n    overwrite=True,\n    paper_bgcolor='hsla(0, 0%, 0%, 0)',\n    plot_bgcolor='hsla(0, 0%, 0%, 0)',\n    modebar=dict(\n      bgcolor='hsla(0, 0%, 0%, 0.5)',\n      color='hsla(0, 0%, 100%, 0.5)',\n    )\n  )\n</code></pre> <p>Mapbox</p> <pre><code>  ## create a map of area, where houses from data set located\n  fig = px.scatter_mapbox(data, #our data set\n                          lat=\"lat\", lon=\"long\", #location\n                          color=\"price\", #select a column for ranking\n                          hover_name=\"price\", \n                          hover_data=[\"bedrooms\", \"bathrooms\"], \n                          color_discrete_sequence=[\"green\"],\n                          size_max=15, \n                          zoom=8, \n                          width=900, height=600, #map size\n                          title =  'Map of area, check location')\n  #style of map\n  fig.update_layout(mapbox_style=\"open-street-map\")\n  fig.show(config={'scrollZoom': False})\n</code></pre>"},{"location":"Tools/Databases/DuckDB/","title":"DuckDB","text":"<pre><code>import duckdb as db\nimport pandas as pd\n</code></pre> <pre><code>class DB():\n  def __init__(self, db_name = \"file.db\"):\n    self.db_name = db_name\n\n  def execute(self, query):\n    with duckdb.connect(self.db_name) as con:\n      try:\n          return con.sql(query).df()\n      except:\n        return False\n\n  def create(self, table_name = \"Series\"):\n    if table_name == \"Series\":\n      query = f\"\"\"\n      create or replace\n      table {table_name}\n      (\n        Date Datetime,\n        Variable String,\n        Value Float,\n\n        PRIMARY KEY (Date, Variable)\n      )\n      \"\"\"\n    elif table_name == \"Variables\":\n      query = f\"\"\"\n      create or replace\n      table {table_name}\n      (\n        id String,\n        realtime_start Datetime,\n        realtime_end  Datetime,\n        title String,\n        observation_start Datetime,\n        observation_end Datetime,\n        frequency String,\n        frequency_short String,\n        units String,\n        units_short String,\n        seasonal_adjustment String,\n        seasonal_adjustment_short String,\n        last_updated Datetime,\n        popularity int,\n        group_popularity int,\n        notes String,\n\n        PRIMARY KEY (id)\n      )\n      \"\"\"\n\n    return self.execute(query)\n\n  def read(self, table_name, pivot=True):\n    query = f\"\"\"\n    select *\n    from {table_name}\n    \"\"\"\n\n    query_result = self.execute(query)\n\n    if query_result is False:\n      return False\n\n    df = query_result\n\n    if pivot:\n      if table_name == \"Series\":\n        df = df.pivot(\n            index = \"Date\",\n            columns = \"Variable\",\n            values = \"Value\"\n        )\n      elif table_name == \"Variables\":\n        pass\n\n    return df\n\n  def upsert(self, table_name, df, melt=True):\n    if table_name == \"Series\":\n      if melt:\n        df = (\n            df\n            .copy()\n            .reset_index()\n            .melt(\n                id_vars = \"Date\",\n                var_name=\"Variable\",\n                value_name = \"Value\"\n            )\n            .dropna()\n        )\n      query = f\"\"\"\n      INSERT INTO {table_name}\n      select * from {var(df)}\n      ON CONFLICT (Date, Variable)\n      do update set Value = EXCLUDED.Value \n      \"\"\"\n    elif table_name == \"Variables\":\n      if melt:\n        pass\n\n      query = f\"\"\"\n      INSERT OR REPLACE INTO {table_name}\n      select * from {var(df)}\n      \"\"\"\n\n    return self.execute(query)\n</code></pre> <pre><code>economic_db = DB(\"economic.db\")\n\n# economic_db.create(\"Variables\")\n# economic_db.upsert(\"Variables\", series_list_df)\neconomic_db.read(\"Variables\")\n\n# economic_db.create(\"Series\")\n# economic_db.upsert(\"Series\", series_data, melt=True)\neconomic_db.read(\"Series\", pivot=True)\n</code></pre>"},{"location":"Tools/DevOps/CML/","title":"CML","text":"<p>Continuous ML</p>"},{"location":"Tools/DevOps/CML/#ml-model-report","title":"ML Model Report","text":"<pre><code>name: model-wine-quality\non: [push]\njobs:\n  run:\n    runs-on: [ubuntu-latest]\n    container: docker://dvcorg/cml-py3:latest\n    steps:\n      - uses: actions/checkout@v2\n      - name: cml_run\n        env:\n          repo_token: ${{ secrets.GITHUB_TOKEN }}\n        run: |\n\n          # Your ML workflow goes here\n          pip install -r requirements.txt\n          python train.py\n\n          echo \"## Model metrics\" &gt; report.md\n          cat metrics.txt &gt;&gt; report.md\n\n          echo \"## Data viz\" &gt;&gt; report.md\n          cml-publish feature_importance.png --md &gt;&gt; report.md\n          cml-publish residuals.png --md &gt;&gt; report.md\n\n          cml-send-comment report.md\n</code></pre>"},{"location":"Tools/DevOps/CML/#dvc-pipeline-for-ml-model-report-compared-to-main-branch","title":"DVC Pipeline for ML Model Report Compared to <code>main</code> branch","text":""},{"location":"Tools/DevOps/CML/#dvcyaml","title":"<code>dvc.yaml</code>","text":"<pre><code>stages:\n  get_data:\n    cmd: python get_data.py\n    deps:\n    - get_data.py\n    outs:\n    - data_raw.csv  \n  process:\n    cmd: python process_data.py\n    deps:\n    - process_data.py\n    - data_raw.csv\n    outs:\n    - data_processed.csv\n  train:\n    cmd: python train.py\n    deps:\n    - train.py\n    - data_processed.csv\n    outs:\n    - by_region.png\n    metrics:\n    - metrics.json:\n        cache: false\n</code></pre>"},{"location":"Tools/DevOps/CML/#ciyml","title":"<code>ci.yml</code>","text":"<pre><code>name: farmers\non: [push]\njobs:\n  run:\n    runs-on: [ubuntu-latest]\n    container: docker://dvcorg/cml-py3:latest\n    steps:\n      - uses: actions/checkout@v2\n      - name: cml_run\n        env:\n          repo_token: ${{ secrets.GITHUB_TOKEN }}\n        run: |\n          pip install -r requirements.txt\n          dvc repro \n\n          git fetch --prune\n          dvc metrics diff --show-md master &gt; report.md\n\n          # Add figure to the report\n          echo \"## Validating results by region\"\n          cml-publish by_region.png --md &gt;&gt; report.md\n          cml-send-comment report.md\n</code></pre>"},{"location":"Tools/DevOps/Git/","title":"Git","text":""},{"location":"Tools/DevOps/Git/01_Basics/","title":"Basics","text":""},{"location":"Tools/DevOps/Git/01_Basics/#cloning","title":"Cloning","text":""},{"location":"Tools/DevOps/Git/01_Basics/#sparse-checkout","title":"Sparse Checkout","text":"<pre><code>git clone --filter=blob:none --sparse --no-checkout https://github.com/user_or_org/repo_name\n\n# move to git folder\ncd repo_name\n\n# set sparse\n# git sparse-checkout set --cone\n\n# select branch\ngit checkout main\n\n# clone specific folder\ngit sparse-checkout add ./folder\n\n# clone specific file\ngit sparse-checkout add ./folder/file\n</code></pre>"},{"location":"Tools/DevOps/Git/01_Basics/#shallow","title":"Shallow","text":"<pre><code>git clone --single-branch --depth=1 --branch main https://github.com/user_or_org/repo_name\n</code></pre>"},{"location":"Tools/DevOps/Git/01_Basics/#refresh-repo","title":"Refresh Repo","text":"<p>Go to repo local folder Right-click &gt; <code>Git Bash here</code> <pre><code>git checkout main\ngit checkout --orphan last\ngit add -A\ngit commit -am \"Repo Refresh\"\ngit branch -D main\ngit branch -m main\ngit gc --aggressive --prune=all\ngit push -f origin main\n\ngit checkout gh-pages\ngit checkout --orphan last\ngit add -A\ngit commit -am \"Repo Refresh\"\ngit branch -D gh-pages\ngit branch -m gh-pages\ngit gc --aggressive --prune=all\ngit push -f origin gh-pages\n</code></pre></p>"},{"location":"Tools/DevOps/Git/01_Basics/#rebase","title":"Rebase","text":"<pre><code>git rebase -i HEAD~10\n</code></pre>"},{"location":"Tools/DevOps/Git/01_Basics/#renamemoving-file","title":"Rename/Moving File","text":"<pre><code>git mv folder_old/filename_old folder_new/filename_new\n</code></pre> <p>This avoids Git thinking that the files were deleted &amp; added, thus reducing repo size</p>"},{"location":"Tools/DevOps/Git/02_CI_CD/","title":"CI/CD","text":""},{"location":"Tools/DevOps/Git/02_CI_CD/#actions","title":"Actions","text":"<p>Create <code>~/.github/workflows/ci.yaml</code></p>"},{"location":"Tools/DevOps/Git/02_CI_CD/#dvc","title":"DVC","text":"<p>Data Version Control</p> <p>You can use an external storage for non-code files.</p> <p>Especially useful for large files</p>"},{"location":"Tools/DevOps/Unit-Testing/","title":"Unit Testing","text":""},{"location":"Tools/DevOps/Unit-Testing/#execution","title":"Execution","text":""},{"location":"Tools/DevOps/Unit-Testing/#install-pytest","title":"Install Pytest","text":"<pre><code>pip install pytest\n</code></pre>"},{"location":"Tools/DevOps/Unit-Testing/#run-the-tests","title":"Run the tests","text":"<pre><code>pytest\n</code></pre>"},{"location":"Tools/DevOps/Unit-Testing/#run-a-specific-file","title":"Run a specific file","text":"<pre><code>pytest test_shopping_cart.py\n</code></pre>"},{"location":"Tools/DevOps/Unit-Testing/#run-a-specific-test","title":"Run a specific test","text":"<pre><code>pytest test_shopping_cart.py::test_can_get_total_price\n</code></pre>"},{"location":"Tools/DevOps/Unit-Testing/#file-naming-convention","title":"File Naming Convention","text":"<p>Files containing tests should start with <code>test_</code></p>"},{"location":"Tools/DevOps/Unit-Testing/#imports","title":"Imports","text":"<pre><code>from unittest.mock import Mock\nfrom item_database import ItemDatabase\nfrom shopping_cart import ShoppingCart\nimport pytest\n</code></pre>"},{"location":"Tools/DevOps/Unit-Testing/#fixture","title":"Fixture","text":"<pre><code>@pytest.fixture\ndef cart():\n    # All setup for the cart here...\n    return ShoppingCart(5)\n</code></pre>"},{"location":"Tools/DevOps/Unit-Testing/#basic-unit-tests","title":"Basic Unit-Tests","text":"<pre><code>def test_can_add_item_to_cart(cart):\n    cart.add(\"apple\")\n    assert cart.size() == 1\n</code></pre> <pre><code>def test_when_item_added_then_cart_contains_item(cart):\n    cart.add(\"apple\")\n    assert \"apple\" in cart.get_items()\n</code></pre> <pre><code>def test_when_add_more_than_max_items_should_fail(cart):\n    for _ in range(5):\n        cart.add(\"apple\")\n\n    with pytest.raises(OverflowError):\n        cart.add(\"apple\")\n</code></pre>"},{"location":"Tools/DevOps/Unit-Testing/#mocking","title":"Mocking","text":"<pre><code>def test_can_get_total_price(cart):\n    cart.add(\"apple\")\n    cart.add(\"orange\")\n    item_database = ItemDatabase()\n\n    def mock_get_item(item: str):\n        if item == \"apple\":\n            return 1.0\n        if item == \"orange\":\n            return 2.0\n\n    item_database.get = Mock(side_effect=mock_get_item)\n    assert cart.get_total_price(item_database) == 3.0\n</code></pre>"},{"location":"Tools/Documentation/Latex/","title":"Latex","text":""},{"location":"Tools/Documentation/Latex/#enclosed-text","title":"Enclosed text","text":"<pre><code>\\enclose{circle}{\\enclose{circle}{S_5}} % mathjax\n\n\\textcircled{R} % latex/katex\n</code></pre>"},{"location":"Tools/Documentation/Latex/#subset","title":"Subset","text":"<pre><code>abc\n\\overset{\n  \\substack{a=1\\\\b=2\\\\c=3}\n}{\n  =\n}\nc\n</code></pre>"},{"location":"Tools/Documentation/Latex/#mathclap","title":"Mathclap","text":"<p>Basically equivalent to <code>position: absolute</code> in CSS Its position does not affect other elements</p> <pre><code>\\mathclap{\\text{Long Text}}\n\n% always user for underbrace and overbrace\nx \\underbrace{y}_{\\text{Long Text}} z \\\\\nx \\underbrace{y}_{\\mathclap{\\text{Long Text}}} z\n</code></pre>"},{"location":"Tools/Documentation/Latex/#align","title":"Align","text":"<ul> <li><code>{aligned}</code></li> <li><code>{alignedat}{1}</code></li> </ul> <pre><code>\\begin{alignedat}{1}\nE[&amp;\\text{PVGO}]\n&amp;&amp;= P_{\\text{Growth}}\n&amp;- P_{\\text{No Growth}}\n\\\\\n&amp;\\text{PVGO}_\\text{Actual}\n&amp;&amp;= P_\\text{Actual}\n&amp;- P_{\\text{No Growth}}\n\\end{alignedat}\n</code></pre>"},{"location":"Tools/Documentation/Mermaidjs/","title":"Mermaidjs","text":""},{"location":"Tools/Documentation/Mermaidjs/#diagram-title","title":"Diagram Title","text":"<pre><code>---\ntitle: Title\n---\nflowchart LR\na --&gt; b</code></pre>"},{"location":"Tools/Documentation/Mermaidjs/#pie-chart","title":"Pie Chart","text":"<pre><code>pie\n\"A\": 1\n\"B\": 3\n\"C\": 6</code></pre>"},{"location":"Tools/FEA/Abaqus/","title":"Abaqus CAE","text":"<p>CAE = Complete Abacus Environment (back-acronym); usually CAE = Computer Aided Engineering</p> <p>Software application used for modeling and analysis of mechanical components and assemblies (pre-processing) and visualizing the finite element analysis result.</p>"},{"location":"Tools/FEA/Abaqus/#references","title":"References","text":"<ul> <li> https://www.youtube.com/playlist?list=PL8JBtohft2fuXL6zj1ZETOTBtM8ydfMd6</li> <li> https://ifcuriousthenlearn.com/blog/2015/04/02/Abaqus-FEA-Scripting-with-python/</li> </ul>"},{"location":"Tools/FEA/Abaqus/01_Introduction/","title":"Introduction","text":"<pre><code>flowchart LR\nsubgraph CAE\n  GUI &amp; CLI &amp; Script --&gt;\n  |Commands| pi[Python&lt;br /&gt;Interpreter] --&gt;\n  ak[Abaqus&lt;br /&gt;Kernel] &amp; rf[Replay Files]\nend</code></pre>"},{"location":"Tools/FEA/Abaqus/01_Introduction/#units","title":"Units","text":"<p>Abacus does not have a unit system. You need to use everything based on SI/SI(mm) units.</p>"},{"location":"Tools/FEA/Abaqus/01_Introduction/#interface","title":"Interface","text":""},{"location":"Tools/FEA/Abaqus/01_Introduction/#tools-tree","title":"Tools Tree","text":"<ul> <li>Model</li> <li>Analysis</li> </ul>"},{"location":"Tools/FEA/Abaqus/01_Introduction/#analysis-tree","title":"Analysis Tree","text":"<ul> <li>Jobs</li> </ul>"},{"location":"Tools/FEA/Abaqus/01_Introduction/#files","title":"Files","text":"Gets updated when Cleared when <code>.cae</code> Project file Interact with GUI Never <code>.rpy</code> Replay Python fileAllows to recreate the modelContains all used actionsOnly 5 recent versions of replay files will be retained Interact with GUI New session starts <code>.jnl</code> Journal/LogStores used model-related commandsAllows to recreate the model if model database gets corrupt Save model database Never <code>.rec</code> Recovery fileStores unused model-related commandsUseful if Abacus aborts unexpectedly Interact with GUI Save CAE file <code>.odb</code> Output DataBase from modelling stageAnalysis result data &amp; model-related info: Nodes, elements, surfaces, sets <code>.inp</code> Input configuration file for analysis stage <code>.rpt</code> Report file images Exported"},{"location":"Tools/FEA/Abaqus/01_Introduction/#types-of-objects","title":"Types of Objects","text":""},{"location":"Tools/FEA/Abaqus/01_Introduction/#properties","title":"Properties","text":"<ul> <li>Deformable</li> <li>Rigid Discrete</li> <li>Rigid Analytical</li> </ul> Object treated as Deformable Block that deforms due to forces Rigid Solid block that cannot be deformed Discrete \u201cmesh\u201d of atomic \u201cdiscrete/finite elements\u201d Analytical Bounding box"},{"location":"Tools/FEA/Abaqus/01_Introduction/#geometry","title":"Geometry","text":"Shell Wire Point"},{"location":"Tools/FEA/Abaqus/01_Introduction/#mesh-element-type","title":"Mesh Element Type","text":"Type TriangularCPS3R Faster computation QuadCPS4R Better geometric precision Quad-Dominated Compromise between both"},{"location":"Tools/FEA/Abaqus/01_Introduction/#views","title":"Views","text":"Feature Edges"},{"location":"Tools/FEA/Abaqus/01_Introduction/#properties-of-mesh","title":"Properties of Mesh","text":"Value when mesh is broken PHILSM 1 PSILSM 0"},{"location":"Tools/FEA/Abaqus/01_Introduction/#export","title":"Export","text":""},{"location":"Tools/FEA/Abaqus/01_Introduction/#image","title":"Image","text":"<ol> <li>Open ODB</li> <li>Display ODB to viewport</li> <li>Set field output to viewport</li> <li>Adjust view commands</li> <li>File &gt; Print &gt; File</li> </ol>"},{"location":"Tools/FEA/Abaqus/01_Introduction/#field-report","title":"Field Report","text":"<p>Method to output data at desired locations, for a single frame within a single step at a time</p>"},{"location":"Tools/FEA/Abaqus/01_Introduction/#procedure","title":"Procedure","text":"<ol> <li>Report</li> <li>Field Output</li> </ol>"},{"location":"Tools/FEA/Abaqus/01_Introduction/#display-group","title":"Display Group","text":"<ol> <li>Create display group</li> <li>Pick from viewport</li> </ol>"},{"location":"Tools/FEA/Abaqus/01_Introduction/#speed-up-simulation","title":"Speed Up Simulation","text":"<ul> <li>Decrease time period to 0.1</li> <li>Increase max increment size</li> <li>Useful if you are just interested in final output only</li> <li>Use more cores</li> <li>Reduce output requests</li> <li>Reduce mesh in unnecessary portions</li> <li>Check element formulation</li> <li>Use Reduced integration/linear geometric order</li> <li>Use small artificial damping</li> </ul>"},{"location":"Tools/FEA/Abaqus/02_Python_Scripting/","title":"Python Scripting","text":"<p>This assumes that you are proficient at Python</p> <p>https://www.youtube.com/watch?v=lesRkx0aPeA&amp;list=PL8JBtohft2fuXL6zj1ZETOTBtM8ydfMd6&amp;index=4</p>"},{"location":"Tools/FEA/Abaqus/02_Python_Scripting/#why-scripting","title":"Why Scripting?","text":"<ul> <li>Interact with underlying Abacus data structure</li> <li>Allows to extend out-of-box functionality</li> <li>Automate repetitive pre/post-processing taks</li> <li>Mathematically post-process FE data</li> <li>Custom GUI</li> <li>Perform parametric studies</li> </ul>"},{"location":"Tools/FEA/Abaqus/02_Python_Scripting/#run-python-scripts","title":"Run Python Scripts","text":"Mode Free Limitations Error Handling CLI Python-Only <code>abaqus python script.py</code> \u2705 Not all features are supported :/ Stop CLI CAE + Viewer <code>abaqus cae noGUI=script.py</code> \u274c Stop CLI Viewer-Only <code>abaqus viewer noGUI=script.py</code> \u274c Stop CLI <code>abaqus cae replay=abaqus.rpy</code> \u274c Ignore &amp; continue CLI <code>abaqus cae recover=my_model.jnl</code> \u274c GUI Menu <code>File &gt; Run Script</code> \u274c Stop GUI Abacus CLI <code>excefile(\"script.py\")</code> \u274c Stop"},{"location":"Tools/FEA/Abaqus/02_Python_Scripting/#basics","title":"Basics","text":"<pre><code># imports\nfrom abaqus import *\nfrom abaqusConstants import *\nfrom caeModules import *\nfrom driverUtils import executeOnCaeStartup\n\n# setup\nexecuteOnCaeStartup()\n\n# Create model\nmodel = mdb.Model(name=\"Model A\")\n\n# creating sketch\ns = myModel.ConstrainedSketch(\n    name = \"__profile__\",\n  sheetSize = 200.0\n)\n\n# Drawing model sketch\ns.Line(\n    point1=(0.0, 0.0),\n  point2=(0.0, 1.0)\n)\ns.Line(\n    point1=(0.0, 1.0),\n  point2=(1.0, 1.0)\n)\ns.Line(\n    point1=(1.0, 1.0),\n  point2=(1.0, 0.0)\n)\ns.Line(\n    point1=(1.0, 0.0),\n  point2=(0.0, 0.0)\n)\n\n# Creating part object\np = (\n  model\n  .Part(\n    name = \"rect_beam\",\n    dimensionality = THREE_D,\n    type=DEFORMABLE_BODY\n  )\n)\n\n# extrude sketch to get the part\np.BaseSolidExtrude(\n    sketch=s,\n  depth=20.0\n)\ns.unsetPrimaryObject()\n\n# setting part to the viewport\n(\n  session\n  .viewports[\"Viewport: 1\"]\n  .setValues(displayedObject=p)\n)\n\n# clear\ndel model.sketches[\"__profile__\"]\n</code></pre> <pre><code># imports\nfrom abaqus import *\nfrom abaqusConstants import *\nimport visualizatino\n\n# opening the db\nmy_odb = (\n    visualization\n  .openOdb(path = \"beam_model.odb\")\n)\n\nviewport = session.viewports[session.currentViewPortName]\n(\n  viewport\n  .setValues(displayedObject=my_odb)\n)\n\n# accessing the step-1 from the ODB\nmystep = my_obd.steps[\"Step-1\"]\n\n# accessing the frames of step1\nframe1 = mystep.frames[-1]\nframe2 = mystep.frames[-2]\n\ndisp1 = frame1.fieledOutputs[\"U\"]\ndisp2 = frame2.fieledOutputs[\"U\"]\n\nstress1 = frame1.fieledOutputs[\"S\"]\nstress2 = frame2.fieledOutputs[\"S\"]\n\ndeltaDisp = disp2 - disp1\ndeltaStress = stress2 - stress1\n\n(\n  viewport\n  .obdDisplay\n  .setDeformedVariables(deltaDisp)\n)\n\n# Plotting thecontour for the new data\n(\n  viewport\n  .obdDisplay\n  .setPrimaryVariable(\n      field = deltaStress,\n    outputPosition = INTEGRATION_POINT,\n    refinement = (INVARIANT, \"Mises\")\n  )\n)\n\nviewport.odbDispaly.display.setValues(\n    plotState=(CONTOURS_ON_DEF)\n)\n</code></pre> <pre><code># saving data to a text file\nwith open(\"delta_displacement.dat\", \"w\") as fout:\n  fout.write(\"%8d, %15.8E, %15.8E, %15.8E\\n\" % tuple([value.nodeLabel, ] + list(value.data)))\n</code></pre>"},{"location":"Tools/FEA/Abaqus/02_Python_Scripting/#step","title":"Step","text":"<p>Attributes</p> <ul> <li>Name</li> <li>Step number</li> <li>nlgoem</li> </ul> <p>Methods</p> <ul> <li>getFrame</li> <li>frames</li> <li>setDefaultField</li> </ul> <pre><code>(\n  odb\n  .steps[\"step-1\"]\n  .frames[1]\n  .fieldOutputs[\"U\"]\n  .values[0]\n  .data\n)\n</code></pre>"},{"location":"Tools/FEA/Abaqus/02_Python_Scripting/#abaqus-objects","title":"Abaqus Objects","text":""},{"location":"Tools/FEA/Abaqus/02_Python_Scripting/#session","title":"Session","text":"<ul> <li>odbs</li> <li>defaultOdbDisplay</li> <li>displayGroups</li> <li>colors</li> <li>printOptions</li> <li>psOptions</li> <li>epsOptions</li> <li>pngOptions</li> <li>xyPlots</li> <li>animationController</li> <li>views</li> <li>viewports</li> <li>paths</li> </ul>"},{"location":"Tools/FEA/Abaqus/02_Python_Scripting/#mdb","title":"Mdb","text":"<ul> <li>models</li> <li>Model<ul> <li>amplitudes</li> <li>parts</li> <li>interactions</li> <li>loads</li> <li>materials</li> <li>steps</li> <li>sections</li> <li>rootAssembly</li> </ul> </li> <li>jobs</li> </ul>"},{"location":"Tools/FEA/Abaqus/02_Python_Scripting/#odb","title":"Odb","text":"<code>import visualization</code><code>openOdb()</code> <code>import odbAccess</code><code>openOdb()</code> Use in Abacus CAE \u2705 \u2705 Use in Abacus Python \u274c \u2705 Multiple ODB accessible simultaneously? \u2705 \u274c Free? \u274c \u2705 <ul> <li>rootAssembly</li> <li>parts</li> <li>sectionCategories</li> <li>steps</li> <li>name</li> <li>nlgoem</li> <li>frames</li> <li>getFrame()</li> <li>setDefaultField()</li> <li>getHistoryRegion()</li> <li>save()</li> <li>close()</li> </ul>"},{"location":"Tools/FEA/Abaqus/02_Python_Scripting/#journalling-options","title":"Journalling options","text":"<pre><code>(\n  session\n  .journalOptions\n  .setValues(\n    replayGeometry=COORDINATE,\n    recoverGeometry=COORDINATE\n  )\n)\n</code></pre> Human-Readable Compressed Index(default) <code>getSequenceFromMask</code> \u274c Coordinate <code>findAt()</code><code>getClosest()</code> \u2705 Index Use entity index \u274c"},{"location":"Tools/FEA/Abaqus/02_Python_Scripting/#compressed-index-mode-default","title":"Compressed Index Mode (default)","text":"<pre><code>all_cells = (\n  mdb\n  .models[\"Model A\"]\n  .parts[\"rect_beam\"]\n  .cells\n)\nselected_cells = (\n  all_cells\n  .getSequencFromMask(\n    mask=(\"[#1 ]\", )\n  )\n)\n</code></pre>"},{"location":"Tools/FEA/Abaqus/02_Python_Scripting/#data-types","title":"Data Types","text":""},{"location":"Tools/FEA/Abaqus/02_Python_Scripting/#symbolic-constants","title":"Symbolic Constants","text":"<pre><code>from abaqusConstants import *\n</code></pre>"},{"location":"Tools/FEA/Abaqus/02_Python_Scripting/#abaqus-boolean","title":"Abaqus Boolean","text":"<p><code>ON, OFF</code></p>"},{"location":"Tools/FEA/Abaqus/02_Python_Scripting/#repositories","title":"Repositories","text":""},{"location":"Tools/FEA/Abaqus/02_Python_Scripting/#export","title":"Export","text":""},{"location":"Tools/FEA/Abaqus/02_Python_Scripting/#images","title":"Images","text":"<pre><code>(\n  session\n  .printToFile(\n    fileName = \"export\",\n    format = PNG,\n    canvasObjects = (\n        session.viewports[\"Viewport: 1\"], \n    )\n  )\n)\n</code></pre>"},{"location":"Tools/FEA/Abaqus/02_Python_Scripting/#field-report","title":"Field Report","text":"<pre><code>session.writeFieldReport(\n    filname=\"report.rpt\",\n  append=ON,\n  sortItem=\"Node Label\",\n  odb=odb,\n  step=0,\n  frame=0,\n  outputPosition=NODAL,\n  variable=((\n    \"S\", INTEGRATION_POINT\n  )),\n)\n</code></pre>"},{"location":"Tools/FEA/Abaqus/03_IDK/","title":"03 IDK","text":"<pre><code>X=[x1,x2,x3 ... xn]\nY=[y1,y2,y3 ... yn]\nZ=[z1,z2,z3 ... zn]\nfor I in range (len(X)):\nfor J in range (len(Y)):\nfor K in range (len(Z)):\nmodelname='P1_'+ str(int(X[I]))+'_'+'P2_'+str(int(Y[J]))+'_'+'P3_'+str(int(Z[K]))\nmymodel=mdb.Model(name=modelname)\n</code></pre> <pre><code># So now you have several Models in Abaqus. Then you write or use Abaqus macro to create your script. At last, you must create a job for each model.\njobname=str(int(X[I]))+'_'+str(int(Y[J]))+'_'+str(int(Z[K]))\n</code></pre> <pre><code>mdb.Job(\n  name=jobname, model=mymodel,description='', type=ANALYSIS, atTime=None, waitMinutes=0, waitHours=0,queue=None, memory=90, memoryUnits=PERCENTAGE,explicitPrecision=SINGLE, nodalOutputPrecision=SINGLE, echoPrint=OFF,modelPrint=OFF, contactPrint=OFF, historyPrint=OFF, userSubroutine='',scratch='', resultsFormat=ODB, parallelizationMethodExplicit=DOMAIN,numDomains=1, activateLoadBalancing=False, multiprocessingMode=DEFAULT,numCpus=1\n       )\n\nmbd.JobFromInputFile(\n\n)\n</code></pre> <pre><code>mdb.jobs[jobname].submit(consistencyChecking=OFF)\nmdb.jobs[jobname].waitForCompletion()\n</code></pre> <pre><code># At last, if you want to run this script without opening Abaqus, open cmd and set a new address to the script.\ncd path\n</code></pre> <pre><code># use this code to run Abaqus\nAbaqus cae noGUI=SCRIPTNAME.py\n</code></pre>"},{"location":"Tools/Google_Sheets/","title":"Google Sheets","text":""},{"location":"Tools/Google_Sheets/gsheets_api/","title":"GSheets API","text":""},{"location":"Tools/Google_Sheets/gsheets_api/#app-script","title":"App Script","text":"<pre><code>const DATA_ENTRY_SHEET_NAME = \"Registration\";\nconst TIME_STAMP_COLUMN_NAME = \"Timestamp\"; // You can edit the name of this column name or you can put blank like this : \"\". Ensure that the same name exist there in the sheet as well.\n\n\nvar sheet = SpreadsheetApp.getActiveSpreadsheet().getSheetByName(DATA_ENTRY_SHEET_NAME);\n\nconst doPost = (request = {}) =&gt; {\n  const { postData: { contents, type } = {} } = request;\n  var data = parseFormData(contents);\n  try {\n    appendToGoogleSheet(data);\n  } catch (e) {\n\n  }\n\n  try {\n    send_confirmation_mail();\n  } catch (e) {\n\n  }\n\n\n return ContentService.createTextOutput(contents).setMimeType(ContentService.MimeType.JSON);\n};\n\nfunction send_confirmation_mail(data) {\n  url = \"https://btf.pythonanywhere.com/send-registration-confirmation?n=\" + data[\"Name\"].replace(\" \", \"+\") + \"&amp;i=\" + data[\"Institution\"].replace(\" \", \"+\") + \"&amp;e=\" + data[\"Email\"].replace(\" \", \"\");\n  UrlFetchApp.fetch(url);\n}\n\nfunction parseFormData(postData) {\n  var data = [];\n  var parameters = postData.split('&amp;');\n  for (var i = 0; i &lt; parameters.length; i++) {\n    var keyValue = parameters[i].split('=');\n    data[keyValue[0]] = decodeURIComponent(keyValue[1]);\n  }\n  return data;\n}\n\nfunction appendToGoogleSheet(data) {\n  if(TIME_STAMP_COLUMN_NAME !==\"\"){\n    data[TIME_STAMP_COLUMN_NAME]=new Date();\n  }\n  var headers = sheet.getRange(1, 1, 1, sheet.getLastColumn()).getValues()[0];\n  var rowData = headers.map(headerFld =&gt; data[headerFld]);\n  sheet.appendRow(rowData);\n}\n</code></pre>"},{"location":"Tools/Google_Sheets/gsheets_api/#client-side","title":"Client-Side","text":"<pre><code>const API_LINK = \"https://script.google.com/macros/s/AKfycbz_FzYhH1h0WZIhvpLicgxWQxqpFnkUGAvLN-oTUdMkImJe_hWqDlaQT8GdPn5MPsVmVA/exec\";\n\nconst Technofest = () =&gt; {\n  const handleSubmit = (event) =&gt; {\n    event.preventDefault();\n    document.getElementById(\"message\").textContent = \"Submitting..\";\n    document.getElementById(\"message\").style.display = \"block\";\n    document.getElementById(\"submit-button\").disabled = true;\n\n    // Collect the form data\n    var formData = new FormData(event.target);\n    var keyValuePairs = [];\n    for (var pair of formData.entries()) {\n      keyValuePairs.push(pair[0] + \"=\" + pair[1]);\n    }\n\n    var formDataString = keyValuePairs.join(\"&amp;\");\n\n    // Send a POST request to your Google Apps Script\n    fetch(API_LINK, {\n      redirect: \"follow\",\n      method: \"POST\",\n      body: formDataString,\n      headers: {\n        \"Content-Type\": \"text/plain;charset=utf-8\",\n      },\n    })\n      .then(function (response) {\n        // Check if the request was successful\n        if (response) {\n          return response; // Assuming your script returns JSON response\n        } else {\n          throw new Error(\"Failed to submit the form.\");\n        }\n      })\n      .then(function (data) {\n        // Display a success message\n        document.getElementById(\"message\").textContent =\n          \"Data submitted successfully!\";\n        document.getElementById(\"message\").style.display = \"block\";\n        document.getElementById(\"message\").style.backgroundColor = \"green\";\n        document.getElementById(\"message\").style.color = \"beige\";\n        document.getElementById(\"submit-button\").disabled = false;\n        event.target.reset();\n\n        setTimeout(function () {\n          document.getElementById(\"message\").textContent = \"\";\n          document.getElementById(\"message\").style.display = \"none\";\n        }, 2600);\n      })\n      .catch(function (error) {\n        // Handle errors, you can display an error message here\n        console.error(error);\n        document.getElementById(\"message\").textContent =\n          \"An error occurred while submitting the form.\" + \": \" + error;\n        document.getElementById(\"message\").style.display = \"block\";\n      });\n  };\n</code></pre>"},{"location":"Tools/Google_Sheets/idk/","title":"Idk","text":"<pre><code>const SHEETID = 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx';\nconst DOCID = 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx';\nconst FOLDERID = 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx';\n\nfunction onOpen(){\n  SpreadsheetApp.getUi()\n  .createMenu('Generate Invoices Workflow')\n  .addItem('Manually Add Month Folder/ Import New Data','xxxx')\n  .addItem('Generate PDF Invoices','sender')\n  .addToUi();\n}\n\nfunction sender () {\n  const sheet = SpreadsheetApp.openById(SHEETID).getSheetByName('InvoiceData');\n  const InvoiceData = sheet.getDataRange().getValues();\n  const rows = InvoiceData.slice(1);\n  // Logger.log(rows);\n\n  const temp = DriveApp.getFileById(DOCID);\n  const folder = DriveApp.getFolderById(FOLDERID);\n\n  //Loop through each spreadsheet row, and for each row, create a new temp document in your drive folder\n  rows.forEach((row,index)=&gt;{\n    const file = temp.makeCopy(folder);\n    const doc = DocumentApp.openById(file.getId());\n    const body = doc.getBody();\n\n    //Loop through the spreadsheets heading values and populate those values into the temp document\n    InvoiceData[0].forEach((heading,i)=&gt;{\n      const header1 = heading.toUpperCase();\n      body.replaceText(`{${header1}}`,row[i]);\n      })\n\n      //Set a name for each document using x, y from the data\n      doc.setName('INV-'+row[0]+' '+row[2]+'.doc');\n      const blob = doc.getAs(MimeType.PDF);\n      doc.saveAndClose();\n      const pdf = folder.createFile(blob).setName('INV-'+row[0]+' '+row[2]+'.pdf');\n      //The followinng code removes the temp doc file, leaving just the PDF files.\n      file.setTrashed(true);\n\n    //TESTING\n    // console.log(header1);\n    //TESTING\n  })\n}\n</code></pre>"},{"location":"Tools/Google_Sheets/mail_merge/","title":"Mail merge","text":"<pre><code>// To learn how to use this script, refer to the documentation:\n// https://developers.google.com/apps-script/samples/automations/mail-merge\n\n/*\nCopyright 2022 Martin Hawksey\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    https://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n*/\n\n/**\n * @OnlyCurrentDoc\n*/\n\n/**\n * Change these to match the column names you are using for email \n * recipient addresses and email sent column.\n*/\nconst RECIPIENT_COL  = \"To\";\nconst CC_COL  = \"CC\";\nconst BCC_COL  = \"BCC\";\nconst EMAIL_SENT_COL = \"Email_Sent\";\nconst SUBJECT_COL = \"Subject\";\n\n/** \n * Creates the menu item \"Mail Merge\" for user to run scripts on drop-down.\n */\nfunction onOpen() {\n  const ui = SpreadsheetApp.getUi();\n  ui.createMenu('Mail Merge')\n      .addItem('Send Emails', 'sendEmails')\n      .addToUi();\n}\n\n/**\n * Sends emails from sheet data.\n * @param {string} subjectLine (optional) for the email draft message\n * @param {Sheet} sheet to read data from\n*/\nfunction sendEmails(subjectLine, sheet=SpreadsheetApp.getActiveSheet()) {\n  // option to skip browser prompt if you want to use this code in other projects\n  if (!subjectLine){\n    subjectLine = Browser.inputBox(\"Mail Merge\", \n                                      \"Type or copy/paste the subject line of the Gmail \" +\n                                      \"draft message you would like to mail merge with:\",\n                                      Browser.Buttons.OK_CANCEL);\n\n    if (subjectLine === \"cancel\" || subjectLine == \"\"){ \n    // If no subject line, finishes up\n    return;\n    }\n  }\n\n  // Gets the draft Gmail message to use as a template\n  const emailTemplate = getGmailTemplateFromDrafts_(subjectLine);\n\n  // Gets the data from the passed sheet\n  const dataRange = sheet.getDataRange();\n  // Fetches displayed values for each row in the Range HT Andrew Roberts \n  // https://mashe.hawksey.info/2020/04/a-bulk-email-mail-merge-with-gmail-and-google-sheets-solution-evolution-using-v8/#comment-187490\n  // @see https://developers.google.com/apps-script/reference/spreadsheet/range#getdisplayvalues\n  const data = dataRange.getDisplayValues();\n\n  // Assumes row 1 contains our column headings\n  const heads = data.shift(); \n\n  // Gets the index of the column named 'Email Status' (Assumes header names are unique)\n  // @see http://ramblings.mcpher.com/Home/excelquirks/gooscript/arrayfunctions\n  const emailSentColIdx = heads.indexOf(EMAIL_SENT_COL);\n\n  // Converts 2d array into an object array\n  // See https://stackoverflow.com/a/22917499/1027723\n  // For a pretty version, see https://mashe.hawksey.info/?p=17869/#comment-184945\n  const obj = data.map(r =&gt; (heads.reduce((o, k, i) =&gt; (o[k] = r[i] || '', o), {})));\n\n  // Creates an array to record sent emails\n  const out = [];\n\n  // Loops through all the rows of data\n  obj.forEach(function(row, rowIdx){\n    // Only sends emails if email_sent cell is blank and not hidden by a filter\n    if (row[EMAIL_SENT_COL] == ''){\n      try {\n        const msgObj = fillInTemplateFromObject_(emailTemplate.message, row);\n\n        // See https://developers.google.com/apps-script/reference/gmail/gmail-app#sendEmail(String,String,String,Object)\n        // If you need to send emails with unicode/emoji characters, use MailApp instead of GmailApp\n        // Uncomment advanced parameters as needed (see docs for limitations)\n        MailApp.sendEmail(\n          row[RECIPIENT_COL],\n          row[SUBJECT_COL],\n          msgObj.text,\n          {\n            htmlBody: msgObj.html,\n            bcc: row[BCC_COL],\n            cc: row[CC_COL],\n            // from: 'bitstechfest@dubai.bits-pilani.ac.in',\n            name: 'BITS Tech Fest',\n            // replyTo: row[CC_COL], // the email address that the reply message is sent when you want the reply to go to an email address that is different than the From: address \n            // noReply: true, // if the email should be sent from a generic no-reply email address (not available to gmail.com users)\n            attachments: emailTemplate.attachments,\n            inlineImages: emailTemplate.inlineImages\n          });\n        // Edits cell to record email sent date\n        out.push([new Date()]);\n      } catch(e) {\n        // modify cell to record error\n        out.push([e.message]);\n      }\n    } else {\n      out.push([row[EMAIL_SENT_COL]]);\n    }\n  });\n\n  // Updates the sheet with new data\n  sheet.getRange(2, emailSentColIdx+1, out.length).setValues(out);\n\n  /**\n   * Get a Gmail draft message by matching the subject line.\n   * @param {string} subject_line to search for draft message\n   * @return {object} containing the subject, plain and html message body and attachments\n  */\n  function getGmailTemplateFromDrafts_(subject_line){\n    try {\n      // get drafts\n      const drafts = GmailApp.getDrafts();\n      // filter the drafts that match subject line\n      const draft = drafts.filter(subjectFilter_(subject_line))[0];\n      // get the message object\n      const msg = draft.getMessage();\n\n      // Handles inline images and attachments so they can be included in the merge\n      // Based on https://stackoverflow.com/a/65813881/1027723\n      // Gets all attachments and inline image attachments\n      const allInlineImages = draft.getMessage().getAttachments({includeInlineImages: true,includeAttachments:false});\n      const attachments = draft.getMessage().getAttachments({includeInlineImages: false});\n      const htmlBody = msg.getBody(); \n\n      // Creates an inline image object with the image name as key \n      // (can't rely on image index as array based on insert order)\n      const img_obj = allInlineImages.reduce((obj, i) =&gt; (obj[i.getName()] = i, obj) ,{});\n\n      //Regexp searches for all img string positions with cid\n      const imgexp = RegExp('&lt;img.*?src=\"cid:(.*?)\".*?alt=\"(.*?)\"[^\\&gt;]+&gt;', 'g');\n      const matches = [...htmlBody.matchAll(imgexp)];\n\n      //Initiates the allInlineImages object\n      const inlineImagesObj = {};\n      // built an inlineImagesObj from inline image matches\n      matches.forEach(match =&gt; inlineImagesObj[match[1]] = img_obj[match[2]]);\n\n      return {message: {subject: subject_line, text: msg.getPlainBody(), html:htmlBody}, \n              attachments: attachments, inlineImages: inlineImagesObj };\n    } catch(e) {\n      throw new Error(\"Oops - can't find Gmail draft\");\n    }\n\n    /**\n     * Filter draft objects with the matching subject linemessage by matching the subject line.\n     * @param {string} subject_line to search for draft message\n     * @return {object} GmailDraft object\n    */\n    function subjectFilter_(subject_line){\n      return function(element) {\n        if (element.getMessage().getSubject() === subject_line) {\n          return element;\n        }\n      }\n    }\n  }\n\n  /**\n   * Fill template string with data object\n   * @see https://stackoverflow.com/a/378000/1027723\n   * @param {string} template string containing {{}} markers which are replaced with data\n   * @param {object} data object used to replace {{}} markers\n   * @return {object} message replaced with data\n  */\n  function fillInTemplateFromObject_(template, data) {\n    // We have two templates one for plain text and the html body\n    // Stringifing the object means we can do a global replace\n    let template_string = JSON.stringify(template);\n\n    // Token replacement\n    template_string = template_string.replace(/{{[^{}]+}}/g, key =&gt; {\n      return escapeData_(data[key.replace(/[{}]+/g, \"\")] || \"\");\n    });\n    return  JSON.parse(template_string);\n  }\n\n  /**\n   * Escape cell data to make JSON safe\n   * @see https://stackoverflow.com/a/9204218/1027723\n   * @param {string} str to escape JSON special characters from\n   * @return {string} escaped string\n  */\n  function escapeData_(str) {\n    return str\n      .replace(/[\\\\]/g, '\\\\\\\\')\n      .replace(/[\\\"]/g, '\\\\\\\"')\n      .replace(/[\\/]/g, '\\\\/')\n      .replace(/[\\b]/g, '\\\\b')\n      .replace(/[\\f]/g, '\\\\f')\n      .replace(/[\\n]/g, '\\\\n')\n      .replace(/[\\r]/g, '\\\\r')\n      .replace(/[\\t]/g, '\\\\t');\n  };\n}\n</code></pre>"},{"location":"Tools/IoT/Arduino/","title":"Arduino","text":""},{"location":"Tools/IoT/Arduino/#rules","title":"Rules","text":"<p>You can use Arduino to make a commercial product following some simple rules.</p> <ul> <li>if you have made your circuit as a derivative of the Arduino board you must release the design files with a CC-BY-SA license like the original cad files</li> <li>If you build your circuit as a shield that plugs on top of an Arduino board all the circuit is yours and you don't have to release anything</li> <li>The programs written on Arduino are yours. if you have modified the core files or one of the libraries you must make your modifications available to everybody</li> <li>You can call your product in any way you like as long as you don't call it Arduino</li> <li>Credits to Arduino not necessary</li> <li>If in the documentation for your product you want to write \"Powered By Arduino\" that would be appreciated</li> <li>There is no revenue sharing for any derivative work (unless it uses the Arduino name see Arduino - Home )</li> <li>If you patent something you have to do that in every place in the world you want to protect your idea. the more countries the more payments</li> <li>Patents reveal your idea to the whole world.. some people don't patent so they don't have to provide any detail about their invention</li> </ul>"},{"location":"Tools/IoT/Arduino/#references","title":"References","text":"<ul> <li>https://www.youtube.com/watch?v=DPqiIzK97K0</li> </ul>"},{"location":"Tools/IoT/Arduino/01_Basics/","title":"Basics","text":""},{"location":"Tools/IoT/Arduino/01_Basics/#arduino-ide","title":"Arduino IDE","text":"<ul> <li>Install Arduino IDE</li> <li><code>Tools</code> &gt; <code>Manage Libaries</code></li> <li>ESP8266/Node MCU</li> <li> <p><code>Preferences</code> &gt; Additional boards manager URLS</p> <ul> <li> <p>https://arduino.esp8266.com/stable/package_esp8266com_index.json</p> </li> <li> <p><code>Tools</code> &gt; <code>Board</code> &gt; <code>Boards Manager</code> &gt; Search <code>ESP8266</code> &gt; <code>Install</code></p> </li> <li> <p>Ensure <code>CH340g Driver</code> installed</p> </li> <li><code>Tools</code> &gt; <code>Board</code> &gt; Select Board</li> <li><code>Tools</code> &gt; <code>Board</code> &gt; Connect to Port</li> <li>Compile &amp; Upload</li> </ul> </li> </ul>"},{"location":"Tools/IoT/Arduino/01_Basics/#simulators","title":"Simulators","text":"<ul> <li>Wokwi (Open-Source)</li> <li>TinkerCad</li> </ul>"},{"location":"Tools/IoT/Arduino/01_Basics/#code","title":"Code","text":""},{"location":"Tools/IoT/Arduino/01_Basics/#skeleton","title":"Skeleton","text":"<pre><code>void setup(){\n  // initialization code\n}\nvoid loop(){\n  // infinitely-looping code\n}\n</code></pre>"},{"location":"Tools/IoT/Arduino/01_Basics/#inputoutputs","title":"Input/Outputs","text":"Function Configuring A GPIO pin cannot be used for both input and output. You need to specify one. <code>pinMode(&lt;pin_number&gt;, &lt;i/o&gt;);</code> <code>pinMode(3, INPUT);</code><code>pinMode(3, OUTPUT);</code> Outputs Digital <code>digitalWrite(&lt;pin_number&gt;, &lt;state&gt;);</code> <code>digitalWrite(3, HIGH); // or digitalWrite(3, 1);</code><code>digitalWrite(3, LOW); // or digitalWrite(3, 0);</code> Analog <code>analogWrite(&lt;pin_number&gt;, value);</code> <code>analogWrite(3, 25);</code> <pre><code>// Code for blinking LED\n\nvoid setup(){\n  pinMode(3, OUTPUT);\n}\nvoid loop(){\n  digitalWrite(3, HIGH);\n  delay(1000); // 1000ms\n\n  digitalWrite(3, LOW);\n  delay(1000); // 1000ms\n}\n</code></pre> <pre><code>// Code for changing LED brightness\n\nvoid setup(){\n  pinMode(3, OUTPUT);\n}\nvoid loop(){\n  for (int i=0; i&lt;=1023; i++) {\n    analogWrite(3, i);\n    delay(1000); // 1000ms\n  }\n}\n</code></pre>"},{"location":"Tools/IoT/Arduino/01_Basics/#serial-monitor","title":"Serial Monitor","text":"<p>Baudrate \\(\\equiv\\) Bitrate</p> <ul> <li>Bitrate is for binary</li> <li>Baudrate is for analog signal</li> </ul> <pre><code>Serial.begin(9600); // ; baudrate // initializes serial monitor\n\nSerial.read() // return ASCII values\n</code></pre> <pre><code>int reading_int;\nchar reading_char;\n\nvoid setup(){\n  Serial.begin(9600);\n}\nvoid loop(){\n  while (Serial.available()){\n    reading_int = Serial.read();\n    reading_char = reading_int;\n\n    Serial.println(reading_int);\n    Serial.println(reading_char);\n  }  \n  delay(500);\n}\n</code></pre>"},{"location":"Tools/IoT/Arduino/01_Basics/#unique-id-for-arduino","title":"Unique ID for Arduino","text":""},{"location":"Tools/IoT/Arduino/01_Basics/#method-1-automatic-using-external-library","title":"Method 1: Automatic (using external library)","text":"<pre><code>#include &lt;ArduinoUniqueID.h&gt; // in the same folder of this note\n\nfor(size_t i = 0; i &lt; UniqueIDsize; i++)\n  Serial.println(UniqueID[i], HEX);\n</code></pre>"},{"location":"Tools/IoT/Arduino/01_Basics/#method-2-automatic","title":"Method 2: Automatic","text":"<pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;unistd.h&gt;\n#include &lt;sys/fcntl.h&gt;\nint main(int ac, char **av) {\n    int fd, i;\n    unsigned char eui[8];\n    fd = open(\"/dev/random\", O_RDONLY);\n    if (fd &lt; 0) {\n        perror(\"can't open /dev/random\");\n        exit(1);\n    }\n    if (read(fd, eui, sizeof(eui)) != sizeof(eui)) {\n        fprintf(stderr, \"couldn't read %zu bytes\\n\", sizeof(eui));\n        exit(1);\n    }\n    eui[0] = (eui[0] &amp; ~1) | 2;\n    for (i = 0; i &lt; sizeof(eui); ++i) {\n        printf(\"%02X%c\", eui[i], i == sizeof(eui)-1 ? '\\n' : '-');\n    }\n    return 0;\n}\n</code></pre>"},{"location":"Tools/IoT/Arduino/01_Basics/#method-3-manualcustom-id","title":"Method 3: Manual/Custom ID","text":"<p>Get the code from <code>TOOLS &gt; Get Board Info</code> or put a custom one</p> <p><code>write_id_to_eeprom.ino</code></p> <p><pre><code>char sID[7] = \"AE0001\";\n\n// do this only once on an Arduino, \n// write the Serial of the Arduino in the first 6 bytes of the EEPROM\n\n#include &lt;EEPROM.h&gt;\n\nvoid setup()\n{\n  Serial.begin(9600);\n  for (int i=0; i&lt;6; i++) {\n    EEPROM.write(i,sID[i]);\n  }\n}\n\nvoid loop() {\n  // \n}\n</code></pre> <code>read_id_from_eeprom.ino</code></p> <pre><code>// reads the Serial of the Arduino from the \n// first 6 bytes of the EEPROM\n\n#include &lt;EEPROM.h&gt;\nchar sID[7];\n\nvoid setup()\n{\n  Serial.begin(9600);\n  for (int i=0; i&lt;6; i++) {\n    sID[i] = EEPROM.read(i);\n  }\n  Serial.println(sID);\n}\n\nvoid loop() {\n  // \n}\n</code></pre>"},{"location":"Tools/IoT/Arduino/01_Basics/#multi-tasking","title":"Multi-Tasking","text":""},{"location":"Tools/IoT/Arduino/01_Basics/#interrupts","title":"Interrupts","text":"Trigger Meaning in Bits High 1 Low 0 Rising 0-1 Falling 1-0 Change 0-1 or 1-0 <pre><code>void my_func() {\n  delay_seconds = 1;\n  delayMicroseconds(delay_seconds * 1000 * 1000);\n\n  if (digitalRead(buttonPin) == LOW)\n  {\n    return ;\n  }\n\n  led_state = !led_state;\n  digitalWrite(ledPin, ledState);\n}\n\nvoid setup(){\n  pinMode(buttonPin, INPUT);\n  pinMode(ledPin, OUTPUT);\n\n  attachInterrupt(buttonPin, my_func, RISING);\n}\n\nvoid loop() {\n  while(WiFi.connected()){\n\n  }\n}\n</code></pre>"},{"location":"Tools/IoT/Arduino/01_Basics/#millis-instead-of-delay","title":"<code>millis()</code>\u00a0instead of <code>delay()</code>","text":"<p>Arduino does not support multi-threading/processing, and hence parallel processing is not possible</p> <pre><code>millis()\n// -&gt; unsigned long\n// -&gt; returns number of ms since Arduino powered up/reset\n</code></pre> <pre><code>unsigned long prevTime = millis();\nunsigned long currentTime;\n\nvoid setup() {\n\n}\n\nvoid loop() {\n  currentTime = millis();\n\n  if (currentTime - prevTime &gt; 1000) {\n    doSomething();\n\n    prevTime = currentTime;\n  }\n}\n</code></pre>"},{"location":"Tools/IoT/Arduino/01_Basics/#example","title":"Example","text":"<pre><code>#define LED1 13\n#define LED2 12\n#define LED3 11\n\n#define BTN 4\n\n// set LED states\nint LED1_state = LOW;\nint brightness = 0;\n\n// previous time for the tasks depending upon time.\nunsigned long prevTime_T1 = millis(); \nunsigned long prevTime_T4 = millis(); \n\n// time intervals for the tasks\nlong interval_T1 = 1000; // blink every 1 second\nlong interval_T4 = 5000; // print brightness of LED3 every 5 seconds\n\nvoid setup() {\n  // put your setup code here, to run once:\n  Serial.begin(9600);\n  pinMode(LED1, OUTPUT);\n  pinMode(LED2, OUTPUT);\n  pinMode(LED3, OUTPUT);\n  pinMode(BTN, INPUT_PULLUP);\n}\n\nvoid loop() {\n  // put your main code here, to run repeatedly:\n  unsigned long currentTime = millis();\n\n  // Task 1 : Blink LED1 (T1)\n  if (currentTime - prevTime_T1 &gt; interval_T1) {\n    LED1_state = !LED1_state;\n    digitalWrite(LED1, LED1_state);\n\n    prevTime_T1 = currentTime;\n  }\n\n  // Task 2 : Glow LED2 when BTN is pressed\n  if (digitalRead(BTN)) {\n    digitalWrite(LED2, LOW);\n  } else {\n    digitalWrite(LED2, HIGH);\n  }\n\n  // Task 3 : Read input from serial monitor (0-255) and then write to LED3\n  if (Serial.available()) {\n    brightness = Serial.parseInt();\n    if (brightness &gt;=0 &amp;&amp; brightness &lt;= 255) {\n      analogWrite(LED3, brightness);\n    }\n  }\n\n  // Task 4 : print the brightness of LED3 in the serial monitor after every 5 seconds\n  if (currentTime - prevTime_T4 &gt; interval_T4) {\n    Serial.print(\"Brightness (0-255): \");\n    Serial.println(brightness);\n\n    prevTime_T4 = currentTime;\n  }\n}\n</code></pre>"},{"location":"Tools/IoT/Arduino/01_Basics/#code-cloning","title":"Code Cloning","text":""},{"location":"Tools/IoT/Arduino/01_Basics/#how-to","title":"How to","text":"<ul> <li>https://www.youtube.com/watch?v=csNdJIIkzo8</li> </ul>"},{"location":"Tools/IoT/Arduino/01_Basics/#protection","title":"Protection","text":"<ul> <li>https://www.youtube.com/watch?v=G3mRMedchJs</li> </ul>"},{"location":"Tools/IoT/Arduino/01_Basics/#code-security","title":"Code Security","text":""},{"location":"Tools/IoT/Arduino/01_Basics/#mainino","title":"<code>main.ino</code>","text":"<pre><code>#include \"secrets.h\"\n\nvoid setup(){\n  Serial.begin(9600);\n  Serial.println(secret_variable);\n}\n</code></pre>"},{"location":"Tools/IoT/Arduino/01_Basics/#secretsh","title":"<code>secrets.h</code>","text":"<pre><code>#define secret_variable \"Secret\";\n</code></pre>"},{"location":"Tools/IoT/Arduino/01_Basics/#_1","title":"Basics","text":""},{"location":"Tools/IoT/Arduino/02_WiFi_Connectivity/","title":"WiFi Connectivity","text":"<pre><code>WiFi.scanNetworks();\nWiFi.SSID(i);\nWiFi.RSSI(i);\nWiFi.begin(ssid, pass);\nWiFi.status();\nWiFi.localIP();\n</code></pre> <pre><code>WiFiServer server(local_port);\nserver.begin();\n</code></pre> <pre><code>WiFiClient = server.available();\nclient.readStringUntil(\"\\r\");\n</code></pre>"},{"location":"Tools/IoT/Arduino/02_WiFi_Connectivity/#show-wifi-networks","title":"Show WiFi Networks","text":"<pre><code>#include &lt;ESP8266WiFi.h&gt;\n\nvoid setup(){\n    Serial.begin(9600);\n}\n\nvoid loop(){\n  Serial.println(\"Scanning WiFi\");\n\n  int no_of_networks = WiFi.scanNetworks(); // only 2.4GHz for NodeMCU\n\n  if (n==0) {\n    Serial.println(\"No networks available\");\n  } else {\n    for (int i=0; i&lt;n; i++) {\n      Serial.println(\n        String(i+1) + \" \" + WiFi.SSID(i) + \" \" + String(WiFi.RSSI(i))\n      );\n    }\n  }\n\n  delay(5000);\n}\n</code></pre>"},{"location":"Tools/IoT/Arduino/02_WiFi_Connectivity/#connect-to-network","title":"Connect to Network","text":"<pre><code>#include &lt;ESP8266WiFi.h&gt;\n\n// variables\n// we need to use these exact types and names, to override the one in the header file\nchar* ssid = \"WiFi Name\";\nchar* pass = \"WiFi Password\";\n\nvoid setup(){\n    Serial.begin(9600);\n  pinMode(D2, OUTPUT);\n\n  Serial.println(\"Connecting to WiFi...\");\n  WiFi.begin(ssid, pass);\n\n\n  while(WiFi.status() != WL_CONNECTED) {\n    Serial.print(\".\");\n    delay(1000);\n  }\n\n  Serial.println(\"Connected to \" + String(ssid) + \" successfully\");\n}\n\nvoid loop(){\n  // code\n\n  delay(5000);\n}\n</code></pre>"},{"location":"Tools/IoT/Arduino/02_WiFi_Connectivity/#local-server","title":"Local Server","text":"<pre><code>#include &lt;ESP8266WiFi.h&gt;\n\n// variables\n// we need to use these exact types and names, to override the one in the header file\nchar* ssid = \"WiFi Name\";\nchar* pass = \"WiFi Password\";\n\nint local_port = 80;\nWiFiServer server(local_port);\n\nvoid setup(){\n    Serial.begin(9600);\n  pinMode(D2, OUTPUT);\n\n  Serial.println(\"Connecting to WiFi...\");\n  WiFi.begin(ssid, pass);\n\n\n  while(WiFi.status() != WL_CONNECTED) {\n    Serial.print(\".\");\n    delay(1000);\n  }\n\n  Serial.println(\"Connected to \" + String(ssid) + \" successfully.\");\n\n  server.begin(local_port);\n\n  local_ip = WiFi.localIP();\n\n  Serial.println(\n    \"Started server on \" + String(local_ip) + \":\" + String(local_port) + \" successfully.\"\n  );\n}\n\nvoid loop(){\n  WiFiClient client = server.available();\n  if (!client) {\n    return;\n  }\n  Serial.println(\"New request received!\");\n  String request = client.readStringUntil(\"\\r\");\n\n  query_path = \"/on\"\n  /*\n  localhost/on turns on LED\n  localhost/off turns off LED\n  */\n  if (request.indexOf(query_path) != -1){\n    digitalWrite(D2, HIGH);\n    Serial.println(\"LED turned on\");\n  }\n  else if (request.indexOf(query_path) != -1){\n    digitalWrite(D2, LOW);\n    Serial.println(\"LED turned off\");\n  } else {\n    Serial.println(\"Invalid request\");\n  }\n\n  client_interaction_code = \"\" +\n    \"&lt;html&gt;&lt;body&gt;\" + \n    \"&lt;button&gt;&lt;a href='/on'&gt;On&lt;/a&gt;&lt;/button&gt;\" +\n    \"&lt;button&gt;&lt;a href='/off'&gt;Off&lt;/a&gt;&lt;/button&gt;\" +\n    \"&lt;/body&gt;&lt;/html&gt;\";\n\n  delay(5000);\n}\n</code></pre>"},{"location":"Tools/IoT/Arduino/02_WiFi_Connectivity/#client","title":"Client","text":"<pre><code>#include &lt;ESP8266WiFi.h&gt;\n\n// variables\n// we need to use these exact types and names, to override the one in the header file\nchar* ssid = \"WiFi Name\";\nchar* pass = \"WiFi Password\";\n\nint local_port = 80;\nWiFiClient client;\n\nchar* api_key = \"\";\nint id_server = ;\nchar ip_server[] = \"\";\n\nvoid setup() {\n  Serial.begin(9600);\n\n  Serial.println(\"Connecting to WiFi ...\");\n  WiFi.begin(ssid, pass);\n  while (WiFi.status() != WL_CONNECTED)\n  {\n    Serial.print(\".\");\n    delay(1000);\n  }\n  Serial.println(\"WiFi connected\");\n\n  // ThingSpeak.begin(client);\n}\n\nvoid loop() {\n  if (client.connect(ip_server, local_port)) {\n    ThingSpeak.setField(1, data);\n    ThingSpeak.writeFields(id_server, api);\n  }\n\n  delay(5000);\n}\n</code></pre>"},{"location":"Tools/IoT/Arduino/02_WiFi_Connectivity/#http-requests","title":"HTTP Requests","text":""},{"location":"Tools/IoT/Arduino/02_WiFi_Connectivity/#send","title":"Send","text":"<pre><code>#include &lt;ESP8266WiFi.h&gt;\n#include &lt;ESP8266HTTPClient.h&gt;\n\n// variables\n// we need to use these exact types and names, to override the one in the header file\nchar* ssid = \"WiFi Name\";\nchar* pass = \"WiFi Password\";\n\nHTTPClient client;\nString api;\nint data;\nint status_code;\nString response;\n\nvoid setup(){\n    Serial.begin(9600);\n  pinMode(D2, OUTPUT);\n\n  Serial.println(\"Connecting to WiFi...\");\n  WiFi.begin(ssid, pass);\n\n\n  while(WiFi.status() != WL_CONNECTED) {\n    Serial.print(\".\");\n    delay(1000);\n  }\n\n  Serial.println(\"Connected to \" + String(ssid) + \" successfully\");\n}\n\nvoid loop(){\n  data = 100;\n  api = \"http://.../insert.php?data=\" + String(data);\n\n  client.begin(api);\n\n  status_code = client.GET();\n  response = client.getString();\n\n  delay(5000);\n}\n</code></pre>"},{"location":"Tools/IoT/Arduino/ArduinoUniqueID_Library/","title":"ArduinoUniqueID","text":"<p>https://github.com/ricaun/ArduinoUniqueID</p>"},{"location":"Tools/NLP/","title":"NLP","text":""},{"location":"Tools/NLP/#references","title":"References","text":"<ul> <li> Topic Modeling and Text Classification with Python for Digital Humanities (DH) | Python Tutorials for Digital Humanities</li> </ul>"},{"location":"Tools/NLP/Spacy/","title":"Spacy","text":""},{"location":"Tools/NLP/Spacy/#references","title":"References","text":"<ul> <li>Advanced NLP with spaCy | Explosion</li> <li>Natural Language Processing with spaCy &amp; Python | freeCodeCamp.org</li> <li>SpaCy for Digital Humanities with Python Tutorials</li> </ul>"},{"location":"Tools/PowerPlatform/","title":"PowerPlatform","text":"<p>Useful low-code tools that come bundled with default O365 license</p>"},{"location":"Tools/PowerPlatform/PowerApps/","title":"PowerApps","text":"<p>Low-Code software that allows \u201ccitizen developers\u201d to create small/medium-scale solutions</p> <p>PowerFX is the prog. Lang, and it is very similar to javascript</p>"},{"location":"Tools/PowerPlatform/PowerApps/01_Basics/","title":"Basics","text":""},{"location":"Tools/PowerPlatform/PowerApps/01_Basics/#variables","title":"Variables","text":"Global Entire app <code>Set(variable, true);</code> Context Local variables to a screen <code>UpdateContext({variable: true});</code>"},{"location":"Tools/PowerPlatform/PowerApps/01_Basics/#basic-functions","title":"Basic Functions","text":"<pre><code>Navigate('Screen Name');\nConcurrent(\n    action1,\n    action2\n);\nSelect('Button Name'); // always should be at end of code block\n</code></pre>"},{"location":"Tools/PowerPlatform/PowerApps/01_Basics/#timer","title":"Timer","text":"<p>It does not work on code view</p>"},{"location":"Tools/PowerPlatform/PowerApps/01_Basics/#gallery","title":"Gallery","text":"<pre><code>Select(\n  Tab,\n  current_tab_selected,\n  Tab_Button\n);\n</code></pre>"},{"location":"Tools/PowerPlatform/PowerApps/01_Getting_Data/","title":"01 Getting Data","text":""},{"location":"Tools/PowerPlatform/PowerApps/01_Getting_Data/#get-data","title":"Get Data","text":""},{"location":"Tools/PowerPlatform/PowerApps/01_Getting_Data/#single-row","title":"Single Row","text":"<pre><code>Lookup(\n  List,\n  Cond1 &amp;&amp; Cond2\n)\n</code></pre>"},{"location":"Tools/PowerPlatform/PowerApps/01_Getting_Data/#multiple-rows","title":"Multiple Rows","text":"<pre><code>Filter(\n  List,\n  Cond1,\n  Cond2\n)\n</code></pre>"},{"location":"Tools/PowerPlatform/PowerApps/01_Getting_Data/#delegability","title":"Delegability","text":"<p>Only concerns online data sources, not collections</p> <ul> <li>Delegable</li> <li><code>Filter()</code></li> <li><code>`Lookup()</code></li> <li>Non-Delegable</li> <li><code>AddColumns</code>, <code>ShowColumns</code>, <code>DropColumns</code>, <code>RemoveColumns</code></li> </ul> <p>Magical blue squigglies only come out on operators, not on functions. So you would not see that showing a warning.</p>"},{"location":"Tools/PowerPlatform/PowerApps/01_Getting_Data/#in-vs-lookup","title":"<code>In vs LookUp</code>","text":"<pre><code>my_list = [1, 2, 3, 4, 5, ..., 1000]\nkey = 2\n\n# in\nfor i in range(len(my_list)):\n  if my_list[i] == key:\n    print(\"found\")\n\n# lookup\nfor i in range(len(my_list)):\n  if my_list[i] == key:\n    print(\"found\")\n    break\n</code></pre>"},{"location":"Tools/PowerPlatform/PowerApps/01_Getting_Data/#idk","title":"IDK","text":"<p>If you are testing delegability, set the delegation limit as 1, and check if everything works</p>"},{"location":"Tools/PowerPlatform/PowerApps/02_Displaying_Data/","title":"02 Displaying Data","text":""},{"location":"Tools/PowerPlatform/PowerApps/02_Displaying_Data/#gallery","title":"Gallery","text":"<p>Use gallery whenever possible instead of individual buttons/boxes</p> Disadvantage Fixed gallery Flexible gallery Loading issues in aspect ratio of app is unlocked <p>Dynamic height for menus container</p> <pre><code>Gallery_Item.Height * Gallery.AllItemsCount\n</code></pre>"},{"location":"Tools/PowerPlatform/PowerApps/02_Displaying_Data/#pivoted-gallery","title":"Pivoted Gallery","text":"<p>If you have a Sharepoint site that is in long-format, but you want a grid view of it.</p> <p>Nested gallery</p>"},{"location":"Tools/PowerPlatform/PowerApps/02_Forms/","title":"Forms","text":"<ul> <li>Gallery   Onselect: Set(SelectedItem, ThisItem)</li> <li>Form   Datasource: Live Source</li> <li>Item: SelectedItem</li> </ul>"},{"location":"Tools/PowerPlatform/PowerApps/02_Forms/#displaymode","title":"DisplayMode","text":"New Edit View"},{"location":"Tools/PowerPlatform/PowerApps/02_Forms/#idk","title":"IDK","text":"<p>For date inputs, Set <code>IsEditable</code> to <code>false</code></p> <p><code>Onsuccess</code></p> <pre><code>Notify(\"Success\");\nBack();\nResetForm(Form_Name);\n</code></pre> <p><code>Onfailure</code></p> <pre><code>Notify(\"Failure\", Form_Name.Error, Form_Name.ErrorKind)\n</code></pre>"},{"location":"Tools/PowerPlatform/PowerApps/02_Forms/#dropdown-for-text-column","title":"Dropdown for text column","text":"<pre><code>// Default_selected_items \n[ LookUp(StorageLocationbyRegion, StorageLocation = Parent.Default) ]\n</code></pre>"},{"location":"Tools/PowerPlatform/PowerApps/02_Forms/#copy-form","title":"Copy Form","text":"<pre><code>If(\n  use_previous_data,\n  First(DropColumns(Table(Create_Item.LastSubmit), \"ID\")),\n  Blank()\n)\n</code></pre>"},{"location":"Tools/PowerPlatform/PowerApps/03_Patching/","title":"Patching","text":""},{"location":"Tools/PowerPlatform/PowerApps/03_Patching/#patch","title":"<code>Patch</code>","text":"<pre><code>Patch(\n    List_Name,\n    {\n        Col1: \"foo\"\n    }\n)\n</code></pre>"},{"location":"Tools/PowerPlatform/PowerApps/03_Patching/#error-handling","title":"Error Handling","text":"<pre><code>Set(\n    errors,\n    Errors(List_Name)\n);\nIf(\n  IsEmpty(errors),\n  Notify(\"Success\", Notification.Success),\n  Notify(First(error).Message, Notification.Error)\n);\n</code></pre>"},{"location":"Tools/PowerPlatform/PowerApps/03_Patching/#advanced","title":"Advanced","text":"<p>For creating record, use</p> <pre><code>Patch(\n  Data_Source,\n  Table({\n    Title:\"Num1\",\n      number:1\n  })\n)\n</code></pre> <p>instead of</p> <pre><code>Patch(\n    dummyData,\n  Defaults(dummyData),\n  {Title:\"Num1\",number:1}\n)\n</code></pre> <p>For updating record, use</p> <pre><code>Patch(dummyData,{ID:1},{Title:\"Num1\",number:1})\n</code></pre> <p>instead of</p> <pre><code>Patch(dummyData,\n  LookUp(dummyData,ID=1),\n  {Title:\"Num1\",number:1}\n)\n</code></pre> <p>Batch patching</p> <pre><code>Patch(\n  Data_Source_Name,\n  ShowColumns(Collection_Name, \"ID\", \"FullName\", \"Status\")\n)\n\n// idk\nForAll(\n    Add_Users_Input.SelectedItems As user_to_add,\n    Collect(\n      users_to_add,\n      Table({\n        Name: user_to_add,\n        Country_Code: LookUp(Choices([@Users].Country_Code), Value=country_code),\n        Superuser: false\n      })\n    )\n);\nIfError(\n    Patch(\n        Users,\n        users_to_add\n    );\n    Notify(\n        \"Successfully added users\",\n        NotificationType.Success\n    );\n    Reset(Add_Users_Input);\n    ,\n    Notify(\n        \"Addition of users failed\",\n        NotificationType.Error\n    );\n\n);\nClear(users_to_add);\n</code></pre> <p>instead of</p> <pre><code>ForAll(\n  colUpdateEmployees,\n  Patch(\n      Employees,\n      LookUp(Employees, ID=colUpdateEmployees[@ID]),\n      {\n        FullName: colUpdateEmployees[@FullName],\n        Active: colUpdateEmployees[@Active]\n      }\n    )\n)\n</code></pre>"},{"location":"Tools/PowerPlatform/PowerApps/03_Patching/#people-column","title":"People Column","text":"<pre><code>Patch(\n  data_source,\n  {\n      Person_Column: {\n        '@odata.type': \"#Microsoft.Azure.Connectors.SharePoint.SPListExpandedUser\",\n        Department: \"\",\n        Claims: \"i:0#.f|membership|\" &amp; User().Email,\n        DisplayName: Office365Users.UserProfileV2(User().Email).displayName,\n        Email: User().Email,\n        JobTitle: \"\",\n        Picture: \"\"\n      }\n  }\n)\n</code></pre>"},{"location":"Tools/PowerPlatform/PowerApps/04_Performance/","title":"Performance","text":""},{"location":"Tools/PowerPlatform/PowerApps/04_Performance/#loading-data","title":"Loading Data","text":"<p>Try to do as much as possible on the server-side. \u2018Lazily load\u2019 only what is required in the current tab onto local collection for repeated operations, such as fuzzy search.</p> <pre><code>sequenceDiagram\nautonumber\nactor u as User\nparticipant a as App\nparticipant c as Current_View_Data_Collection\nparticipant il as List\n\nu -&gt;&gt; a: Open App\na -&gt;&gt; u: View Login page with countries\nu -&gt;&gt; a: Login into a single country\n\nu -&gt;&gt; a: Select View (Tab - eg: Active, Completed)\n\na -&gt;&gt; il: Request data of current 1) region 2) view\nil -&gt;&gt; c: Download\nc -&gt;&gt; a: Status filter options\nc -&gt;&gt; a: Return Current View Data\na -&gt;&gt; u: View\n\nu -&gt;&gt; a: Select Status / Search Name, ID, Code\na -&gt;&gt; c: Filter\nc -&gt;&gt; a: Return filtered Current View Data\na -&gt;&gt; u: View</code></pre> <pre><code>          flowchart LR\n          onvisible --&gt; refresh_button --&gt; t[\"TabGallery&gt;Button&gt;OnSelect\"]</code></pre> <p><code>TabGallery&gt;Button&gt;OnSelect</code></p> <pre><code>Set(\n    data_loading,\n    true\n); // this is for toggling any loading animations you have\nSet(\n    new_tab_selected,\n    ThisItem.ID\n);\nIf(\n    new_tab_selected = varTabSelected,\n    Blank(),\n    Reset(Status_Dropdown)\n);\nSet(\n    varTabSelected,\n    new_tab_selected\n);\n// repetition of filter() is required to overcome delegation issue\nClearCollect( // non-delegable\n    current_view_data,\n    Switch( // non-delegable\n      varTabSelected,\n      1,\n      Filter(\n        my_list,\n        Region.Value = country,\n        Status.Value = \"Active\"\n      ),\n      2,\n      Filter(\n        current_region_data,\n        Region.Value = country,\n        Status.Value = \"Cancelled\"\n      )\n    )\n);\nSet(\n    data_loading,\n    false\n);\n</code></pre> <p><code>Refresh Button</code></p> <pre><code>Select(\n    Tab,\n    varTabSelected,\n    Tab_Button\n);\n</code></pre>"},{"location":"Tools/PowerPlatform/PowerApps/04_Performance/#avoid-unnecessarily-data-requests","title":"Avoid unnecessarily data requests","text":"<pre><code>// Onvisible of page\nIf(\n  !loadapp,\n  ClearCollect(collection,Filter(datasource,condition));\n  UpdateContext({loadapp: true})\n)\n</code></pre> <ul> <li>Never use <code>Refresh()</code>. Data sources are already refreshed on start</li> <li>Forms &amp; patch() connected to a data source (not collection) will automatically refresh that same data source (not collection)</li> <li>Even a designated refresh button does not require this. Just use <code>ClearCollect()</code></li> </ul>"},{"location":"Tools/PowerPlatform/PowerApps/04_Performance/#concurrent-execution","title":"Concurrent Execution","text":"<pre><code>Concurrent(\n  function_1(),\n    function_2(),\n    ...,\n    function_n()\n)\n\n// function can be Set(), clear(), collect(), ...\n</code></pre>"},{"location":"Tools/PowerPlatform/PowerApps/04_Performance/#delegable-in","title":"Delegable <code>in</code>","text":"<pre><code>Ungroup(\n  ForAll(\n    Selectbox.SelectedItems, // list of column values\n    {\n      ItemsGroupedByFilterValue: Filter(\n      ListWithDelegationIssues,\n      County.Value = CurrentFilterValue\n      )\n    }\n  ),\n  \"ItemsGroupedByFilterValue\"\n)\n</code></pre>"},{"location":"Tools/PowerPlatform/PowerApps/04_Performance/#optimizing-gallery","title":"Optimizing Gallery","text":"<ul> <li>Set <code>DelayItemLoading</code> to <code>true</code></li> <li>Set <code>LoadingSpinner</code> to <code>LoadingSpinner.Data</code> or <code>LoadingSpinner.Controls</code></li> <li> <p>Eliminate multi-screen dependency</p> </li> <li> <p>This will prevent Powerapps from load everything from other screen</p> </li> <li> <p>For example: Use global variable instead of <code>gallery.selected</code> for form</p> <p>row selection: <code>Onselect</code> will be <code>Set(current_item, ThisItem)</code></p> </li> </ul>"},{"location":"Tools/PowerPlatform/PowerApps/04_Performance/#settings","title":"Settings","text":"<p>Enable <code>DelayedLoad</code></p> <p>Speed up your app's start time by setting on-demand screen expression calls.</p> <p>Enable <code>Keep recently visited screens in memory</code></p> <p>Enable <code>Enhanced performance for hidden controls</code></p> <p>Hidden controls will not be created until they become visible</p>"},{"location":"Tools/PowerPlatform/PowerApps/04_Performance/#misc","title":"Misc","text":"<ul> <li>Use components wherever possible</li> <li>Use <code>Lookup()</code> instead of <code>First(Filter())</code></li> <li><code>Lookup()</code> is delegable, <code>First(Filter())</code> is non-delegable, <code>First(Filter())</code>\u00a0will load all the data into memory then perform the <code>Filter()</code>, then perform the <code>First()</code></li> <li>Lookup will stop at first match, <code>First(Filter())</code> will search the entire data even after first match and then return the first</li> <li>Change order of filters to further optimize logic</li> <li>Use Named Formulas</li> <li>Go to <code>App</code> property</li> <li>Formula</li> <li>Use Formula checker -&gt; Performance Warnings - Unused Variables troubleshooting and some unused controls in the app</li> <li>Disable <code>onstart</code></li> <li>Move things from <code>onstart</code> to <code>onvisible</code> to improve initial load</li> <li><code>Delay Output</code> of text control</li> <li>Progress Message</li> </ul>"},{"location":"Tools/PowerPlatform/PowerApps/05_Misc/","title":"Miscellaneous","text":"<p>Need help with classifying these concepts</p>"},{"location":"Tools/PowerPlatform/PowerApps/05_Misc/#onvisible","title":"Onvisible","text":"<pre><code>Set(downtime_status, false);\nIf(\n    downtime_status,\n    Notify(\"Due to downtime associated with global deployment, please use the app after an hour.\", NotificationType.Error),\n    false\n);\n\nConcurrent(\n    Set(\n        primary_font,\n        \"Segoe UI\"\n    ),\n    Set(\n        grey_font_color,\n        RGBA(\n            0,\n            0,\n            0,\n            0.6\n        )\n    )\n);\n\nSet(\n    user_details,\n    Office365Users.MyProfile()\n);\n</code></pre>"},{"location":"Tools/PowerPlatform/PowerApps/05_Misc/#filtering","title":"Filtering","text":""},{"location":"Tools/PowerPlatform/PowerApps/05_Misc/#conditional-filtering","title":"Conditional Filtering","text":"<pre><code>Filter(\n    collection,\n  If(\n    Field_1_Filter_Case_1,\n    Field_2_Filter_Case_2\n    )\n)\n// instead of \nIf(\n  Filter_1(),\n  Filter_2()\n)\n</code></pre>"},{"location":"Tools/PowerPlatform/PowerApps/05_Misc/#fuzzy-search","title":"Fuzzy Search","text":"<p>Fuzzy search only works for collections! Use the performance tips to optimize the loading of data from datasource into collections.</p> <pre><code>// &lt;- or operation of search strings\ntrue in ForAll()\n\n// &lt;- and operation of search strings\nNot(false in ForAll())\n</code></pre> <pre><code>Filter(\n  collection_name,\n  If(\n    Not(false in ForAll( // &lt;- and: match all substrings\n      Split(\n        Trim(Substitute(Search_Input.Text, \" \", \"\")), // Trim(Search_Input.Text),\n        \" \"\n      ) As substring,\n      true in ForAll( // &lt;- or: match any column\n        [\n          Col1,\n            Col2,\n            Col3\n        ] As column, // columns to search\n        substring.Value in Substitute(column, \" \", \"\")\n            // Use substring.Value.Value for choice columns\n    )\n    )),\n    true,\n    false\n  )\n)\n</code></pre>"},{"location":"Tools/PowerPlatform/PowerApps/05_Misc/#alerts","title":"Alerts","text":"<pre><code>Notify( \"Wrong\", NotificationType.Warning, 4000 )\n// Message, Type, Timeout\n</code></pre> NotificationType Purpose NotificationType.Error Displays the message as an error. NotificationType.Information (Default) Displays the message as informational. NotificationType.Success Displays the message as success. NotificationType.Warning Displays the message as a warning. <p>Set <code>App.ConfirmExit</code> to <code>true</code></p>"},{"location":"Tools/PowerPlatform/PowerApps/05_Misc/#alternating-colors","title":"Alternating Colors","text":"<p>https://devoworx.net/alternate-row-color-in-gallery-powerapps/#alternate-row-color-in-gallery </p> <pre><code>With(\n{\n    Items:List_or_List_Name\n},\nForAll(\n    Sequence(CountRows(Items)),\n    Patch(\n        Last(\n            FirstN(Items,Value)),\n            {rowNumber: Value}\n        )\n    )\n)\n</code></pre> <pre><code>If(\n  Mod(ThisItem.rowNumber,2) = 0,\n  RGBA(240, 240, 240, 1),RGBA(255, 255, 255, 1)\n)\n</code></pre>"},{"location":"Tools/PowerPlatform/PowerApps/05_Misc/#embed-powerbi","title":"Embed PowerBI","text":"<ul> <li> <p>PowerBI &gt; <code>Embed</code> &gt; <code>Website/Portal</code></p> </li> <li> <p>PowerApps &gt; <code>Insert</code> &gt; <code>Charts</code> &gt; <code>PowerBI Tile</code></p> </li> <li>Select it</li> <li><code>TileUrl</code> &gt; Paste embed link</li> </ul>"},{"location":"Tools/PowerPlatform/PowerApps/05_Misc/#idk","title":"IDK","text":"<ul> <li> <p>Use <code>Select()</code> for re-using code by calling a button</p> </li> <li> <p>Mainly useful for onvisible to call the refresh button</p> </li> <li> <p>Default for multi-select columns</p> <ul> <li><code>{Value: ThisItem.Purpose}</code></li> </ul> </li> </ul>"},{"location":"Tools/PowerPlatform/PowerApps/05_Misc/#home-page-onvisible","title":"Home page onvisible","text":"<pre><code>Set(downtime_status, false);\nIf(\n    downtime_status,\n    Notify(\"Due to downtime associated with global deployment, please use the app after an hour.\", NotificationType.Error),\n    false\n);\n\nConcurrent(\n    Set(\n      user_details,\n      Office365Users.MyProfile()\n  ),\n    Set(\n        primary_font,\n        Font.'Segoe UI' // \"Custom Font\"\n    ),\n    Set(\n        grey_font_color,\n      RGBA(0,0,0,0.6)\n    ),\n    Set(\n        app_name,\n        \"Thahir App\"\n    )\n);\n</code></pre> <p>Onvisible only once</p> <pre><code>If(\n  !loadapp,\n  ClearCollect(\n    collection,\n    Filter(datasource,condition)\n  );\n  UpdateContext({loadapp: true}),\n  Blank()\n)\n</code></pre>"},{"location":"Tools/PowerPlatform/PowerApps/05_Misc/#app-name","title":"App Name","text":"<pre><code>app_name &amp; \" | \" &amp; App.ActiveScreen.Name\n// set app_name globally\n</code></pre>"},{"location":"Tools/PowerPlatform/PowerApps/05_Misc/#get-first-names","title":"Get first names","text":"<p>Multi-person column</p> <pre><code>// Ahmed\n// Ahamed\n// Mohammed\n\nForAll(\n      users_all As user,\n      First(\n          Split(\n              Last(\n                  Split(\n                      user.Name.DisplayName,\n                      \", \"\n                  )\n              ).Value,\n              \" \"\n          )\n      ).Value\n  )\n</code></pre> <p>From a List to a string</p> <pre><code>// Ahmed; Ahamed; Mohammed\n\nConcat(\n    ForAll(\n        users_all As user,\n        First(\n            Split(\n                Last(\n                    Split(\n                        user.Name.DisplayName,\n                        \"; \"\n                    )\n                ).Value,\n                \" \"\n            )\n        ).Value\n    ),\n    Value,\n    Char(10)\n)\n</code></pre> <p>From a table to a string for a cell in a Gallery</p> <pre><code>// Ahmed; Ahamed; Mohammed\nConcat(\n    ForAll(\n        ThisItem.Owner As ItemOwner,\n        First(\n            Split(\n                Last(\n                    Split(\n                        ItemOwner.DisplayName,\n                        \"; \"\n                    )\n                ).Value,\n                \" \"\n            )\n        ).Value\n    ),\n    Value,\n    \", \"\n)\n</code></pre> <p>default selected items for a dropdown</p> <pre><code>// Ahmed; Ahamed; Mohammed\nFilter(\n    AddColumns(\n        ForAll(\n            users_all,\n            Name\n        ),\n        \"FirstName\",\n        First(\n            Split(\n                Last(\n                    Split(\n                        DisplayName,\n                        \"; \"\n                    )\n                ).Value,\n                \" \"\n            )\n        ).Value\n    ),\n    Email in ForAll(\n        ThisItem.Owner, // don't use Parent.Default, as it gives weird results\n        Email\n    )\n)\n</code></pre>"},{"location":"Tools/PowerPlatform/PowerApps/05_Misc/#distinct-values-from-choice-column","title":"Distinct Values from Choice Column","text":"<p>Single-Select</p> <pre><code>Distinct(\n    ForAll(\n        List,\n        Column\n    ),\n    Value\n)\n</code></pre> <p>Multi-Select</p> <pre><code>Distinct(\n    Ungroup(\n        ForAll(\n            ForAll(\n                List,\n                Column\n            ),\n            Value\n        ),\n        \"Value\"\n    ),\n    Value\n)\n</code></pre>"},{"location":"Tools/PowerPlatform/PowerApps/05_Misc/#union-of-2-tables","title":"Union of 2 tables","text":"<pre><code>Ungroup(\n  Table(\n    {MyTables: TableA},\n    {MyTables: TableB}\n  ),\n  \"MyTables\"\n)\n</code></pre> <pre><code>Ungroup(\n    ForAll(\n        Conditions_Time_Points_Gallery.AllItems,\n        {\n            MyTables: ForAll(\n                Samples_Count_Gallery.AllItems,\n                Samples_Count_Input.Text\n            )\n        }\n    ),\n    \"MyTables\"\n)\n</code></pre>"},{"location":"Tools/PowerPlatform/PowerApps/05_Misc/#replicating-database-style-input","title":"Replicating database-style input","text":""},{"location":"Tools/PowerPlatform/PowerApps/05_Misc/#input-boxes","title":"Input boxes","text":"<p>Onselect</p> <pre><code>Select(Update Record Button);\n</code></pre>"},{"location":"Tools/PowerPlatform/PowerApps/05_Misc/#update-record-button","title":"Update Record Button","text":"<pre><code>If(\n    LookUp(\n        collection_modified,\n        ID = ThisItem.ID,\n        true\n    ),\n    Blank(),\n    Collect(\n        collection_modified,\n        ThisItem\n    )\n);\nUpdateIf(\n    collection_modified,\n    ID = ThisItem.ID,\n    {\n        Comments: Comments_Input.Value\n    }\n);\n</code></pre>"},{"location":"Tools/PowerPlatform/PowerApps/05_Misc/#submit-button","title":"Submit Button","text":"<pre><code>Patch(\n    Success_Criteria,\n    ShowColumns(\n      collection_modified,\n      \"ID\",\n      \"Col1\",\n      \"Col2\"\n    )\n);\n</code></pre>"},{"location":"Tools/PowerPlatform/PowerApps/05_Misc/#deep-linking","title":"Deep Linking","text":""},{"location":"Tools/PowerPlatform/PowerApps/05_Misc/#apponvisible","title":"<code>App.Onvisible</code>","text":"<pre><code>Set(\n    param_item_id,\n    Param(\"item_id\")\n);\nIf(\n  !IsBlank(param_item_id),\n    Set(\n    selected_stability_code,\n    param_item_id\n  );\n    Select(go_to_manage_item);\n)\n</code></pre>"},{"location":"Tools/PowerPlatform/PowerApps/05_Misc/#go_to_manage_item-button","title":"<code>go_to_manage_item</code> button","text":"<p>Create a button to do the navigation, because PowerApp does not allow <code>Navigate()</code> in onvisible</p> <pre><code>Navigate('Screen_to_Manage_Item');\n</code></pre>"},{"location":"Tools/PowerPlatform/PowerApps/05_Misc/#generate-link","title":"Generate Link","text":"<pre><code>https://&lt;applink&gt;?param1=value1&amp;param2=value2&amp;param3=value3\n</code></pre> <p>For example,</p> <pre><code>https://&lt;applink&gt;?item_id=100\n</code></pre>"},{"location":"Tools/PowerPlatform/PowerAutomate/","title":"PowerAutomate","text":"<p>Automation tool</p> <ul> <li>Triggers: What starts the flow</li> <li>Automated: Database, Email</li> <li>Instant: Manually start, PowerApps</li> <li>Scheduled: Time-based</li> <li>Desktop</li> <li>Connectors: Actions</li> <li>Types<ul> <li>Standard</li> <li>Premium </li> </ul> </li> <li>Examples<ul> <li>Send HTTP API requests</li> <li>CRUD items</li> <li>SharePoint</li> <li>SQL</li> </ul> </li> </ul>"},{"location":"Tools/PowerPlatform/PowerAutomate/#flow-failures","title":"Flow Failures","text":"<p>If a flow keeps failing, Microsoft will automatically suspend it.</p>"},{"location":"Tools/PowerPlatform/PowerAutomate/01/","title":"01","text":""},{"location":"Tools/PowerPlatform/PowerAutomate/01/#get-items","title":"Get items","text":"<p>By default, <code>Get items</code> only returns the first 100 items. (don't believe the message it shows that <code>default = all</code>)</p> <p>Set <code>Top Count</code> to 5000.</p>"},{"location":"Tools/PowerPlatform/PowerAutomate/01/#odata-filter","title":"Odata Filter","text":"<pre><code>numeric_column eq number\nstring_column eq 'string'\n\nlookup_column/Id eq number\nlookup_column/subfield eq 'string'\n</code></pre> eq lt gt"},{"location":"Tools/PowerPlatform/PowerAutomate/01/#to-get-more-than-5000-items","title":"To get more than 5000 items","text":"<ol> <li>Create a variable <code>lower_limit_id=0</code></li> <li>Get items with ID &gt; lower_limit &amp;&amp; ID &lt; lower_limit + 5000</li> <li><code>lower_limit=lower_limit + 5000</code></li> <li>Perform step 2 until the <code>lower_limit_id &gt; max(id of items you just got)</code></li> <li>Finally, do <code>union</code></li> </ol> <p>ID = Auto-Generated column</p>"},{"location":"Tools/PowerPlatform/PowerAutomate/01/#concurrency-control","title":"Concurrency Control","text":"<p>Better to limit database related operations to 1</p> <p></p>"},{"location":"Tools/PowerPlatform/PowerAutomate/01/#copy-file","title":"Copy file","text":""},{"location":"Tools/PowerPlatform/PowerAutomate/01/#union","title":"Union","text":"<pre><code>union(\n  collection_1: object|array,\n  collection_2: object|array\n)\n</code></pre>"},{"location":"Tools/PowerPlatform/PowerBi/","title":"PowerBi","text":"<p>Visualization software used for Business Intelligence </p>"},{"location":"Tools/PowerPlatform/PowerBi/#advantages","title":"Advantages","text":"<ul> <li>Easy to maintain</li> <li>Drag &amp; drop and Coding</li> <li>Embed everything in a single shareable \u201c.pbix\u201d file</li> <li>Does not support live queries very well. Need to use very high frequency refresh schedule. You can even use PowerAutomate to refresh</li> </ul>"},{"location":"Tools/PowerPlatform/PowerBi/#disadvantages","title":"Disadvantages","text":"<ul> <li>Not as flexible as hand-coding a dashboard in Python or so</li> <li>Might get too big for very large datasets</li> </ul>"},{"location":"Tools/PowerPlatform/PowerBi/#coding-languages","title":"Coding languages","text":"<ul> <li>PowerQuery</li> <li>DAX (Data Analysis Expressions)</li> </ul>"},{"location":"Tools/PowerPlatform/PowerBi/#idk","title":"IDK","text":"<p>PowerBi Desktop \\(\\ne\\) PowerBi Cloud</p> <p>PowerBi cloud every uploaded dashboard gets split into Report &amp; Semantic Model</p>"},{"location":"Tools/PowerPlatform/PowerBi/01/","title":"01","text":""},{"location":"Tools/PowerPlatform/PowerBi/01/#sharepoint-import-optimization","title":"SharePoint Import Optimization","text":""},{"location":"Tools/PowerPlatform/PowerBi/01/#use-version-20","title":"Use Version 2.0","text":"<ul> <li>`Default``</li> <li><code>`All</code> will request a lot of unnecessarily data</li> </ul> <p>If Version 2.0 does not work for some reason such as</p> <p>Sharepoint online list 1.0 connector is the only option for Power BI refresh data over 5K items  </p>"},{"location":"Tools/PowerPlatform/PowerBi/01/#custom-request","title":"Custom Request","text":""},{"location":"Tools/PowerPlatform/PowerBi/01/#option-1","title":"Option 1","text":"<pre><code>let\n  tenantname = \"&lt;domain&gt;\",  // eg: \"domain.sharepoint.com\"\n  sitename = \"&lt;site&gt;\",      // eg: \"SiteName\"; if a subsite use \"Site/SubSite\"\n  listname = \"&lt;list&gt;\",      // eg: \"ListName\"\n\n  siteurl = \"https://\" &amp; tenantname &amp; \"/sites/\" &amp; sitename,  // use ... /sites/&lt;your site name&gt;/&lt;your subsite name&gt; if applicable                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n  itemcount = Json.Document(Web.Contents(siteurl, [RelativePath = \"_api/web/lists/GetByTitle('\" &amp; listname &amp; \"')/items?$select=ID&amp;$orderby=ID%20desc&amp;$top=1\", Headers = [Accept = \"application/json\"]]))[value]{0}[ID],\n  StartIDs = List.Numbers(0, Number.RoundUp(itemcount / 5000), 5000),\n  ConvertToTable = Table.FromList(StartIDs, Splitter.SplitByNothing(), null, null, ExtraValues.Error),\n  Add_EndIDs = Table.AddColumn(ConvertToTable, \"Addition\", each [Column1] + 4999, type number),\n  RenamedColumns = Table.RenameColumns(Add_EndIDs, {{\"Column1\", \"StartID\"}, {\"Addition\", \"EndID\"}}),\n  #\"Changed Type\" = Table.TransformColumnTypes(RenamedColumns, {{\"StartID\", type text}, {\"EndID\", type text}}),\n\n  //Comment in only one of the fieldselect lines below, defining your select and expand columns using the example syntax shown                                                                                                                            \n  // fieldselect = \"&amp;$top=5000\",  // all fields with no expansion                                                                                                                                                                                                                         \n  // fieldselect = \"&amp;$top=5000&amp;$select=ID,Title,Date,PersonColumn,ChoiceColumn,LookupColumn\", // list desired fields (no expansion) -No Spaces!                                                                                                                                            \n  fieldselect = \"&amp;$top=5000&amp;$select=ID,Title,Date,PersonColumn/LastName,PersonColumn/FirstName,ChoiceColumn,LookupColumn/Title,LookupColumn/Project,LookupColumn/ProjectStatus&amp;$expand=PersonColumn,LookupColumn\", //expand list fields - No Spaces!                                                                                                                                                                                                                                                    \n\n  GetData = Table.AddColumn(#\"Changed Type\", \"Items\", each Json.Document(Web.Contents(siteurl, [RelativePath = \"_api/web/lists/GetByTitle('\" &amp; listname &amp; \"')/items?$filter=(ID ge \" &amp; [StartID] &amp; \") and (ID le \" &amp; [EndID] &amp; \")\" &amp; fieldselect, Headers = [Accept = \"application/json\"]]))[value]),\n  #\"Removed Other Columns\" = Table.SelectColumns(GetData, {\"Items\"}),\n  #\"Expanded Items\" = Table.ExpandListColumn(#\"Removed Other Columns\", \"Items\")\nin\n  #\"Expanded Items\"\n</code></pre>"},{"location":"Tools/PowerPlatform/PowerBi/01/#option-2-elegant-but-sequential-requests-slower","title":"Option 2: Elegant, but sequential requests (slower)","text":"<pre><code>let\n  tenantname = \"&lt;domain&gt;\",  // eg: \"domain.sharepoint.com\"\n  sitename = \"&lt;site&gt;\",      // eg: \"SiteName\"; if a subsite use \"Site/SubSite\"\n  listname = \"&lt;list&gt;\",      // eg: \"ListName\"\n\n  siteurl = \"https://\" &amp; tenantname &amp; \"/sites/\" &amp; sitename,  // use ... /sites/&lt;your site name&gt;/&lt;your subsite name&gt; if applicable                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n\n  //Comment in only one of the fieldselect lines below, defining your select and expand columns using the example syntax shown                                                                                                                            \n  // fieldselect = \"&amp;$top=5000\", // all fields with no expansion                                                              \n  // fieldselect = \"&amp;$top=5000&amp;$select=ID,Title,Date,PersonColumn,ChoiceColumn,LookupColumn\", // list desired fields (no expansion) -No Spaces!                                                                                                                                            \n  fieldselect = \"&amp;$top=5000&amp;$select=ID,Title,Date,PersonColumn/LastName,PersonColumn/FirstName,ChoiceColumn,LookupColumn/Title,LookupColumn/Project,LookupColumn/ProjectStatus&amp;$expand=PersonColumn,LookupColumn\",  //expand list fields - No Spaces!                                                                                                   \n\n  InitialWebCall = Json.Document(Web.Contents(siteurl, [RelativePath = \"_api/web/lists/GetByTitle('\" &amp; listname &amp; \"')/items?$skipToken=Paged=TRUE\" &amp; fieldselect, Headers = [Accept = \"application/json\"]])),\n  datalist = List.Generate(() =&gt; InitialWebCall, each List.Count([value]) &gt; 0, each try Json.Document(Web.Contents(siteurl, [RelativePath = \"_api\" &amp; Text.AfterDelimiter([odata.nextLink], \"_api\"), Headers = [Accept = \"application/json\"]])) otherwise [value = {}], each [value]),\n  #\"Converted to Table\" = Table.FromList(datalist, Splitter.SplitByNothing(), null, null, ExtraValues.Error),\n  #\"Expanded Column1\" = Table.ExpandListColumn(#\"Converted to Table\", \"Column1\")\nin\n  #\"Expanded Column1\"\n</code></pre>"},{"location":"Tools/PowerPlatform/PowerBi/01/#generate-range","title":"Generate Range","text":"<ul> <li>Create New column</li> </ul> <p>Type</p> <p>Number Range</p> <pre><code>{\n  1..3\n}\n</code></pre> <p>Date Range</p> <pre><code>{\n  Number.From([Start_Date]) .. Number.From([End_Date])\n}\n</code></pre> <p>Expand new column</p> <p>Change data type as required</p>"},{"location":"Tools/PowerPlatform/SharePoint/","title":"SharePoint","text":"<p>Microsoft service that has both File Storage &amp; Relational Database capabilities.</p> <p>Mix of OneDrive &amp; MySQL</p> <p>Previously it was Microsoft Lists.</p>"},{"location":"Tools/PowerPlatform/SharePoint/#advantages","title":"Advantages","text":"<ol> <li>Free (within default O365 license)</li> <li>Audit Trail (Version history of every record/file)</li> <li>Flexible &amp; Feature-Rich</li> <li>LookUp are very useful</li> <li>Easily to integrate with PowerPlatform ecosystem, and also to Python using SharePlum</li> <li>Export query, which is a read-only view into the backend</li> </ol>"},{"location":"Tools/PowerPlatform/SharePoint/#disadvantages","title":"Disadvantages","text":"<ol> <li>It does not give as feature-rich queries/API compared to other solutions like SQL Server/Dataverse</li> <li>LookUp columns has limitations</li> <li>Setting up permissions is not easy</li> <li>Export query only works on Windows</li> </ol>"},{"location":"Tools/PowerPlatform/SharePoint/#sharepoint-vs-dataverse","title":"SharePoint vs Dataverse","text":"SharePoint Dataverse Cost Advantages Disadvantages Comments"},{"location":"Tools/PowerPlatform/SharePoint/#hub-of-all-sharepoint-sites","title":"Hub of All SharePoint sites","text":"<p><code>https://&lt;subdomain&gt;.sharepoint.com/_layouts/15/sharepoint.aspx?v=activities&amp;spStartSource=spappbar</code></p>"},{"location":"Tools/PowerPlatform/SharePoint/#vocabulary","title":"Vocabulary","text":"Normal Language SharePoint Database Site Table List"},{"location":"Tools/PowerPlatform/SharePoint/#most-significant-read-only-columns","title":"Most Significant Read-Only Columns","text":"<ol> <li>ID</li> <li>Created By</li> <li>Modified By</li> <li>Created</li> <li>Modified</li> </ol>"},{"location":"Tools/PowerPlatform/SharePoint/#creating-new-site","title":"Creating New Site","text":""},{"location":"Tools/PowerPlatform/SharePoint/#steps","title":"Steps","text":""},{"location":"Tools/PowerPlatform/SharePoint/#important-configuration","title":"Important Configuration","text":"<ol> <li>Site Settings</li> <li>View all site sittings</li> <li>Regional settings</li> <li>Make sure that time zone &amp; locale are set correctly</li> </ol>"},{"location":"Tools/PowerPlatform/SharePoint/#creating-new-list","title":"Creating New List","text":"<ol> <li>Home Page of the Site</li> <li>Click <code>New</code></li> <li>Choose existing list or blank template</li> </ol>"},{"location":"Tools/PowerPlatform/SharePoint/#editing-list","title":"Editing List","text":"<p><code>Edit in Grid View</code> is your best friend!!! You can only bulk-edit only &lt;100 items at a time</p> <p>Behaves like a normal spreadsheet (copy, paste, etc)</p>"},{"location":"Tools/PowerPlatform/SharePoint/#schema-validation","title":"Schema Validation","text":"<ol> <li>Enforce unique (unique)</li> <li>Requires information (not null)</li> </ol> <p>If you use both, you are basically enforcing primary key</p>"},{"location":"Tools/PowerPlatform/SharePoint/#column-types","title":"Column Types","text":"Type Comment Advantages Disadvantages Text Max char \\([1, 255]\\) Searchable on SharePointFilterable Max limit Multiple Lines of Text No max limit I think not Searchable on SharePointNot filterable Choice Should be used when you have a pre-defined list of options Very fastLooks nice Not easy to maintain the list; hence, should not be used when options are dynamic DateTime Try to do everything in GMT Does not store date time, it stores in serial date time format Dates &amp; Times are not very well-managed in Microsoft ecosystem Person Number FloatYou can control precision to be 0 to enforce integer Yes/No Boolean LookUp Enforcing FKBecomes very slow/unusable after the list you are looking up from has &gt; 5000 items Calculated <code>Show more column types</code> &gt; CalculatedHelps creating regular database view-style calculated column Formula only gets executed when the row is updated; for eg, <code>Today()</code> cannot be used due to this"},{"location":"Tools/PowerPlatform/SharePoint/#recycle-bin","title":"Recycle Bin","text":"<ul> <li>Level 1</li> <li>Level 2</li> </ul>"},{"location":"Tools/PowerPlatform/SharePoint/#permissions","title":"Permissions","text":""},{"location":"Tools/PowerPlatform/SharePoint/#requirement","title":"Requirement","text":"Role Access Owners Should have full-control Superusers Should have privileged control over the items, but cannot access the backend Users Can only access the app No access No access"},{"location":"Tools/PowerPlatform/SharePoint/#solution","title":"Solution","text":""},{"location":"Tools/PowerPlatform/SharePoint/#maintain-sharepoint-list","title":"Maintain SharePoint List","text":"<pre><code>erDiagram\nUsers {\n    LookUp Region FK\n    Person Email \"NOT NULL\"\n    Choice User_Type \"NOT NULL; [User, SuperUser, Admin]\"\n}</code></pre>"},{"location":"Tools/PowerPlatform/SharePoint/#sharepoint-permissions","title":"SharePoint Permissions","text":"<ol> <li>Go to site permissions</li> <li>Advanced permission settings</li> <li>Create permissions group and give permissions according to the below</li> <li>Create custom permission levels according to the below</li> </ol> Role Sharepoint Permission Level List Permission Level Owners Full-control Full-control Superusers Contribute (Custom) Contribute (Custom) Users Read (Custom) Contribute (Custom) No access No access No access Level Steps Custom Read Read access permission level with only <code>Open  -  Allows users to open a Web site, list, or folder in order to access items inside that container.</code> Custom Contribute Contribute access permission level with 1. <code>Add Items  -  Add items to lists and add documents to document libraries.</code> 2. <code>Edit Items  -  Edit items in lists, edit documents in document libraries, and customize Web Part Pages in document libraries.</code> 3. <code>View Items  -  View items in lists and documents in document libraries.</code> 4. <code>View Pages  -  View pages in a Web site.</code> 5. <code>Open  -  Allows users to open a Web site, list, or folder in order to access items inside that container.</code>"},{"location":"Tools/PowerPlatform/SharePoint/#trick-to-do-this-faster","title":"Trick to do this faster","text":"<ul> <li> <p>First, edit the permission of the new group <code>Users</code> to <code>Custom Contribute</code></p> </li> <li> <p>Then go to each list\u2019s settings and specify do not inherit from parent</p> </li> <li> <p>Come back to site settings, and edit the permission to <code>Read</code></p> </li> <li> <p>Add <code>Everyone but external users</code> to <code>Users</code></p> <ul> <li>Do this last to avoid any accidental access</li> </ul> </li> <li> <p>This way you don't have to edit list settings each time, only the site settings :)</p> </li> </ul>"},{"location":"Tools/Programming_Languages/PHP/","title":"PHP","text":""},{"location":"Tools/Programming_Languages/PHP/#basic-block","title":"Basic Block","text":"<pre><code>&lt;?php\n// code\necho \"Hello world!\";\n?&gt;\n</code></pre>"},{"location":"Tools/Programming_Languages/PHP/#declaration","title":"Declaration","text":"<p>Data types not required</p> <pre><code>$my_var = \"\";\n</code></pre>"},{"location":"Tools/Programming_Languages/PHP/#get-query-parameters","title":"Get query parameters","text":"<pre><code>$query_param = $_GET[\"query_param\"];\n</code></pre>"},{"location":"Tools/Programming_Languages/PHP/#dates","title":"Dates","text":"<pre><code>data_default_timezone_set(\"Asia/Kolkata\");\n\n$date = date(\"Y-m-d\");\n$time = date(\"H:i:s\");\n</code></pre>"},{"location":"Tools/Programming_Languages/Python/","title":"Python","text":"<p>Since it is an interpreted language, it may be slow for very large projects, but should be fine for most projects</p>"},{"location":"Tools/Programming_Languages/Python/01/","title":"Python","text":""},{"location":"Tools/Programming_Languages/Python/01/#import-package","title":"Import Package","text":"<pre><code>import foo\nfrom foo import bar\nfrom foo import *\n\nfrom foo.idk import bar\nfrom foo.idk import *\n</code></pre> <p>All the above take the same duration; no performance difference</p>"},{"location":"Tools/Programming_Languages/Python/01/#garbage-collection","title":"Garbage Collection","text":"<p>Manually collecting will be faster than automatic</p> <pre><code>import gc\n\ng0, g1, g2 = gc.get_threshold() # default: 700, 10, 10\n# gc.set_threshold(10_000, 10, 10)\n\ngc.collect(generation=0)\ngc.set_threshold(0)\ngc.disable()\ngc.freeze()\n\nheavy_code() # like ML, database\n\ngc.unfreeze()\ngc.set_threshold(10_000, 10, 10)\ngc.enable()\ngc.collect(generation=0)\n\n\n# exit\n# don't cleanup on exit\natexit.register(os._exit, 0) # only for Python &lt; 3.6\n</code></pre>"},{"location":"Tools/Programming_Languages/Python/01/#idk","title":"IDK","text":"<p>A collection before POSIX <code>fork()</code> call may free pages for future allocation which can cause copy-on-write too</p> <p>Hence</p> <ol> <li>Parent process</li> <li>disable garbage collector</li> <li>freeze before fork</li> <li>Child process</li> <li>Enable garbage collector</li> </ol>"},{"location":"Tools/Programming_Languages/Python/01/#machine-learning","title":"Machine Learning","text":"<pre><code>gc.set_threshold(0)\ngc.disable()\n\nfor epoch in range(n_epochs):\n  for batch in batch_data_loader:\n    # train\n    # eval\n      gc.collect(0)\n\ngc.collect()\n\n# exit\natexit.register(os._exit, 0) # only for Python &lt; 3.6\n</code></pre>"},{"location":"Tools/Programming_Languages/Python/01/#idk_1","title":"IDK","text":"<ul> <li><code>gc.disable()</code> will sometimes got overridden by another library calling <code>gc.enable()</code></li> </ul>"},{"location":"Tools/Programming_Languages/Python/01/#number-formatting","title":"Number Formatting","text":"<pre><code>number = 333.43\n\n\"{:02d}\".format(1)      ## leading zeroes\n\"{:2f}\".format(number)  ## floating point rounding\n\nf\"{x:z}\" ## rounds negative 0\nf\"{x:z.1f}\"\n</code></pre>"},{"location":"Tools/Programming_Languages/Python/01/#hex-to-rgba","title":"Hex to RGBA","text":"<pre><code>def hex_to_rgba(h, alpha):\n    '''\n    converts color value in hex format to rgba format with alpha transparency\n    '''\n    return \"rgba\" + str(tuple([int(h.lstrip('#')[i:i+2], 16) for i in (0, 2, 4)] + [alpha]))\n</code></pre>"},{"location":"Tools/Programming_Languages/Python/01/#custom-rounding","title":"Custom Rounding","text":"<p><pre><code>def round_f(x, places, percentage=False):\n  if percentage:\n    x *= 100\n\n  string = f\"{x:z.{places}f}\"\n\n  if places &gt; 0:\n    string = string.rstrip('0').rstrip('.')\n\n  if percentage:\n    string += \"%\"\n\n  return string\n</code></pre> <pre><code>def round_s(x, significant_decimals, max_digits=None, percentage=False):\n  if percentage:\n    x *= 100\n\n  if max_digits is None:\n    max_digits = min(significant_decimals * 2, 4)\n\n  decimal_digits = str(x).split(\".\")[1]\n  pos_first_non_zero = len(decimal_digits) - len(decimal_digits.lstrip(\"0\")) \n  pos = pos_first_non_zero + significant_decimals\n\n  return round_f(x, min(pos, max_digits))\n</code></pre></p>"},{"location":"Tools/Programming_Languages/Python/01/#text","title":"Text","text":"<p><pre><code>names = names.split(\"\\n\")\nnames = names.split(\",\")\n\n## remove empty strings from string list\nnames = list(filter(None, names))\n</code></pre> <pre><code>class color:\n PURPLE = '\\033[95m'\n CYAN = '\\033[96m'\n DARKCYAN = '\\033[36m'\n BLUE = '\\033[94m'\n GREEN = '\\033[92m'\n YELLOW = '\\033[93m'\n RED = '\\033[91m'\n BOLD = '\\033[1m'\n UNDERLINE = '\\033[4m'\n END = '\\033[0m'\n\nprint(color.BOLD + 'Hello World !' + color.END)\n</code></pre></p>"},{"location":"Tools/Programming_Languages/Python/01/#idk_2","title":"IDK","text":""},{"location":"Tools/Programming_Languages/Python/01/#name-of-script","title":"Name of script","text":"<p><pre><code>__file__\n</code></pre> Useful for pages in Streamlit</p>"},{"location":"Tools/Programming_Languages/Python/01/#name-of-calling-function","title":"Name of calling function","text":"<p><pre><code>__name__\n</code></pre> Useful for checking if this is a program or a library</p>"},{"location":"Tools/Programming_Languages/Python/01/#input-hidden-textpassword","title":"Input Hidden Text/Password","text":"<pre><code>from getpass import getpass\nsender_password = getpass(\"Password: \")\n</code></pre>"},{"location":"Tools/Programming_Languages/Python/01/#date-time","title":"Date-Time","text":"<p>Refer to Python DateTime Formats <pre><code>dob = '05/02/1985'\ndob = datetime.strptime(dob, '%d/%m/%Y')\n</code></pre></p>"},{"location":"Tools/Programming_Languages/Python/01/#find-home-directory","title":"Find Home Directory","text":"<p>\u2705 This is cross-platform <pre><code>from pathlib import Path\nhome = str(Path.home())\n</code></pre></p>"},{"location":"Tools/Programming_Languages/Python/01/#delete","title":"Delete","text":""},{"location":"Tools/Programming_Languages/Python/01/#move-file-to-recycle-bin","title":"Move file to Recycle Bin","text":"<pre><code>  from send2trash import send2trash\n\n  send2trash(\"test_folder\")\n\n  send2trash(\"test.csv\")\n\n  for file_name in glob.glob(os.path.join(directory, '*.mov')):\n    file = os.path.join(directory, file_name)\n    send2trash(file)\n    print(\"Deleted\", file)\n</code></pre>"},{"location":"Tools/Programming_Languages/Python/01/#permanently-delete-file","title":"\u274c Permanently Delete File","text":"<pre><code>  os.remove(file)\n\n  for file_name in glob.glob(os.path.join(directory, '*.mov')):\n    file = os.path.join(directory, file_name)\n    os.remove(file)\n    print(\"Deleted\", file, \"Permanently\")\n</code></pre>"},{"location":"Tools/Programming_Languages/Python/01/#permanently-delete-folder","title":"\u274c Permanently Delete Folder","text":"<pre><code>  import shutil\n  shutil.rmtree(path)\n</code></pre>"},{"location":"Tools/Programming_Languages/Python/01/#copy","title":"Copy","text":"<pre><code>import shutil\n\nshutil.copy_file(src, dest) ## contents of file\nshutil.copy() ## copy_file() + permission mode\nshutil.copy2() ## copy() + copies metadata\n</code></pre>"},{"location":"Tools/Programming_Languages/Python/01/#list-all-files-in-a-directory","title":"List all files in a directory","text":"<pre><code>import os\n\nfor file_name in os.listdir(\"./data\"):\n  file = os.path.join(directory, file_name)\n  print(file)\n</code></pre>"},{"location":"Tools/Programming_Languages/Python/01/#create-a-folder","title":"Create a folder","text":"<pre><code>newpath = r'C:\\Program Files\\arbitrary' \nif not os.path.exists(newpath):\n    os.makedirs(newpath)\n</code></pre>"},{"location":"Tools/Programming_Languages/Python/01/#get-only-files-of-a-particular-type-using-glob","title":"Get only files of a particular type using <code>glob</code>","text":"<pre><code>for file_name in glob.glob(os.path.join(directory, '*.mp4')):\n  file = os.path.join(directory, file_name)\n    print(file)\n</code></pre>"},{"location":"Tools/Programming_Languages/Python/01/#get-files-of-multiple-types-using-glob","title":"Get files of Multiple Types using <code>glob</code>","text":"<pre><code># better\n\nfrom pathlib import Path\nall_note_paths = (\n    p.resolve() for p in Path(\"./\").glob(\"**/*\") if p.suffix in [\n        \".md\", \".css\", \".js\", \".html\"\n    ]\n)\n</code></pre> <pre><code>def list_files(images_dir):\n  l = []\n\n  for type in [\"jpg\", \"jpeg\", \"png\"]:\n    this_type_files = glob.glob(\n      os.path.join(images_dir, \"**\", f\"*.{type}\"),\n      recursive = True\n    )\n    l += this_type_files\n  return l\n</code></pre>"},{"location":"Tools/Programming_Languages/Python/01/#get-filename-with-extension","title":"Get filename with extension","text":"<pre><code>import os\nfile_name_with_ext = os.path.basename(\"a/b/c\")\n</code></pre>"},{"location":"Tools/Programming_Languages/Python/01/#get-folder-name","title":"Get folder name","text":"<pre><code>filename = \"folder/file.mp4\"\nos.path.dirname(filename)\n\nfilename = \"folder/folder/file.mp4\"\nos.path.basename(os.path.dirname(filename))\n</code></pre>"},{"location":"Tools/Programming_Languages/Python/01/#get-extension-only","title":"Get extension only","text":"<pre><code>import os\next = os.path.splitext(filename)[1]\n</code></pre>"},{"location":"Tools/Programming_Languages/Python/01/#get-filename-only","title":"Get filename only","text":"<pre><code>import os\n\ndef get_filename(file):\n  file_name = os.path.basename(file)\n  file_name_without_ext = os.path.splitext(file_name)[0]\n\n  ## using the above\n  new_file_name = os.path.splitext(file_name)[0] ## + \"_Copy\" + os.path.splitext(file_name)[1]\n\n  return new_file_name\n</code></pre>"},{"location":"Tools/Programming_Languages/Python/01/#files-in-directory-and-sub-directory","title":"Files in directory and sub-directory","text":"<pre><code>from os import walk\nfiles = []\n\n## specific directory\nfiles = []\nfor (dirpath, dirnames, filenames) in walk(\"./data\"):\n    files.extend(filenames)\n    break\n\n## directory and subdirectories\nfor (dirpath, dirnames, filenames) in walk(\".\"):\n    files.extend(filenames)\n\nfiles\n</code></pre>"},{"location":"Tools/Programming_Languages/Python/01/#search-substring-in-string","title":"Search Substring in String","text":"<pre><code>my_string[:-3] == \"pdf\"\nmy_string.find(\"pdf\")\n\"pdf\" in my_string\n\nmy_string.index(\"pdf\")\n</code></pre>"},{"location":"Tools/Programming_Languages/Python/01/#idk_3","title":"IDK","text":"<pre><code>for (i, file) in enumerate(files):\nprint(i, \"is\", file)\n</code></pre>"},{"location":"Tools/Programming_Languages/Python/01/#class","title":"Class","text":"<pre><code>object.__members__\nobject.__methods__\n\ndir(object)\n</code></pre>"},{"location":"Tools/Programming_Languages/Python/01/#inspect","title":"Inspect","text":"<p>Inspect class <pre><code>def func_of_class(class_name):\n  return [\n    func for func in dir(class_name)\n    if callable(getattr(class_name, func))\n    and not func.startswith(\"__\")\n  ]\n\nobj = my_class()\nfor func in func_of_class(type(obj)):\n  getattr(obj, func)(arg)\n</code></pre> Inspect Function Getting arguments of a function <pre><code>  getfullargspec(model_equation).args\n</code></pre> Dynamically get the code of a python function <pre><code>Inspect.getsource\n</code></pre> Dynamically run python code <pre><code>python_code = \"\"\"\nprint(\"hello world\")\n\"\"\"\nexec(python_code)\n</code></pre></p>"},{"location":"Tools/Programming_Languages/Python/01/#create-virtual-environment","title":"Create virtual environment","text":"<p><pre><code>python -m venv \"./venv\"\npython -m venv \"C:blah/blah/venv\"\n</code></pre> Switch to this virtual environment If you get an error when a powershell script runs, run this code in Powershell (admin) <pre><code>  Set-ExecutionPolicy RemoteSigned\n</code></pre></p>"},{"location":"Tools/Programming_Languages/Python/01/#traverse-list-with-index-and-value","title":"Traverse list with index and value","text":"<pre><code>for i, val in enumerate(list):\nprint(i)\nprint(val)\n</code></pre>"},{"location":"Tools/Programming_Languages/Python/01/#cli","title":"CLI","text":""},{"location":"Tools/Programming_Languages/Python/01/#argparse","title":"Argparse","text":"<pre><code>import argparse\n\nTIME_THRESHOLD = 10\nHASH_SIZE = 4\n\nif __name__ == \"__main__\":\n  parser = argparse.ArgumentParser(description = \"Group similar images\")\n\n  parser.add_argument(\"--tt\", type = int, help = f\"Time Threshold (seconds), default = {TIME_THRESHOLD}\")\n  parser.add_argument(\"--hs\", type = int, help = f\"Hash Size, default = {HASH_SIZE}\")\n\n  args = parser.parse_args()\n\n  TIME_THRESHOLD = args.tt\n  HASH_SIZE = args.hs\n</code></pre>"},{"location":"Tools/Programming_Languages/Python/01/#adding-to-path-using-setuptools","title":"Adding to path using setuptools","text":"<p>https://python-packaging.readthedocs.io/en/latest/command-line-scripts.html</p>"},{"location":"Tools/Programming_Languages/Python/01/#import-classesfunctions-from-another-python-file","title":"Import classes/functions from another python file","text":"<pre><code>from my_other_file import my_func, MyClass\n</code></pre>"},{"location":"Tools/Programming_Languages/Python/01/#get-classes-of-modulepython-file","title":"Get classes of module/Python file","text":"<pre><code>def get_classes_of_module(module):\n    m = []\n    import importlib, inspect\n    for name, c in inspect.getmembers(importlib.import_module(\"utils.models\"), inspect.isclass):\n        if c.__module__ == 'utils.models':\n            m.append(c)\n    return m\n</code></pre>"},{"location":"Tools/Programming_Languages/Python/01/#caching","title":"Caching","text":"<pre><code>from functools import cache\nimport time\n\n@cache\ndef function():\ntime.sleep(10) ## this will be skipped by cache\nreturn 1\n</code></pre>"},{"location":"Tools/Programming_Languages/Python/01/#get-variable-name","title":"Get variable name","text":"<pre><code>import inspect\n\ndef var(var):\n    current_frame = inspect.currentframe()\n    caller_frame = inspect.getouterframes(current_frame)[1]\n    local_vars = caller_frame.frame.f_locals\n\n    for name, value in local_vars.items():\n        if value is var:\n            return name\n\nvar = \"Hello\"\nvar_name = var(var)\n</code></pre> <pre><code># doesn't work inside a function\n\nvar = \"Hello\"\nvar_name = f\"{var=}\".split(\"=\")[0]\n</code></pre>"},{"location":"Tools/Programming_Languages/Python/01/#memory-usage","title":"Memory Usage","text":"<pre><code>def get_memory_usage():\n  process = Process(os.getpid())\n  mb = process.memory_info().rss/(1024**2)\n  return mb\n</code></pre>"},{"location":"Tools/Programming_Languages/Python/01/#lazy-imports","title":"Lazy Imports","text":"<pre><code>class LazyImport:\n  def __init__(self, module_name):\n    self.module_name = module_name\n    self._module = None\n\n  def __getattr__(self, attr):\n    if self._module is None:\n      self._module = importlib.import_module(self.module_name)\n    return getattr(self._module, attr)\n\nnp = LazyImport(\"numpy\")\nnp.array([0, 1, 2])\n</code></pre>"},{"location":"Tools/Programming_Languages/Python/01/#save-file","title":"Save File","text":"<pre><code>def save_file(file, file_name, location):\n    with open(os.path.join(location, file_name), \"wb\") as f:\n        f.write(file.getbuffer())\n</code></pre>"},{"location":"Tools/Programming_Languages/Python/02_Creating_Package/","title":"Creating Packages","text":""},{"location":"Tools/Programming_Languages/Python/02_Creating_Package/#folder-structure","title":"Folder Structure","text":"<pre><code>base_folder\n- package_name\n    - init.py\n    - package_name.py\n- setup.py\n- requirements.txt\n</code></pre>"},{"location":"Tools/Programming_Languages/Python/02_Creating_Package/#package_namepy","title":"<code>package_name.py</code>","text":"<pre><code>class Package_Name:\n  pass\n</code></pre>"},{"location":"Tools/Programming_Languages/Python/02_Creating_Package/#initpy","title":"<code>init.py</code>","text":"<pre><code>from package_name import Package_Name\n</code></pre>"},{"location":"Tools/Programming_Languages/Python/02_Creating_Package/#setuppy","title":"<code>setup.py</code>","text":"<pre><code>\"\"\"\nAuthor: Ahmed Thahir\n\nThis is free and unencumbered software released into the public domain.\n\nAnyone is free to copy, modify, publish, use, compile, sell, or\ndistribute this software, either in source code form or as a compiled\nbinary, for any purpose, commercial or non-commercial, and by any\nmeans.\n\nIn jurisdictions that recognize copyright laws, the author or authors\nof this software dedicate any and all copyright interest in the\nsoftware to the public domain. We make this dedication for the benefit\nof the public at large and to the detriment of our heirs and\nsuccessors. We intend this dedication to be an overt act of\nrelinquishment in perpetuity of all present and future rights to this\nsoftware under copyright law.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\nEXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\nMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\nIN NO EVENT SHALL THE AUTHORS BE LIABLE FOR ANY CLAIM, DAMAGES OR\nOTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\nARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\nOTHER DEALINGS IN THE SOFTWARE.\n\nFor more information, please refer to &lt;http://unlicense.org/&gt;\n\"\"\"\n\ntry:\n    from setuptools import setup, find_packages\nexcept ImportError:\n    from distutils.core import setup, find_packages\n\nVERSION = \"0.0.1\"\nDESCRIPTION = \"St. Louis Federal Reserve FRED API\"\n\nimport os\nhere = os.path.abspath(os.path.dirname(__file__))\n\nwith open(os.path.join(here, \"requirements.txt\"), \"r\", encoding=\"utf-8\") as f:\n    requirements = f.read().splitlines()\n\nwith open(os.path.join(here, \"README.md\"), \"r\", encoding=\"utf-8\") as f:\n    long_description = \"\\n\" + f.read()\n\nsetup(\n    name = \"fredx\",\n    version = VERSION,\n    description = DESCRIPTION,\n\n    long_description = long_description,\n    long_description_content_type = \"text/markdown\",\n\n    keywords = [\"fred\", \"api\", \"federal reserve\", \"st. louis fed\", \"async\"],\n    author = \"Ahmed Thahir\",\n    author_email = \"ahmedthahir2002@gmail.com\",\n    url = \"https://github.com/AhmedThahir/fredx\",\n    license = \"MIT\",\n    packages = find_packages(),\n    install_requires = requirements,\n    classifiers = [\n        \"Development Status :: 1 - Planning\",\n        \"Intended Audience :: Developers\",\n        \"Programming Language :: Python :: 3\",\n        \"Operating System :: Unix\",\n        \"Operating System :: MacOS :: MacOS X\",\n        \"Operating System :: Microsoft :: Windows\",\n        \"Topic :: Internet\",\n        \"Topic :: Internet :: WWW/HTTP\",\n        \"Natural Language :: English\",\n    ],\n)\n</code></pre>"},{"location":"Tools/Programming_Languages/Python/02_Creating_Package/#building","title":"Building","text":"<pre><code>pip install setuptools\npython setup.py sdist bdist_wheel\n</code></pre>"},{"location":"Tools/Programming_Languages/Python/02_Creating_Package/#uploading","title":"Uploading","text":"<pre><code>pip install twine\ntwine upload dist/*\n</code></pre>"},{"location":"Tools/Programming_Languages/Python/03_Concurrency/","title":"Concurrency","text":""},{"location":"Tools/Programming_Languages/Python/03_Concurrency/#async","title":"Async","text":""},{"location":"Tools/Programming_Languages/Python/03_Concurrency/#multi-threading","title":"Multi-Threading","text":""},{"location":"Tools/Programming_Languages/Python/03_Concurrency/#multi-processing","title":"Multi-Processing","text":""},{"location":"Tools/Programming_Languages/Python/03_Concurrency/#subprocess","title":"Subprocess","text":"<p>```python for chunk in range(1, 10):         command = f\"command -- {chunk}\"</p> <pre><code>    command_list = shlex.split(command)\n    procs.append(\n        subprocess.Popen(\n    command_list,\n    shell=True,\n    text=True\n  )\n    )\n\n    time.sleep(3)\n# some gap between starting up subprocesses\n</code></pre> <p>for p in procs:   p.wait()</p>"},{"location":"Tools/Video/Manim/","title":"Manim","text":""},{"location":"Tools/Video/Manim/#workflow","title":"Workflow","text":"<ol> <li>render manim animation</li> <li>import into video editor</li> <li>record voiceover</li> <li>use lossless cut/shutter encoder/ffmpeg to attach the audio</li> </ol>"},{"location":"Tools/Video/Manim/#installation","title":"Installation","text":"<p>make sure to add everything to system path</p> <p>Advanced System Settings &gt; Environmental Variables &gt; Path &gt; Add &gt; folder</p>"},{"location":"Tools/Video/Manim/#dependencies","title":"Dependencies","text":"<ol> <li>python    python using anaconda</li> <li>latex</li> <li>manim</li> <li><code>conda install pycairo</code></li> <li><code>pip install manim</code></li> <li>ffmpeg</li> </ol>"},{"location":"Tools/Video/Manim/#macos","title":"Macos","text":"<pre><code>python -m pip install -r ~/Desktop/manim-master/requirements.txt\n</code></pre> <p>install latex mactex</p> <p>ffmpeg</p> <pre><code>sudo chown -R $(whoami) /usr/local/bin\nmv ffmpeg ffplay ffprobe /usr/local/bin\n</code></pre>"},{"location":"Tools/Video/Manim/#text","title":"Text","text":"<p>Tex for text</p> <p>MathTex for math</p> <p><code>self.play(Write(t1), run_time = 5), self.wait(delay)</code></p>"},{"location":"Tools/Video/Manim/#matrices","title":"Matrices","text":"<pre><code>Matrix([\n  (\"10\", \"20\", \"45\"),\n    (\"45\", \"97  \", \"123\"),\n    (\"133\", \"56\", \"75\")],\n  size= 0.5\n)\n</code></pre>"},{"location":"Tools/Video/Manim/#screen","title":"Screen","text":"<p><code>self.clear()</code> clears screen</p>"},{"location":"Tools/Video/Manim/#example-program","title":"Example Program","text":""},{"location":"Tools/Video/Manim/#test","title":"Test","text":"<pre><code>from manim import *\n\n## delay in seconds\ndelay = 0.1 ## bw animations\nendDelay = 2 ## end \n\nclass testAnimation(Scene):\n    def construct(self):\n        t1 = Tex(\"Introduction to Equations\", color = BLUE).to_corner(UP)\n        self.play(Write(t1), run_time = 5), self.wait(delay)\n\n        t2 = MathTex(\"x^2 + 2x\").to_corner(UP).next_to(t1, DOWN)\n        self.play(Write(t2)), self.wait(delay)\n\n        t3 = Tex(\"A basic quadratic equation\", color = GOLD_B).to_corner(RIGHT)\n        self.play(Transform(t1, t3)), self.wait(delay)\n\n        t4 = t3.shift(DOWN)\n        self.play(Transform(t1, t4)), self.wait(endDelay)\n\n        page1 = VGroup(t1, t2)\n\n        ## self.clear()\n\n        t5 = MathTex(r\"h(x) = \\theta X \\text{ or } X\\theta\")\n\n        self.play(Write(t5)), self.wait(endDelay)\n\n        page2 = VGroup(t5)\n</code></pre>"},{"location":"Tools/Video/Manim/#poisson-dist","title":"Poisson Dist","text":"<pre><code>from manim import *\n\n## sizing\nh1 = 2\nh2 = 1.5\npwidth = 12\npline  = 1.2\n\n## delay in seconds\ndelay = 0.1 ## b/w animations\nendDelay = 2 ## slide end\n\nclass TestAnimation(Scene):\n    def construct(self):\n        ## page 1\n        t = [0] ## start array from 1\n\n        t.append(Tex(\"Introduction to Statistics\", color = PINK).to_corner(UP)), t[-1].scale(h1)\n        t.append(Tex(\"Poisson Distribution\", color = GOLD_B).next_to(t[1], DOWN)), t[-1].scale(h2)\n        t.append(Text(\"\"\"\n                The Poisson Distribution is a discrete probability distribution\n                that measures the probability that a certain number of independent \n                events occur within a certain interval/continuum.\n                \"\"\", line_spacing= pline) )\n        t[-1].width = pwidth\n\n        for i in range(1, 3+1): ## 1 - 3\n            if(i == 3):\n                self.play(Write(t[i]), run_time = 10)\n            else:\n                self.play(Write(t[i]))\n            self.wait(delay)\n        self.wait(endDelay)\n\n        ## page 2\n        self.clear()\n        t = [0]\n\n        t.append(Tex(\"Formula\", color =  GOLD_B).to_corner(UP)), t[1].scale(h1)\n        t.append(MathTex(r\"P\\left( x \\right) = \\frac{ {e^{ - \\mu } \\mu ^x } }{ {x!} }\"))\n        t.append(MathTex(\"x = 0, 1, 2, ...\").next_to(t[2]))\n\n        for i in range(1, 3+1):\n            if (i == 2):\n                self.play(Write(t[i]), run_time = 3), self.wait(delay)\n                t[i].generate_target()\n                t[i].target.shift(LEFT * 2)\n                self.play(MoveToTarget(t[i])), self.wait(delay)\n            else:\n                self.play(Write(t[i]))\n            self.wait(delay)\n        self.wait(endDelay)\n\n        ## thank you\n        self.clear()\n\n        t = Tex(\"Thank you\", color = GOLD_B)\n        t.scale(h1)\n        self.play(Write(t)), self.wait(endDelay)\n</code></pre>"},{"location":"Tools/Video/Manim/#example","title":"Example","text":"<pre><code>from manim import *\n\n## sizing\nh1 = 2\nh2 = 1.5\npwidth = 12\npline  = 1.2\n\n## delay in seconds\ndelay = 0.1 ## b/w animations\nendDelay = 2 ## slide end\n\nclass TestAnimation(Scene):\n  def construct(self):\n      ## page 1\n      t = [0] ## start array from 1\n\n      t.append(Tex(\"Introduction to Statistics\", color = PINK).to_corner(UP)), t[-1].scale(h1)\n      t.append(Tex(\"Poisson Distribution\", color = GOLD_B).next_to(t[1], DOWN)), t[-1].scale(h2)\n      t.append(Text(\"\"\"\n              The Poisson Distribution is a discrete probability distribution\n              that measures the probability that a certain number of independent \n              events occur within a certain interval/continuum.\n              \"\"\", line_spacing= pline) )\n      t[-1].width = pwidth\n\n      for i in range(1, 3+1): ## 1 - 3\n          if(i == 3):\n              self.play(Write(t[i]), run_time = 10)\n          else:\n              self.play(Write(t[i]))\n          self.wait(delay)\n      self.wait(endDelay)\n\n      ## page 2\n      self.clear()\n      t = [0]\n\n      t.append(Tex(\"Formula\", color =  GOLD_B).to_corner(UP)), t[1].scale(h1)\n      t.append(MathTex(r\"P\\left( x \\right) = \\frac{ {e^{ - \\mu } \\mu ^x } }{ {x!} }\"))\n      t.append(MathTex(\"x = 0, 1, 2, ...\").next_to(t[2]))\n\n      for i in range(1, 3+1):\n          if (i == 2):\n              self.play(Write(t[i]), run_time = 3), self.wait(delay)\n              t[i].generate_target()\n              t[i].target.shift(LEFT * 2)\n              self.play(MoveToTarget(t[i])), self.wait(delay)\n          else:\n              self.play(Write(t[i]))\n          self.wait(delay)\n      self.wait(endDelay)\n\n      ## thank you\n      self.clear()\n\n      t = Tex(\"Thank you\", color = GOLD_B)\n      t.scale(h1)\n      self.play(Write(t)), self.wait(endDelay)\n</code></pre>"},{"location":"Tools/Video/Manim/#running","title":"Running","text":""},{"location":"Tools/Video/Manim/#cli","title":"CLI","text":"<pre><code>manim -hq -p file.py Scene_Name -w test.mp4\n\nmanimgl -o -f file.py Scene_Name -w test.mp4\n</code></pre> <ul> <li>`manim test.py -p -ql``</li> <li><code>`manim test.py -p -ql</code></li> <li>manim test.py -p -qh`</li> </ul>"},{"location":"Tools/Video/Manim/#within-py-script","title":"Within <code>.py</code> script","text":"<pre><code>command = \"manim -hq -p file.py Scene_Name\"\n\nimport subprocess\nsubprocess.Popen(command)\n</code></pre>"},{"location":"Tools/Video/Manim/#references","title":"References","text":"<ul> <li>Mathematical Animations WITH EASE | Benjamin Hackl</li> </ul>"},{"location":"Tools/Video/OBS%20Studio/01/","title":"01","text":""},{"location":"Tools/Video/OBS%20Studio/01/#live-pseudo-motion-blur","title":"Live [Pseudo] Motion Blur","text":"<p>I know this is a very old question, i found a new method where you can use more than 2 or 3 Game Capture screens by putting a second Game Capture duplicated by Duplicate (Reference) inside a Group Folder and editing the group folder's filters with Color Correction then adding 30 - 70 ms of Render delay (render delay may depend on how high your fps is. mine is currently on 40 ms on 50 PAL) then there you go.</p> <p>This method is better since using too many Window Captures (for me) lags my OBS.</p>"},{"location":"Tools/Video/auto-editor/","title":"Index","text":"<p>Always use <code>--keep-tracks-separate</code> to avoid weird syncing issues</p>"},{"location":"Tools/Video/auto-editor/#my-usual-settings","title":"My usual settings","text":"<pre><code>auto-editor input.mp4 --sounded-speed 1 --silent-speed 0 --edit \"audio:stream=1,threshold=-30dB,mincut=1s\" --my-ffmpeg --extras \"-c:v hevc_nvenc -cq 30 -c:a copy -preset ultrafast\" --keep-tracks-separate\n</code></pre>"},{"location":"Tools/Video/auto-editor/#check-file-properties","title":"Check File Properties","text":"<pre><code>auto-editor info test.mkv\n</code></pre>"},{"location":"Tools/Video/auto-editor/#default-settings","title":"Default Settings","text":"<pre><code>## all default settings\nauto-editor test.mkv --show-ffmpeg-debug\n\nauto-editor --scale --help\nauto-editor --margin --help\n</code></pre>"},{"location":"Tools/Video/auto-editor/#output-stats-preview","title":"Output Stats Preview","text":"<pre><code>auto-editor input.mp4 --stats\n</code></pre>"},{"location":"Tools/Video/auto-editor/#custom-output-name","title":"Custom Output Name","text":"<pre><code>auto-editor input.mp4 --output-file output.mp4\n</code></pre>"},{"location":"Tools/Video/auto-editor/#analyze-different-audio-tracks","title":"Analyze different audio tracks","text":"<pre><code>auto-editor multi-track.mov --edit \"audio:stream=1 audio:threshold=10%\" --keep-tracks-separate\nauto-editor multi-track.mov --edit \"(or audio:stream=0 audio:threshold=10%,stream=1)\" --keep-tracks-separate\n</code></pre>"},{"location":"Tools/Video/auto-editor/#gpu-cqp","title":"GPU + CQP","text":"<pre><code>#CQP\nauto-editor video.mp4 --video-codec hevc_nvenc --my-ffmpeg --extras \"-rc vbr_hq -qmin 0 -cq 30\" --keep-tracks-separate\n\nauto-editor test.mkv --video-codec hevc_nvenc --my-ffmpeg --keep-tracks-separate\nauto-editor test.mkv --video-codec h264_nvenc --my-ffmpeg --keep-tracks-separate\n</code></pre>"},{"location":"Tools/Video/auto-editor/#premiere","title":"Premiere","text":"<pre><code>auto-editor example.mp4 --export premiere --keep-tracks-separate\n</code></pre>"},{"location":"Tools/Video/auto-editor/#speed-of-quiet-and-loud-parts","title":"Speed of quiet and loud parts","text":"<pre><code>auto-editor video.mp4 --silent-speed 10.0 --sounded-speed 1.5 --keep-tracks-separate\n</code></pre>"},{"location":"Tools/Video/auto-editor/#analyze-only-a-particular-track","title":"Analyze only a particular track","text":"<pre><code>auto-editor video.mp4 --edit \"audio:stream=2 audio:threshold=4%\" --keep-tracks-separate\n\nauto-editor video.mp4 --edit \"(or audio:stream=2 audio:threshold=4%,stream=0)\" --keep-tracks-separate\n</code></pre>"},{"location":"Tools/Video/auto-editor/#running-from-python-script","title":"Running from Python Script","text":""},{"location":"Tools/Video/auto-editor/#process-single-video","title":"Process Single Video","text":"<pre><code>  import subprocess\n\n  video = \"test.mp4\"\n\n  command = r'auto-editor \"{}\" --video-codec hevc_nvenc --my-ffmpeg --keep-tracks-separate'.format(video)\n  subprocess.Popen(command)\n</code></pre>"},{"location":"Tools/Video/auto-editor/#process-multiple-videos","title":"Process Multiple Videos","text":"<pre><code>  import subprocess\n  import os\n\n  def find_videos(directory):\n    videos = []\n    for filename in os.listdir(directory):\n      extension = filename.split(\".\")[-1]\n      if(extension in [\"mp4\", \"mkv\"]):\n        video = os.path.join(directory, filename)\n        videos.append(video)\n    return videos\n\n  def process_video(video):\n    command = r'auto-editor \"{}\" --video-codec hevc_nvenc --my-ffmpeg --keep-tracks-separate'.format(video)\n    subprocess.Popen(command)\n\n  if __name__ == \"__main__\":\n    directory = os.getcwd()\n    videos = find_videos(directory)\n\n    for video in videos:\n      process_video(video)\n</code></pre>"},{"location":"Tools/Video/auto-editor/#lossless","title":"Lossless","text":"<pre><code>import numpy as np\nimport time\nimport re\nimport sys\nimport json\nimport os\nimport concurrent.futures as multi\n\ndef cleanup():\n  files_to_remove = []\n  for file in os.listdir():\n    for unwanted in [\"_segment\"]:\n      if unwanted in file.lower():\n        files_to_remove.append(file)\n  with multi.ThreadPoolExecutor() as executor:\n    executor.map(os.remove, files_to_remove)\n\ndef get_fps(input_file):\n  info = os.popen(f'ffprobe -i \"{input_file}\" 2&gt;&amp;1').read()\n  match = re.search(r'\\s([\\d\\.]*)\\sfps', info)\n  if match:\n    return float(match.group(1))\n  return 0.0\n\ndef get_keyframe_interval(input_file):\n  \"\"\"\n  Get average keyframe interval\n  \"\"\"\n\n  start_time_to_read = 1\n  max_seconds_to_read = 5\n  info = os.popen(f\"\"\"\n  ffprobe -read_intervals {start_time_to_read}%+{max_seconds_to_read} -select_streams v -show_entries frame=pts_time -of csv=p=0 -skip_frame nokey -v 0 -hide_banner -i {input_file}\n  \"\"\").read()\n\n  keyframe_time_points = np.array(info.split(\"\\n\"))\n  keyframe_time_points = keyframe_time_points[\n    (keyframe_time_points != \"\")\n  ]\n  keyframe_time_points = keyframe_time_points.astype(np.float32)\n  keyframe_interval = np.round(np.mean(np.diff(keyframe_time_points))).astype(int)\n  return keyframe_interval\n\ndef process_json(input_file, fps, json):\n  extension = os.path.splitext(input_file)[1]\n  cmd = []\n  with open('_segments.txt', 'w') as f:\n    sounded_chunks = json[\"v\"][0]\n    for segment_number, sounded_chunk in enumerate(sounded_chunks):\n      segment_file_name = f\"_segment{segment_number}{extension}\"\n\n      f.write(f'file {segment_file_name}\\n')\n\n      offset_time = sounded_chunk[\"offset\"] / fps\n      start_time = sounded_chunk[\"start\"] / fps\n      speed = sounded_chunk[\"speed\"]\n      duration = (sounded_chunk[\"dur\"]/speed) / fps\n\n      cmd.append(f\"\"\"\n      ffmpeg -hide_banner -loglevel error \\\n      -ss {offset_time + start_time} \\\n      {(\n        f\"-itsscale {1/speed}\"\n        if speed==1\n        else \"\"\n      )} \\\n      -i \"{input_file}\" \\\n      -t {duration} \\\n      -avoid_negative_ts make_zero \\\n      {(\n        \"-c:a copy\"\n        if speed==1\n        else f\"-af volume=0 -af atempo={speed}\"\n      )} \\\n      -c:v copy \\\n      -map_metadata 0 -movflags use_metadata_tags -movflags '+faststart' -default_mode infer_no_subs -ignore_unknown -y \\\n      {segment_file_name}\n      \"\"\")\n\n    with multi.ProcessPoolExecutor() as executor:\n      executor.map(os.system, cmd)\n\ndef combine_segments(input_file):\n  os.system(f\"\"\"\n  ffmpeg  -hide_banner -loglevel error \\\n  -f concat -safe 0 -protocol_whitelist 'file,pipe,fd' \\\n  -i _segments.txt \\\n  -c copy \\\n  '-disposition' default -movflags use_metadata_tags -movflags '+faststart' -default_mode infer_no_subs -ignore_unknown -y \\\n  \"{os.path.splitext(input_file)[0]}_STRIPPED{os.path.splitext(input_file)[1]}\"\n  \"\"\")\n\ndef process_file(input_file):\n  fps_val = get_fps(input_file)\n  keyframe_interval = get_keyframe_interval(input_file)\n\n  if fps_val &lt;= 0.0:\n    print(f'Unable to determine FPS of {input_file}')\n    return\n\n  os.system(f'auto-editor \"{input_file}\" --edit \"audio:threshold=-40dB,mincut={max([1, 1 + keyframe_interval])}s\" --export json')\n  json_file = os.path.splitext(input_file)[0] + '_ALTERED.json'\n  with open(json_file) as f:\n    json_data = json.load(f)\n  process_json(input_file, fps_val, json_data)\n  os.remove(json_file)\n  combine_segments(input_file)\n\ndef iterate_files():\n  for input_file in sys.argv[1:]:\n    process_file(input_file)\n\ndef main():\n  cleanup()\n\n  start_time = time.time()\n  iterate_files()\n  print(\"--- %s seconds ---\" % (time.time() - start_time))\n\n  cleanup()\n\nif __name__ == \"__main__\":\n  main()\n</code></pre>"},{"location":"Tools/Video/auto-editor/#editing-logic","title":"Editing logic","text":"<pre><code>default: audio (only)\n\nEditing Methods:\n - audio: General audio detection\n - motion: Motion detection specialized for real life noisy video\n - pixeldiff: Detect when a certain amount of pixels have changed between frames\n - random: Set silent/loud randomly based on a random or preset seed\n - none: Do not modify the media in anyway (Mark all sections as \"loud\")\n - all: Cut out everything out (Mark all sections as \"silent\")\n\nAttribute Defaults:\n - audio\n    - threshold: 4% (number)\n    - stream: 0 (natural | \"all\")\n - motion\n    - threshold: 2% (number)\n    - stream: 0 (natural | \"all\")\n    - blur: 9 (natural)\n    - width: 400 (natural)\n - pixeldiff\n    - threshold: 1 (natural)\n    - stream: 0 (natural | \"all\")\n - random\n    - threshold: 0.5 (number)\n    - seed: RANDOMLY-GENERATED (int)\n\nOperators:\n - and\n   - usage: $METHOD and $METHOD\n - or\n   - usage: $METHOD or $METHOD\n - xor\n   - usage: $METHOD xor $METHOD\n - not\n   - usage: not $METHOD\nExamples:\n  --edit audio\n  --edit audio:stream=1\n  --edit audio:threshold=4%\n  --edit audio:threshold=0.03\n  --edit motion\n  --edit motion:threshold=2%,blur=3\n  --edit audio:threshold=4% or motion:threshold=2%,blur=3\n  --edit none\n  --edit all\n</code></pre>"},{"location":"Tools/Video/ffmpeg/","title":"Index","text":""},{"location":"Tools/Video/ffmpeg/#installation","title":"Installation","text":""},{"location":"Tools/Video/ffmpeg/#macos","title":"Macos","text":""},{"location":"Tools/Video/ffmpeg/#download-ffmpeg-and-ffprobe","title":"Download ffmpeg and ffprobe","text":"<p>Go to https://ffmpeg.org/download.html and click the Apple logo in the \"Get packages &amp; executable files\" section.</p> <p>Click \"Static builds for macOS 64-bit\".</p> <p>You'll see two options for downloading ffmpeg. Choose the one with the shorter filename; this will look like  <code>ffmpeg-&lt;versionNumber&gt;.7z</code> , where  <code>&lt;versionNumber&gt;</code>  is something like  <code>4.3.1</code> .</p> <p>Underneath this heading, click \"Download as ZIP\".</p> <p>Scroll down the page until you see ffprobe. Choosing the shorter filename, under  <code>ffprobe-&lt;versionNumber&gt;.7z</code> , click \"Download the file as ZIP\".</p> <p>If a popup appears after clicking the download link, press \"allow\" or \"save\".</p> <p>Open your Downloads folder, and double-click  <code>ffmpeg-&lt;versionNumber&gt;.zip</code> . This will extract it using the Archive Utility and create an executable  <code>ffmpeg</code>  file in Downloads.</p> <p>Repeat this step for  <code>ffprobe</code> .</p> <p>You should now have two executables, called  <code>ffmpeg</code>  and  <code>ffprobe</code> .</p>"},{"location":"Tools/Video/ffmpeg/#move-the-downloaded-files-to-the-right-location","title":"Move the downloaded files to the right location","text":"<p>Open your home folder.</p> <p>Your home folder has the same name as your user account. The easiest way to find it is to open Finder, and use the keyboard shortcut  <code>command + shift + H</code>  or in the menu bar select Go &gt; Home.</p> <p>You should see folders such as Desktop, Applications, and Downloads in this folder.</p> <p>Create a new folder called  <code>audio-orchestrator-ffmpeg</code>  in your home folder.</p> <p>Go to File &gt; New folder or use the shortcut  <code>command + shift + N</code> , type or enter the folder name, and press  <code>return</code>  to confirm.</p> <p>Open your new  <code>audio-orchestrator-ffmpeg</code>  folder by double-clicking it.</p> <p>Create a new folder called  <code>bin</code>  in  <code>audio-orchestrator-ffmpeg</code> .</p> <p>Move the  <code>ffmpeg</code>  and  <code>ffprobe</code>  files from  <code>Downloads</code>  into this  <code>bin</code>  folder.</p> <p>You should now have two files,  <code>ffmpeg</code>  and  <code>ffprobe</code> , in your  <code>~/audio-orchestrator-ffmpeg/bin/</code>  folder      ffmpeg and ffprobe executables with the required folder structure </p>"},{"location":"Tools/Video/ffmpeg/#authorise-ffmpeg-and-ffprobe","title":"Authorise ffmpeg and ffprobe","text":"<p>Double-click the file called  <code>ffmpeg</code> .</p> <p>You should see an error message \"ffmpeg can\u2019t be opened because it is from an unidentified developer\". Click \"OK\".</p> <p>Go to System Preferences &gt; Security and Privacy and click on the General tab.</p> <p>At the bottom of the window you will see a message saying that ffmpeg was blocked. Click \"Open Anyway\".</p> <p>If you do not see this message in the General tab, double-click  <code>ffmpeg</code>  again.</p> <p>You may have to click the \"unlock\" button and enter your password to be able to click \"Open Anyway\".</p> <p>If you see another popup that says \u201cffmpeg is from an unidentified developer. Are you sure you want to open it?\u201d, click \"Open\". If you don\u2019t get this popup, just go to the same file and double-click it again.</p> <p>When you double-click the file, a Terminal window may open. Keep the terminal open until you see a message confirming you can close it.</p> <p>Repeat authorisation steps (a) to (f) for the file called  <code>ffprobe</code> .</p>"},{"location":"Tools/Video/ffmpeg/#arguments","title":"Arguments","text":"<p>-ss seek</p> <p>-t duration</p> <p>-to end time point</p> <p>-i video.mp4 (must come after the above)</p> <p>-c codec</p> <p>-o output</p>"},{"location":"Tools/Video/ffmpeg/#list-of-encoders","title":"List of encoders","text":"<pre><code>ffmpeg -encoders\n</code></pre>"},{"location":"Tools/Video/ffmpeg/#get-video-fps","title":"Get video FPS","text":"<pre><code>ffmpeg -i filename 2&gt;&amp;1 | sed -n \"s/.*, \\(.*\\) fp.*/\\1/p\"\n</code></pre>"},{"location":"Tools/Video/ffmpeg/#collect-all-keyframes","title":"Collect all keyframes","text":"<pre><code>ffmpeg -skip_frame nokey -i 2.flv -vsync 0 -r 30 -f\n</code></pre>"},{"location":"Tools/Video/ffmpeg/#preview","title":"Preview","text":"<pre><code>ffplay -i video.mp4 -ss 00:01:02 -t 00:30:00\n</code></pre> <p>Start at 1 minute 2 seconds with a duration of 30 seconds</p>"},{"location":"Tools/Video/ffmpeg/#lossless","title":"Lossless","text":""},{"location":"Tools/Video/ffmpeg/#losslessly-trim","title":"Losslessly Trim","text":"<p>Leave a bit of extra padding in cut points to prevent overcut</p> <pre><code>  ffmpeg -ss 00:01:02 -t 00:30:00 -i video.mp4 -c copy video_clip.mp4\n</code></pre> <p>If you have trouble using the video in Final Cut Pro, try the .mov extension like this:</p> <pre><code>    ffmpeg -i video.mp4 -ss 00:01:02 -t 00:30:00 -codec copy video_clip.mov\n</code></pre>"},{"location":"Tools/Video/ffmpeg/#lossless-concatmerge","title":"Lossless Concat/Merge","text":"<pre><code>  ffmpeg -f concat -safe 0 -i file_list.txt -c copy output.mp4\n</code></pre> <p><code>file_list.txt</code></p> <pre><code>    file 'clip1.mp4'\n    file 'clip2.mp4'\n    file 'clip3.mp4'\n</code></pre> <p>With chapters</p> <pre><code>    import subprocess\n    import os\n    import re\n\n    def make_chapters_metadata(list_mp4: list):\n        print(f\"Making metadata source file\")\n\n        chapters = {}\n        for single_mp4 in list_mp4:\n            number = single_mp4.removesuffix(\".m4a\")\n            cmd = f\"ffprobe -v quiet -of csv=p=0 -show_entries format=duration '{folder}/{single_mp4}'\"\n            print(f\"{cmd=}\")\n            duration_in_microseconds_ = subprocess.run(cmd, shell=True, capture_output=True)\n            duration_in_microseconds__ = duration_in_microseconds_.stdout.decode().strip().replace(\".\", \"\")\n            print(f\"{duration_in_microseconds_=}\")\n            duration_in_microseconds = int(duration_in_microseconds__)\n            chapters[number] = {\"duration\": duration_in_microseconds}\n\n        print(f\"{chapters=}\")\n        chapters[list_mp4[0].removesuffix(\".m4a\")][\"start\"] = 0\n        for n in range(1, len(chapters)):\n            chapter = list_mp4[n-1].removesuffix(\".m4a\")\n            next_chapter = list_mp4[n].removesuffix(\".m4a\")\n            chapters[chapter][\"end\"] = chapters[chapter][\"start\"] + chapters[chapter][\"duration\"]\n            chapters[next_chapter][\"start\"] = chapters[chapter][\"end\"] + 1\n        last_chapter = list_mp4[len(chapters)-1].removesuffix(\".m4a\")\n        chapters[last_chapter][\"end\"] = chapters[last_chapter][\"start\"] + chapters[last_chapter][\"duration\"]\n\n        metadatafile = f\"{folder}/combined.metadata.txt\"\n        with open(metadatafile, \"w+\") as m:\n            m.writelines(\";FFMETADATA1\\n\")\n            for chapter in chapters:\n                ch_meta = \"\"\"\n    [CHAPTER]\n    TIMEBASE=1/1000000\n    START={}\n    END={}\n    title={}\n    \"\"\".format(chapters[chapter][\"start\"], chapters[chapter][\"end\"], chapter)\n                m.writelines(ch_meta)\n\n\n    def concatenate_all_to_one_with_chapters():\n        print(f\"Concatenating list of mp4 to combined.mp4\")\n        metadatafile = f\"{folder}/combined.metadata.txt\"\n        subprocess.run([\"ffmpeg\", \"-hide_banner\", \"-y\", \"-safe\", \"0\", \"-f\", \"concat\", \"-i\", \"list_mp4.txt\", \"-c\", \"copy\", \"-i\", f\"{metadatafile}\", \"-map_metadata\", \"1\", \"combined.m4a\"])\n\n    if __name__ == '__main__':\n\n        folder = \".\"  ## Specify folder where the files 0001.mp4... are\n        ## concatenate_all_to_one_with_chapters()\n        ## exit(0)\n\n        list_mp4 = [f for f in os.listdir(folder) if f.endswith('.m4a')]\n        list_mp4.sort()\n        print(f\"{list_mp4=}\")\n\n        ## Make the list of mp4 in ffmpeg format\n        if os.path.isfile(\"list_mp4.txt\"):\n            os.remove(\"list_mp4.txt\")\n        for filename_mp4 in list_mp4:\n            with open(\"list_mp4.txt\", \"a\") as f:\n                line = f\"file '{filename_mp4}'\\n\"\n                f.write(line)\n\n        make_chapters_metadata(list_mp4)\n        concatenate_all_to_one_with_chapters()\n</code></pre>"},{"location":"Tools/Video/ffmpeg/#lossless-speed-change","title":"Lossless Speed Change","text":"<pre><code>  ffmpeg -itsscale 1/{new_speed} -i input.mp4 -c copy output.mp4\n</code></pre>"},{"location":"Tools/Video/ffmpeg/#lossless-compressiontimelapse-extract-only-i-frames","title":"Lossless Compression/Timelapse (extract only i-frames)","text":"<pre><code>  ## drop non-keyframes\n  ffmpeg -itsscale 1/{new_speed} -i input.mov -c:v copy -an -bsf:v \"noise=drop=not(key)\" output.mp4\n\n  ## select every k frames\n  ## won't work all some frames won't be keyframes (use method 3 instead)\n  ffmpeg -itsscale 1/{new_speed} -i input.mov -c:v copy -an -bsf:v \"noise=drop=mod(n\\,{select_frame_frequency})\" output.mp4\n\n  ## both\n  ffmpeg -itsscale 1/{new_speed} -i input.mov -c:v copy -an -bsf:v \"noise=drop=not(key),noise=drop=mod(n\\,select_frame_frequency)\" output.mp4\n</code></pre>"},{"location":"Tools/Video/ffmpeg/#ffprobe","title":"ffprobe","text":""},{"location":"Tools/Video/ffmpeg/#limit-frames","title":"Limit frames","text":"<pre><code>  ffprobe -read_intervals {intervals} -i {input_file}\n</code></pre> <pre><code>  INTERVAL  ::= [START|+START_OFFSET][%[END|+END_OFFSET]]\n  INTERVALS ::= INTERVAL[,INTERVALS]\n</code></pre>"},{"location":"Tools/Video/ffmpeg/#get-keyframe-timestamps","title":"Get keyframe timestamps","text":"<pre><code>  ffprobe -select_streams v -show_entries frame=pict_type,pts_time -of csv=p=0 -skip_frame nokey -v 0 -hide_banner -i input.mp4\n</code></pre>"},{"location":"Tools/Video/ffmpeg/#get-number-of-keyframes","title":"Get number of keyframes","text":"<pre><code>  ffprobe -hide_banner -of compact=p=0:nk=1 -select_streams v:0 -count_frames -show_entries stream=nb_read_frames -skip_frame nokey -v 0 -i input.mp4\n</code></pre>"},{"location":"Tools/Video/ffmpeg/#get-video-duration","title":"Get video duration","text":"<pre><code>  ## only time points\n  ffprobe -select_streams v -show_entries frame=pts_time -of csv=p=0 -skip_frame nokey -v 0 -hide_banner -i INPUT.mov\n\n  ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 -i input.mp4\n</code></pre> <p>Get keyframe interval</p> <pre><code>  ffprobe -read_intervals {start_time_to_read}%+{max_seconds_to_read} -select_streams v -show_entries frame=pts_time -of csv=p=0 -skip_frame nokey -v 0 -hide_banner -i {input_file}\n\n  ## another approach\n  ffprobe -loglevel error -skip_frame nokey -select_streams v:0 -show_entries frame=pkt_pts_time -of csv=print_section=0 input.mp4\n</code></pre>"},{"location":"Tools/Video/ffmpeg/#ffplay","title":"ffplay","text":"<pre><code>ffplay -fs -noborder -nostats input.mp4\n</code></pre>"},{"location":"Tools/Video/ffmpeg/#video-filters","title":"Video Filters","text":""},{"location":"Tools/Video/ffmpeg/#split-screen","title":"Split Screen","text":"<pre><code>  ffmpeg -i input_1.mp4 -i input_2.mp4 -filter_complex \"[0:v:0]pad=iw*2:ih[bg]; [bg][1:v:0]overlay=w\" output.mp4\n</code></pre>"},{"location":"Tools/Video/ffmpeg/#lens-correction","title":"Lens Correction","text":"<pre><code>  ffmpeg -i in.mp4 -vf \"lenscorrection=cx=0.5:cy=0.5:k1=-0.227:k2=-0.022\" out.mp4\n</code></pre>"},{"location":"Tools/Video/ffmpeg/#stabilization","title":"Stabilization","text":"<pre><code>  ffmpeg -i input.mp4 -vf vidstabdetect=shakiness=10 -f null -\n  ffmpeg -i input.mp4 -vf vidstabtransform=smoothing=50,unsharp=5:5:0.8:3:3:0.4 output.mp4\n</code></pre>"},{"location":"Tools/Video/ffmpeg/#color-correction","title":"Color Correction","text":"<pre><code>  ffmpeg -i input.mp4 -vf eq=brightness=1.2:contrast=1.2:saturation=2 -c:a copy output.mp4\n</code></pre>"},{"location":"Tools/Video/ffmpeg/#remove-duplicate-frames","title":"Remove Duplicate Frames","text":"<p>Mpdecimate</p> <pre><code>    ffmpeg -i input.mp4 -vf mpdecimate,setpts=N/FRAME_RATE/TB -vsync vfr -an -c:a copy output.mp4\n</code></pre> <p>Lossless</p> <pre><code>    #!/bin/bash\n\n    ## lld -- lossless decimator/de-duplicator\n\n    ## This script takes a compatible all-I-frame video file and losslessly deduplicates to a new video file.\n    ## The new file will have the same fps as the old file, though. Changing fps is left to the user --\n    ##   it's not simple for e.g. ProRes to do this and remain \"legal\".\n    #\n    ## You may want to tweak the configuration of mpdecimate below. For me, \"hi\" is disabled, because even between my\n    ## duplicate frames there typically existed one 8x8 block that would exceed a high value, for some reason.\n    ## lo and frac may need tweaking for your source material: https://ffmpeg.org/ffmpeg-filters.html#mpdecimate\n    ## After running, you can investigate the output of mpdecimate in the temp directory file mpdecimate.txt.\n    #\n    ## This script assumes no audio associated with the footage -- it would likely need to be modified to handle\n    ## footage with audio\n    #\n    ## max frame number in file is 99,999,999 frames (~38 days at 29.97fps)\n    #\n    ## Development/discussion thread here: https://forum.videohelp.com/threads/383274-de-duplicating-decimating-ProRes-losslessly#post2483571\n\n    echo -e \"\\nlld version 1.0\\n\"\n\n    if [ -z $1 ]; then echo -e \"Usage: ddpr file [expected_fps [debug]]\\n  where expected_fps is the actual frame rate (used for info purposes only -- will not affect processing)\\n  note: lld will create file.tempdir in current directory\\n  If you specify \\\"debug\\\", lld will leave the temporary directory behind for evaluating logs, etc.\\n\"; exit; fi\n\n    fname=$1\n    newfps=$2\n    debug=$3\n\n    if [ ! -f \"$fname\" ]; then\n      echo \"$fname does not exist. Doing nothing.\"\n      exit 1\n    fi\n\n    if [ -e \"$fname\".tempdir ]; then\n      echo \"$fname\".tempdir already exists. Doing nothing.\n      exit 1\n    fi\n\n    if [ -e \"$fname.dedup.mov\" ]; then\n      echo \"$fname.dedup.mov already exists. Doing nothing.\"\n      exit 1\n    fi\n\n    fext=`echo $fname | sed -r 's/.*(\\.[^.]*)$/\\1/'`\n    fextcnt=`echo -n $fext | wc -c`\n\n    if [ $fextcnt -le 1 ]; then\n      echo \"Couldn't detect filename extension. Doing nothing.\"\n      exit 1\n    fi\n\n    mkdir \"$fname\".tempdir\n    mkdir \"$fname\".tempdir/frames\n\n    cd \"$fname\".tempdir\n\n    fps=`ffprobe -v 0 -of compact=p=0 -select_streams 0 -show_entries stream=r_frame_rate ../$fname | sed 's/^r_frame_rate=/scale=15;/g' | bc` \n\n    echo \"$fps fps detected. Detecting duplicates...\"\n\n    flen=`echo \"scale=15;1/$fps\" | bc`\n    flenoffset=`echo \"scale=15;.1*$flen\" | bc`\n\n    ffmpeg -i ../$fname -vf mpdecimate=max=1:hi=999999999:lo=64*3:frac=0.4 -loglevel debug -f null - &gt; mpdecimate.txt 2&gt;&amp;1\n\n    fcnt=`cat mpdecimate.txt | grep \"frames successfully decoded\" | sed -r 's/^(.*) frames .*$/\\1/'`\n\n    cat mpdecimate.txt | grep Parsed | grep keep | sed -r 's/^.*pts_time:(.*) drop.*/\\1/' &gt; ts.txt\n\n    if [ ! -f ts.txt ]; then\n      echo \"ts.txt not created. Exiting.\"\n      exit 1\n    fi\n\n    res=`cat ts.txt | wc -l`\n\n    if [ $res -le \"0\" ]; then\n     echo \"ts.txt didn't generate correctly. Exiting.\"\n     exit 1\n    fi\n\n    ## The offset version is necessary because apparently ffmpeg doesn't find the nearest frame to the timestamp, but the next frame.\n    ## As a result, occasional rounding errors mean that the wrong frame would be targeted when extracting them from the original file.\n    ## The offset is a 10% backwards shift in the timestamp to guarantee that the next frame found will be the correct frame.\n    cat ts.txt | awk \"{print \\$1-$flenoffset}\" | bc &gt; ts2.txt\n\n    if [ ! -f ts2.txt ]; then\n      echo \"ts2.txt not created. Exiting.\"\n      exit 1\n    fi\n\n    res=`cat ts2.txt | wc -l`\n\n    if [ $res -le \"0\" ]; then\n     echo \"ts2.txt didn't generate correctly. Exiting.\"\n     exit 1\n    fi\n\n    newfr=`echo \"scale=3;$fps*($res/$fcnt)\" | bc`\n\n    echo \"Detected $res good frames out of $fcnt total frames = detected actual frame rate of $newfr fps\"\n\n    if [ ! -z $newfps ]; then\n      fpserr=`echo \"scale=3;100*($newfr/$newfps - 1)\" | bc`\n\n      fpserrfirstchar=`echo $fpserr | sed -r 's/^(.).*$/\\1/g'`\n      if [ $fpserrfirstchar == \".\" ]; then\n        fpserr=`echo 0$fpserr`\n      fi\n      fpserrfirstchars=`echo $fpserr | sed -r 's/^(..).*$/\\1/g'`\n      if [ $fpserrfirstchars == \"-.\" ]; then\n        fpserr=`echo $fpserr | sed -r 's/^.(.*)$/\\1/g'`\n        fpserr=`echo \"-0\"$fpserr`\n      fi\n\n      echo \"$fpserr% error from expected $newfps fps.\"\n    fi\n\n    echo \"Generating segment video files...\"\n\n    ## doing -ss after -i because before is not accurate in this case for some reason (first few seconds work fine, then starts going off the rails)\n    cat ts2.txt | awk \"{printf \\\"ffmpeg -i ../$1 -ss %s -t 0$flen -vcodec copy -acodec copy frames/%08d$fext\\n\\\",\\$1,f;f++}\" &gt; ffmpeg_frame_extraction_commands.txt\n\n    if [ ! -f ffmpeg_frame_extraction_commands.txt ]; then\n      echo \"ffmpeg_frame_extraction_commands.txt not created. Exiting.\"\n      exit 1\n    fi\n\n    res=`cat ffmpeg_frame_extraction_commands.txt | wc -l`\n\n    if [ $res -le \"0\" ]; then\n     echo \"ffmpeg_frame_extraction_commands.txt didn't generate correctly. Exiting.\"\n     exit 1\n    fi\n\n    source ffmpeg_frame_extraction_commands.txt &gt; ffmpeg_frame_extraction_results.log 2&gt;&amp;1\n\n    res=`/bin/ls -1 frames/*$fext | wc -l`\n\n    if [ $res -le \"0\" ]; then\n      echo \"No segment files generated. Exiting.\"\n      exit 1\n    fi\n\n    echo \"Generating segment logfile for concatenation and concatenating...\"\n\n    /bin/ls -1 frames/*$fext | sed -r \"s/(.*)/file '\\1'/\" &gt; segment_files_list.txt\n\n    if [ ! -f segment_files_list.txt ]; then\n      echo \"segment_files_list.txt not created. Exiting.\"\n      exit 1\n    fi\n\n    res=`cat segment_files_list.txt | wc -l`\n\n    if [ $res -le \"0\" ]; then\n     echo \"segment_files_list.txt didn't generate correctly. Exiting.\"\n     exit 1\n    fi\n\n    ffmpeg -f concat -i segment_files_list.txt -c copy ../$fname.dedup$fext &gt; ffmpeg_concatenation_results.log 2&gt;&amp;1\n\n    if [ -z $debug ]; then\n     echo \"Deleting temporary directory. (Use e.g. \\\"lld file.mov 18 debug\\\" to prevent this.)\"\n     echo \"Removing temporary directory: $fname.tempdir\"\n     cd ..\n     rm -rf \"$fname\".tempdir\n    else\n     echo \"Temporary directory $fname.tempdir left behind.\"\n    fi\n\n    echo -e \"Done. Resulting filename: $fname.dedup$fext\\n\"\n</code></pre>"},{"location":"Tools/Video/ffmpeg/#motion-blur","title":"Motion Blur","text":"<pre><code>  ffmpeg -i input.mp4 -vf tmix=frames={no_of_frames_to_blend} output.mp4\n</code></pre>"},{"location":"Tools/Video/ffmpeg/#upscaling","title":"Upscaling","text":"<pre><code>  ffmpeg -i input.mp4 -vf \"scale={scale}:flags=neighbor\" -c:v nvenc_hevc -crf 30 -preset ultrafast output.mp4\n</code></pre> <p>Scale</p> <pre><code>    3840:-1\n    -1:2160\n    iw*2:ih*2\n</code></pre>"},{"location":"Tools/Video/ffmpeg/#web-optimization","title":"Web Optimization","text":"<pre><code>ffmpeg -i input.mp4 -c copy -movflags faststart output.mp4\n</code></pre> <p>Fast start is for internet streaming as it puts header at the begining of the file. When you play file from HDD it doesn't matter</p> <p>Only for MP4, M4A, M4V, MOV</p>"},{"location":"Tools/Web_Dev/","title":"Web Development","text":"HTML CSS JS Hyper Text Markup Language Cascading StyleSheets JavaScript Body, Structure Styling, Interactions UI UI UX Very similar to XML"},{"location":"Tools/Web_Dev/#colors","title":"Colors","text":"HSL Hue, Saturation, Luminosity RGB Red, Green, Blue HEX Hexadecimal LAB Luminosity, A, B Hue Type of color (red, green, blue) Saturation Intensity of color Luminosity Brightness of color <p>Optional parameter - A: Alpha (Opacity)</p> Transparency Opacity How much transparent How much opaque 0% Completely visible Completely invisible 100% Completely invisible Completely visible"},{"location":"Tools/Web_Dev/#references","title":"References","text":"<ul> <li> Web Dev Simplified | Introduction to Web Development</li> <li> w3schools</li> </ul>"},{"location":"Tools/Web_Dev/HTML/","title":"HTML","text":"<p>Auto-refresh page every 5sec</p> <pre><code>&lt;meta http-equiv=\"refresh\" content=\"5\"&gt;\n</code></pre>"},{"location":"Tools/Web_Dev/HTML/#autocompletesuggestions","title":"Autocomplete/Suggestions","text":""},{"location":"Tools/Web_Dev/HTML/#text","title":"Text","text":"<pre><code>&lt;label for=\"city_input\"&gt;City&lt;/label&gt;\n&lt;input type=\"text\" id=\"city_input\" list=\"cities_list\" /&gt;\n&lt;datalist id=\"cities_list\"&gt;\n    &lt;option value=\"Dubai\"&gt;My city&lt;/option&gt;\n    &lt;option value=\"Kayal\"&gt;My hometown&lt;/option&gt;\n&lt;/datalist&gt;\n</code></pre>"},{"location":"Tools/Web_Dev/HTML/#colors","title":"Colors","text":"<pre><code>&lt;label for=\"color_picker\"&gt;Pick a color&lt;/label&gt;\n&lt;input type=\"color\" id=\"color_picker\" list=\"colors_list\" /&gt;\n&lt;datalist id=\"colors_list\"&gt;\n    &lt;option value=\"#155AF0\"&gt;Primary Color&lt;/option&gt;\n    &lt;option value=\"#FFF\"&gt;Secondary Color&lt;/option&gt;\n&lt;/datalist&gt;\n</code></pre>"},{"location":"Tools/Web_Dev/MERN/","title":"MERN Stack","text":"Role MongoDB Database Express.js Code web server in javascript React.js Front-end Node.js Server-side javascript"},{"location":"Tools/Web_Dev/MERN/#references","title":"References","text":"<ul> <li> FreeCodeCamp | MERN Stack Course</li> </ul>"},{"location":"Tools/Web_Dev/MERN/MongoDB/","title":"MongoDB","text":"<p>MongoDB does not require explicit creation. If you try to access something that doesn\u2019t exist, MongoDB will create it for you.</p>"},{"location":"Tools/Web_Dev/MERN/MongoDB/#install","title":"Install","text":"<ol> <li>Install <code>MongoDB Community</code></li> <li>Install <code>mongosh</code> (shell)</li> </ol>"},{"location":"Tools/Web_Dev/MERN/MongoDB/#vocabulary","title":"Vocabulary","text":"Relational MongoDB Database Database Table Collection Column Key Row Document Index Index Join $lookup Foreign Key Reference"},{"location":"Tools/Web_Dev/MERN/MongoDB/#data-format","title":"Data Format","text":"<p>BSON (Binary JSON): very similar to json</p>"},{"location":"Tools/Web_Dev/MERN/MongoDB/#mongosh","title":"Mongosh","text":"<pre><code>mongosh // enter\nexit // exit\n</code></pre>"},{"location":"Tools/Web_Dev/MERN/MongoDB/#ddl","title":"DDL","text":"<pre><code>show dbs\nuse appdb\nshow collections\ndb.dropDatabase()\n</code></pre>"},{"location":"Tools/Web_Dev/MERN/MongoDB/#dml","title":"DML","text":""},{"location":"Tools/Web_Dev/MERN/MongoDB/#create","title":"Create","text":"<pre><code>db.users.insertOne({\n    name: \"Ahmed\"\n})\ndb.users.insertMany([\n  {\n    name: \"Ahmed\"\n  },\n    {\n    name: \"Thahir\"\n  }\n])\n</code></pre>"},{"location":"Tools/Web_Dev/MERN/MongoDB/#read","title":"Read","text":"<pre><code>db.users.findOne()\ndb.users.find() // equiv to select *\n</code></pre>"},{"location":"Tools/Web_Dev/MERN/MongoDB/#find-functions","title":"Find functions","text":"<pre><code>.sort({\n    name:1, // -1\n    age: 1\n})\n.skip(5)\n.limit(10)\n</code></pre>"},{"location":"Tools/Web_Dev/MERN/MongoDB/#count","title":"Count","text":"<pre><code>db.users.countDocuments({\n  age: 10\n})\n</code></pre>"},{"location":"Tools/Web_Dev/MERN/MongoDB/#update","title":"Update","text":"<p><code>$set</code></p> <pre><code>db.users.updateOne(\n  {\n    age: 26\n    },\n  {\n    $set: {age: 27}\n  }\n)\ndb.users.updateMany(\n  {\n    age: 26\n    },\n  {\n    $set: {age: 27}\n  }\n)\n</code></pre> <pre><code>$set\n$inc\n$rename: {name: \"Ahmed\"}\n$unset: {name: \"\"} // removes key; doesn't set to null\n$push: {hobbies: \"Swimming\"} // adding to array key\n$pull: {hobbies: \"Swimming\"} // remove from array key\n</code></pre>"},{"location":"Tools/Web_Dev/MERN/MongoDB/#replace","title":"Replace","text":"<pre><code>db.users.replaceOne(\n  {\n    age: 26\n    },\n  {\n    name: \"Thahir\"\n  }\n)\ndb.users.replace(\n  {\n    age: 26\n    },\n  {\n    name: \"Thahir\"\n  }\n)\n</code></pre>"},{"location":"Tools/Web_Dev/MERN/MongoDB/#delete","title":"Delete","text":"<pre><code>db.users.deleteOne(\n  {\n    name: \"Thahir\"\n  }\n)\ndb.users.deleteMany(\n  {\n    name: \"Thahir\"\n  }\n)\n</code></pre>"},{"location":"Tools/Web_Dev/MERN/MongoDB/#filtering","title":"Filtering","text":"<pre><code>// filter Thahir and return only name and age\ndb.users.find(\n  {\n    name: \"Thahir\"\n  },\n  {\n    name: 1,\n    age:1,\n    _id: 0\n  }\n)\n</code></pre>"},{"location":"Tools/Web_Dev/MERN/MongoDB/#complex-filters","title":"Complex Filters","text":"<pre><code>db.users.find(\n  {\n    first_name: {$eq: \"Ahmed\"},\n    age: {$gte: 50},\n  },\n  {\n    name: 1,\n    age:1,\n    _id: 0\n  }\n)\n</code></pre> <pre><code>{$eq: \"Thahir\"}\n{$ne: \"Thahir\"}\n{$gte: 10}\n{$in: [\n  \"Ahmed\",\n  \"Thahir\",\n  5\n]}\n{$nin: [\n  \"Ahmed\",\n  \"Thahir\",\n  5\n]}\n{$exists: true} // only checks if key exists; hence includes documents with null\n{$exists: false}\n</code></pre>"},{"location":"Tools/Web_Dev/MERN/MongoDB/#filter-operations","title":"Filter Operations","text":"<pre><code>// not\n{\n    not: [\n    {filter_1: \"enset\"}\n  ]\n}\n\n// and\n{\n    filter_1: \"enset\",\n  filter_2: \"enset\"\n}\n\n{\n    $and: [\n    {filter_1: \"enset\"},\n      {filter_2: \"enset\"}\n  ]\n}\n\n// or\n{\n    $or: [\n    {filter_1: \"enset\"},\n      {filter_2: \"enset\"}\n  ]\n}\n</code></pre>"},{"location":"Tools/Web_Dev/MERN/MongoDB/#comparing-keys","title":"Comparing keys","text":"<pre><code>db.users.find({\n  $expr: {\n    $gt: [\"$debt\", \"$balance\"]\n  }\n})\n</code></pre>"},{"location":"Tools/Web_Dev/MERN/MongoDB/#nested-keys","title":"Nested keys","text":"<pre><code>db.users.find({\n  \"address.street\": \"Testing\"\n})\n</code></pre>"},{"location":"Tools/Web_Dev/MERN/MongoDB/#_1","title":"MongoDB","text":""},{"location":"Tools/Web_Dev/MERN/MongoDB/#references","title":"References","text":"<ul> <li> Web Dev Simplified | MongoDB Crash Course</li> </ul>"},{"location":"Tools/Web_Dev/MERN/Node/","title":"NodeJS","text":"<p>Open source server environment, that allows you to run JavaScript on the server, ie allows JS to run outside the browser.</p>"},{"location":"Tools/Web_Dev/MERN/Node/#installation","title":"Installation","text":"<p>https://nodejs.org</p>"},{"location":"Tools/Web_Dev/MERN/Node/#web-server","title":"Web Server","text":""},{"location":"Tools/Web_Dev/MERN/Node/#indexhtml","title":"<code>index.html</code>","text":"<pre><code>&lt;html&gt;\n  &lt;body&gt;\n    \"Hello world!\"\n  &lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"Tools/Web_Dev/MERN/Node/#appjs","title":"<code>app.js</code>","text":"<pre><code>// imports\nconst http = require(\"http\")\nconst fs = require(\"fs\") // file system\n\nconst port = 3000\n\nconst server = http.createServer(function (req, res){\n\n  fs.readFile(\n    \"index.html\",\n    function(error, data) {\n      if (error) {\n        res.writeHead(404)\n        res.write(\"Error: File not found\")\n      } else {\n        res.writeHead(200, {\n          \"Content-Type\": \"text/html\"\n        })\n        res.write(data)\n      }\n    }\n    res.end()\n  )\n\n  // res.end()\n})\n\nserver.listen(port, function(error) {\n  if (error) {\n    console.log(\"Something went wrong: \", error)\n  } else {\n    console.log(\"Server listening on port \" + port)\n  }\n})\n</code></pre> <pre><code>node app.js\n</code></pre>"},{"location":"Tools/Web_Dev/MERN/Node_Express/","title":"Node &amp; Express","text":"<pre><code>cd folder\nnpm init -y\n</code></pre> <pre><code>npm install -g express cors mongodb dotenv nodemon\n</code></pre> <code>nodemon</code> Automatically restarts node application when file changes"},{"location":"Tools/Web_Dev/MERN/Node_Express/#usage","title":"Usage","text":"<ol> <li>Start up server</li> </ol> <pre><code>nodemon server\n</code></pre> <ol> <li>Go to <code>localhost:5000/api/v1/restaurants</code></li> </ol>"},{"location":"Tools/Web_Dev/MERN/Node_Express/#file-tree","title":"File Tree","text":"<pre><code>- backend\n    - api\n        - restaurants.controller.js\n        - restaurants.route.js\n        - reviews.controller.js\n  - dao (Data Access Object)\n      - restaurantsDAO.js\n        - reviewsDAO.js\n    - node_modules\n\n  - .env\n  - index.js\n  - package-lock.json\n  - package.json\n  - server.js\n</code></pre>"},{"location":"Tools/Web_Dev/MERN/Node_Express/#packagejson","title":"<code>package.json</code>","text":"<pre><code>{\n  \"name\": \"backend\",\n  \"version\": \"1.0.0\",\n  \"description\": \"\",\n  \"main\": \"index.js\",\n  \"type\": \"module\", // allows use to import statements from es6\n  \"scripts\": {\n    \"test\": \"echo \\\"Error: no test specified\\\" &amp;&amp; exit 1\"\n  },\n  \"keywords\": [],\n  \"author\": \"\",\n  \"license\": \"ISC\",\n  \"dependencies\": {\n    \"body-parser\": \"^1.19.0\",\n    \"bson\": \"^4.2.2\",\n    \"cors\": \"^2.8.5\",\n    \"dotenv\": \"^8.2.0\",\n    \"express\": \"^4.17.1\",\n    \"mongodb\": \"^3.6.4\"\n  }\n}\n</code></pre>"},{"location":"Tools/Web_Dev/MERN/Node_Express/#serverjs","title":"<code>server.js</code>","text":"<pre><code>import express from \"express\"\nimport cors from \"cors\"\nimport restaurants from \"./api/restaurants.route.js\"\n\nconst app = express()\n\napp.use(cors())\napp.use(express.json()) // allow server to accept &amp; read json in requests\n\n// default route\napp.use(\n  \"/api/v1/restaurants\",\n  restaurants\n)\n\n// fallback route\napp.use(\n  \"*\",\n  (req, res) =&gt; res.status(404).json({ error: \"not found\"})\n)\n\n// export the app as a module,\n// so that it can be imported in another file that accesses the database\nexport default app\n</code></pre>"},{"location":"Tools/Web_Dev/MERN/Node_Express/#env","title":"<code>.env</code>","text":"<p>Set the uri of the database</p> <pre><code>PORT = 5000\n\nDB_URI = blah_blah\nNS = sample_restaurants\n</code></pre>"},{"location":"Tools/Web_Dev/MERN/Node_Express/#indexjs","title":"<code>index.js</code>","text":"<pre><code>import app from \"./server.js\"\nimport mongodb from \"mongodb\"\nimport dotenv from \"dotenv\"\nimport RestaurantsDAO from \"./dao/restaurantsDAO.js\"\nimport ReviewsDAO from \"./dao/reviewsDAO.js\"\n</code></pre> <pre><code>dotenv.config()\nconst MongoClient = mongodb.MongoClient\n\nconst port = process.env.PORT || 8000\n\nMongoClient.connect(\n  process.env.RESTREVIEWS_DB_URI, // connect to database\n  {\n    poolSize: 50,\n    wtimeout: 2500,\n    useNewUrlParse: true }\n  )\n  .catch(err =&gt; {                                   // check for error\n    console.error(err.stack)\n    process.exit(1)\n  })\n  .then(async client =&gt; {                   // start webserver\n    await RestaurantsDAO.injectDB(client)\n    await ReviewsDAO.injectDB(client)\n    app.listen(port, () =&gt; {\n      console.log(`listening on port ${port}`)\n    })\n  })\n)\n</code></pre>"},{"location":"Tools/Web_Dev/MERN/Node_Express/#restaurantscontrollerjs","title":"<code>restaurants.controller.js</code>","text":"<pre><code>import RestaurantsDAO from \"../dao/restaurantsDAO.js\"\n\nexport default class RestaurantsController {\n  static async apiGetRestaurants(req, res, next) {\n    const restaurantsPerPage = req.query.restaurantsPerPage ? parseInt(req.query.restaurantsPerPage, 10) : 20\n    const page = req.query.page ? parseInt(req.query.page, 10) : 0\n\n    let filters = {}\n    if (req.query.cuisine) {\n      filters.cuisine = req.query.cuisine\n    } else if (req.query.zipcode) {\n      filters.zipcode = req.query.zipcode\n    } else if (req.query.name) {\n      filters.name = req.query.name\n    }\n\n    const { restaurantsList, totalNumRestaurants } = await RestaurantsDAO.getRestaurants({\n      filters,\n      page,\n      restaurantsPerPage,\n    })\n\n    let response = {\n      restaurants: restaurantsList,\n      page: page,\n      filters: filters,\n      entries_per_page: restaurantsPerPage,\n      total_results: totalNumRestaurants,\n    }\n    res.json(response)\n  }\n  static async apiGetRestaurantById(req, res, next) {\n    try {\n      let id = req.params.id || {}\n      let restaurant = await RestaurantsDAO.getRestaurantByID(id)\n      if (!restaurant) {\n        res.status(404).json({ error: \"Not found\" })\n        return\n      }\n      res.json(restaurant)\n    } catch (e) {\n      console.log(`api, ${e}`)\n      res.status(500).json({ error: e })\n    }\n  }\n\n  static async apiGetRestaurantCuisines(req, res, next) {\n    try {\n      let cuisines = await RestaurantsDAO.getCuisines()\n      res.json(cuisines)\n    } catch (e) {\n      console.log(`api, ${e}`)\n      res.status(500).json({ error: e })\n    }\n  }\n}\n</code></pre>"},{"location":"Tools/Web_Dev/MERN/Node_Express/#restaurantsroutejs","title":"<code>restaurants.route.js</code>","text":"<pre><code>import express from \"express\"\nimport RestaurantsCtrl from \"./restaurants.controller.js\"\nimport ReviewsCtrl from \"./reviews.controller.js\"\n\nconst router = express.Router()\n\nrouter.route(\"/\").get(RestaurantsCtrl.apiGetRestaurants)\nrouter.route(\"/id/:id\").get(RestaurantsCtrl.apiGetRestaurantById)\nrouter.route(\"/cuisines\").get(RestaurantsCtrl.apiGetRestaurantCuisines)\n\nrouter\n  .route(\"/review\")\n  .post(ReviewsCtrl.apiPostReview)\n  .put(ReviewsCtrl.apiUpdateReview)\n  .delete(ReviewsCtrl.apiDeleteReview)\n\nexport default router\n</code></pre>"},{"location":"Tools/Web_Dev/MERN/Node_Express/#reviewscontrollerjs","title":"<code>reviews.controller.js</code>","text":"<pre><code>import ReviewsDAO from \"../dao/reviewsDAO.js\"\n\nexport default class ReviewsController {\n  static async apiPostReview(req, res, next) {\n    try {\n      const restaurantId = req.body.restaurant_id\n      const review = req.body.text\n      const userInfo = {\n        name: req.body.name,\n        _id: req.body.user_id\n      }\n      const date = new Date()\n\n      const ReviewResponse = await ReviewsDAO.addReview(\n        restaurantId,\n        userInfo,\n        review,\n        date,\n      )\n      res.json({ status: \"success\" })\n    } catch (e) {\n      res.status(500).json({ error: e.message })\n    }\n  }\n\n  static async apiUpdateReview(req, res, next) {\n    try {\n      const reviewId = req.body.review_id\n      const text = req.body.text\n      const date = new Date()\n\n      const reviewResponse = await ReviewsDAO.updateReview(\n        reviewId,\n        req.body.user_id,\n        text,\n        date,\n      )\n\n      var { error } = reviewResponse\n      if (error) {\n        res.status(400).json({ error })\n      }\n\n      if (reviewResponse.modifiedCount === 0) {\n        throw new Error(\n          \"unable to update review - user may not be original poster\",\n        )\n      }\n\n      res.json({ status: \"success\" })\n    } catch (e) {\n      res.status(500).json({ error: e.message })\n    }\n  }\n\n  static async apiDeleteReview(req, res, next) {\n    try {\n      const reviewId = req.query.id\n      const userId = req.body.user_id\n      console.log(reviewId)\n      const reviewResponse = await ReviewsDAO.deleteReview(\n        reviewId,\n        userId,\n      )\n      res.json({ status: \"success\" })\n    } catch (e) {\n      res.status(500).json({ error: e.message })\n    }\n  }\n\n}\n</code></pre>"},{"location":"Tools/Web_Dev/MERN/Node_Express/#restaurantsdaojs","title":"<code>restaurantsDAO.js</code>","text":"<pre><code>import mongodb from \"mongodb\"\nconst ObjectId = mongodb.ObjectID\nlet restaurants\n\nexport default class RestaurantsDAO {\n  static async injectDB(conn) {\n    if (restaurants) {\n      return\n    }\n    try {\n      restaurants = await conn.db(process.env.RESTREVIEWS_NS).collection(\"restaurants\")\n    } catch (e) {\n      console.error(\n        `Unable to establish a collection handle in restaurantsDAO: ${e}`,\n      )\n    }\n  }\n\n  static async getRestaurants({\n    filters = null,\n    page = 0,\n    restaurantsPerPage = 20,\n  } = {}) {\n    let query\n    if (filters) {\n      if (\"name\" in filters) {\n        query = { $text: { $search: filters[\"name\"] } } // $text needs to be configured in mongodb\n      } else if (\"cuisine\" in filters) {\n        query = { \"cuisine\": { $eq: filters[\"cuisine\"] } }\n      } else if (\"zipcode\" in filters) {\n        query = { \"address.zipcode\": { $eq: filters[\"zipcode\"] } }\n      }\n    }\n\n    let cursor\n\n    try {\n      cursor = await restaurants\n        .find(query)\n    } catch (e) {\n      console.error(`Unable to issue find command, ${e}`)\n      return { restaurantsList: [], totalNumRestaurants: 0 }\n    }\n\n    const displayCursor = cursor.limit(restaurantsPerPage).skip(restaurantsPerPage * page)\n\n    try {\n      const restaurantsList = await displayCursor.toArray()\n      const totalNumRestaurants = await restaurants.countDocuments(query)\n\n      return { restaurantsList, totalNumRestaurants }\n    } catch (e) {\n      console.error(\n        `Unable to convert cursor to array or problem counting documents, ${e}`,\n      )\n      return { restaurantsList: [], totalNumRestaurants: 0 }\n    }\n  }\n  static async getRestaurantByID(id) {\n    try {\n      const pipeline = [\n        {\n            $match: {\n                _id: new ObjectId(id),\n            },\n        },\n              {\n                  $lookup: {\n                      from: \"reviews\",\n                      let: {\n                          id: \"$_id\",\n                      },\n                      pipeline: [\n                          {\n                              $match: {\n                                  $expr: {\n                                      $eq: [\"$restaurant_id\", \"$$id\"],\n                                  },\n                              },\n                          },\n                          {\n                              $sort: {\n                                  date: -1,\n                              },\n                          },\n                      ],\n                      as: \"reviews\",\n                  },\n              },\n              {\n                  $addFields: {\n                      reviews: \"$reviews\",\n                  },\n              },\n          ]\n      return await restaurants.aggregate(pipeline).next()\n    } catch (e) {\n      console.error(`Something went wrong in getRestaurantByID: ${e}`)\n      throw e\n    }\n  }\n\n  static async getCuisines() {\n    let cuisines = []\n    try {\n      cuisines = await restaurants.distinct(\"cuisine\")\n      return cuisines\n    } catch (e) {\n      console.error(`Unable to get cuisines, ${e}`)\n      return cuisines\n    }\n  }\n}\n</code></pre>"},{"location":"Tools/Web_Dev/MERN/Node_Express/#reviewsdaojs","title":"<code>reviewsDAO.js</code>","text":"<pre><code>import mongodb from \"mongodb\"\nconst ObjectId = mongodb.ObjectID\n\nlet reviews\n\nexport default class ReviewsDAO {\n  static async injectDB(conn) {\n    if (reviews) {\n      return\n    }\n    try {\n      reviews = await conn.db(process.env.RESTREVIEWS_NS).collection(\"reviews\")\n    } catch (e) {\n      console.error(`Unable to establish collection handles in userDAO: ${e}`)\n    }\n  }\n\n  static async addReview(restaurantId, user, review, date) {\n    try {\n      const reviewDoc = { name: user.name,\n          user_id: user._id,\n          date: date,\n          text: review,\n          restaurant_id: ObjectId(restaurantId), }\n\n      return await reviews.insertOne(reviewDoc)\n    } catch (e) {\n      console.error(`Unable to post review: ${e}`)\n      return { error: e }\n    }\n  }\n\n  static async updateReview(reviewId, userId, text, date) {\n    try {\n      const updateResponse = await reviews.updateOne(\n        { user_id: userId, _id: ObjectId(reviewId)},\n        { $set: { text: text, date: date  } },\n      )\n\n      return updateResponse\n    } catch (e) {\n      console.error(`Unable to update review: ${e}`)\n      return { error: e }\n    }\n  }\n\n  static async deleteReview(reviewId, userId) {\n\n    try {\n      const deleteResponse = await reviews.deleteOne({\n        _id: ObjectId(reviewId),\n        user_id: userId,\n      })\n\n      return deleteResponse\n    } catch (e) {\n      console.error(`Unable to delete review: ${e}`)\n      return { error: e }\n    }\n  }\n\n}\n</code></pre>"},{"location":"Tools/Web_Dev/MERN/Node_Express/#references","title":"References","text":"<ul> <li> Web Dev Simplified | Your First Node.js Web Server</li> <li> Wed Dev Simplified | Learn Express JS In 35 Minutes</li> <li> Web Dev Simplified | Build A REST API With Node.js, Express, &amp; MongoDB - Quick</li> </ul>"},{"location":"Tools/Web_Dev/MERN/React/","title":"React","text":""}]}